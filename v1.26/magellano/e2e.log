I0901 10:29:28.974125      19 e2e.go:126] Starting e2e run "8adb3b46-bd81-48ef-adc4-5eb7453ac860" on Ginkgo node 1
Sep  1 10:29:29.029: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1693564168 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Sep  1 10:29:29.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 10:29:29.245: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep  1 10:29:29.265: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  1 10:29:29.297: INFO: 16 / 16 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  1 10:29:29.297: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Sep  1 10:29:29.297: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  1 10:29:29.304: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
Sep  1 10:29:29.304: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep  1 10:29:29.304: INFO: e2e test version: v1.26.8
Sep  1 10:29:29.305: INFO: kube-apiserver version: v1.26.8
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Sep  1 10:29:29.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 10:29:29.310: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.069 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Sep  1 10:29:29.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 10:29:29.245: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Sep  1 10:29:29.265: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Sep  1 10:29:29.297: INFO: 16 / 16 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Sep  1 10:29:29.297: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
    Sep  1 10:29:29.297: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Sep  1 10:29:29.304: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
    Sep  1 10:29:29.304: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Sep  1 10:29:29.304: INFO: e2e test version: v1.26.8
    Sep  1 10:29:29.305: INFO: kube-apiserver version: v1.26.8
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Sep  1 10:29:29.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 10:29:29.310: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:29:29.362
Sep  1 10:29:29.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 10:29:29.363
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:29:29.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:29:29.385
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/01/23 10:29:29.388
Sep  1 10:29:29.398: INFO: Waiting up to 5m0s for pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651" in namespace "emptydir-5239" to be "Succeeded or Failed"
Sep  1 10:29:29.403: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Pending", Reason="", readiness=false. Elapsed: 4.69995ms
Sep  1 10:29:31.407: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008806726s
Sep  1 10:29:33.406: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008048467s
Sep  1 10:29:35.407: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008799294s
Sep  1 10:29:37.408: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010559761s
Sep  1 10:29:39.409: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Running", Reason="", readiness=true. Elapsed: 10.010958871s
Sep  1 10:29:41.406: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Running", Reason="", readiness=false. Elapsed: 12.008451424s
Sep  1 10:29:43.408: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.009915282s
STEP: Saw pod success 09/01/23 10:29:43.408
Sep  1 10:29:43.408: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651" satisfied condition "Succeeded or Failed"
Sep  1 10:29:43.411: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651 container test-container: <nil>
STEP: delete the pod 09/01/23 10:29:43.43
Sep  1 10:29:43.444: INFO: Waiting for pod pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651 to disappear
Sep  1 10:29:43.448: INFO: Pod pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 10:29:43.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5239" for this suite. 09/01/23 10:29:43.453
------------------------------
• [SLOW TEST] [14.097 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:29:29.362
    Sep  1 10:29:29.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 10:29:29.363
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:29:29.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:29:29.385
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/01/23 10:29:29.388
    Sep  1 10:29:29.398: INFO: Waiting up to 5m0s for pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651" in namespace "emptydir-5239" to be "Succeeded or Failed"
    Sep  1 10:29:29.403: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Pending", Reason="", readiness=false. Elapsed: 4.69995ms
    Sep  1 10:29:31.407: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008806726s
    Sep  1 10:29:33.406: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008048467s
    Sep  1 10:29:35.407: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008799294s
    Sep  1 10:29:37.408: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010559761s
    Sep  1 10:29:39.409: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Running", Reason="", readiness=true. Elapsed: 10.010958871s
    Sep  1 10:29:41.406: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Running", Reason="", readiness=false. Elapsed: 12.008451424s
    Sep  1 10:29:43.408: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.009915282s
    STEP: Saw pod success 09/01/23 10:29:43.408
    Sep  1 10:29:43.408: INFO: Pod "pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651" satisfied condition "Succeeded or Failed"
    Sep  1 10:29:43.411: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651 container test-container: <nil>
    STEP: delete the pod 09/01/23 10:29:43.43
    Sep  1 10:29:43.444: INFO: Waiting for pod pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651 to disappear
    Sep  1 10:29:43.448: INFO: Pod pod-2d81a9e5-fa23-4085-a53e-c80ca7b2b651 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:29:43.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5239" for this suite. 09/01/23 10:29:43.453
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:29:43.459
Sep  1 10:29:43.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename dns 09/01/23 10:29:43.462
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:29:43.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:29:43.482
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 09/01/23 10:29:43.486
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8112.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local; sleep 1; done
 09/01/23 10:29:43.491
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8112.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local; sleep 1; done
 09/01/23 10:29:43.491
STEP: creating a pod to probe DNS 09/01/23 10:29:43.492
STEP: submitting the pod to kubernetes 09/01/23 10:29:43.492
Sep  1 10:29:43.501: INFO: Waiting up to 15m0s for pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f" in namespace "dns-8112" to be "running"
Sep  1 10:29:43.505: INFO: Pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990173ms
Sep  1 10:29:45.537: INFO: Pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03601331s
Sep  1 10:29:47.574: INFO: Pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073330727s
Sep  1 10:29:49.513: INFO: Pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011854651s
Sep  1 10:29:51.509: INFO: Pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008444619s
Sep  1 10:29:53.510: INFO: Pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f": Phase="Running", Reason="", readiness=true. Elapsed: 10.00860032s
Sep  1 10:29:53.510: INFO: Pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f" satisfied condition "running"
STEP: retrieving the pod 09/01/23 10:29:53.51
STEP: looking for the results for each expected name from probers 09/01/23 10:29:53.513
Sep  1 10:29:53.522: INFO: DNS probes using dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f succeeded

STEP: deleting the pod 09/01/23 10:29:53.522
STEP: changing the externalName to bar.example.com 09/01/23 10:29:53.536
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8112.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local; sleep 1; done
 09/01/23 10:29:53.561
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8112.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local; sleep 1; done
 09/01/23 10:29:53.561
STEP: creating a second pod to probe DNS 09/01/23 10:29:53.561
STEP: submitting the pod to kubernetes 09/01/23 10:29:53.562
Sep  1 10:29:53.574: INFO: Waiting up to 15m0s for pod "dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0" in namespace "dns-8112" to be "running"
Sep  1 10:29:53.580: INFO: Pod "dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.910473ms
Sep  1 10:29:55.584: INFO: Pod "dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009105497s
Sep  1 10:29:57.584: INFO: Pod "dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0": Phase="Running", Reason="", readiness=true. Elapsed: 4.009501942s
Sep  1 10:29:57.584: INFO: Pod "dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0" satisfied condition "running"
STEP: retrieving the pod 09/01/23 10:29:57.584
STEP: looking for the results for each expected name from probers 09/01/23 10:29:57.588
Sep  1 10:29:57.594: INFO: File wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  1 10:29:57.598: INFO: File jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  1 10:29:57.598: INFO: Lookups using dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 failed for: [wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local]

Sep  1 10:30:02.603: INFO: File wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  1 10:30:02.607: INFO: File jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  1 10:30:02.607: INFO: Lookups using dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 failed for: [wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local]

Sep  1 10:30:07.607: INFO: File wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  1 10:30:07.611: INFO: File jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  1 10:30:07.611: INFO: Lookups using dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 failed for: [wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local]

Sep  1 10:30:12.605: INFO: File wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  1 10:30:12.610: INFO: File jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  1 10:30:12.610: INFO: Lookups using dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 failed for: [wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local]

Sep  1 10:30:17.609: INFO: DNS probes using dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 succeeded

STEP: deleting the pod 09/01/23 10:30:17.609
STEP: changing the service to type=ClusterIP 09/01/23 10:30:17.628
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8112.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local; sleep 1; done
 09/01/23 10:30:17.701
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8112.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local; sleep 1; done
 09/01/23 10:30:17.702
STEP: creating a third pod to probe DNS 09/01/23 10:30:17.702
STEP: submitting the pod to kubernetes 09/01/23 10:30:17.717
Sep  1 10:30:17.744: INFO: Waiting up to 15m0s for pod "dns-test-745c8b83-57ba-464f-8ea1-ffbe666c74b3" in namespace "dns-8112" to be "running"
Sep  1 10:30:17.753: INFO: Pod "dns-test-745c8b83-57ba-464f-8ea1-ffbe666c74b3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.864206ms
Sep  1 10:30:19.758: INFO: Pod "dns-test-745c8b83-57ba-464f-8ea1-ffbe666c74b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013673649s
Sep  1 10:30:21.758: INFO: Pod "dns-test-745c8b83-57ba-464f-8ea1-ffbe666c74b3": Phase="Running", Reason="", readiness=true. Elapsed: 4.013371136s
Sep  1 10:30:21.758: INFO: Pod "dns-test-745c8b83-57ba-464f-8ea1-ffbe666c74b3" satisfied condition "running"
STEP: retrieving the pod 09/01/23 10:30:21.758
STEP: looking for the results for each expected name from probers 09/01/23 10:30:21.762
Sep  1 10:30:21.773: INFO: DNS probes using dns-test-745c8b83-57ba-464f-8ea1-ffbe666c74b3 succeeded

STEP: deleting the pod 09/01/23 10:30:21.773
STEP: deleting the test externalName service 09/01/23 10:30:21.789
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  1 10:30:21.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8112" for this suite. 09/01/23 10:30:21.826
------------------------------
• [SLOW TEST] [38.389 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:29:43.459
    Sep  1 10:29:43.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename dns 09/01/23 10:29:43.462
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:29:43.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:29:43.482
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 09/01/23 10:29:43.486
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8112.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local; sleep 1; done
     09/01/23 10:29:43.491
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8112.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local; sleep 1; done
     09/01/23 10:29:43.491
    STEP: creating a pod to probe DNS 09/01/23 10:29:43.492
    STEP: submitting the pod to kubernetes 09/01/23 10:29:43.492
    Sep  1 10:29:43.501: INFO: Waiting up to 15m0s for pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f" in namespace "dns-8112" to be "running"
    Sep  1 10:29:43.505: INFO: Pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990173ms
    Sep  1 10:29:45.537: INFO: Pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03601331s
    Sep  1 10:29:47.574: INFO: Pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073330727s
    Sep  1 10:29:49.513: INFO: Pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011854651s
    Sep  1 10:29:51.509: INFO: Pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008444619s
    Sep  1 10:29:53.510: INFO: Pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f": Phase="Running", Reason="", readiness=true. Elapsed: 10.00860032s
    Sep  1 10:29:53.510: INFO: Pod "dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f" satisfied condition "running"
    STEP: retrieving the pod 09/01/23 10:29:53.51
    STEP: looking for the results for each expected name from probers 09/01/23 10:29:53.513
    Sep  1 10:29:53.522: INFO: DNS probes using dns-test-aae9ff91-845b-4292-a0a5-f82d8aad544f succeeded

    STEP: deleting the pod 09/01/23 10:29:53.522
    STEP: changing the externalName to bar.example.com 09/01/23 10:29:53.536
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8112.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local; sleep 1; done
     09/01/23 10:29:53.561
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8112.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local; sleep 1; done
     09/01/23 10:29:53.561
    STEP: creating a second pod to probe DNS 09/01/23 10:29:53.561
    STEP: submitting the pod to kubernetes 09/01/23 10:29:53.562
    Sep  1 10:29:53.574: INFO: Waiting up to 15m0s for pod "dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0" in namespace "dns-8112" to be "running"
    Sep  1 10:29:53.580: INFO: Pod "dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.910473ms
    Sep  1 10:29:55.584: INFO: Pod "dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009105497s
    Sep  1 10:29:57.584: INFO: Pod "dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0": Phase="Running", Reason="", readiness=true. Elapsed: 4.009501942s
    Sep  1 10:29:57.584: INFO: Pod "dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0" satisfied condition "running"
    STEP: retrieving the pod 09/01/23 10:29:57.584
    STEP: looking for the results for each expected name from probers 09/01/23 10:29:57.588
    Sep  1 10:29:57.594: INFO: File wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  1 10:29:57.598: INFO: File jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  1 10:29:57.598: INFO: Lookups using dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 failed for: [wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local]

    Sep  1 10:30:02.603: INFO: File wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  1 10:30:02.607: INFO: File jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  1 10:30:02.607: INFO: Lookups using dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 failed for: [wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local]

    Sep  1 10:30:07.607: INFO: File wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  1 10:30:07.611: INFO: File jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  1 10:30:07.611: INFO: Lookups using dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 failed for: [wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local]

    Sep  1 10:30:12.605: INFO: File wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  1 10:30:12.610: INFO: File jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local from pod  dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  1 10:30:12.610: INFO: Lookups using dns-8112/dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 failed for: [wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local]

    Sep  1 10:30:17.609: INFO: DNS probes using dns-test-5808a1c7-d589-4ee3-8554-43acd66392e0 succeeded

    STEP: deleting the pod 09/01/23 10:30:17.609
    STEP: changing the service to type=ClusterIP 09/01/23 10:30:17.628
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8112.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8112.svc.cluster.local; sleep 1; done
     09/01/23 10:30:17.701
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8112.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8112.svc.cluster.local; sleep 1; done
     09/01/23 10:30:17.702
    STEP: creating a third pod to probe DNS 09/01/23 10:30:17.702
    STEP: submitting the pod to kubernetes 09/01/23 10:30:17.717
    Sep  1 10:30:17.744: INFO: Waiting up to 15m0s for pod "dns-test-745c8b83-57ba-464f-8ea1-ffbe666c74b3" in namespace "dns-8112" to be "running"
    Sep  1 10:30:17.753: INFO: Pod "dns-test-745c8b83-57ba-464f-8ea1-ffbe666c74b3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.864206ms
    Sep  1 10:30:19.758: INFO: Pod "dns-test-745c8b83-57ba-464f-8ea1-ffbe666c74b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013673649s
    Sep  1 10:30:21.758: INFO: Pod "dns-test-745c8b83-57ba-464f-8ea1-ffbe666c74b3": Phase="Running", Reason="", readiness=true. Elapsed: 4.013371136s
    Sep  1 10:30:21.758: INFO: Pod "dns-test-745c8b83-57ba-464f-8ea1-ffbe666c74b3" satisfied condition "running"
    STEP: retrieving the pod 09/01/23 10:30:21.758
    STEP: looking for the results for each expected name from probers 09/01/23 10:30:21.762
    Sep  1 10:30:21.773: INFO: DNS probes using dns-test-745c8b83-57ba-464f-8ea1-ffbe666c74b3 succeeded

    STEP: deleting the pod 09/01/23 10:30:21.773
    STEP: deleting the test externalName service 09/01/23 10:30:21.789
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:30:21.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8112" for this suite. 09/01/23 10:30:21.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:30:21.872
Sep  1 10:30:21.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename job 09/01/23 10:30:21.874
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:30:21.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:30:21.904
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 09/01/23 10:30:21.91
STEP: Ensuring active pods == parallelism 09/01/23 10:30:21.919
STEP: Orphaning one of the Job's Pods 09/01/23 10:30:25.923
Sep  1 10:30:26.443: INFO: Successfully updated pod "adopt-release-df4qz"
STEP: Checking that the Job readopts the Pod 09/01/23 10:30:26.443
Sep  1 10:30:26.443: INFO: Waiting up to 15m0s for pod "adopt-release-df4qz" in namespace "job-1651" to be "adopted"
Sep  1 10:30:26.447: INFO: Pod "adopt-release-df4qz": Phase="Running", Reason="", readiness=true. Elapsed: 4.269583ms
Sep  1 10:30:28.451: INFO: Pod "adopt-release-df4qz": Phase="Running", Reason="", readiness=true. Elapsed: 2.007828996s
Sep  1 10:30:28.451: INFO: Pod "adopt-release-df4qz" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 09/01/23 10:30:28.452
Sep  1 10:30:28.966: INFO: Successfully updated pod "adopt-release-df4qz"
STEP: Checking that the Job releases the Pod 09/01/23 10:30:28.966
Sep  1 10:30:28.966: INFO: Waiting up to 15m0s for pod "adopt-release-df4qz" in namespace "job-1651" to be "released"
Sep  1 10:30:28.970: INFO: Pod "adopt-release-df4qz": Phase="Running", Reason="", readiness=true. Elapsed: 3.719894ms
Sep  1 10:30:30.975: INFO: Pod "adopt-release-df4qz": Phase="Running", Reason="", readiness=true. Elapsed: 2.009114878s
Sep  1 10:30:30.975: INFO: Pod "adopt-release-df4qz" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  1 10:30:30.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1651" for this suite. 09/01/23 10:30:30.98
------------------------------
• [SLOW TEST] [9.116 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:30:21.872
    Sep  1 10:30:21.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename job 09/01/23 10:30:21.874
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:30:21.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:30:21.904
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 09/01/23 10:30:21.91
    STEP: Ensuring active pods == parallelism 09/01/23 10:30:21.919
    STEP: Orphaning one of the Job's Pods 09/01/23 10:30:25.923
    Sep  1 10:30:26.443: INFO: Successfully updated pod "adopt-release-df4qz"
    STEP: Checking that the Job readopts the Pod 09/01/23 10:30:26.443
    Sep  1 10:30:26.443: INFO: Waiting up to 15m0s for pod "adopt-release-df4qz" in namespace "job-1651" to be "adopted"
    Sep  1 10:30:26.447: INFO: Pod "adopt-release-df4qz": Phase="Running", Reason="", readiness=true. Elapsed: 4.269583ms
    Sep  1 10:30:28.451: INFO: Pod "adopt-release-df4qz": Phase="Running", Reason="", readiness=true. Elapsed: 2.007828996s
    Sep  1 10:30:28.451: INFO: Pod "adopt-release-df4qz" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 09/01/23 10:30:28.452
    Sep  1 10:30:28.966: INFO: Successfully updated pod "adopt-release-df4qz"
    STEP: Checking that the Job releases the Pod 09/01/23 10:30:28.966
    Sep  1 10:30:28.966: INFO: Waiting up to 15m0s for pod "adopt-release-df4qz" in namespace "job-1651" to be "released"
    Sep  1 10:30:28.970: INFO: Pod "adopt-release-df4qz": Phase="Running", Reason="", readiness=true. Elapsed: 3.719894ms
    Sep  1 10:30:30.975: INFO: Pod "adopt-release-df4qz": Phase="Running", Reason="", readiness=true. Elapsed: 2.009114878s
    Sep  1 10:30:30.975: INFO: Pod "adopt-release-df4qz" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:30:30.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1651" for this suite. 09/01/23 10:30:30.98
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:30:30.988
Sep  1 10:30:30.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir-wrapper 09/01/23 10:30:30.989
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:30:31.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:30:31.012
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 09/01/23 10:30:31.016
STEP: Creating RC which spawns configmap-volume pods 09/01/23 10:30:31.26
Sep  1 10:30:31.352: INFO: Pod name wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687: Found 3 pods out of 5
Sep  1 10:30:36.370: INFO: Pod name wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/01/23 10:30:36.37
Sep  1 10:30:36.370: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:30:36.375: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.377392ms
Sep  1 10:30:38.379: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009151399s
Sep  1 10:30:40.379: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008385981s
Sep  1 10:30:42.380: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009241574s
Sep  1 10:30:44.380: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00984327s
Sep  1 10:30:46.379: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00858817s
Sep  1 10:30:48.380: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh": Phase="Running", Reason="", readiness=true. Elapsed: 12.009915291s
Sep  1 10:30:48.380: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh" satisfied condition "running"
Sep  1 10:30:48.380: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-bwpcp" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:30:48.383: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-bwpcp": Phase="Running", Reason="", readiness=true. Elapsed: 3.158481ms
Sep  1 10:30:48.384: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-bwpcp" satisfied condition "running"
Sep  1 10:30:48.384: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-k5pdr" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:30:48.387: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-k5pdr": Phase="Running", Reason="", readiness=true. Elapsed: 3.359764ms
Sep  1 10:30:48.387: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-k5pdr" satisfied condition "running"
Sep  1 10:30:48.388: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-kl7nl" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:30:48.391: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-kl7nl": Phase="Running", Reason="", readiness=true. Elapsed: 3.441293ms
Sep  1 10:30:48.391: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-kl7nl" satisfied condition "running"
Sep  1 10:30:48.391: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-pdnpz" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:30:48.394: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-pdnpz": Phase="Running", Reason="", readiness=true. Elapsed: 3.204727ms
Sep  1 10:30:48.394: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-pdnpz" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687 in namespace emptydir-wrapper-2014, will wait for the garbage collector to delete the pods 09/01/23 10:30:48.394
Sep  1 10:30:48.455: INFO: Deleting ReplicationController wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687 took: 6.563426ms
Sep  1 10:30:48.556: INFO: Terminating ReplicationController wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687 pods took: 101.11832ms
STEP: Creating RC which spawns configmap-volume pods 09/01/23 10:30:51.761
Sep  1 10:30:51.776: INFO: Pod name wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033: Found 0 pods out of 5
Sep  1 10:30:56.784: INFO: Pod name wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/01/23 10:30:56.784
Sep  1 10:30:56.784: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:30:56.788: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km": Phase="Pending", Reason="", readiness=false. Elapsed: 3.445699ms
Sep  1 10:30:58.792: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00777902s
Sep  1 10:31:00.792: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007564032s
Sep  1 10:31:02.798: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013268107s
Sep  1 10:31:04.792: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007580925s
Sep  1 10:31:06.794: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008978384s
Sep  1 10:31:08.793: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km": Phase="Running", Reason="", readiness=true. Elapsed: 12.0085137s
Sep  1 10:31:08.793: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km" satisfied condition "running"
Sep  1 10:31:08.793: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-llxbr" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:31:08.797: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-llxbr": Phase="Running", Reason="", readiness=true. Elapsed: 3.790679ms
Sep  1 10:31:08.797: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-llxbr" satisfied condition "running"
Sep  1 10:31:08.797: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-qpkxq" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:31:08.801: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-qpkxq": Phase="Running", Reason="", readiness=true. Elapsed: 3.863612ms
Sep  1 10:31:08.801: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-qpkxq" satisfied condition "running"
Sep  1 10:31:08.801: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-t8zfp" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:31:08.805: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-t8zfp": Phase="Running", Reason="", readiness=true. Elapsed: 4.010268ms
Sep  1 10:31:08.805: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-t8zfp" satisfied condition "running"
Sep  1 10:31:08.805: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-zzw56" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:31:08.809: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-zzw56": Phase="Running", Reason="", readiness=true. Elapsed: 3.492483ms
Sep  1 10:31:08.809: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-zzw56" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033 in namespace emptydir-wrapper-2014, will wait for the garbage collector to delete the pods 09/01/23 10:31:08.809
Sep  1 10:31:08.870: INFO: Deleting ReplicationController wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033 took: 6.434868ms
Sep  1 10:31:08.971: INFO: Terminating ReplicationController wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033 pods took: 101.353723ms
STEP: Creating RC which spawns configmap-volume pods 09/01/23 10:31:12.379
Sep  1 10:31:12.394: INFO: Pod name wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88: Found 0 pods out of 5
Sep  1 10:31:17.405: INFO: Pod name wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/01/23 10:31:17.405
Sep  1 10:31:17.405: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:31:17.409: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.431155ms
Sep  1 10:31:19.413: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007995766s
Sep  1 10:31:21.415: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009801541s
Sep  1 10:31:23.415: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009647255s
Sep  1 10:31:25.416: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010380907s
Sep  1 10:31:27.418: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012888184s
Sep  1 10:31:29.418: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9": Phase="Running", Reason="", readiness=true. Elapsed: 12.012699398s
Sep  1 10:31:29.419: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9" satisfied condition "running"
Sep  1 10:31:29.419: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-cjzm2" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:31:29.430: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-cjzm2": Phase="Running", Reason="", readiness=true. Elapsed: 10.944086ms
Sep  1 10:31:29.431: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-cjzm2" satisfied condition "running"
Sep  1 10:31:29.431: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-vvcfm" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:31:29.438: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-vvcfm": Phase="Running", Reason="", readiness=true. Elapsed: 7.547698ms
Sep  1 10:31:29.439: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-vvcfm" satisfied condition "running"
Sep  1 10:31:29.439: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-x5vsf" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:31:29.448: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-x5vsf": Phase="Running", Reason="", readiness=true. Elapsed: 8.389235ms
Sep  1 10:31:29.448: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-x5vsf" satisfied condition "running"
Sep  1 10:31:29.448: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-x6q72" in namespace "emptydir-wrapper-2014" to be "running"
Sep  1 10:31:29.453: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-x6q72": Phase="Running", Reason="", readiness=true. Elapsed: 5.462703ms
Sep  1 10:31:29.453: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-x6q72" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88 in namespace emptydir-wrapper-2014, will wait for the garbage collector to delete the pods 09/01/23 10:31:29.453
Sep  1 10:31:29.560: INFO: Deleting ReplicationController wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88 took: 21.317843ms
Sep  1 10:31:29.861: INFO: Terminating ReplicationController wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88 pods took: 301.197069ms
STEP: Cleaning up the configMaps 09/01/23 10:31:32.562
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 10:31:33.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-2014" for this suite. 09/01/23 10:31:33.34
------------------------------
• [SLOW TEST] [62.364 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:30:30.988
    Sep  1 10:30:30.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir-wrapper 09/01/23 10:30:30.989
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:30:31.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:30:31.012
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 09/01/23 10:30:31.016
    STEP: Creating RC which spawns configmap-volume pods 09/01/23 10:30:31.26
    Sep  1 10:30:31.352: INFO: Pod name wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687: Found 3 pods out of 5
    Sep  1 10:30:36.370: INFO: Pod name wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/01/23 10:30:36.37
    Sep  1 10:30:36.370: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:30:36.375: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.377392ms
    Sep  1 10:30:38.379: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009151399s
    Sep  1 10:30:40.379: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008385981s
    Sep  1 10:30:42.380: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009241574s
    Sep  1 10:30:44.380: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00984327s
    Sep  1 10:30:46.379: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00858817s
    Sep  1 10:30:48.380: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh": Phase="Running", Reason="", readiness=true. Elapsed: 12.009915291s
    Sep  1 10:30:48.380: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-422dh" satisfied condition "running"
    Sep  1 10:30:48.380: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-bwpcp" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:30:48.383: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-bwpcp": Phase="Running", Reason="", readiness=true. Elapsed: 3.158481ms
    Sep  1 10:30:48.384: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-bwpcp" satisfied condition "running"
    Sep  1 10:30:48.384: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-k5pdr" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:30:48.387: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-k5pdr": Phase="Running", Reason="", readiness=true. Elapsed: 3.359764ms
    Sep  1 10:30:48.387: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-k5pdr" satisfied condition "running"
    Sep  1 10:30:48.388: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-kl7nl" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:30:48.391: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-kl7nl": Phase="Running", Reason="", readiness=true. Elapsed: 3.441293ms
    Sep  1 10:30:48.391: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-kl7nl" satisfied condition "running"
    Sep  1 10:30:48.391: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-pdnpz" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:30:48.394: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-pdnpz": Phase="Running", Reason="", readiness=true. Elapsed: 3.204727ms
    Sep  1 10:30:48.394: INFO: Pod "wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687-pdnpz" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687 in namespace emptydir-wrapper-2014, will wait for the garbage collector to delete the pods 09/01/23 10:30:48.394
    Sep  1 10:30:48.455: INFO: Deleting ReplicationController wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687 took: 6.563426ms
    Sep  1 10:30:48.556: INFO: Terminating ReplicationController wrapped-volume-race-d8742705-45ce-4056-997e-ceab77513687 pods took: 101.11832ms
    STEP: Creating RC which spawns configmap-volume pods 09/01/23 10:30:51.761
    Sep  1 10:30:51.776: INFO: Pod name wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033: Found 0 pods out of 5
    Sep  1 10:30:56.784: INFO: Pod name wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/01/23 10:30:56.784
    Sep  1 10:30:56.784: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:30:56.788: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km": Phase="Pending", Reason="", readiness=false. Elapsed: 3.445699ms
    Sep  1 10:30:58.792: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00777902s
    Sep  1 10:31:00.792: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007564032s
    Sep  1 10:31:02.798: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013268107s
    Sep  1 10:31:04.792: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007580925s
    Sep  1 10:31:06.794: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008978384s
    Sep  1 10:31:08.793: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km": Phase="Running", Reason="", readiness=true. Elapsed: 12.0085137s
    Sep  1 10:31:08.793: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-7q2km" satisfied condition "running"
    Sep  1 10:31:08.793: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-llxbr" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:31:08.797: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-llxbr": Phase="Running", Reason="", readiness=true. Elapsed: 3.790679ms
    Sep  1 10:31:08.797: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-llxbr" satisfied condition "running"
    Sep  1 10:31:08.797: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-qpkxq" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:31:08.801: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-qpkxq": Phase="Running", Reason="", readiness=true. Elapsed: 3.863612ms
    Sep  1 10:31:08.801: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-qpkxq" satisfied condition "running"
    Sep  1 10:31:08.801: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-t8zfp" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:31:08.805: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-t8zfp": Phase="Running", Reason="", readiness=true. Elapsed: 4.010268ms
    Sep  1 10:31:08.805: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-t8zfp" satisfied condition "running"
    Sep  1 10:31:08.805: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-zzw56" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:31:08.809: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-zzw56": Phase="Running", Reason="", readiness=true. Elapsed: 3.492483ms
    Sep  1 10:31:08.809: INFO: Pod "wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033-zzw56" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033 in namespace emptydir-wrapper-2014, will wait for the garbage collector to delete the pods 09/01/23 10:31:08.809
    Sep  1 10:31:08.870: INFO: Deleting ReplicationController wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033 took: 6.434868ms
    Sep  1 10:31:08.971: INFO: Terminating ReplicationController wrapped-volume-race-bb87db2b-9463-4e79-927e-23ff540e9033 pods took: 101.353723ms
    STEP: Creating RC which spawns configmap-volume pods 09/01/23 10:31:12.379
    Sep  1 10:31:12.394: INFO: Pod name wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88: Found 0 pods out of 5
    Sep  1 10:31:17.405: INFO: Pod name wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/01/23 10:31:17.405
    Sep  1 10:31:17.405: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:31:17.409: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.431155ms
    Sep  1 10:31:19.413: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007995766s
    Sep  1 10:31:21.415: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009801541s
    Sep  1 10:31:23.415: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009647255s
    Sep  1 10:31:25.416: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010380907s
    Sep  1 10:31:27.418: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012888184s
    Sep  1 10:31:29.418: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9": Phase="Running", Reason="", readiness=true. Elapsed: 12.012699398s
    Sep  1 10:31:29.419: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-474r9" satisfied condition "running"
    Sep  1 10:31:29.419: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-cjzm2" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:31:29.430: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-cjzm2": Phase="Running", Reason="", readiness=true. Elapsed: 10.944086ms
    Sep  1 10:31:29.431: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-cjzm2" satisfied condition "running"
    Sep  1 10:31:29.431: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-vvcfm" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:31:29.438: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-vvcfm": Phase="Running", Reason="", readiness=true. Elapsed: 7.547698ms
    Sep  1 10:31:29.439: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-vvcfm" satisfied condition "running"
    Sep  1 10:31:29.439: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-x5vsf" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:31:29.448: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-x5vsf": Phase="Running", Reason="", readiness=true. Elapsed: 8.389235ms
    Sep  1 10:31:29.448: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-x5vsf" satisfied condition "running"
    Sep  1 10:31:29.448: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-x6q72" in namespace "emptydir-wrapper-2014" to be "running"
    Sep  1 10:31:29.453: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-x6q72": Phase="Running", Reason="", readiness=true. Elapsed: 5.462703ms
    Sep  1 10:31:29.453: INFO: Pod "wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88-x6q72" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88 in namespace emptydir-wrapper-2014, will wait for the garbage collector to delete the pods 09/01/23 10:31:29.453
    Sep  1 10:31:29.560: INFO: Deleting ReplicationController wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88 took: 21.317843ms
    Sep  1 10:31:29.861: INFO: Terminating ReplicationController wrapped-volume-race-53047a87-fb36-4c9f-88c2-20cd0841af88 pods took: 301.197069ms
    STEP: Cleaning up the configMaps 09/01/23 10:31:32.562
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:31:33.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-2014" for this suite. 09/01/23 10:31:33.34
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:31:33.359
Sep  1 10:31:33.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-runtime 09/01/23 10:31:33.361
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:31:33.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:31:33.404
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 09/01/23 10:31:33.413
STEP: wait for the container to reach Succeeded 09/01/23 10:31:33.446
STEP: get the container status 09/01/23 10:31:37.497
STEP: the container should be terminated 09/01/23 10:31:37.508
STEP: the termination message should be set 09/01/23 10:31:37.508
Sep  1 10:31:37.508: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 09/01/23 10:31:37.508
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  1 10:31:37.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3516" for this suite. 09/01/23 10:31:37.571
------------------------------
• [4.247 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:31:33.359
    Sep  1 10:31:33.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-runtime 09/01/23 10:31:33.361
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:31:33.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:31:33.404
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 09/01/23 10:31:33.413
    STEP: wait for the container to reach Succeeded 09/01/23 10:31:33.446
    STEP: get the container status 09/01/23 10:31:37.497
    STEP: the container should be terminated 09/01/23 10:31:37.508
    STEP: the termination message should be set 09/01/23 10:31:37.508
    Sep  1 10:31:37.508: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 09/01/23 10:31:37.508
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:31:37.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3516" for this suite. 09/01/23 10:31:37.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:31:37.609
Sep  1 10:31:37.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 10:31:37.611
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:31:37.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:31:37.663
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/01/23 10:31:37.675
Sep  1 10:31:37.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 10:31:44.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:32:01.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7964" for this suite. 09/01/23 10:32:01.674
------------------------------
• [SLOW TEST] [24.072 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:31:37.609
    Sep  1 10:31:37.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 10:31:37.611
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:31:37.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:31:37.663
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/01/23 10:31:37.675
    Sep  1 10:31:37.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 10:31:44.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:32:01.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7964" for this suite. 09/01/23 10:32:01.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:32:01.689
Sep  1 10:32:01.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 10:32:01.691
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:01.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:01.708
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-9651/configmap-test-9468d6fd-aabe-4b2a-845e-98c77d9f2638 09/01/23 10:32:01.712
STEP: Creating a pod to test consume configMaps 09/01/23 10:32:01.717
Sep  1 10:32:01.729: INFO: Waiting up to 5m0s for pod "pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89" in namespace "configmap-9651" to be "Succeeded or Failed"
Sep  1 10:32:01.733: INFO: Pod "pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89": Phase="Pending", Reason="", readiness=false. Elapsed: 3.788914ms
Sep  1 10:32:03.737: INFO: Pod "pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008152806s
Sep  1 10:32:05.737: INFO: Pod "pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007797955s
STEP: Saw pod success 09/01/23 10:32:05.737
Sep  1 10:32:05.737: INFO: Pod "pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89" satisfied condition "Succeeded or Failed"
Sep  1 10:32:05.741: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89 container env-test: <nil>
STEP: delete the pod 09/01/23 10:32:05.76
Sep  1 10:32:05.772: INFO: Waiting for pod pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89 to disappear
Sep  1 10:32:05.775: INFO: Pod pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 10:32:05.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9651" for this suite. 09/01/23 10:32:05.779
------------------------------
• [4.095 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:32:01.689
    Sep  1 10:32:01.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 10:32:01.691
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:01.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:01.708
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-9651/configmap-test-9468d6fd-aabe-4b2a-845e-98c77d9f2638 09/01/23 10:32:01.712
    STEP: Creating a pod to test consume configMaps 09/01/23 10:32:01.717
    Sep  1 10:32:01.729: INFO: Waiting up to 5m0s for pod "pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89" in namespace "configmap-9651" to be "Succeeded or Failed"
    Sep  1 10:32:01.733: INFO: Pod "pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89": Phase="Pending", Reason="", readiness=false. Elapsed: 3.788914ms
    Sep  1 10:32:03.737: INFO: Pod "pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008152806s
    Sep  1 10:32:05.737: INFO: Pod "pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007797955s
    STEP: Saw pod success 09/01/23 10:32:05.737
    Sep  1 10:32:05.737: INFO: Pod "pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89" satisfied condition "Succeeded or Failed"
    Sep  1 10:32:05.741: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89 container env-test: <nil>
    STEP: delete the pod 09/01/23 10:32:05.76
    Sep  1 10:32:05.772: INFO: Waiting for pod pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89 to disappear
    Sep  1 10:32:05.775: INFO: Pod pod-configmaps-b32f2896-fea7-4e91-bfcb-7490f69ebd89 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:32:05.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9651" for this suite. 09/01/23 10:32:05.779
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:32:05.788
Sep  1 10:32:05.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename dns 09/01/23 10:32:05.79
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:05.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:05.81
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/01/23 10:32:05.814
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/01/23 10:32:05.815
STEP: creating a pod to probe DNS 09/01/23 10:32:05.815
STEP: submitting the pod to kubernetes 09/01/23 10:32:05.815
Sep  1 10:32:05.826: INFO: Waiting up to 15m0s for pod "dns-test-938f8605-3e38-466a-8266-302e6409a941" in namespace "dns-3169" to be "running"
Sep  1 10:32:05.829: INFO: Pod "dns-test-938f8605-3e38-466a-8266-302e6409a941": Phase="Pending", Reason="", readiness=false. Elapsed: 3.330644ms
Sep  1 10:32:07.836: INFO: Pod "dns-test-938f8605-3e38-466a-8266-302e6409a941": Phase="Running", Reason="", readiness=true. Elapsed: 2.009676228s
Sep  1 10:32:07.836: INFO: Pod "dns-test-938f8605-3e38-466a-8266-302e6409a941" satisfied condition "running"
STEP: retrieving the pod 09/01/23 10:32:07.836
STEP: looking for the results for each expected name from probers 09/01/23 10:32:07.84
Sep  1 10:32:07.866: INFO: DNS probes using dns-3169/dns-test-938f8605-3e38-466a-8266-302e6409a941 succeeded

STEP: deleting the pod 09/01/23 10:32:07.867
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  1 10:32:07.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3169" for this suite. 09/01/23 10:32:07.897
------------------------------
• [2.197 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:32:05.788
    Sep  1 10:32:05.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename dns 09/01/23 10:32:05.79
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:05.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:05.81
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/01/23 10:32:05.814
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/01/23 10:32:05.815
    STEP: creating a pod to probe DNS 09/01/23 10:32:05.815
    STEP: submitting the pod to kubernetes 09/01/23 10:32:05.815
    Sep  1 10:32:05.826: INFO: Waiting up to 15m0s for pod "dns-test-938f8605-3e38-466a-8266-302e6409a941" in namespace "dns-3169" to be "running"
    Sep  1 10:32:05.829: INFO: Pod "dns-test-938f8605-3e38-466a-8266-302e6409a941": Phase="Pending", Reason="", readiness=false. Elapsed: 3.330644ms
    Sep  1 10:32:07.836: INFO: Pod "dns-test-938f8605-3e38-466a-8266-302e6409a941": Phase="Running", Reason="", readiness=true. Elapsed: 2.009676228s
    Sep  1 10:32:07.836: INFO: Pod "dns-test-938f8605-3e38-466a-8266-302e6409a941" satisfied condition "running"
    STEP: retrieving the pod 09/01/23 10:32:07.836
    STEP: looking for the results for each expected name from probers 09/01/23 10:32:07.84
    Sep  1 10:32:07.866: INFO: DNS probes using dns-3169/dns-test-938f8605-3e38-466a-8266-302e6409a941 succeeded

    STEP: deleting the pod 09/01/23 10:32:07.867
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:32:07.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3169" for this suite. 09/01/23 10:32:07.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:32:07.993
Sep  1 10:32:07.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename svcaccounts 09/01/23 10:32:07.994
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:08.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:08.102
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 09/01/23 10:32:08.107
STEP: watching for the ServiceAccount to be added 09/01/23 10:32:08.145
STEP: patching the ServiceAccount 09/01/23 10:32:08.148
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/01/23 10:32:08.183
STEP: deleting the ServiceAccount 09/01/23 10:32:08.214
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  1 10:32:08.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9679" for this suite. 09/01/23 10:32:08.337
------------------------------
• [0.398 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:32:07.993
    Sep  1 10:32:07.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename svcaccounts 09/01/23 10:32:07.994
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:08.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:08.102
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 09/01/23 10:32:08.107
    STEP: watching for the ServiceAccount to be added 09/01/23 10:32:08.145
    STEP: patching the ServiceAccount 09/01/23 10:32:08.148
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/01/23 10:32:08.183
    STEP: deleting the ServiceAccount 09/01/23 10:32:08.214
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:32:08.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9679" for this suite. 09/01/23 10:32:08.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:32:08.393
Sep  1 10:32:08.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename hostport 09/01/23 10:32:08.394
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:08.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:08.52
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/01/23 10:32:08.538
Sep  1 10:32:08.579: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8456" to be "running and ready"
Sep  1 10:32:08.607: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 27.893719ms
Sep  1 10:32:08.608: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:32:10.612: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03249174s
Sep  1 10:32:10.612: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:32:12.611: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031798076s
Sep  1 10:32:12.611: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:32:14.612: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 6.032281118s
Sep  1 10:32:14.612: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  1 10:32:14.612: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.16.0.4 on the node which pod1 resides and expect scheduled 09/01/23 10:32:14.612
Sep  1 10:32:14.621: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8456" to be "running and ready"
Sep  1 10:32:14.625: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.201599ms
Sep  1 10:32:14.625: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:32:16.629: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008488852s
Sep  1 10:32:16.630: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:32:18.631: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.009639206s
Sep  1 10:32:18.631: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  1 10:32:18.631: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.16.0.4 but use UDP protocol on the node which pod2 resides 09/01/23 10:32:18.631
Sep  1 10:32:18.638: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8456" to be "running and ready"
Sep  1 10:32:18.641: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.944899ms
Sep  1 10:32:18.641: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:32:20.646: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007868471s
Sep  1 10:32:20.646: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:32:22.646: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.008296923s
Sep  1 10:32:22.646: INFO: The phase of Pod pod3 is Running (Ready = true)
Sep  1 10:32:22.646: INFO: Pod "pod3" satisfied condition "running and ready"
Sep  1 10:32:22.653: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8456" to be "running and ready"
Sep  1 10:32:22.656: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.971912ms
Sep  1 10:32:22.656: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:32:24.660: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.007329817s
Sep  1 10:32:24.661: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Sep  1 10:32:24.661: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/01/23 10:32:24.664
Sep  1 10:32:24.664: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.16.0.4 http://127.0.0.1:54323/hostname] Namespace:hostport-8456 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 10:32:24.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 10:32:24.665: INFO: ExecWithOptions: Clientset creation
Sep  1 10:32:24.665: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8456/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.16.0.4+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.4, port: 54323 09/01/23 10:32:24.868
Sep  1 10:32:24.869: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.16.0.4:54323/hostname] Namespace:hostport-8456 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 10:32:24.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 10:32:24.870: INFO: ExecWithOptions: Clientset creation
Sep  1 10:32:24.870: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8456/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.16.0.4%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.4, port: 54323 UDP 09/01/23 10:32:25.024
Sep  1 10:32:25.024: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.16.0.4 54323] Namespace:hostport-8456 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 10:32:25.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 10:32:25.025: INFO: ExecWithOptions: Clientset creation
Sep  1 10:32:25.026: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8456/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.16.0.4+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Sep  1 10:32:30.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-8456" for this suite. 09/01/23 10:32:30.17
------------------------------
• [SLOW TEST] [21.784 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:32:08.393
    Sep  1 10:32:08.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename hostport 09/01/23 10:32:08.394
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:08.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:08.52
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/01/23 10:32:08.538
    Sep  1 10:32:08.579: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8456" to be "running and ready"
    Sep  1 10:32:08.607: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 27.893719ms
    Sep  1 10:32:08.608: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:32:10.612: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03249174s
    Sep  1 10:32:10.612: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:32:12.611: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031798076s
    Sep  1 10:32:12.611: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:32:14.612: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 6.032281118s
    Sep  1 10:32:14.612: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  1 10:32:14.612: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.16.0.4 on the node which pod1 resides and expect scheduled 09/01/23 10:32:14.612
    Sep  1 10:32:14.621: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8456" to be "running and ready"
    Sep  1 10:32:14.625: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.201599ms
    Sep  1 10:32:14.625: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:32:16.629: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008488852s
    Sep  1 10:32:16.630: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:32:18.631: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.009639206s
    Sep  1 10:32:18.631: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  1 10:32:18.631: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.16.0.4 but use UDP protocol on the node which pod2 resides 09/01/23 10:32:18.631
    Sep  1 10:32:18.638: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8456" to be "running and ready"
    Sep  1 10:32:18.641: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.944899ms
    Sep  1 10:32:18.641: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:32:20.646: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007868471s
    Sep  1 10:32:20.646: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:32:22.646: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.008296923s
    Sep  1 10:32:22.646: INFO: The phase of Pod pod3 is Running (Ready = true)
    Sep  1 10:32:22.646: INFO: Pod "pod3" satisfied condition "running and ready"
    Sep  1 10:32:22.653: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8456" to be "running and ready"
    Sep  1 10:32:22.656: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.971912ms
    Sep  1 10:32:22.656: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:32:24.660: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.007329817s
    Sep  1 10:32:24.661: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Sep  1 10:32:24.661: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/01/23 10:32:24.664
    Sep  1 10:32:24.664: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.16.0.4 http://127.0.0.1:54323/hostname] Namespace:hostport-8456 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 10:32:24.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 10:32:24.665: INFO: ExecWithOptions: Clientset creation
    Sep  1 10:32:24.665: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8456/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.16.0.4+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.4, port: 54323 09/01/23 10:32:24.868
    Sep  1 10:32:24.869: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.16.0.4:54323/hostname] Namespace:hostport-8456 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 10:32:24.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 10:32:24.870: INFO: ExecWithOptions: Clientset creation
    Sep  1 10:32:24.870: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8456/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.16.0.4%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.4, port: 54323 UDP 09/01/23 10:32:25.024
    Sep  1 10:32:25.024: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.16.0.4 54323] Namespace:hostport-8456 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 10:32:25.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 10:32:25.025: INFO: ExecWithOptions: Clientset creation
    Sep  1 10:32:25.026: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8456/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.16.0.4+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:32:30.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-8456" for this suite. 09/01/23 10:32:30.17
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:32:30.178
Sep  1 10:32:30.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 10:32:30.183
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:30.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:30.206
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Sep  1 10:32:30.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/01/23 10:32:33.982
Sep  1 10:32:33.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 create -f -'
Sep  1 10:32:35.858: INFO: stderr: ""
Sep  1 10:32:35.858: INFO: stdout: "e2e-test-crd-publish-openapi-1978-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  1 10:32:35.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 delete e2e-test-crd-publish-openapi-1978-crds test-foo'
Sep  1 10:32:35.958: INFO: stderr: ""
Sep  1 10:32:35.959: INFO: stdout: "e2e-test-crd-publish-openapi-1978-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep  1 10:32:35.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 apply -f -'
Sep  1 10:32:36.554: INFO: stderr: ""
Sep  1 10:32:36.554: INFO: stdout: "e2e-test-crd-publish-openapi-1978-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  1 10:32:36.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 delete e2e-test-crd-publish-openapi-1978-crds test-foo'
Sep  1 10:32:36.657: INFO: stderr: ""
Sep  1 10:32:36.657: INFO: stdout: "e2e-test-crd-publish-openapi-1978-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/01/23 10:32:36.657
Sep  1 10:32:36.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 create -f -'
Sep  1 10:32:37.244: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/01/23 10:32:37.244
Sep  1 10:32:37.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 create -f -'
Sep  1 10:32:37.812: INFO: rc: 1
Sep  1 10:32:37.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 apply -f -'
Sep  1 10:32:38.396: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/01/23 10:32:38.397
Sep  1 10:32:38.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 create -f -'
Sep  1 10:32:38.990: INFO: rc: 1
Sep  1 10:32:38.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 apply -f -'
Sep  1 10:32:39.584: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 09/01/23 10:32:39.584
Sep  1 10:32:39.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 explain e2e-test-crd-publish-openapi-1978-crds'
Sep  1 10:32:40.161: INFO: stderr: ""
Sep  1 10:32:40.161: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1978-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 09/01/23 10:32:40.161
Sep  1 10:32:40.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 explain e2e-test-crd-publish-openapi-1978-crds.metadata'
Sep  1 10:32:40.710: INFO: stderr: ""
Sep  1 10:32:40.710: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1978-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep  1 10:32:40.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 explain e2e-test-crd-publish-openapi-1978-crds.spec'
Sep  1 10:32:41.294: INFO: stderr: ""
Sep  1 10:32:41.294: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1978-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep  1 10:32:41.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 explain e2e-test-crd-publish-openapi-1978-crds.spec.bars'
Sep  1 10:32:41.881: INFO: stderr: ""
Sep  1 10:32:41.881: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1978-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/01/23 10:32:41.882
Sep  1 10:32:41.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 explain e2e-test-crd-publish-openapi-1978-crds.spec.bars2'
Sep  1 10:32:42.466: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:32:46.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8872" for this suite. 09/01/23 10:32:46.295
------------------------------
• [SLOW TEST] [16.128 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:32:30.178
    Sep  1 10:32:30.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 10:32:30.183
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:30.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:30.206
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Sep  1 10:32:30.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/01/23 10:32:33.982
    Sep  1 10:32:33.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 create -f -'
    Sep  1 10:32:35.858: INFO: stderr: ""
    Sep  1 10:32:35.858: INFO: stdout: "e2e-test-crd-publish-openapi-1978-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep  1 10:32:35.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 delete e2e-test-crd-publish-openapi-1978-crds test-foo'
    Sep  1 10:32:35.958: INFO: stderr: ""
    Sep  1 10:32:35.959: INFO: stdout: "e2e-test-crd-publish-openapi-1978-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Sep  1 10:32:35.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 apply -f -'
    Sep  1 10:32:36.554: INFO: stderr: ""
    Sep  1 10:32:36.554: INFO: stdout: "e2e-test-crd-publish-openapi-1978-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep  1 10:32:36.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 delete e2e-test-crd-publish-openapi-1978-crds test-foo'
    Sep  1 10:32:36.657: INFO: stderr: ""
    Sep  1 10:32:36.657: INFO: stdout: "e2e-test-crd-publish-openapi-1978-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/01/23 10:32:36.657
    Sep  1 10:32:36.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 create -f -'
    Sep  1 10:32:37.244: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/01/23 10:32:37.244
    Sep  1 10:32:37.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 create -f -'
    Sep  1 10:32:37.812: INFO: rc: 1
    Sep  1 10:32:37.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 apply -f -'
    Sep  1 10:32:38.396: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/01/23 10:32:38.397
    Sep  1 10:32:38.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 create -f -'
    Sep  1 10:32:38.990: INFO: rc: 1
    Sep  1 10:32:38.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 --namespace=crd-publish-openapi-8872 apply -f -'
    Sep  1 10:32:39.584: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 09/01/23 10:32:39.584
    Sep  1 10:32:39.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 explain e2e-test-crd-publish-openapi-1978-crds'
    Sep  1 10:32:40.161: INFO: stderr: ""
    Sep  1 10:32:40.161: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1978-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 09/01/23 10:32:40.161
    Sep  1 10:32:40.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 explain e2e-test-crd-publish-openapi-1978-crds.metadata'
    Sep  1 10:32:40.710: INFO: stderr: ""
    Sep  1 10:32:40.710: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1978-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Sep  1 10:32:40.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 explain e2e-test-crd-publish-openapi-1978-crds.spec'
    Sep  1 10:32:41.294: INFO: stderr: ""
    Sep  1 10:32:41.294: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1978-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Sep  1 10:32:41.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 explain e2e-test-crd-publish-openapi-1978-crds.spec.bars'
    Sep  1 10:32:41.881: INFO: stderr: ""
    Sep  1 10:32:41.881: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1978-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/01/23 10:32:41.882
    Sep  1 10:32:41.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-8872 explain e2e-test-crd-publish-openapi-1978-crds.spec.bars2'
    Sep  1 10:32:42.466: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:32:46.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8872" for this suite. 09/01/23 10:32:46.295
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:32:46.306
Sep  1 10:32:46.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pods 09/01/23 10:32:46.308
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:46.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:46.329
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 09/01/23 10:32:46.332
Sep  1 10:32:46.344: INFO: created test-pod-1
Sep  1 10:32:46.351: INFO: created test-pod-2
Sep  1 10:32:46.361: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 09/01/23 10:32:46.362
Sep  1 10:32:46.363: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1031' to be running and ready
Sep  1 10:32:46.384: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  1 10:32:46.384: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  1 10:32:46.384: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  1 10:32:46.384: INFO: 0 / 3 pods in namespace 'pods-1031' are running and ready (0 seconds elapsed)
Sep  1 10:32:46.384: INFO: expected 0 pod replicas in namespace 'pods-1031', 0 are Running and Ready.
Sep  1 10:32:46.384: INFO: POD         NODE                                    PHASE    GRACE  CONDITIONS
Sep  1 10:32:46.384: INFO: test-pod-1  k8s-worker-1.c.operations-lab.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  }]
Sep  1 10:32:46.384: INFO: test-pod-2  k8s-worker-1.c.operations-lab.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  }]
Sep  1 10:32:46.385: INFO: test-pod-3  k8s-worker-1.c.operations-lab.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  }]
Sep  1 10:32:46.385: INFO: 
Sep  1 10:32:48.393: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  1 10:32:48.393: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  1 10:32:48.393: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  1 10:32:48.393: INFO: 0 / 3 pods in namespace 'pods-1031' are running and ready (2 seconds elapsed)
Sep  1 10:32:48.393: INFO: expected 0 pod replicas in namespace 'pods-1031', 0 are Running and Ready.
Sep  1 10:32:48.394: INFO: POD         NODE                                    PHASE    GRACE  CONDITIONS
Sep  1 10:32:48.394: INFO: test-pod-1  k8s-worker-1.c.operations-lab.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  }]
Sep  1 10:32:48.394: INFO: test-pod-2  k8s-worker-1.c.operations-lab.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  }]
Sep  1 10:32:48.394: INFO: test-pod-3  k8s-worker-1.c.operations-lab.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  }]
Sep  1 10:32:48.394: INFO: 
Sep  1 10:32:50.395: INFO: 3 / 3 pods in namespace 'pods-1031' are running and ready (4 seconds elapsed)
Sep  1 10:32:50.395: INFO: expected 0 pod replicas in namespace 'pods-1031', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 09/01/23 10:32:50.42
Sep  1 10:32:50.423: INFO: Pod quantity 3 is different from expected quantity 0
Sep  1 10:32:51.432: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  1 10:32:52.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1031" for this suite. 09/01/23 10:32:52.435
------------------------------
• [SLOW TEST] [6.140 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:32:46.306
    Sep  1 10:32:46.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pods 09/01/23 10:32:46.308
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:46.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:46.329
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 09/01/23 10:32:46.332
    Sep  1 10:32:46.344: INFO: created test-pod-1
    Sep  1 10:32:46.351: INFO: created test-pod-2
    Sep  1 10:32:46.361: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 09/01/23 10:32:46.362
    Sep  1 10:32:46.363: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1031' to be running and ready
    Sep  1 10:32:46.384: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  1 10:32:46.384: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  1 10:32:46.384: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  1 10:32:46.384: INFO: 0 / 3 pods in namespace 'pods-1031' are running and ready (0 seconds elapsed)
    Sep  1 10:32:46.384: INFO: expected 0 pod replicas in namespace 'pods-1031', 0 are Running and Ready.
    Sep  1 10:32:46.384: INFO: POD         NODE                                    PHASE    GRACE  CONDITIONS
    Sep  1 10:32:46.384: INFO: test-pod-1  k8s-worker-1.c.operations-lab.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  }]
    Sep  1 10:32:46.384: INFO: test-pod-2  k8s-worker-1.c.operations-lab.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  }]
    Sep  1 10:32:46.385: INFO: test-pod-3  k8s-worker-1.c.operations-lab.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  }]
    Sep  1 10:32:46.385: INFO: 
    Sep  1 10:32:48.393: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  1 10:32:48.393: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  1 10:32:48.393: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  1 10:32:48.393: INFO: 0 / 3 pods in namespace 'pods-1031' are running and ready (2 seconds elapsed)
    Sep  1 10:32:48.393: INFO: expected 0 pod replicas in namespace 'pods-1031', 0 are Running and Ready.
    Sep  1 10:32:48.394: INFO: POD         NODE                                    PHASE    GRACE  CONDITIONS
    Sep  1 10:32:48.394: INFO: test-pod-1  k8s-worker-1.c.operations-lab.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  }]
    Sep  1 10:32:48.394: INFO: test-pod-2  k8s-worker-1.c.operations-lab.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  }]
    Sep  1 10:32:48.394: INFO: test-pod-3  k8s-worker-1.c.operations-lab.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:32:46 +0000 UTC  }]
    Sep  1 10:32:48.394: INFO: 
    Sep  1 10:32:50.395: INFO: 3 / 3 pods in namespace 'pods-1031' are running and ready (4 seconds elapsed)
    Sep  1 10:32:50.395: INFO: expected 0 pod replicas in namespace 'pods-1031', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 09/01/23 10:32:50.42
    Sep  1 10:32:50.423: INFO: Pod quantity 3 is different from expected quantity 0
    Sep  1 10:32:51.432: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:32:52.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1031" for this suite. 09/01/23 10:32:52.435
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:32:52.454
Sep  1 10:32:52.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename runtimeclass 09/01/23 10:32:52.456
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:52.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:52.482
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  1 10:32:52.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5013" for this suite. 09/01/23 10:32:52.498
------------------------------
• [0.051 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:32:52.454
    Sep  1 10:32:52.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename runtimeclass 09/01/23 10:32:52.456
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:52.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:52.482
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:32:52.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5013" for this suite. 09/01/23 10:32:52.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:32:52.505
Sep  1 10:32:52.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 10:32:52.508
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:52.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:52.53
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/01/23 10:32:52.534
Sep  1 10:32:52.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2100 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep  1 10:32:52.647: INFO: stderr: ""
Sep  1 10:32:52.647: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 09/01/23 10:32:52.647
STEP: verifying the pod e2e-test-httpd-pod was created 09/01/23 10:32:57.698
Sep  1 10:32:57.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2100 get pod e2e-test-httpd-pod -o json'
Sep  1 10:32:57.855: INFO: stderr: ""
Sep  1 10:32:57.855: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-09-01T10:32:52Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2100\",\n        \"resourceVersion\": \"8333\",\n        \"uid\": \"5409188f-9c0f-4820-8aa9-a1341bcf51f4\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-7skbj\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-worker-1.c.operations-lab.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-7skbj\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-01T10:32:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-01T10:32:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-01T10:32:57Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-01T10:32:52Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://58ef61e3e57f6d7f69c5c70ae29bf98c31ee65fb169eea175e0dbc0fa6b89176\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-09-01T10:32:56Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.0.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.10.0.208\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.10.0.208\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-09-01T10:32:52Z\"\n    }\n}\n"
STEP: replace the image in the pod 09/01/23 10:32:57.856
Sep  1 10:32:57.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2100 replace -f -'
Sep  1 10:33:00.211: INFO: stderr: ""
Sep  1 10:33:00.211: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 09/01/23 10:33:00.211
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Sep  1 10:33:00.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2100 delete pods e2e-test-httpd-pod'
Sep  1 10:33:02.675: INFO: stderr: ""
Sep  1 10:33:02.675: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 10:33:02.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2100" for this suite. 09/01/23 10:33:02.685
------------------------------
• [SLOW TEST] [10.192 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:32:52.505
    Sep  1 10:32:52.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 10:32:52.508
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:32:52.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:32:52.53
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/01/23 10:32:52.534
    Sep  1 10:32:52.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2100 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep  1 10:32:52.647: INFO: stderr: ""
    Sep  1 10:32:52.647: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 09/01/23 10:32:52.647
    STEP: verifying the pod e2e-test-httpd-pod was created 09/01/23 10:32:57.698
    Sep  1 10:32:57.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2100 get pod e2e-test-httpd-pod -o json'
    Sep  1 10:32:57.855: INFO: stderr: ""
    Sep  1 10:32:57.855: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-09-01T10:32:52Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2100\",\n        \"resourceVersion\": \"8333\",\n        \"uid\": \"5409188f-9c0f-4820-8aa9-a1341bcf51f4\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-7skbj\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-worker-1.c.operations-lab.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-7skbj\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-01T10:32:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-01T10:32:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-01T10:32:57Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-01T10:32:52Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://58ef61e3e57f6d7f69c5c70ae29bf98c31ee65fb169eea175e0dbc0fa6b89176\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-09-01T10:32:56Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.0.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.10.0.208\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.10.0.208\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-09-01T10:32:52Z\"\n    }\n}\n"
    STEP: replace the image in the pod 09/01/23 10:32:57.856
    Sep  1 10:32:57.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2100 replace -f -'
    Sep  1 10:33:00.211: INFO: stderr: ""
    Sep  1 10:33:00.211: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 09/01/23 10:33:00.211
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Sep  1 10:33:00.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2100 delete pods e2e-test-httpd-pod'
    Sep  1 10:33:02.675: INFO: stderr: ""
    Sep  1 10:33:02.675: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:33:02.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2100" for this suite. 09/01/23 10:33:02.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:33:02.698
Sep  1 10:33:02.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename disruption 09/01/23 10:33:02.699
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:02.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:02.738
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 09/01/23 10:33:02.746
STEP: Updating PodDisruptionBudget status 09/01/23 10:33:02.76
STEP: Waiting for all pods to be running 09/01/23 10:33:02.773
Sep  1 10:33:02.790: INFO: running pods: 0 < 1
STEP: locating a running pod 09/01/23 10:33:04.794
STEP: Waiting for the pdb to be processed 09/01/23 10:33:04.807
STEP: Patching PodDisruptionBudget status 09/01/23 10:33:04.815
STEP: Waiting for the pdb to be processed 09/01/23 10:33:04.827
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  1 10:33:04.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4332" for this suite. 09/01/23 10:33:04.838
------------------------------
• [2.148 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:33:02.698
    Sep  1 10:33:02.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename disruption 09/01/23 10:33:02.699
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:02.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:02.738
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 09/01/23 10:33:02.746
    STEP: Updating PodDisruptionBudget status 09/01/23 10:33:02.76
    STEP: Waiting for all pods to be running 09/01/23 10:33:02.773
    Sep  1 10:33:02.790: INFO: running pods: 0 < 1
    STEP: locating a running pod 09/01/23 10:33:04.794
    STEP: Waiting for the pdb to be processed 09/01/23 10:33:04.807
    STEP: Patching PodDisruptionBudget status 09/01/23 10:33:04.815
    STEP: Waiting for the pdb to be processed 09/01/23 10:33:04.827
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:33:04.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4332" for this suite. 09/01/23 10:33:04.838
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:33:04.847
Sep  1 10:33:04.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename secrets 09/01/23 10:33:04.849
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:04.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:04.868
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-1718/secret-test-ff917399-62b8-4fcd-af4c-1e9e9f883976 09/01/23 10:33:04.871
STEP: Creating a pod to test consume secrets 09/01/23 10:33:04.876
Sep  1 10:33:04.887: INFO: Waiting up to 5m0s for pod "pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650" in namespace "secrets-1718" to be "Succeeded or Failed"
Sep  1 10:33:04.893: INFO: Pod "pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650": Phase="Pending", Reason="", readiness=false. Elapsed: 5.85305ms
Sep  1 10:33:06.897: INFO: Pod "pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009663922s
Sep  1 10:33:08.898: INFO: Pod "pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010127196s
STEP: Saw pod success 09/01/23 10:33:08.898
Sep  1 10:33:08.898: INFO: Pod "pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650" satisfied condition "Succeeded or Failed"
Sep  1 10:33:08.902: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650 container env-test: <nil>
STEP: delete the pod 09/01/23 10:33:08.909
Sep  1 10:33:08.924: INFO: Waiting for pod pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650 to disappear
Sep  1 10:33:08.927: INFO: Pod pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  1 10:33:08.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1718" for this suite. 09/01/23 10:33:08.933
------------------------------
• [4.097 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:33:04.847
    Sep  1 10:33:04.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename secrets 09/01/23 10:33:04.849
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:04.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:04.868
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-1718/secret-test-ff917399-62b8-4fcd-af4c-1e9e9f883976 09/01/23 10:33:04.871
    STEP: Creating a pod to test consume secrets 09/01/23 10:33:04.876
    Sep  1 10:33:04.887: INFO: Waiting up to 5m0s for pod "pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650" in namespace "secrets-1718" to be "Succeeded or Failed"
    Sep  1 10:33:04.893: INFO: Pod "pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650": Phase="Pending", Reason="", readiness=false. Elapsed: 5.85305ms
    Sep  1 10:33:06.897: INFO: Pod "pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009663922s
    Sep  1 10:33:08.898: INFO: Pod "pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010127196s
    STEP: Saw pod success 09/01/23 10:33:08.898
    Sep  1 10:33:08.898: INFO: Pod "pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650" satisfied condition "Succeeded or Failed"
    Sep  1 10:33:08.902: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650 container env-test: <nil>
    STEP: delete the pod 09/01/23 10:33:08.909
    Sep  1 10:33:08.924: INFO: Waiting for pod pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650 to disappear
    Sep  1 10:33:08.927: INFO: Pod pod-configmaps-063c6b51-aeca-4db2-9592-afc1516d7650 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:33:08.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1718" for this suite. 09/01/23 10:33:08.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:33:08.952
Sep  1 10:33:08.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename init-container 09/01/23 10:33:08.953
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:08.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:08.974
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 09/01/23 10:33:08.977
Sep  1 10:33:08.977: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:33:13.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4031" for this suite. 09/01/23 10:33:13.724
------------------------------
• [4.781 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:33:08.952
    Sep  1 10:33:08.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename init-container 09/01/23 10:33:08.953
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:08.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:08.974
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 09/01/23 10:33:08.977
    Sep  1 10:33:08.977: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:33:13.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4031" for this suite. 09/01/23 10:33:13.724
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:33:13.738
Sep  1 10:33:13.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 10:33:13.739
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:13.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:13.764
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-c93ea2f4-30c3-4d53-af80-d3831e114171 09/01/23 10:33:13.767
STEP: Creating secret with name secret-projected-all-test-volume-2e93a47c-60f5-4606-9390-5120dbcd3087 09/01/23 10:33:13.774
STEP: Creating a pod to test Check all projections for projected volume plugin 09/01/23 10:33:13.781
Sep  1 10:33:13.801: INFO: Waiting up to 5m0s for pod "projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318" in namespace "projected-8076" to be "Succeeded or Failed"
Sep  1 10:33:13.818: INFO: Pod "projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318": Phase="Pending", Reason="", readiness=false. Elapsed: 17.230899ms
Sep  1 10:33:15.822: INFO: Pod "projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318": Phase="Running", Reason="", readiness=false. Elapsed: 2.020803806s
Sep  1 10:33:17.822: INFO: Pod "projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021236315s
STEP: Saw pod success 09/01/23 10:33:17.822
Sep  1 10:33:17.822: INFO: Pod "projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318" satisfied condition "Succeeded or Failed"
Sep  1 10:33:17.825: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318 container projected-all-volume-test: <nil>
STEP: delete the pod 09/01/23 10:33:17.831
Sep  1 10:33:17.845: INFO: Waiting for pod projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318 to disappear
Sep  1 10:33:17.848: INFO: Pod projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Sep  1 10:33:17.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8076" for this suite. 09/01/23 10:33:17.853
------------------------------
• [4.120 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:33:13.738
    Sep  1 10:33:13.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 10:33:13.739
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:13.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:13.764
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-c93ea2f4-30c3-4d53-af80-d3831e114171 09/01/23 10:33:13.767
    STEP: Creating secret with name secret-projected-all-test-volume-2e93a47c-60f5-4606-9390-5120dbcd3087 09/01/23 10:33:13.774
    STEP: Creating a pod to test Check all projections for projected volume plugin 09/01/23 10:33:13.781
    Sep  1 10:33:13.801: INFO: Waiting up to 5m0s for pod "projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318" in namespace "projected-8076" to be "Succeeded or Failed"
    Sep  1 10:33:13.818: INFO: Pod "projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318": Phase="Pending", Reason="", readiness=false. Elapsed: 17.230899ms
    Sep  1 10:33:15.822: INFO: Pod "projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318": Phase="Running", Reason="", readiness=false. Elapsed: 2.020803806s
    Sep  1 10:33:17.822: INFO: Pod "projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021236315s
    STEP: Saw pod success 09/01/23 10:33:17.822
    Sep  1 10:33:17.822: INFO: Pod "projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318" satisfied condition "Succeeded or Failed"
    Sep  1 10:33:17.825: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318 container projected-all-volume-test: <nil>
    STEP: delete the pod 09/01/23 10:33:17.831
    Sep  1 10:33:17.845: INFO: Waiting for pod projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318 to disappear
    Sep  1 10:33:17.848: INFO: Pod projected-volume-0e0f482b-2243-4f04-bf8c-0bcec0fca318 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:33:17.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8076" for this suite. 09/01/23 10:33:17.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:33:17.863
Sep  1 10:33:17.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 10:33:17.866
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:17.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:17.882
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 09/01/23 10:33:17.886
Sep  1 10:33:17.893: INFO: Waiting up to 5m0s for pod "pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d" in namespace "emptydir-8294" to be "Succeeded or Failed"
Sep  1 10:33:17.897: INFO: Pod "pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.912259ms
Sep  1 10:33:19.901: INFO: Pod "pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007892867s
Sep  1 10:33:21.902: INFO: Pod "pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008308503s
STEP: Saw pod success 09/01/23 10:33:21.902
Sep  1 10:33:21.902: INFO: Pod "pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d" satisfied condition "Succeeded or Failed"
Sep  1 10:33:21.905: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d container test-container: <nil>
STEP: delete the pod 09/01/23 10:33:21.912
Sep  1 10:33:21.923: INFO: Waiting for pod pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d to disappear
Sep  1 10:33:21.927: INFO: Pod pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 10:33:21.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8294" for this suite. 09/01/23 10:33:21.932
------------------------------
• [4.074 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:33:17.863
    Sep  1 10:33:17.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 10:33:17.866
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:17.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:17.882
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 09/01/23 10:33:17.886
    Sep  1 10:33:17.893: INFO: Waiting up to 5m0s for pod "pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d" in namespace "emptydir-8294" to be "Succeeded or Failed"
    Sep  1 10:33:17.897: INFO: Pod "pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.912259ms
    Sep  1 10:33:19.901: INFO: Pod "pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007892867s
    Sep  1 10:33:21.902: INFO: Pod "pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008308503s
    STEP: Saw pod success 09/01/23 10:33:21.902
    Sep  1 10:33:21.902: INFO: Pod "pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d" satisfied condition "Succeeded or Failed"
    Sep  1 10:33:21.905: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d container test-container: <nil>
    STEP: delete the pod 09/01/23 10:33:21.912
    Sep  1 10:33:21.923: INFO: Waiting for pod pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d to disappear
    Sep  1 10:33:21.927: INFO: Pod pod-fd9d8222-fd2c-4ec7-b740-8913113b7f3d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:33:21.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8294" for this suite. 09/01/23 10:33:21.932
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:33:21.943
Sep  1 10:33:21.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename daemonsets 09/01/23 10:33:21.945
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:21.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:21.966
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
STEP: Creating simple DaemonSet "daemon-set" 09/01/23 10:33:21.985
STEP: Check that daemon pods launch on every node of the cluster. 09/01/23 10:33:21.991
Sep  1 10:33:21.998: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 10:33:22.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:33:22.002: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:33:23.006: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 10:33:23.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:33:23.010: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:33:24.008: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 10:33:24.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 10:33:24.012: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:33:25.008: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 10:33:25.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 10:33:25.011: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:33:26.007: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 10:33:26.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 10:33:26.010: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:33:27.007: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 10:33:27.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 10:33:27.012: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:33:28.007: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 10:33:28.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 10:33:28.011: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:33:29.007: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 10:33:29.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  1 10:33:29.011: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 09/01/23 10:33:29.014
STEP: DeleteCollection of the DaemonSets 09/01/23 10:33:29.018
STEP: Verify that ReplicaSets have been deleted 09/01/23 10:33:29.025
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
Sep  1 10:33:29.060: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8726"},"items":null}

Sep  1 10:33:29.070: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8726"},"items":[{"metadata":{"name":"daemon-set-47wpg","generateName":"daemon-set-","namespace":"daemonsets-6954","uid":"56f7a669-9fd2-48c6-9dab-effd47da0b43","resourceVersion":"8719","creationTimestamp":"2023-09-01T10:33:22Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0e415089-5759-4a44-82fa-dd35214d2556","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-01T10:33:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e415089-5759-4a44-82fa-dd35214d2556\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-01T10:33:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-k8gjg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-k8gjg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-worker-2.c.operations-lab.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-worker-2.c.operations-lab.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:22Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:22Z"}],"hostIP":"172.16.0.4","podIP":"10.10.1.110","podIPs":[{"ip":"10.10.1.110"}],"startTime":"2023-09-01T10:33:22Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-01T10:33:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://17f78dc85f915922b417f7723824e318dc02ad92c960dedf37576e6c4c61be75","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-nmnnp","generateName":"daemon-set-","namespace":"daemonsets-6954","uid":"f5ba6291-336e-4151-b4e6-522e27f11c10","resourceVersion":"8682","creationTimestamp":"2023-09-01T10:33:22Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0e415089-5759-4a44-82fa-dd35214d2556","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-01T10:33:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e415089-5759-4a44-82fa-dd35214d2556\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-01T10:33:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8kd92","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8kd92","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-worker-1.c.operations-lab.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-worker-1.c.operations-lab.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:22Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:23Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:23Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:22Z"}],"hostIP":"172.16.0.3","podIP":"10.10.0.201","podIPs":[{"ip":"10.10.0.201"}],"startTime":"2023-09-01T10:33:22Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-01T10:33:22Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d25e899e81192e4cf5f8bbc4f9185a4f11f8f30c944b82a77f846834e09ea951","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:33:29.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6954" for this suite. 09/01/23 10:33:29.108
------------------------------
• [SLOW TEST] [7.171 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:33:21.943
    Sep  1 10:33:21.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename daemonsets 09/01/23 10:33:21.945
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:21.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:21.966
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:834
    STEP: Creating simple DaemonSet "daemon-set" 09/01/23 10:33:21.985
    STEP: Check that daemon pods launch on every node of the cluster. 09/01/23 10:33:21.991
    Sep  1 10:33:21.998: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 10:33:22.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:33:22.002: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:33:23.006: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 10:33:23.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:33:23.010: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:33:24.008: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 10:33:24.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 10:33:24.012: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:33:25.008: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 10:33:25.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 10:33:25.011: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:33:26.007: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 10:33:26.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 10:33:26.010: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:33:27.007: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 10:33:27.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 10:33:27.012: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:33:28.007: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 10:33:28.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 10:33:28.011: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:33:29.007: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 10:33:29.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  1 10:33:29.011: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 09/01/23 10:33:29.014
    STEP: DeleteCollection of the DaemonSets 09/01/23 10:33:29.018
    STEP: Verify that ReplicaSets have been deleted 09/01/23 10:33:29.025
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    Sep  1 10:33:29.060: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8726"},"items":null}

    Sep  1 10:33:29.070: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8726"},"items":[{"metadata":{"name":"daemon-set-47wpg","generateName":"daemon-set-","namespace":"daemonsets-6954","uid":"56f7a669-9fd2-48c6-9dab-effd47da0b43","resourceVersion":"8719","creationTimestamp":"2023-09-01T10:33:22Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0e415089-5759-4a44-82fa-dd35214d2556","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-01T10:33:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e415089-5759-4a44-82fa-dd35214d2556\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-01T10:33:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-k8gjg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-k8gjg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-worker-2.c.operations-lab.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-worker-2.c.operations-lab.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:22Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:22Z"}],"hostIP":"172.16.0.4","podIP":"10.10.1.110","podIPs":[{"ip":"10.10.1.110"}],"startTime":"2023-09-01T10:33:22Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-01T10:33:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://17f78dc85f915922b417f7723824e318dc02ad92c960dedf37576e6c4c61be75","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-nmnnp","generateName":"daemon-set-","namespace":"daemonsets-6954","uid":"f5ba6291-336e-4151-b4e6-522e27f11c10","resourceVersion":"8682","creationTimestamp":"2023-09-01T10:33:22Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0e415089-5759-4a44-82fa-dd35214d2556","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-01T10:33:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e415089-5759-4a44-82fa-dd35214d2556\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-01T10:33:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8kd92","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8kd92","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-worker-1.c.operations-lab.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-worker-1.c.operations-lab.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:22Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:23Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:23Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-01T10:33:22Z"}],"hostIP":"172.16.0.3","podIP":"10.10.0.201","podIPs":[{"ip":"10.10.0.201"}],"startTime":"2023-09-01T10:33:22Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-01T10:33:22Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d25e899e81192e4cf5f8bbc4f9185a4f11f8f30c944b82a77f846834e09ea951","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:33:29.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6954" for this suite. 09/01/23 10:33:29.108
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:33:29.118
Sep  1 10:33:29.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename replicaset 09/01/23 10:33:29.121
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:29.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:29.147
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Sep  1 10:33:29.151: INFO: Creating ReplicaSet my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c
Sep  1 10:33:29.161: INFO: Pod name my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c: Found 0 pods out of 1
Sep  1 10:33:34.167: INFO: Pod name my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c: Found 1 pods out of 1
Sep  1 10:33:34.167: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c" is running
Sep  1 10:33:34.167: INFO: Waiting up to 5m0s for pod "my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c-8g9hr" in namespace "replicaset-4421" to be "running"
Sep  1 10:33:34.172: INFO: Pod "my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c-8g9hr": Phase="Running", Reason="", readiness=true. Elapsed: 4.645862ms
Sep  1 10:33:34.172: INFO: Pod "my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c-8g9hr" satisfied condition "running"
Sep  1 10:33:34.172: INFO: Pod "my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c-8g9hr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 10:33:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 10:33:30 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 10:33:30 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 10:33:29 +0000 UTC Reason: Message:}])
Sep  1 10:33:34.172: INFO: Trying to dial the pod
Sep  1 10:33:39.184: INFO: Controller my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c: Got expected result from replica 1 [my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c-8g9hr]: "my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c-8g9hr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  1 10:33:39.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4421" for this suite. 09/01/23 10:33:39.19
------------------------------
• [SLOW TEST] [10.081 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:33:29.118
    Sep  1 10:33:29.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename replicaset 09/01/23 10:33:29.121
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:29.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:29.147
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Sep  1 10:33:29.151: INFO: Creating ReplicaSet my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c
    Sep  1 10:33:29.161: INFO: Pod name my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c: Found 0 pods out of 1
    Sep  1 10:33:34.167: INFO: Pod name my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c: Found 1 pods out of 1
    Sep  1 10:33:34.167: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c" is running
    Sep  1 10:33:34.167: INFO: Waiting up to 5m0s for pod "my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c-8g9hr" in namespace "replicaset-4421" to be "running"
    Sep  1 10:33:34.172: INFO: Pod "my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c-8g9hr": Phase="Running", Reason="", readiness=true. Elapsed: 4.645862ms
    Sep  1 10:33:34.172: INFO: Pod "my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c-8g9hr" satisfied condition "running"
    Sep  1 10:33:34.172: INFO: Pod "my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c-8g9hr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 10:33:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 10:33:30 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 10:33:30 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 10:33:29 +0000 UTC Reason: Message:}])
    Sep  1 10:33:34.172: INFO: Trying to dial the pod
    Sep  1 10:33:39.184: INFO: Controller my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c: Got expected result from replica 1 [my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c-8g9hr]: "my-hostname-basic-0b12d737-35d1-405f-a9d3-9d774ded473c-8g9hr", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:33:39.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4421" for this suite. 09/01/23 10:33:39.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:33:39.205
Sep  1 10:33:39.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 10:33:39.206
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:39.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:39.229
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 09/01/23 10:33:39.233
Sep  1 10:33:39.244: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb" in namespace "downward-api-511" to be "Succeeded or Failed"
Sep  1 10:33:39.248: INFO: Pod "downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.841174ms
Sep  1 10:33:41.252: INFO: Pod "downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007909995s
Sep  1 10:33:43.254: INFO: Pod "downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009281767s
STEP: Saw pod success 09/01/23 10:33:43.254
Sep  1 10:33:43.254: INFO: Pod "downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb" satisfied condition "Succeeded or Failed"
Sep  1 10:33:43.257: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb container client-container: <nil>
STEP: delete the pod 09/01/23 10:33:43.263
Sep  1 10:33:43.279: INFO: Waiting for pod downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb to disappear
Sep  1 10:33:43.282: INFO: Pod downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  1 10:33:43.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-511" for this suite. 09/01/23 10:33:43.286
------------------------------
• [4.091 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:33:39.205
    Sep  1 10:33:39.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 10:33:39.206
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:39.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:39.229
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 09/01/23 10:33:39.233
    Sep  1 10:33:39.244: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb" in namespace "downward-api-511" to be "Succeeded or Failed"
    Sep  1 10:33:39.248: INFO: Pod "downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.841174ms
    Sep  1 10:33:41.252: INFO: Pod "downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007909995s
    Sep  1 10:33:43.254: INFO: Pod "downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009281767s
    STEP: Saw pod success 09/01/23 10:33:43.254
    Sep  1 10:33:43.254: INFO: Pod "downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb" satisfied condition "Succeeded or Failed"
    Sep  1 10:33:43.257: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb container client-container: <nil>
    STEP: delete the pod 09/01/23 10:33:43.263
    Sep  1 10:33:43.279: INFO: Waiting for pod downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb to disappear
    Sep  1 10:33:43.282: INFO: Pod downwardapi-volume-2432913b-e67a-48a5-afba-3078c30517bb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:33:43.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-511" for this suite. 09/01/23 10:33:43.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:33:43.305
Sep  1 10:33:43.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename replication-controller 09/01/23 10:33:43.306
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:43.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:43.328
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 09/01/23 10:33:43.331
STEP: When the matched label of one of its pods change 09/01/23 10:33:43.339
Sep  1 10:33:43.344: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  1 10:33:48.349: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 09/01/23 10:33:48.361
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  1 10:33:49.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8017" for this suite. 09/01/23 10:33:49.384
------------------------------
• [SLOW TEST] [6.089 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:33:43.305
    Sep  1 10:33:43.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename replication-controller 09/01/23 10:33:43.306
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:43.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:43.328
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 09/01/23 10:33:43.331
    STEP: When the matched label of one of its pods change 09/01/23 10:33:43.339
    Sep  1 10:33:43.344: INFO: Pod name pod-release: Found 0 pods out of 1
    Sep  1 10:33:48.349: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/01/23 10:33:48.361
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:33:49.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8017" for this suite. 09/01/23 10:33:49.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:33:49.398
Sep  1 10:33:49.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 10:33:49.4
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:49.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:49.423
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 09/01/23 10:33:49.427
Sep  1 10:33:49.436: INFO: Waiting up to 5m0s for pod "pod-0155a69e-58a8-42d7-9a61-29c62986c2e2" in namespace "emptydir-6584" to be "Succeeded or Failed"
Sep  1 10:33:49.440: INFO: Pod "pod-0155a69e-58a8-42d7-9a61-29c62986c2e2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.396992ms
Sep  1 10:33:51.444: INFO: Pod "pod-0155a69e-58a8-42d7-9a61-29c62986c2e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007657539s
Sep  1 10:33:53.444: INFO: Pod "pod-0155a69e-58a8-42d7-9a61-29c62986c2e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007940375s
STEP: Saw pod success 09/01/23 10:33:53.445
Sep  1 10:33:53.445: INFO: Pod "pod-0155a69e-58a8-42d7-9a61-29c62986c2e2" satisfied condition "Succeeded or Failed"
Sep  1 10:33:53.456: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-0155a69e-58a8-42d7-9a61-29c62986c2e2 container test-container: <nil>
STEP: delete the pod 09/01/23 10:33:53.463
Sep  1 10:33:53.475: INFO: Waiting for pod pod-0155a69e-58a8-42d7-9a61-29c62986c2e2 to disappear
Sep  1 10:33:53.481: INFO: Pod pod-0155a69e-58a8-42d7-9a61-29c62986c2e2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 10:33:53.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6584" for this suite. 09/01/23 10:33:53.486
------------------------------
• [4.096 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:33:49.398
    Sep  1 10:33:49.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 10:33:49.4
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:49.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:49.423
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/01/23 10:33:49.427
    Sep  1 10:33:49.436: INFO: Waiting up to 5m0s for pod "pod-0155a69e-58a8-42d7-9a61-29c62986c2e2" in namespace "emptydir-6584" to be "Succeeded or Failed"
    Sep  1 10:33:49.440: INFO: Pod "pod-0155a69e-58a8-42d7-9a61-29c62986c2e2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.396992ms
    Sep  1 10:33:51.444: INFO: Pod "pod-0155a69e-58a8-42d7-9a61-29c62986c2e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007657539s
    Sep  1 10:33:53.444: INFO: Pod "pod-0155a69e-58a8-42d7-9a61-29c62986c2e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007940375s
    STEP: Saw pod success 09/01/23 10:33:53.445
    Sep  1 10:33:53.445: INFO: Pod "pod-0155a69e-58a8-42d7-9a61-29c62986c2e2" satisfied condition "Succeeded or Failed"
    Sep  1 10:33:53.456: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-0155a69e-58a8-42d7-9a61-29c62986c2e2 container test-container: <nil>
    STEP: delete the pod 09/01/23 10:33:53.463
    Sep  1 10:33:53.475: INFO: Waiting for pod pod-0155a69e-58a8-42d7-9a61-29c62986c2e2 to disappear
    Sep  1 10:33:53.481: INFO: Pod pod-0155a69e-58a8-42d7-9a61-29c62986c2e2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:33:53.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6584" for this suite. 09/01/23 10:33:53.486
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:33:53.495
Sep  1 10:33:53.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 10:33:53.497
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:53.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:53.516
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 10:33:53.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-617" for this suite. 09/01/23 10:33:53.561
------------------------------
• [0.072 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:33:53.495
    Sep  1 10:33:53.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 10:33:53.497
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:53.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:53.516
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:33:53.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-617" for this suite. 09/01/23 10:33:53.561
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:33:53.573
Sep  1 10:33:53.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename deployment 09/01/23 10:33:53.575
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:53.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:53.598
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Sep  1 10:33:53.610: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  1 10:33:58.618: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/01/23 10:33:58.618
Sep  1 10:33:58.618: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/01/23 10:33:58.629
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  1 10:33:58.646: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1950  7c669eaa-4caa-4433-8838-a976c4fffed8 9078 1 2023-09-01 10:33:58 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-09-01 10:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00ca1c1c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep  1 10:33:58.660: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-1950  877cdd84-d413-421b-8701-1ef9c3e5216b 9081 1 2023-09-01 10:33:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 7c669eaa-4caa-4433-8838-a976c4fffed8 0xc00c92fae7 0xc00c92fae8}] [] [{kube-controller-manager Update apps/v1 2023-09-01 10:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c669eaa-4caa-4433-8838-a976c4fffed8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c92fb78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  1 10:33:58.660: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep  1 10:33:58.660: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1950  6684c3cd-8ffc-4c1b-8268-0e85901c193c 9079 1 2023-09-01 10:33:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 7c669eaa-4caa-4433-8838-a976c4fffed8 0xc00c92f9bf 0xc00c92f9d0}] [] [{e2e.test Update apps/v1 2023-09-01 10:33:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:33:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-01 10:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"7c669eaa-4caa-4433-8838-a976c4fffed8\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00c92fa88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  1 10:33:58.675: INFO: Pod "test-cleanup-controller-lznj2" is available:
&Pod{ObjectMeta:{test-cleanup-controller-lznj2 test-cleanup-controller- deployment-1950  e9c51b3c-d67d-44c5-beb5-9621b50b5d1a 9053 0 2023-09-01 10:33:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 6684c3cd-8ffc-4c1b-8268-0e85901c193c 0xc00ca1c60f 0xc00ca1c620}] [] [{kube-controller-manager Update v1 2023-09-01 10:33:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6684c3cd-8ffc-4c1b-8268-0e85901c193c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 10:33:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ql988,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ql988,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:33:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:33:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:33:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:33:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.183,StartTime:2023-09-01 10:33:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 10:33:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6a6d7bad8cac0f29a306fef12408bb9096a64327732bedb2d2d117a94941360b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 10:33:58.676: INFO: Pod "test-cleanup-deployment-7698ff6f6b-tg5jn" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-tg5jn test-cleanup-deployment-7698ff6f6b- deployment-1950  532e0f11-41d8-4d12-9e5d-f3644dcf84ba 9082 0 2023-09-01 10:33:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 877cdd84-d413-421b-8701-1ef9c3e5216b 0xc00ca1c7f7 0xc00ca1c7f8}] [] [{kube-controller-manager Update v1 2023-09-01 10:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"877cdd84-d413-421b-8701-1ef9c3e5216b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ssgf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ssgf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  1 10:33:58.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1950" for this suite. 09/01/23 10:33:58.699
------------------------------
• [SLOW TEST] [5.142 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:33:53.573
    Sep  1 10:33:53.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename deployment 09/01/23 10:33:53.575
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:53.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:53.598
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Sep  1 10:33:53.610: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Sep  1 10:33:58.618: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/01/23 10:33:58.618
    Sep  1 10:33:58.618: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/01/23 10:33:58.629
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  1 10:33:58.646: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1950  7c669eaa-4caa-4433-8838-a976c4fffed8 9078 1 2023-09-01 10:33:58 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-09-01 10:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00ca1c1c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Sep  1 10:33:58.660: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-1950  877cdd84-d413-421b-8701-1ef9c3e5216b 9081 1 2023-09-01 10:33:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 7c669eaa-4caa-4433-8838-a976c4fffed8 0xc00c92fae7 0xc00c92fae8}] [] [{kube-controller-manager Update apps/v1 2023-09-01 10:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c669eaa-4caa-4433-8838-a976c4fffed8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c92fb78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  1 10:33:58.660: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Sep  1 10:33:58.660: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1950  6684c3cd-8ffc-4c1b-8268-0e85901c193c 9079 1 2023-09-01 10:33:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 7c669eaa-4caa-4433-8838-a976c4fffed8 0xc00c92f9bf 0xc00c92f9d0}] [] [{e2e.test Update apps/v1 2023-09-01 10:33:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:33:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-01 10:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"7c669eaa-4caa-4433-8838-a976c4fffed8\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00c92fa88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  1 10:33:58.675: INFO: Pod "test-cleanup-controller-lznj2" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-lznj2 test-cleanup-controller- deployment-1950  e9c51b3c-d67d-44c5-beb5-9621b50b5d1a 9053 0 2023-09-01 10:33:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 6684c3cd-8ffc-4c1b-8268-0e85901c193c 0xc00ca1c60f 0xc00ca1c620}] [] [{kube-controller-manager Update v1 2023-09-01 10:33:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6684c3cd-8ffc-4c1b-8268-0e85901c193c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 10:33:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ql988,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ql988,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:33:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:33:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:33:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:33:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.183,StartTime:2023-09-01 10:33:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 10:33:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6a6d7bad8cac0f29a306fef12408bb9096a64327732bedb2d2d117a94941360b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 10:33:58.676: INFO: Pod "test-cleanup-deployment-7698ff6f6b-tg5jn" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-tg5jn test-cleanup-deployment-7698ff6f6b- deployment-1950  532e0f11-41d8-4d12-9e5d-f3644dcf84ba 9082 0 2023-09-01 10:33:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 877cdd84-d413-421b-8701-1ef9c3e5216b 0xc00ca1c7f7 0xc00ca1c7f8}] [] [{kube-controller-manager Update v1 2023-09-01 10:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"877cdd84-d413-421b-8701-1ef9c3e5216b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ssgf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ssgf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:33:58.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1950" for this suite. 09/01/23 10:33:58.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:33:58.716
Sep  1 10:33:58.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-probe 09/01/23 10:33:58.719
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:58.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:58.751
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 in namespace container-probe-6166 09/01/23 10:33:58.755
Sep  1 10:33:58.766: INFO: Waiting up to 5m0s for pod "liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2" in namespace "container-probe-6166" to be "not pending"
Sep  1 10:33:58.776: INFO: Pod "liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.402557ms
Sep  1 10:34:00.780: INFO: Pod "liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013523035s
Sep  1 10:34:00.780: INFO: Pod "liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2" satisfied condition "not pending"
Sep  1 10:34:00.780: INFO: Started pod liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 in namespace container-probe-6166
STEP: checking the pod's current state and verifying that restartCount is present 09/01/23 10:34:00.78
Sep  1 10:34:00.783: INFO: Initial restart count of pod liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 is 0
Sep  1 10:34:20.826: INFO: Restart count of pod container-probe-6166/liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 is now 1 (20.042323442s elapsed)
Sep  1 10:34:40.881: INFO: Restart count of pod container-probe-6166/liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 is now 2 (40.097626659s elapsed)
Sep  1 10:35:00.935: INFO: Restart count of pod container-probe-6166/liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 is now 3 (1m0.151635337s elapsed)
Sep  1 10:35:20.985: INFO: Restart count of pod container-probe-6166/liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 is now 4 (1m20.202010283s elapsed)
Sep  1 10:36:27.155: INFO: Restart count of pod container-probe-6166/liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 is now 5 (2m26.37137178s elapsed)
STEP: deleting the pod 09/01/23 10:36:27.155
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  1 10:36:27.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6166" for this suite. 09/01/23 10:36:27.173
------------------------------
• [SLOW TEST] [148.494 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:33:58.716
    Sep  1 10:33:58.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-probe 09/01/23 10:33:58.719
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:33:58.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:33:58.751
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 in namespace container-probe-6166 09/01/23 10:33:58.755
    Sep  1 10:33:58.766: INFO: Waiting up to 5m0s for pod "liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2" in namespace "container-probe-6166" to be "not pending"
    Sep  1 10:33:58.776: INFO: Pod "liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.402557ms
    Sep  1 10:34:00.780: INFO: Pod "liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013523035s
    Sep  1 10:34:00.780: INFO: Pod "liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2" satisfied condition "not pending"
    Sep  1 10:34:00.780: INFO: Started pod liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 in namespace container-probe-6166
    STEP: checking the pod's current state and verifying that restartCount is present 09/01/23 10:34:00.78
    Sep  1 10:34:00.783: INFO: Initial restart count of pod liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 is 0
    Sep  1 10:34:20.826: INFO: Restart count of pod container-probe-6166/liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 is now 1 (20.042323442s elapsed)
    Sep  1 10:34:40.881: INFO: Restart count of pod container-probe-6166/liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 is now 2 (40.097626659s elapsed)
    Sep  1 10:35:00.935: INFO: Restart count of pod container-probe-6166/liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 is now 3 (1m0.151635337s elapsed)
    Sep  1 10:35:20.985: INFO: Restart count of pod container-probe-6166/liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 is now 4 (1m20.202010283s elapsed)
    Sep  1 10:36:27.155: INFO: Restart count of pod container-probe-6166/liveness-39ad368e-ab0f-477c-a97f-1a808c2005f2 is now 5 (2m26.37137178s elapsed)
    STEP: deleting the pod 09/01/23 10:36:27.155
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:36:27.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6166" for this suite. 09/01/23 10:36:27.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:36:27.221
Sep  1 10:36:27.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename csistoragecapacity 09/01/23 10:36:27.224
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:36:27.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:36:27.246
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 09/01/23 10:36:27.249
STEP: getting /apis/storage.k8s.io 09/01/23 10:36:27.253
STEP: getting /apis/storage.k8s.io/v1 09/01/23 10:36:27.255
STEP: creating 09/01/23 10:36:27.256
STEP: watching 09/01/23 10:36:27.284
Sep  1 10:36:27.284: INFO: starting watch
STEP: getting 09/01/23 10:36:27.293
STEP: listing in namespace 09/01/23 10:36:27.296
STEP: listing across namespaces 09/01/23 10:36:27.298
STEP: patching 09/01/23 10:36:27.301
STEP: updating 09/01/23 10:36:27.308
Sep  1 10:36:27.314: INFO: waiting for watch events with expected annotations in namespace
Sep  1 10:36:27.314: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 09/01/23 10:36:27.315
STEP: deleting a collection 09/01/23 10:36:27.325
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Sep  1 10:36:27.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-543" for this suite. 09/01/23 10:36:27.343
------------------------------
• [0.130 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:36:27.221
    Sep  1 10:36:27.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename csistoragecapacity 09/01/23 10:36:27.224
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:36:27.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:36:27.246
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 09/01/23 10:36:27.249
    STEP: getting /apis/storage.k8s.io 09/01/23 10:36:27.253
    STEP: getting /apis/storage.k8s.io/v1 09/01/23 10:36:27.255
    STEP: creating 09/01/23 10:36:27.256
    STEP: watching 09/01/23 10:36:27.284
    Sep  1 10:36:27.284: INFO: starting watch
    STEP: getting 09/01/23 10:36:27.293
    STEP: listing in namespace 09/01/23 10:36:27.296
    STEP: listing across namespaces 09/01/23 10:36:27.298
    STEP: patching 09/01/23 10:36:27.301
    STEP: updating 09/01/23 10:36:27.308
    Sep  1 10:36:27.314: INFO: waiting for watch events with expected annotations in namespace
    Sep  1 10:36:27.314: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 09/01/23 10:36:27.315
    STEP: deleting a collection 09/01/23 10:36:27.325
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:36:27.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-543" for this suite. 09/01/23 10:36:27.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:36:27.356
Sep  1 10:36:27.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename secrets 09/01/23 10:36:27.358
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:36:27.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:36:27.382
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-8c74523c-0a3a-416b-b922-cf4dbd65866f 09/01/23 10:36:27.386
STEP: Creating a pod to test consume secrets 09/01/23 10:36:27.392
Sep  1 10:36:27.404: INFO: Waiting up to 5m0s for pod "pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af" in namespace "secrets-5201" to be "Succeeded or Failed"
Sep  1 10:36:27.417: INFO: Pod "pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af": Phase="Pending", Reason="", readiness=false. Elapsed: 12.614244ms
Sep  1 10:36:29.423: INFO: Pod "pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af": Phase="Running", Reason="", readiness=true. Elapsed: 2.018813962s
Sep  1 10:36:31.421: INFO: Pod "pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af": Phase="Running", Reason="", readiness=false. Elapsed: 4.016447279s
Sep  1 10:36:33.424: INFO: Pod "pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019609777s
STEP: Saw pod success 09/01/23 10:36:33.424
Sep  1 10:36:33.424: INFO: Pod "pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af" satisfied condition "Succeeded or Failed"
Sep  1 10:36:33.428: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af container secret-volume-test: <nil>
STEP: delete the pod 09/01/23 10:36:33.448
Sep  1 10:36:33.462: INFO: Waiting for pod pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af to disappear
Sep  1 10:36:33.465: INFO: Pod pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  1 10:36:33.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5201" for this suite. 09/01/23 10:36:33.469
------------------------------
• [SLOW TEST] [6.122 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:36:27.356
    Sep  1 10:36:27.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename secrets 09/01/23 10:36:27.358
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:36:27.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:36:27.382
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-8c74523c-0a3a-416b-b922-cf4dbd65866f 09/01/23 10:36:27.386
    STEP: Creating a pod to test consume secrets 09/01/23 10:36:27.392
    Sep  1 10:36:27.404: INFO: Waiting up to 5m0s for pod "pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af" in namespace "secrets-5201" to be "Succeeded or Failed"
    Sep  1 10:36:27.417: INFO: Pod "pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af": Phase="Pending", Reason="", readiness=false. Elapsed: 12.614244ms
    Sep  1 10:36:29.423: INFO: Pod "pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af": Phase="Running", Reason="", readiness=true. Elapsed: 2.018813962s
    Sep  1 10:36:31.421: INFO: Pod "pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af": Phase="Running", Reason="", readiness=false. Elapsed: 4.016447279s
    Sep  1 10:36:33.424: INFO: Pod "pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019609777s
    STEP: Saw pod success 09/01/23 10:36:33.424
    Sep  1 10:36:33.424: INFO: Pod "pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af" satisfied condition "Succeeded or Failed"
    Sep  1 10:36:33.428: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af container secret-volume-test: <nil>
    STEP: delete the pod 09/01/23 10:36:33.448
    Sep  1 10:36:33.462: INFO: Waiting for pod pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af to disappear
    Sep  1 10:36:33.465: INFO: Pod pod-secrets-7b2418b7-ddde-4fc9-9734-d6346ca9b4af no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:36:33.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5201" for this suite. 09/01/23 10:36:33.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:36:33.491
Sep  1 10:36:33.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pods 09/01/23 10:36:33.492
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:36:33.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:36:33.516
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 09/01/23 10:36:33.519
STEP: submitting the pod to kubernetes 09/01/23 10:36:33.519
Sep  1 10:36:33.528: INFO: Waiting up to 5m0s for pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae" in namespace "pods-8409" to be "running and ready"
Sep  1 10:36:33.532: INFO: Pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.570995ms
Sep  1 10:36:33.532: INFO: The phase of Pod pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:36:35.537: INFO: Pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae": Phase="Running", Reason="", readiness=true. Elapsed: 2.008142318s
Sep  1 10:36:35.537: INFO: The phase of Pod pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae is Running (Ready = true)
Sep  1 10:36:35.537: INFO: Pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/01/23 10:36:35.54
STEP: updating the pod 09/01/23 10:36:35.543
Sep  1 10:36:36.056: INFO: Successfully updated pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae"
Sep  1 10:36:36.056: INFO: Waiting up to 5m0s for pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae" in namespace "pods-8409" to be "running"
Sep  1 10:36:36.062: INFO: Pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae": Phase="Running", Reason="", readiness=true. Elapsed: 5.343332ms
Sep  1 10:36:36.062: INFO: Pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 09/01/23 10:36:36.062
Sep  1 10:36:36.070: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  1 10:36:36.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8409" for this suite. 09/01/23 10:36:36.075
------------------------------
• [2.594 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:36:33.491
    Sep  1 10:36:33.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pods 09/01/23 10:36:33.492
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:36:33.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:36:33.516
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 09/01/23 10:36:33.519
    STEP: submitting the pod to kubernetes 09/01/23 10:36:33.519
    Sep  1 10:36:33.528: INFO: Waiting up to 5m0s for pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae" in namespace "pods-8409" to be "running and ready"
    Sep  1 10:36:33.532: INFO: Pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.570995ms
    Sep  1 10:36:33.532: INFO: The phase of Pod pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:36:35.537: INFO: Pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae": Phase="Running", Reason="", readiness=true. Elapsed: 2.008142318s
    Sep  1 10:36:35.537: INFO: The phase of Pod pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae is Running (Ready = true)
    Sep  1 10:36:35.537: INFO: Pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/01/23 10:36:35.54
    STEP: updating the pod 09/01/23 10:36:35.543
    Sep  1 10:36:36.056: INFO: Successfully updated pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae"
    Sep  1 10:36:36.056: INFO: Waiting up to 5m0s for pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae" in namespace "pods-8409" to be "running"
    Sep  1 10:36:36.062: INFO: Pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae": Phase="Running", Reason="", readiness=true. Elapsed: 5.343332ms
    Sep  1 10:36:36.062: INFO: Pod "pod-update-bc0eb38a-884a-4295-88a8-cf7bc907caae" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 09/01/23 10:36:36.062
    Sep  1 10:36:36.070: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:36:36.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8409" for this suite. 09/01/23 10:36:36.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:36:36.093
Sep  1 10:36:36.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename deployment 09/01/23 10:36:36.095
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:36:36.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:36:36.123
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Sep  1 10:36:36.127: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  1 10:36:36.137: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  1 10:36:41.143: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/01/23 10:36:41.143
Sep  1 10:36:41.143: INFO: Creating deployment "test-rolling-update-deployment"
Sep  1 10:36:41.151: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  1 10:36:41.163: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  1 10:36:43.171: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  1 10:36:43.173: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  1 10:36:43.183: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-393  d7c413dd-5c95-490d-894b-99a0ea262e98 10179 1 2023-09-01 10:36:41 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-09-01 10:36:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:36:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037070a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-01 10:36:41 +0000 UTC,LastTransitionTime:2023-09-01 10:36:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-09-01 10:36:42 +0000 UTC,LastTransitionTime:2023-09-01 10:36:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  1 10:36:43.186: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-393  96ac2181-b7ba-49a6-86c6-d2a6c1199dd8 10165 1 2023-09-01 10:36:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d7c413dd-5c95-490d-894b-99a0ea262e98 0xc003707587 0xc003707588}] [] [{kube-controller-manager Update apps/v1 2023-09-01 10:36:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7c413dd-5c95-490d-894b-99a0ea262e98\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:36:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037079e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  1 10:36:43.186: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  1 10:36:43.187: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-393  085e0840-8da6-4266-92f7-cadfc968515a 10178 2 2023-09-01 10:36:36 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d7c413dd-5c95-490d-894b-99a0ea262e98 0xc003707457 0xc003707458}] [] [{e2e.test Update apps/v1 2023-09-01 10:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:36:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7c413dd-5c95-490d-894b-99a0ea262e98\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:36:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003707518 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  1 10:36:43.190: INFO: Pod "test-rolling-update-deployment-7549d9f46d-ttgbr" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-ttgbr test-rolling-update-deployment-7549d9f46d- deployment-393  76da004c-da78-458c-abf0-e6d0708b1249 10164 0 2023-09-01 10:36:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 96ac2181-b7ba-49a6-86c6-d2a6c1199dd8 0xc00c0f02d7 0xc00c0f02d8}] [] [{kube-controller-manager Update v1 2023-09-01 10:36:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96ac2181-b7ba-49a6-86c6-d2a6c1199dd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 10:36:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdpv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdpv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:36:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:36:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:36:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:36:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.22,StartTime:2023-09-01 10:36:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 10:36:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://a6b279681911455ae861c974c65f1c37798033669b193afba1a09fac50af0ab6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  1 10:36:43.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-393" for this suite. 09/01/23 10:36:43.195
------------------------------
• [SLOW TEST] [7.110 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:36:36.093
    Sep  1 10:36:36.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename deployment 09/01/23 10:36:36.095
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:36:36.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:36:36.123
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Sep  1 10:36:36.127: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Sep  1 10:36:36.137: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  1 10:36:41.143: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/01/23 10:36:41.143
    Sep  1 10:36:41.143: INFO: Creating deployment "test-rolling-update-deployment"
    Sep  1 10:36:41.151: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Sep  1 10:36:41.163: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Sep  1 10:36:43.171: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Sep  1 10:36:43.173: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  1 10:36:43.183: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-393  d7c413dd-5c95-490d-894b-99a0ea262e98 10179 1 2023-09-01 10:36:41 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-09-01 10:36:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:36:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037070a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-01 10:36:41 +0000 UTC,LastTransitionTime:2023-09-01 10:36:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-09-01 10:36:42 +0000 UTC,LastTransitionTime:2023-09-01 10:36:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  1 10:36:43.186: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-393  96ac2181-b7ba-49a6-86c6-d2a6c1199dd8 10165 1 2023-09-01 10:36:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d7c413dd-5c95-490d-894b-99a0ea262e98 0xc003707587 0xc003707588}] [] [{kube-controller-manager Update apps/v1 2023-09-01 10:36:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7c413dd-5c95-490d-894b-99a0ea262e98\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:36:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037079e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  1 10:36:43.186: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Sep  1 10:36:43.187: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-393  085e0840-8da6-4266-92f7-cadfc968515a 10178 2 2023-09-01 10:36:36 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d7c413dd-5c95-490d-894b-99a0ea262e98 0xc003707457 0xc003707458}] [] [{e2e.test Update apps/v1 2023-09-01 10:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:36:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7c413dd-5c95-490d-894b-99a0ea262e98\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:36:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003707518 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  1 10:36:43.190: INFO: Pod "test-rolling-update-deployment-7549d9f46d-ttgbr" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-ttgbr test-rolling-update-deployment-7549d9f46d- deployment-393  76da004c-da78-458c-abf0-e6d0708b1249 10164 0 2023-09-01 10:36:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 96ac2181-b7ba-49a6-86c6-d2a6c1199dd8 0xc00c0f02d7 0xc00c0f02d8}] [] [{kube-controller-manager Update v1 2023-09-01 10:36:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96ac2181-b7ba-49a6-86c6-d2a6c1199dd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 10:36:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdpv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdpv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:36:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:36:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:36:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:36:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.22,StartTime:2023-09-01 10:36:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 10:36:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://a6b279681911455ae861c974c65f1c37798033669b193afba1a09fac50af0ab6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:36:43.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-393" for this suite. 09/01/23 10:36:43.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:36:43.213
Sep  1 10:36:43.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename secrets 09/01/23 10:36:43.215
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:36:43.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:36:43.237
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-259fa9cd-7352-40ec-b5d9-89fbde79d304 09/01/23 10:36:43.264
STEP: Creating a pod to test consume secrets 09/01/23 10:36:43.274
Sep  1 10:36:43.286: INFO: Waiting up to 5m0s for pod "pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042" in namespace "secrets-9438" to be "Succeeded or Failed"
Sep  1 10:36:43.293: INFO: Pod "pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042": Phase="Pending", Reason="", readiness=false. Elapsed: 7.486733ms
Sep  1 10:36:45.298: INFO: Pod "pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011683099s
Sep  1 10:36:47.297: INFO: Pod "pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011634361s
STEP: Saw pod success 09/01/23 10:36:47.298
Sep  1 10:36:47.298: INFO: Pod "pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042" satisfied condition "Succeeded or Failed"
Sep  1 10:36:47.301: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042 container secret-volume-test: <nil>
STEP: delete the pod 09/01/23 10:36:47.308
Sep  1 10:36:47.368: INFO: Waiting for pod pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042 to disappear
Sep  1 10:36:47.381: INFO: Pod pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  1 10:36:47.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9438" for this suite. 09/01/23 10:36:47.411
STEP: Destroying namespace "secret-namespace-6453" for this suite. 09/01/23 10:36:47.439
------------------------------
• [4.264 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:36:43.213
    Sep  1 10:36:43.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename secrets 09/01/23 10:36:43.215
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:36:43.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:36:43.237
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-259fa9cd-7352-40ec-b5d9-89fbde79d304 09/01/23 10:36:43.264
    STEP: Creating a pod to test consume secrets 09/01/23 10:36:43.274
    Sep  1 10:36:43.286: INFO: Waiting up to 5m0s for pod "pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042" in namespace "secrets-9438" to be "Succeeded or Failed"
    Sep  1 10:36:43.293: INFO: Pod "pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042": Phase="Pending", Reason="", readiness=false. Elapsed: 7.486733ms
    Sep  1 10:36:45.298: INFO: Pod "pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011683099s
    Sep  1 10:36:47.297: INFO: Pod "pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011634361s
    STEP: Saw pod success 09/01/23 10:36:47.298
    Sep  1 10:36:47.298: INFO: Pod "pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042" satisfied condition "Succeeded or Failed"
    Sep  1 10:36:47.301: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042 container secret-volume-test: <nil>
    STEP: delete the pod 09/01/23 10:36:47.308
    Sep  1 10:36:47.368: INFO: Waiting for pod pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042 to disappear
    Sep  1 10:36:47.381: INFO: Pod pod-secrets-4e6946a4-8b4c-46c9-8964-0ca1ec9d9042 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:36:47.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9438" for this suite. 09/01/23 10:36:47.411
    STEP: Destroying namespace "secret-namespace-6453" for this suite. 09/01/23 10:36:47.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:36:47.483
Sep  1 10:36:47.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-probe 09/01/23 10:36:47.485
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:36:47.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:36:47.606
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c in namespace container-probe-1903 09/01/23 10:36:47.612
Sep  1 10:36:47.669: INFO: Waiting up to 5m0s for pod "busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c" in namespace "container-probe-1903" to be "not pending"
Sep  1 10:36:47.678: INFO: Pod "busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.200369ms
Sep  1 10:36:49.703: INFO: Pod "busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c": Phase="Running", Reason="", readiness=true. Elapsed: 2.033096182s
Sep  1 10:36:49.703: INFO: Pod "busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c" satisfied condition "not pending"
Sep  1 10:36:49.703: INFO: Started pod busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c in namespace container-probe-1903
STEP: checking the pod's current state and verifying that restartCount is present 09/01/23 10:36:49.703
Sep  1 10:36:49.709: INFO: Initial restart count of pod busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c is 0
Sep  1 10:37:39.871: INFO: Restart count of pod container-probe-1903/busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c is now 1 (50.161368839s elapsed)
STEP: deleting the pod 09/01/23 10:37:39.871
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  1 10:37:39.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1903" for this suite. 09/01/23 10:37:39.895
------------------------------
• [SLOW TEST] [52.447 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:36:47.483
    Sep  1 10:36:47.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-probe 09/01/23 10:36:47.485
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:36:47.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:36:47.606
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c in namespace container-probe-1903 09/01/23 10:36:47.612
    Sep  1 10:36:47.669: INFO: Waiting up to 5m0s for pod "busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c" in namespace "container-probe-1903" to be "not pending"
    Sep  1 10:36:47.678: INFO: Pod "busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.200369ms
    Sep  1 10:36:49.703: INFO: Pod "busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c": Phase="Running", Reason="", readiness=true. Elapsed: 2.033096182s
    Sep  1 10:36:49.703: INFO: Pod "busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c" satisfied condition "not pending"
    Sep  1 10:36:49.703: INFO: Started pod busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c in namespace container-probe-1903
    STEP: checking the pod's current state and verifying that restartCount is present 09/01/23 10:36:49.703
    Sep  1 10:36:49.709: INFO: Initial restart count of pod busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c is 0
    Sep  1 10:37:39.871: INFO: Restart count of pod container-probe-1903/busybox-ee322533-55b1-4e9d-8c57-3c30e357f76c is now 1 (50.161368839s elapsed)
    STEP: deleting the pod 09/01/23 10:37:39.871
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:37:39.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1903" for this suite. 09/01/23 10:37:39.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:37:39.943
Sep  1 10:37:39.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 10:37:39.945
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:37:39.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:37:39.975
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 10:37:40.025
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 10:37:40.795
STEP: Deploying the webhook pod 09/01/23 10:37:40.803
STEP: Wait for the deployment to be ready 09/01/23 10:37:40.82
Sep  1 10:37:40.831: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/01/23 10:37:42.843
STEP: Verifying the service has paired with the endpoint 09/01/23 10:37:42.863
Sep  1 10:37:43.864: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/01/23 10:37:43.868
STEP: Registering slow webhook via the AdmissionRegistration API 09/01/23 10:37:43.868
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/01/23 10:37:43.895
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/01/23 10:37:44.907
STEP: Registering slow webhook via the AdmissionRegistration API 09/01/23 10:37:44.907
STEP: Having no error when timeout is longer than webhook latency 09/01/23 10:37:45.943
STEP: Registering slow webhook via the AdmissionRegistration API 09/01/23 10:37:45.943
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/01/23 10:37:50.991
STEP: Registering slow webhook via the AdmissionRegistration API 09/01/23 10:37:50.991
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:37:56.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6758" for this suite. 09/01/23 10:37:56.194
STEP: Destroying namespace "webhook-6758-markers" for this suite. 09/01/23 10:37:56.239
------------------------------
• [SLOW TEST] [16.332 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:37:39.943
    Sep  1 10:37:39.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 10:37:39.945
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:37:39.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:37:39.975
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 10:37:40.025
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 10:37:40.795
    STEP: Deploying the webhook pod 09/01/23 10:37:40.803
    STEP: Wait for the deployment to be ready 09/01/23 10:37:40.82
    Sep  1 10:37:40.831: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/01/23 10:37:42.843
    STEP: Verifying the service has paired with the endpoint 09/01/23 10:37:42.863
    Sep  1 10:37:43.864: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/01/23 10:37:43.868
    STEP: Registering slow webhook via the AdmissionRegistration API 09/01/23 10:37:43.868
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/01/23 10:37:43.895
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/01/23 10:37:44.907
    STEP: Registering slow webhook via the AdmissionRegistration API 09/01/23 10:37:44.907
    STEP: Having no error when timeout is longer than webhook latency 09/01/23 10:37:45.943
    STEP: Registering slow webhook via the AdmissionRegistration API 09/01/23 10:37:45.943
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/01/23 10:37:50.991
    STEP: Registering slow webhook via the AdmissionRegistration API 09/01/23 10:37:50.991
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:37:56.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6758" for this suite. 09/01/23 10:37:56.194
    STEP: Destroying namespace "webhook-6758-markers" for this suite. 09/01/23 10:37:56.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:37:56.281
Sep  1 10:37:56.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 10:37:56.283
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:37:56.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:37:56.324
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 09/01/23 10:37:56.328
Sep  1 10:37:56.345: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe" in namespace "projected-5116" to be "Succeeded or Failed"
Sep  1 10:37:56.351: INFO: Pod "downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.493205ms
Sep  1 10:37:58.355: INFO: Pod "downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010593281s
Sep  1 10:38:00.355: INFO: Pod "downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010003697s
STEP: Saw pod success 09/01/23 10:38:00.355
Sep  1 10:38:00.355: INFO: Pod "downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe" satisfied condition "Succeeded or Failed"
Sep  1 10:38:00.358: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe container client-container: <nil>
STEP: delete the pod 09/01/23 10:38:00.364
Sep  1 10:38:00.380: INFO: Waiting for pod downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe to disappear
Sep  1 10:38:00.383: INFO: Pod downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  1 10:38:00.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5116" for this suite. 09/01/23 10:38:00.388
------------------------------
• [4.113 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:37:56.281
    Sep  1 10:37:56.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 10:37:56.283
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:37:56.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:37:56.324
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 09/01/23 10:37:56.328
    Sep  1 10:37:56.345: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe" in namespace "projected-5116" to be "Succeeded or Failed"
    Sep  1 10:37:56.351: INFO: Pod "downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.493205ms
    Sep  1 10:37:58.355: INFO: Pod "downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010593281s
    Sep  1 10:38:00.355: INFO: Pod "downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010003697s
    STEP: Saw pod success 09/01/23 10:38:00.355
    Sep  1 10:38:00.355: INFO: Pod "downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe" satisfied condition "Succeeded or Failed"
    Sep  1 10:38:00.358: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe container client-container: <nil>
    STEP: delete the pod 09/01/23 10:38:00.364
    Sep  1 10:38:00.380: INFO: Waiting for pod downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe to disappear
    Sep  1 10:38:00.383: INFO: Pod downwardapi-volume-ae593dcf-c0ea-4d79-b052-a409b2e16cbe no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:38:00.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5116" for this suite. 09/01/23 10:38:00.388
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:38:00.398
Sep  1 10:38:00.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 10:38:00.4
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:38:00.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:38:00.424
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 09/01/23 10:38:00.428
Sep  1 10:38:00.439: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387" in namespace "projected-5219" to be "Succeeded or Failed"
Sep  1 10:38:00.450: INFO: Pod "downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387": Phase="Pending", Reason="", readiness=false. Elapsed: 9.901645ms
Sep  1 10:38:02.455: INFO: Pod "downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015067845s
Sep  1 10:38:04.454: INFO: Pod "downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014671387s
STEP: Saw pod success 09/01/23 10:38:04.454
Sep  1 10:38:04.455: INFO: Pod "downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387" satisfied condition "Succeeded or Failed"
Sep  1 10:38:04.458: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387 container client-container: <nil>
STEP: delete the pod 09/01/23 10:38:04.466
Sep  1 10:38:04.483: INFO: Waiting for pod downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387 to disappear
Sep  1 10:38:04.488: INFO: Pod downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  1 10:38:04.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5219" for this suite. 09/01/23 10:38:04.494
------------------------------
• [4.105 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:38:00.398
    Sep  1 10:38:00.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 10:38:00.4
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:38:00.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:38:00.424
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 09/01/23 10:38:00.428
    Sep  1 10:38:00.439: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387" in namespace "projected-5219" to be "Succeeded or Failed"
    Sep  1 10:38:00.450: INFO: Pod "downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387": Phase="Pending", Reason="", readiness=false. Elapsed: 9.901645ms
    Sep  1 10:38:02.455: INFO: Pod "downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015067845s
    Sep  1 10:38:04.454: INFO: Pod "downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014671387s
    STEP: Saw pod success 09/01/23 10:38:04.454
    Sep  1 10:38:04.455: INFO: Pod "downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387" satisfied condition "Succeeded or Failed"
    Sep  1 10:38:04.458: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387 container client-container: <nil>
    STEP: delete the pod 09/01/23 10:38:04.466
    Sep  1 10:38:04.483: INFO: Waiting for pod downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387 to disappear
    Sep  1 10:38:04.488: INFO: Pod downwardapi-volume-a7457548-b60a-4e63-834b-c55ed34e7387 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:38:04.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5219" for this suite. 09/01/23 10:38:04.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:38:04.51
Sep  1 10:38:04.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename dns 09/01/23 10:38:04.511
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:38:04.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:38:04.533
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 09/01/23 10:38:04.536
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3664.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3664.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3664.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3664.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 40.96.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.96.40_udp@PTR;check="$$(dig +tcp +noall +answer +search 40.96.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.96.40_tcp@PTR;sleep 1; done
 09/01/23 10:38:04.564
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3664.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3664.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3664.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3664.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 40.96.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.96.40_udp@PTR;check="$$(dig +tcp +noall +answer +search 40.96.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.96.40_tcp@PTR;sleep 1; done
 09/01/23 10:38:04.564
STEP: creating a pod to probe DNS 09/01/23 10:38:04.565
STEP: submitting the pod to kubernetes 09/01/23 10:38:04.565
Sep  1 10:38:04.585: INFO: Waiting up to 15m0s for pod "dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e" in namespace "dns-3664" to be "running"
Sep  1 10:38:04.595: INFO: Pod "dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.521343ms
Sep  1 10:38:06.600: INFO: Pod "dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013998449s
Sep  1 10:38:06.600: INFO: Pod "dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e" satisfied condition "running"
STEP: retrieving the pod 09/01/23 10:38:06.6
STEP: looking for the results for each expected name from probers 09/01/23 10:38:06.603
Sep  1 10:38:06.609: INFO: Unable to read wheezy_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:06.614: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:06.619: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:06.624: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:06.647: INFO: Unable to read jessie_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:06.650: INFO: Unable to read jessie_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:06.653: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:06.658: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:06.675: INFO: Lookups using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e failed for: [wheezy_udp@dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_udp@dns-test-service.dns-3664.svc.cluster.local jessie_tcp@dns-test-service.dns-3664.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local]

Sep  1 10:38:11.682: INFO: Unable to read wheezy_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:11.687: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:11.701: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:11.708: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:11.734: INFO: Unable to read jessie_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:11.740: INFO: Unable to read jessie_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:11.747: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:11.752: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:11.775: INFO: Lookups using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e failed for: [wheezy_udp@dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_udp@dns-test-service.dns-3664.svc.cluster.local jessie_tcp@dns-test-service.dns-3664.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local]

Sep  1 10:38:16.682: INFO: Unable to read wheezy_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:16.688: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:16.694: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:16.699: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:16.720: INFO: Unable to read jessie_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:16.725: INFO: Unable to read jessie_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:16.729: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:16.735: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:16.754: INFO: Lookups using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e failed for: [wheezy_udp@dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_udp@dns-test-service.dns-3664.svc.cluster.local jessie_tcp@dns-test-service.dns-3664.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local]

Sep  1 10:38:21.681: INFO: Unable to read wheezy_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:21.687: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:21.692: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:21.697: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:21.721: INFO: Unable to read jessie_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:21.727: INFO: Unable to read jessie_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:21.733: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:21.737: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:21.760: INFO: Lookups using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e failed for: [wheezy_udp@dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_udp@dns-test-service.dns-3664.svc.cluster.local jessie_tcp@dns-test-service.dns-3664.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local]

Sep  1 10:38:26.682: INFO: Unable to read wheezy_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:26.688: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:26.698: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:26.703: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:26.724: INFO: Unable to read jessie_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:26.729: INFO: Unable to read jessie_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:26.734: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:26.740: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:26.757: INFO: Lookups using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e failed for: [wheezy_udp@dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_udp@dns-test-service.dns-3664.svc.cluster.local jessie_tcp@dns-test-service.dns-3664.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local]

Sep  1 10:38:31.681: INFO: Unable to read wheezy_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:31.685: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:31.689: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:31.693: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:31.716: INFO: Unable to read jessie_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:31.720: INFO: Unable to read jessie_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:31.726: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:31.731: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:31.750: INFO: Lookups using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e failed for: [wheezy_udp@dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_udp@dns-test-service.dns-3664.svc.cluster.local jessie_tcp@dns-test-service.dns-3664.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local]

Sep  1 10:38:36.683: INFO: Unable to read wheezy_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:36.719: INFO: Unable to read jessie_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:36.724: INFO: Unable to read jessie_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
Sep  1 10:38:36.751: INFO: Lookups using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e failed for: [wheezy_udp@dns-test-service.dns-3664.svc.cluster.local jessie_udp@dns-test-service.dns-3664.svc.cluster.local jessie_tcp@dns-test-service.dns-3664.svc.cluster.local]

Sep  1 10:38:41.769: INFO: DNS probes using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e succeeded

STEP: deleting the pod 09/01/23 10:38:41.77
STEP: deleting the test service 09/01/23 10:38:41.804
STEP: deleting the test headless service 09/01/23 10:38:41.878
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  1 10:38:41.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3664" for this suite. 09/01/23 10:38:41.922
------------------------------
• [SLOW TEST] [37.428 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:38:04.51
    Sep  1 10:38:04.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename dns 09/01/23 10:38:04.511
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:38:04.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:38:04.533
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 09/01/23 10:38:04.536
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3664.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3664.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3664.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3664.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 40.96.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.96.40_udp@PTR;check="$$(dig +tcp +noall +answer +search 40.96.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.96.40_tcp@PTR;sleep 1; done
     09/01/23 10:38:04.564
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3664.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3664.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3664.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3664.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3664.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 40.96.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.96.40_udp@PTR;check="$$(dig +tcp +noall +answer +search 40.96.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.96.40_tcp@PTR;sleep 1; done
     09/01/23 10:38:04.564
    STEP: creating a pod to probe DNS 09/01/23 10:38:04.565
    STEP: submitting the pod to kubernetes 09/01/23 10:38:04.565
    Sep  1 10:38:04.585: INFO: Waiting up to 15m0s for pod "dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e" in namespace "dns-3664" to be "running"
    Sep  1 10:38:04.595: INFO: Pod "dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.521343ms
    Sep  1 10:38:06.600: INFO: Pod "dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013998449s
    Sep  1 10:38:06.600: INFO: Pod "dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e" satisfied condition "running"
    STEP: retrieving the pod 09/01/23 10:38:06.6
    STEP: looking for the results for each expected name from probers 09/01/23 10:38:06.603
    Sep  1 10:38:06.609: INFO: Unable to read wheezy_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:06.614: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:06.619: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:06.624: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:06.647: INFO: Unable to read jessie_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:06.650: INFO: Unable to read jessie_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:06.653: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:06.658: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:06.675: INFO: Lookups using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e failed for: [wheezy_udp@dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_udp@dns-test-service.dns-3664.svc.cluster.local jessie_tcp@dns-test-service.dns-3664.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local]

    Sep  1 10:38:11.682: INFO: Unable to read wheezy_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:11.687: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:11.701: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:11.708: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:11.734: INFO: Unable to read jessie_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:11.740: INFO: Unable to read jessie_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:11.747: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:11.752: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:11.775: INFO: Lookups using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e failed for: [wheezy_udp@dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_udp@dns-test-service.dns-3664.svc.cluster.local jessie_tcp@dns-test-service.dns-3664.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local]

    Sep  1 10:38:16.682: INFO: Unable to read wheezy_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:16.688: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:16.694: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:16.699: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:16.720: INFO: Unable to read jessie_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:16.725: INFO: Unable to read jessie_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:16.729: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:16.735: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:16.754: INFO: Lookups using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e failed for: [wheezy_udp@dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_udp@dns-test-service.dns-3664.svc.cluster.local jessie_tcp@dns-test-service.dns-3664.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local]

    Sep  1 10:38:21.681: INFO: Unable to read wheezy_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:21.687: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:21.692: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:21.697: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:21.721: INFO: Unable to read jessie_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:21.727: INFO: Unable to read jessie_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:21.733: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:21.737: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:21.760: INFO: Lookups using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e failed for: [wheezy_udp@dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_udp@dns-test-service.dns-3664.svc.cluster.local jessie_tcp@dns-test-service.dns-3664.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local]

    Sep  1 10:38:26.682: INFO: Unable to read wheezy_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:26.688: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:26.698: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:26.703: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:26.724: INFO: Unable to read jessie_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:26.729: INFO: Unable to read jessie_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:26.734: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:26.740: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:26.757: INFO: Lookups using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e failed for: [wheezy_udp@dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_udp@dns-test-service.dns-3664.svc.cluster.local jessie_tcp@dns-test-service.dns-3664.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local]

    Sep  1 10:38:31.681: INFO: Unable to read wheezy_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:31.685: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:31.689: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:31.693: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:31.716: INFO: Unable to read jessie_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:31.720: INFO: Unable to read jessie_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:31.726: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:31.731: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:31.750: INFO: Lookups using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e failed for: [wheezy_udp@dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@dns-test-service.dns-3664.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_udp@dns-test-service.dns-3664.svc.cluster.local jessie_tcp@dns-test-service.dns-3664.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3664.svc.cluster.local]

    Sep  1 10:38:36.683: INFO: Unable to read wheezy_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:36.719: INFO: Unable to read jessie_udp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:36.724: INFO: Unable to read jessie_tcp@dns-test-service.dns-3664.svc.cluster.local from pod dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e: the server could not find the requested resource (get pods dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e)
    Sep  1 10:38:36.751: INFO: Lookups using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e failed for: [wheezy_udp@dns-test-service.dns-3664.svc.cluster.local jessie_udp@dns-test-service.dns-3664.svc.cluster.local jessie_tcp@dns-test-service.dns-3664.svc.cluster.local]

    Sep  1 10:38:41.769: INFO: DNS probes using dns-3664/dns-test-5ff1480e-56f3-4d9b-8ca2-7069b51e420e succeeded

    STEP: deleting the pod 09/01/23 10:38:41.77
    STEP: deleting the test service 09/01/23 10:38:41.804
    STEP: deleting the test headless service 09/01/23 10:38:41.878
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:38:41.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3664" for this suite. 09/01/23 10:38:41.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:38:41.955
Sep  1 10:38:41.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 10:38:41.958
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:38:41.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:38:41.985
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 09/01/23 10:38:41.99
Sep  1 10:38:41.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2163 create -f -'
Sep  1 10:38:43.223: INFO: stderr: ""
Sep  1 10:38:43.223: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/01/23 10:38:43.223
Sep  1 10:38:44.227: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  1 10:38:44.227: INFO: Found 0 / 1
Sep  1 10:38:45.227: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  1 10:38:45.227: INFO: Found 1 / 1
Sep  1 10:38:45.227: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 09/01/23 10:38:45.228
Sep  1 10:38:45.230: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  1 10:38:45.230: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  1 10:38:45.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2163 patch pod agnhost-primary-8vmlg -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  1 10:38:45.340: INFO: stderr: ""
Sep  1 10:38:45.340: INFO: stdout: "pod/agnhost-primary-8vmlg patched\n"
STEP: checking annotations 09/01/23 10:38:45.34
Sep  1 10:38:45.344: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  1 10:38:45.344: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 10:38:45.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2163" for this suite. 09/01/23 10:38:45.349
------------------------------
• [3.401 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:38:41.955
    Sep  1 10:38:41.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 10:38:41.958
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:38:41.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:38:41.985
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 09/01/23 10:38:41.99
    Sep  1 10:38:41.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2163 create -f -'
    Sep  1 10:38:43.223: INFO: stderr: ""
    Sep  1 10:38:43.223: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/01/23 10:38:43.223
    Sep  1 10:38:44.227: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  1 10:38:44.227: INFO: Found 0 / 1
    Sep  1 10:38:45.227: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  1 10:38:45.227: INFO: Found 1 / 1
    Sep  1 10:38:45.227: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 09/01/23 10:38:45.228
    Sep  1 10:38:45.230: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  1 10:38:45.230: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  1 10:38:45.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2163 patch pod agnhost-primary-8vmlg -p {"metadata":{"annotations":{"x":"y"}}}'
    Sep  1 10:38:45.340: INFO: stderr: ""
    Sep  1 10:38:45.340: INFO: stdout: "pod/agnhost-primary-8vmlg patched\n"
    STEP: checking annotations 09/01/23 10:38:45.34
    Sep  1 10:38:45.344: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  1 10:38:45.344: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:38:45.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2163" for this suite. 09/01/23 10:38:45.349
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:38:45.357
Sep  1 10:38:45.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename lease-test 09/01/23 10:38:45.359
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:38:45.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:38:45.386
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Sep  1 10:38:45.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-7845" for this suite. 09/01/23 10:38:45.465
------------------------------
• [0.118 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:38:45.357
    Sep  1 10:38:45.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename lease-test 09/01/23 10:38:45.359
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:38:45.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:38:45.386
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:38:45.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-7845" for this suite. 09/01/23 10:38:45.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:38:45.477
Sep  1 10:38:45.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 10:38:45.479
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:38:45.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:38:45.502
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 10:38:45.521
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 10:38:46.373
STEP: Deploying the webhook pod 09/01/23 10:38:46.383
STEP: Wait for the deployment to be ready 09/01/23 10:38:46.401
Sep  1 10:38:46.412: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/01/23 10:38:48.423
STEP: Verifying the service has paired with the endpoint 09/01/23 10:38:48.441
Sep  1 10:38:49.442: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/01/23 10:38:49.447
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/01/23 10:38:49.471
STEP: Creating a dummy validating-webhook-configuration object 09/01/23 10:38:49.494
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/01/23 10:38:49.505
STEP: Creating a dummy mutating-webhook-configuration object 09/01/23 10:38:49.516
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/01/23 10:38:49.528
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:38:49.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9941" for this suite. 09/01/23 10:38:49.658
STEP: Destroying namespace "webhook-9941-markers" for this suite. 09/01/23 10:38:49.674
------------------------------
• [4.216 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:38:45.477
    Sep  1 10:38:45.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 10:38:45.479
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:38:45.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:38:45.502
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 10:38:45.521
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 10:38:46.373
    STEP: Deploying the webhook pod 09/01/23 10:38:46.383
    STEP: Wait for the deployment to be ready 09/01/23 10:38:46.401
    Sep  1 10:38:46.412: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/01/23 10:38:48.423
    STEP: Verifying the service has paired with the endpoint 09/01/23 10:38:48.441
    Sep  1 10:38:49.442: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/01/23 10:38:49.447
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/01/23 10:38:49.471
    STEP: Creating a dummy validating-webhook-configuration object 09/01/23 10:38:49.494
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/01/23 10:38:49.505
    STEP: Creating a dummy mutating-webhook-configuration object 09/01/23 10:38:49.516
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/01/23 10:38:49.528
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:38:49.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9941" for this suite. 09/01/23 10:38:49.658
    STEP: Destroying namespace "webhook-9941-markers" for this suite. 09/01/23 10:38:49.674
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:38:49.697
Sep  1 10:38:49.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 10:38:49.699
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:38:49.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:38:49.733
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8261 09/01/23 10:38:49.738
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/01/23 10:38:49.762
STEP: creating service externalsvc in namespace services-8261 09/01/23 10:38:49.762
STEP: creating replication controller externalsvc in namespace services-8261 09/01/23 10:38:49.796
I0901 10:38:49.813968      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8261, replica count: 2
I0901 10:38:52.864887      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 09/01/23 10:38:52.868
Sep  1 10:38:52.888: INFO: Creating new exec pod
Sep  1 10:38:52.900: INFO: Waiting up to 5m0s for pod "execpodxc878" in namespace "services-8261" to be "running"
Sep  1 10:38:52.906: INFO: Pod "execpodxc878": Phase="Pending", Reason="", readiness=false. Elapsed: 5.317342ms
Sep  1 10:38:54.909: INFO: Pod "execpodxc878": Phase="Running", Reason="", readiness=true. Elapsed: 2.009067299s
Sep  1 10:38:54.909: INFO: Pod "execpodxc878" satisfied condition "running"
Sep  1 10:38:54.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-8261 exec execpodxc878 -- /bin/sh -x -c nslookup clusterip-service.services-8261.svc.cluster.local'
Sep  1 10:38:55.170: INFO: stderr: "+ nslookup clusterip-service.services-8261.svc.cluster.local\n"
Sep  1 10:38:55.170: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-8261.svc.cluster.local\tcanonical name = externalsvc.services-8261.svc.cluster.local.\nName:\texternalsvc.services-8261.svc.cluster.local\nAddress: 10.102.119.156\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8261, will wait for the garbage collector to delete the pods 09/01/23 10:38:55.17
Sep  1 10:38:55.248: INFO: Deleting ReplicationController externalsvc took: 20.477889ms
Sep  1 10:38:55.349: INFO: Terminating ReplicationController externalsvc pods took: 100.623336ms
Sep  1 10:38:57.593: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 10:38:57.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8261" for this suite. 09/01/23 10:38:57.626
------------------------------
• [SLOW TEST] [7.938 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:38:49.697
    Sep  1 10:38:49.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 10:38:49.699
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:38:49.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:38:49.733
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8261 09/01/23 10:38:49.738
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/01/23 10:38:49.762
    STEP: creating service externalsvc in namespace services-8261 09/01/23 10:38:49.762
    STEP: creating replication controller externalsvc in namespace services-8261 09/01/23 10:38:49.796
    I0901 10:38:49.813968      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8261, replica count: 2
    I0901 10:38:52.864887      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 09/01/23 10:38:52.868
    Sep  1 10:38:52.888: INFO: Creating new exec pod
    Sep  1 10:38:52.900: INFO: Waiting up to 5m0s for pod "execpodxc878" in namespace "services-8261" to be "running"
    Sep  1 10:38:52.906: INFO: Pod "execpodxc878": Phase="Pending", Reason="", readiness=false. Elapsed: 5.317342ms
    Sep  1 10:38:54.909: INFO: Pod "execpodxc878": Phase="Running", Reason="", readiness=true. Elapsed: 2.009067299s
    Sep  1 10:38:54.909: INFO: Pod "execpodxc878" satisfied condition "running"
    Sep  1 10:38:54.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-8261 exec execpodxc878 -- /bin/sh -x -c nslookup clusterip-service.services-8261.svc.cluster.local'
    Sep  1 10:38:55.170: INFO: stderr: "+ nslookup clusterip-service.services-8261.svc.cluster.local\n"
    Sep  1 10:38:55.170: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-8261.svc.cluster.local\tcanonical name = externalsvc.services-8261.svc.cluster.local.\nName:\texternalsvc.services-8261.svc.cluster.local\nAddress: 10.102.119.156\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8261, will wait for the garbage collector to delete the pods 09/01/23 10:38:55.17
    Sep  1 10:38:55.248: INFO: Deleting ReplicationController externalsvc took: 20.477889ms
    Sep  1 10:38:55.349: INFO: Terminating ReplicationController externalsvc pods took: 100.623336ms
    Sep  1 10:38:57.593: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:38:57.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8261" for this suite. 09/01/23 10:38:57.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:38:57.643
Sep  1 10:38:57.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 10:38:57.644
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:38:57.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:38:57.675
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-5afdb810-6079-4fa5-9cff-711b50c5a558 09/01/23 10:38:57.678
STEP: Creating a pod to test consume secrets 09/01/23 10:38:57.684
Sep  1 10:38:57.702: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896" in namespace "projected-4354" to be "Succeeded or Failed"
Sep  1 10:38:57.707: INFO: Pod "pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896": Phase="Pending", Reason="", readiness=false. Elapsed: 5.103965ms
Sep  1 10:38:59.713: INFO: Pod "pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010450954s
Sep  1 10:39:01.712: INFO: Pod "pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010077642s
STEP: Saw pod success 09/01/23 10:39:01.712
Sep  1 10:39:01.713: INFO: Pod "pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896" satisfied condition "Succeeded or Failed"
Sep  1 10:39:01.716: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/01/23 10:39:01.722
Sep  1 10:39:01.738: INFO: Waiting for pod pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896 to disappear
Sep  1 10:39:01.741: INFO: Pod pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  1 10:39:01.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4354" for this suite. 09/01/23 10:39:01.746
------------------------------
• [4.112 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:38:57.643
    Sep  1 10:38:57.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 10:38:57.644
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:38:57.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:38:57.675
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-5afdb810-6079-4fa5-9cff-711b50c5a558 09/01/23 10:38:57.678
    STEP: Creating a pod to test consume secrets 09/01/23 10:38:57.684
    Sep  1 10:38:57.702: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896" in namespace "projected-4354" to be "Succeeded or Failed"
    Sep  1 10:38:57.707: INFO: Pod "pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896": Phase="Pending", Reason="", readiness=false. Elapsed: 5.103965ms
    Sep  1 10:38:59.713: INFO: Pod "pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010450954s
    Sep  1 10:39:01.712: INFO: Pod "pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010077642s
    STEP: Saw pod success 09/01/23 10:39:01.712
    Sep  1 10:39:01.713: INFO: Pod "pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896" satisfied condition "Succeeded or Failed"
    Sep  1 10:39:01.716: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/01/23 10:39:01.722
    Sep  1 10:39:01.738: INFO: Waiting for pod pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896 to disappear
    Sep  1 10:39:01.741: INFO: Pod pod-projected-secrets-6e7a136e-0031-4c80-ab9f-7470dcafe896 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:39:01.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4354" for this suite. 09/01/23 10:39:01.746
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:39:01.755
Sep  1 10:39:01.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename conformance-tests 09/01/23 10:39:01.758
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:39:01.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:39:01.776
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 09/01/23 10:39:01.782
Sep  1 10:39:01.782: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Sep  1 10:39:01.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-5256" for this suite. 09/01/23 10:39:01.794
------------------------------
• [0.053 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:39:01.755
    Sep  1 10:39:01.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename conformance-tests 09/01/23 10:39:01.758
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:39:01.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:39:01.776
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 09/01/23 10:39:01.782
    Sep  1 10:39:01.782: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:39:01.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-5256" for this suite. 09/01/23 10:39:01.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:39:01.81
Sep  1 10:39:01.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 10:39:01.812
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:39:01.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:39:01.832
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Sep  1 10:39:01.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2014 create -f -'
Sep  1 10:39:02.561: INFO: stderr: ""
Sep  1 10:39:02.561: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep  1 10:39:02.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2014 create -f -'
Sep  1 10:39:03.798: INFO: stderr: ""
Sep  1 10:39:03.798: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/01/23 10:39:03.798
Sep  1 10:39:04.801: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  1 10:39:04.801: INFO: Found 1 / 1
Sep  1 10:39:04.801: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  1 10:39:04.804: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  1 10:39:04.804: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  1 10:39:04.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2014 describe pod agnhost-primary-kbnrl'
Sep  1 10:39:04.921: INFO: stderr: ""
Sep  1 10:39:04.921: INFO: stdout: "Name:             agnhost-primary-kbnrl\nNamespace:        kubectl-2014\nPriority:         0\nService Account:  default\nNode:             k8s-worker-1.c.operations-lab.internal/172.16.0.3\nStart Time:       Fri, 01 Sep 2023 10:39:02 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.10.0.73\nIPs:\n  IP:           10.10.0.73\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://b74330e73b9bbe3d5b9c34026febab5baba23f8117664ba80d862d5b1ecce03f\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 01 Sep 2023 10:39:03 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dpwdz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-dpwdz:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-2014/agnhost-primary-kbnrl to k8s-worker-1.c.operations-lab.internal\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Sep  1 10:39:04.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2014 describe rc agnhost-primary'
Sep  1 10:39:05.050: INFO: stderr: ""
Sep  1 10:39:05.050: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2014\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-kbnrl\n"
Sep  1 10:39:05.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2014 describe service agnhost-primary'
Sep  1 10:39:05.191: INFO: stderr: ""
Sep  1 10:39:05.191: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2014\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.101.220.37\nIPs:               10.101.220.37\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.10.0.73:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  1 10:39:05.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2014 describe node k8s-control-plane.c.operations-lab.internal'
Sep  1 10:39:05.356: INFO: stderr: ""
Sep  1 10:39:05.356: INFO: stdout: "Name:               k8s-control-plane.c.operations-lab.internal\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-control-plane.c.operations-lab.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 01 Sep 2023 10:07:53 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-control-plane.c.operations-lab.internal\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 01 Sep 2023 10:38:59 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 01 Sep 2023 10:21:36 +0000   Fri, 01 Sep 2023 10:21:36 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Fri, 01 Sep 2023 10:34:45 +0000   Fri, 01 Sep 2023 10:07:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 01 Sep 2023 10:34:45 +0000   Fri, 01 Sep 2023 10:07:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 01 Sep 2023 10:34:45 +0000   Fri, 01 Sep 2023 10:07:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 01 Sep 2023 10:34:45 +0000   Fri, 01 Sep 2023 10:21:36 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.16.0.2\n  Hostname:    k8s-control-plane.c.operations-lab.internal\nCapacity:\n  cpu:                2\n  ephemeral-storage:  50620216Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8120776Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  46651590989\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8018376Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 6f418680c39a9867e71db0905d183980\n  System UUID:                6f418680-c39a-9867-e71d-b0905d183980\n  Boot ID:                    9d2995b4-fd8f-46cd-9de6-72484a0cd3fb\n  Kernel Version:             6.2.0-1012-gcp\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.22\n  Kubelet Version:            v1.26.8\n  Kube-Proxy Version:         v1.26.8\nPodCIDR:                      10.10.0.0/24\nPodCIDRs:                     10.10.0.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                   ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-d6p6h                                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\n  kube-system                 etcd-k8s-control-plane.c.operations-lab.internal                       100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         31m\n  kube-system                 kube-apiserver-k8s-control-plane.c.operations-lab.internal             250m (12%)    0 (0%)      0 (0%)           0 (0%)         31m\n  kube-system                 kube-controller-manager-k8s-control-plane.c.operations-lab.internal    200m (10%)    0 (0%)      0 (0%)           0 (0%)         31m\n  kube-system                 kube-proxy-v4hpj                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         30m\n  kube-system                 kube-scheduler-k8s-control-plane.c.operations-lab.internal             100m (5%)     0 (0%)      0 (0%)           0 (0%)         31m\n  monitoring-system           node-exporter-cfmm5                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-ntzx9                0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m52s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                650m (32%)  0 (0%)\n  memory             100Mi (1%)  0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 30m                kube-proxy       \n  Normal   NodeAllocatableEnforced  31m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  31m (x5 over 31m)  kubelet          Node k8s-control-plane.c.operations-lab.internal status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    31m (x5 over 31m)  kubelet          Node k8s-control-plane.c.operations-lab.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     31m (x5 over 31m)  kubelet          Node k8s-control-plane.c.operations-lab.internal status is now: NodeHasSufficientPID\n  Warning  InvalidDiskCapacity      31m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  31m                kubelet          Node k8s-control-plane.c.operations-lab.internal status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    31m                kubelet          Node k8s-control-plane.c.operations-lab.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     31m                kubelet          Node k8s-control-plane.c.operations-lab.internal status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  31m                kubelet          Updated Node Allocatable limit across pods\n  Normal   Starting                 31m                kubelet          Starting kubelet.\n  Normal   RegisteredNode           30m                node-controller  Node k8s-control-plane.c.operations-lab.internal event: Registered Node k8s-control-plane.c.operations-lab.internal in Controller\n  Normal   NodeReady                17m                kubelet          Node k8s-control-plane.c.operations-lab.internal status is now: NodeReady\n"
Sep  1 10:39:05.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2014 describe namespace kubectl-2014'
Sep  1 10:39:05.464: INFO: stderr: ""
Sep  1 10:39:05.464: INFO: stdout: "Name:         kubectl-2014\nLabels:       e2e-framework=kubectl\n              e2e-run=8adb3b46-bd81-48ef-adc4-5eb7453ac860\n              kubernetes.io/metadata.name=kubectl-2014\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 10:39:05.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2014" for this suite. 09/01/23 10:39:05.47
------------------------------
• [3.666 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:39:01.81
    Sep  1 10:39:01.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 10:39:01.812
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:39:01.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:39:01.832
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Sep  1 10:39:01.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2014 create -f -'
    Sep  1 10:39:02.561: INFO: stderr: ""
    Sep  1 10:39:02.561: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Sep  1 10:39:02.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2014 create -f -'
    Sep  1 10:39:03.798: INFO: stderr: ""
    Sep  1 10:39:03.798: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/01/23 10:39:03.798
    Sep  1 10:39:04.801: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  1 10:39:04.801: INFO: Found 1 / 1
    Sep  1 10:39:04.801: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep  1 10:39:04.804: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  1 10:39:04.804: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  1 10:39:04.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2014 describe pod agnhost-primary-kbnrl'
    Sep  1 10:39:04.921: INFO: stderr: ""
    Sep  1 10:39:04.921: INFO: stdout: "Name:             agnhost-primary-kbnrl\nNamespace:        kubectl-2014\nPriority:         0\nService Account:  default\nNode:             k8s-worker-1.c.operations-lab.internal/172.16.0.3\nStart Time:       Fri, 01 Sep 2023 10:39:02 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.10.0.73\nIPs:\n  IP:           10.10.0.73\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://b74330e73b9bbe3d5b9c34026febab5baba23f8117664ba80d862d5b1ecce03f\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 01 Sep 2023 10:39:03 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dpwdz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-dpwdz:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-2014/agnhost-primary-kbnrl to k8s-worker-1.c.operations-lab.internal\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Sep  1 10:39:04.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2014 describe rc agnhost-primary'
    Sep  1 10:39:05.050: INFO: stderr: ""
    Sep  1 10:39:05.050: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2014\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-kbnrl\n"
    Sep  1 10:39:05.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2014 describe service agnhost-primary'
    Sep  1 10:39:05.191: INFO: stderr: ""
    Sep  1 10:39:05.191: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2014\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.101.220.37\nIPs:               10.101.220.37\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.10.0.73:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Sep  1 10:39:05.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2014 describe node k8s-control-plane.c.operations-lab.internal'
    Sep  1 10:39:05.356: INFO: stderr: ""
    Sep  1 10:39:05.356: INFO: stdout: "Name:               k8s-control-plane.c.operations-lab.internal\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-control-plane.c.operations-lab.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 01 Sep 2023 10:07:53 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-control-plane.c.operations-lab.internal\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 01 Sep 2023 10:38:59 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 01 Sep 2023 10:21:36 +0000   Fri, 01 Sep 2023 10:21:36 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Fri, 01 Sep 2023 10:34:45 +0000   Fri, 01 Sep 2023 10:07:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 01 Sep 2023 10:34:45 +0000   Fri, 01 Sep 2023 10:07:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 01 Sep 2023 10:34:45 +0000   Fri, 01 Sep 2023 10:07:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 01 Sep 2023 10:34:45 +0000   Fri, 01 Sep 2023 10:21:36 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.16.0.2\n  Hostname:    k8s-control-plane.c.operations-lab.internal\nCapacity:\n  cpu:                2\n  ephemeral-storage:  50620216Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8120776Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  46651590989\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8018376Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 6f418680c39a9867e71db0905d183980\n  System UUID:                6f418680-c39a-9867-e71d-b0905d183980\n  Boot ID:                    9d2995b4-fd8f-46cd-9de6-72484a0cd3fb\n  Kernel Version:             6.2.0-1012-gcp\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.22\n  Kubelet Version:            v1.26.8\n  Kube-Proxy Version:         v1.26.8\nPodCIDR:                      10.10.0.0/24\nPodCIDRs:                     10.10.0.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                   ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-d6p6h                                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\n  kube-system                 etcd-k8s-control-plane.c.operations-lab.internal                       100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         31m\n  kube-system                 kube-apiserver-k8s-control-plane.c.operations-lab.internal             250m (12%)    0 (0%)      0 (0%)           0 (0%)         31m\n  kube-system                 kube-controller-manager-k8s-control-plane.c.operations-lab.internal    200m (10%)    0 (0%)      0 (0%)           0 (0%)         31m\n  kube-system                 kube-proxy-v4hpj                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         30m\n  kube-system                 kube-scheduler-k8s-control-plane.c.operations-lab.internal             100m (5%)     0 (0%)      0 (0%)           0 (0%)         31m\n  monitoring-system           node-exporter-cfmm5                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-ntzx9                0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m52s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                650m (32%)  0 (0%)\n  memory             100Mi (1%)  0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 30m                kube-proxy       \n  Normal   NodeAllocatableEnforced  31m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  31m (x5 over 31m)  kubelet          Node k8s-control-plane.c.operations-lab.internal status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    31m (x5 over 31m)  kubelet          Node k8s-control-plane.c.operations-lab.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     31m (x5 over 31m)  kubelet          Node k8s-control-plane.c.operations-lab.internal status is now: NodeHasSufficientPID\n  Warning  InvalidDiskCapacity      31m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  31m                kubelet          Node k8s-control-plane.c.operations-lab.internal status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    31m                kubelet          Node k8s-control-plane.c.operations-lab.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     31m                kubelet          Node k8s-control-plane.c.operations-lab.internal status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  31m                kubelet          Updated Node Allocatable limit across pods\n  Normal   Starting                 31m                kubelet          Starting kubelet.\n  Normal   RegisteredNode           30m                node-controller  Node k8s-control-plane.c.operations-lab.internal event: Registered Node k8s-control-plane.c.operations-lab.internal in Controller\n  Normal   NodeReady                17m                kubelet          Node k8s-control-plane.c.operations-lab.internal status is now: NodeReady\n"
    Sep  1 10:39:05.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-2014 describe namespace kubectl-2014'
    Sep  1 10:39:05.464: INFO: stderr: ""
    Sep  1 10:39:05.464: INFO: stdout: "Name:         kubectl-2014\nLabels:       e2e-framework=kubectl\n              e2e-run=8adb3b46-bd81-48ef-adc4-5eb7453ac860\n              kubernetes.io/metadata.name=kubectl-2014\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:39:05.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2014" for this suite. 09/01/23 10:39:05.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:39:05.478
Sep  1 10:39:05.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 10:39:05.48
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:39:05.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:39:05.506
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 09/01/23 10:39:05.509
Sep  1 10:39:05.516: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c" in namespace "projected-8831" to be "Succeeded or Failed"
Sep  1 10:39:05.519: INFO: Pod "downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.04011ms
Sep  1 10:39:07.525: INFO: Pod "downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008491445s
Sep  1 10:39:09.524: INFO: Pod "downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007281636s
STEP: Saw pod success 09/01/23 10:39:09.524
Sep  1 10:39:09.524: INFO: Pod "downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c" satisfied condition "Succeeded or Failed"
Sep  1 10:39:09.528: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c container client-container: <nil>
STEP: delete the pod 09/01/23 10:39:09.534
Sep  1 10:39:09.553: INFO: Waiting for pod downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c to disappear
Sep  1 10:39:09.556: INFO: Pod downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  1 10:39:09.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8831" for this suite. 09/01/23 10:39:09.56
------------------------------
• [4.091 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:39:05.478
    Sep  1 10:39:05.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 10:39:05.48
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:39:05.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:39:05.506
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 09/01/23 10:39:05.509
    Sep  1 10:39:05.516: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c" in namespace "projected-8831" to be "Succeeded or Failed"
    Sep  1 10:39:05.519: INFO: Pod "downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.04011ms
    Sep  1 10:39:07.525: INFO: Pod "downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008491445s
    Sep  1 10:39:09.524: INFO: Pod "downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007281636s
    STEP: Saw pod success 09/01/23 10:39:09.524
    Sep  1 10:39:09.524: INFO: Pod "downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c" satisfied condition "Succeeded or Failed"
    Sep  1 10:39:09.528: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c container client-container: <nil>
    STEP: delete the pod 09/01/23 10:39:09.534
    Sep  1 10:39:09.553: INFO: Waiting for pod downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c to disappear
    Sep  1 10:39:09.556: INFO: Pod downwardapi-volume-7c0db12e-3545-472e-ae0d-925dd0966f2c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:39:09.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8831" for this suite. 09/01/23 10:39:09.56
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:39:09.572
Sep  1 10:39:09.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename sched-preemption 09/01/23 10:39:09.574
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:39:09.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:39:09.599
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  1 10:39:09.625: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  1 10:40:09.672: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 09/01/23 10:40:09.676
Sep  1 10:40:09.718: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep  1 10:40:09.732: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep  1 10:40:09.772: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep  1 10:40:09.781: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/01/23 10:40:09.781
Sep  1 10:40:09.782: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5328" to be "running"
Sep  1 10:40:09.787: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.749919ms
Sep  1 10:40:11.793: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.011425705s
Sep  1 10:40:11.793: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep  1 10:40:11.793: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5328" to be "running"
Sep  1 10:40:11.797: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.641417ms
Sep  1 10:40:11.797: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  1 10:40:11.797: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5328" to be "running"
Sep  1 10:40:11.800: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.767416ms
Sep  1 10:40:11.800: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  1 10:40:11.800: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5328" to be "running"
Sep  1 10:40:11.803: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.208457ms
Sep  1 10:40:11.804: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/01/23 10:40:11.804
Sep  1 10:40:11.809: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5328" to be "running"
Sep  1 10:40:11.815: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.451748ms
Sep  1 10:40:13.819: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009287077s
Sep  1 10:40:15.820: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010079179s
Sep  1 10:40:17.820: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.01020915s
Sep  1 10:40:17.820: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:40:17.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5328" for this suite. 09/01/23 10:40:17.876
------------------------------
• [SLOW TEST] [68.310 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:39:09.572
    Sep  1 10:39:09.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename sched-preemption 09/01/23 10:39:09.574
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:39:09.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:39:09.599
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  1 10:39:09.625: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  1 10:40:09.672: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 09/01/23 10:40:09.676
    Sep  1 10:40:09.718: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep  1 10:40:09.732: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep  1 10:40:09.772: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep  1 10:40:09.781: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/01/23 10:40:09.781
    Sep  1 10:40:09.782: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5328" to be "running"
    Sep  1 10:40:09.787: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.749919ms
    Sep  1 10:40:11.793: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.011425705s
    Sep  1 10:40:11.793: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep  1 10:40:11.793: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5328" to be "running"
    Sep  1 10:40:11.797: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.641417ms
    Sep  1 10:40:11.797: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  1 10:40:11.797: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5328" to be "running"
    Sep  1 10:40:11.800: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.767416ms
    Sep  1 10:40:11.800: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  1 10:40:11.800: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5328" to be "running"
    Sep  1 10:40:11.803: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.208457ms
    Sep  1 10:40:11.804: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/01/23 10:40:11.804
    Sep  1 10:40:11.809: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5328" to be "running"
    Sep  1 10:40:11.815: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.451748ms
    Sep  1 10:40:13.819: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009287077s
    Sep  1 10:40:15.820: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010079179s
    Sep  1 10:40:17.820: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.01020915s
    Sep  1 10:40:17.820: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:40:17.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5328" for this suite. 09/01/23 10:40:17.876
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:40:17.887
Sep  1 10:40:17.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pods 09/01/23 10:40:17.889
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:17.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:17.913
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Sep  1 10:40:17.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: creating the pod 09/01/23 10:40:17.918
STEP: submitting the pod to kubernetes 09/01/23 10:40:17.918
Sep  1 10:40:17.927: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30" in namespace "pods-2599" to be "running and ready"
Sep  1 10:40:17.931: INFO: Pod "pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30": Phase="Pending", Reason="", readiness=false. Elapsed: 3.479041ms
Sep  1 10:40:17.931: INFO: The phase of Pod pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:40:19.935: INFO: Pod "pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30": Phase="Running", Reason="", readiness=true. Elapsed: 2.007934369s
Sep  1 10:40:19.935: INFO: The phase of Pod pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30 is Running (Ready = true)
Sep  1 10:40:19.935: INFO: Pod "pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  1 10:40:20.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2599" for this suite. 09/01/23 10:40:20.125
------------------------------
• [2.245 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:40:17.887
    Sep  1 10:40:17.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pods 09/01/23 10:40:17.889
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:17.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:17.913
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Sep  1 10:40:17.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: creating the pod 09/01/23 10:40:17.918
    STEP: submitting the pod to kubernetes 09/01/23 10:40:17.918
    Sep  1 10:40:17.927: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30" in namespace "pods-2599" to be "running and ready"
    Sep  1 10:40:17.931: INFO: Pod "pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30": Phase="Pending", Reason="", readiness=false. Elapsed: 3.479041ms
    Sep  1 10:40:17.931: INFO: The phase of Pod pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:40:19.935: INFO: Pod "pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30": Phase="Running", Reason="", readiness=true. Elapsed: 2.007934369s
    Sep  1 10:40:19.935: INFO: The phase of Pod pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30 is Running (Ready = true)
    Sep  1 10:40:19.935: INFO: Pod "pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:40:20.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2599" for this suite. 09/01/23 10:40:20.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:40:20.135
Sep  1 10:40:20.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pods 09/01/23 10:40:20.137
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:20.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:20.158
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 09/01/23 10:40:20.162
STEP: submitting the pod to kubernetes 09/01/23 10:40:20.162
STEP: verifying QOS class is set on the pod 09/01/23 10:40:20.174
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Sep  1 10:40:20.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1263" for this suite. 09/01/23 10:40:20.188
------------------------------
• [0.062 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:40:20.135
    Sep  1 10:40:20.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pods 09/01/23 10:40:20.137
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:20.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:20.158
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 09/01/23 10:40:20.162
    STEP: submitting the pod to kubernetes 09/01/23 10:40:20.162
    STEP: verifying QOS class is set on the pod 09/01/23 10:40:20.174
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:40:20.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1263" for this suite. 09/01/23 10:40:20.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:40:20.202
Sep  1 10:40:20.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 10:40:20.203
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:20.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:20.226
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-7422 09/01/23 10:40:20.23
STEP: creating service affinity-clusterip in namespace services-7422 09/01/23 10:40:20.23
STEP: creating replication controller affinity-clusterip in namespace services-7422 09/01/23 10:40:20.246
I0901 10:40:20.255262      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7422, replica count: 3
I0901 10:40:23.306510      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  1 10:40:23.321: INFO: Creating new exec pod
Sep  1 10:40:23.328: INFO: Waiting up to 5m0s for pod "execpod-affinitylfgfp" in namespace "services-7422" to be "running"
Sep  1 10:40:23.334: INFO: Pod "execpod-affinitylfgfp": Phase="Pending", Reason="", readiness=false. Elapsed: 5.454616ms
Sep  1 10:40:25.338: INFO: Pod "execpod-affinitylfgfp": Phase="Running", Reason="", readiness=true. Elapsed: 2.009739605s
Sep  1 10:40:25.338: INFO: Pod "execpod-affinitylfgfp" satisfied condition "running"
Sep  1 10:40:26.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-7422 exec execpod-affinitylfgfp -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Sep  1 10:40:26.554: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep  1 10:40:26.554: INFO: stdout: ""
Sep  1 10:40:26.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-7422 exec execpod-affinitylfgfp -- /bin/sh -x -c nc -v -z -w 2 10.98.254.254 80'
Sep  1 10:40:26.769: INFO: stderr: "+ nc -v -z -w 2 10.98.254.254 80\nConnection to 10.98.254.254 80 port [tcp/http] succeeded!\n"
Sep  1 10:40:26.769: INFO: stdout: ""
Sep  1 10:40:26.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-7422 exec execpod-affinitylfgfp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.254.254:80/ ; done'
Sep  1 10:40:27.081: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n"
Sep  1 10:40:27.082: INFO: stdout: "\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l"
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
Sep  1 10:40:27.082: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7422, will wait for the garbage collector to delete the pods 09/01/23 10:40:27.1
Sep  1 10:40:27.162: INFO: Deleting ReplicationController affinity-clusterip took: 5.768253ms
Sep  1 10:40:27.262: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.414139ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 10:40:29.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7422" for this suite. 09/01/23 10:40:29.83
------------------------------
• [SLOW TEST] [9.634 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:40:20.202
    Sep  1 10:40:20.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 10:40:20.203
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:20.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:20.226
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-7422 09/01/23 10:40:20.23
    STEP: creating service affinity-clusterip in namespace services-7422 09/01/23 10:40:20.23
    STEP: creating replication controller affinity-clusterip in namespace services-7422 09/01/23 10:40:20.246
    I0901 10:40:20.255262      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7422, replica count: 3
    I0901 10:40:23.306510      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  1 10:40:23.321: INFO: Creating new exec pod
    Sep  1 10:40:23.328: INFO: Waiting up to 5m0s for pod "execpod-affinitylfgfp" in namespace "services-7422" to be "running"
    Sep  1 10:40:23.334: INFO: Pod "execpod-affinitylfgfp": Phase="Pending", Reason="", readiness=false. Elapsed: 5.454616ms
    Sep  1 10:40:25.338: INFO: Pod "execpod-affinitylfgfp": Phase="Running", Reason="", readiness=true. Elapsed: 2.009739605s
    Sep  1 10:40:25.338: INFO: Pod "execpod-affinitylfgfp" satisfied condition "running"
    Sep  1 10:40:26.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-7422 exec execpod-affinitylfgfp -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Sep  1 10:40:26.554: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Sep  1 10:40:26.554: INFO: stdout: ""
    Sep  1 10:40:26.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-7422 exec execpod-affinitylfgfp -- /bin/sh -x -c nc -v -z -w 2 10.98.254.254 80'
    Sep  1 10:40:26.769: INFO: stderr: "+ nc -v -z -w 2 10.98.254.254 80\nConnection to 10.98.254.254 80 port [tcp/http] succeeded!\n"
    Sep  1 10:40:26.769: INFO: stdout: ""
    Sep  1 10:40:26.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-7422 exec execpod-affinitylfgfp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.254.254:80/ ; done'
    Sep  1 10:40:27.081: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.254.254:80/\n"
    Sep  1 10:40:27.082: INFO: stdout: "\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l\naffinity-clusterip-kwp7l"
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Received response from host: affinity-clusterip-kwp7l
    Sep  1 10:40:27.082: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-7422, will wait for the garbage collector to delete the pods 09/01/23 10:40:27.1
    Sep  1 10:40:27.162: INFO: Deleting ReplicationController affinity-clusterip took: 5.768253ms
    Sep  1 10:40:27.262: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.414139ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:40:29.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7422" for this suite. 09/01/23 10:40:29.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:40:29.838
Sep  1 10:40:29.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 10:40:29.841
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:29.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:29.865
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/01/23 10:40:29.869
Sep  1 10:40:29.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8486 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep  1 10:40:29.966: INFO: stderr: ""
Sep  1 10:40:29.966: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 09/01/23 10:40:29.966
Sep  1 10:40:29.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8486 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Sep  1 10:40:30.858: INFO: stderr: ""
Sep  1 10:40:30.858: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/01/23 10:40:30.859
Sep  1 10:40:30.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8486 delete pods e2e-test-httpd-pod'
Sep  1 10:40:32.673: INFO: stderr: ""
Sep  1 10:40:32.673: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 10:40:32.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8486" for this suite. 09/01/23 10:40:32.678
------------------------------
• [2.846 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:40:29.838
    Sep  1 10:40:29.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 10:40:29.841
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:29.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:29.865
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/01/23 10:40:29.869
    Sep  1 10:40:29.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8486 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep  1 10:40:29.966: INFO: stderr: ""
    Sep  1 10:40:29.966: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 09/01/23 10:40:29.966
    Sep  1 10:40:29.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8486 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Sep  1 10:40:30.858: INFO: stderr: ""
    Sep  1 10:40:30.858: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/01/23 10:40:30.859
    Sep  1 10:40:30.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8486 delete pods e2e-test-httpd-pod'
    Sep  1 10:40:32.673: INFO: stderr: ""
    Sep  1 10:40:32.673: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:40:32.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8486" for this suite. 09/01/23 10:40:32.678
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:40:32.684
Sep  1 10:40:32.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename sched-pred 09/01/23 10:40:32.688
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:32.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:32.725
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  1 10:40:32.728: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  1 10:40:32.738: INFO: Waiting for terminating namespaces to be deleted...
Sep  1 10:40:32.743: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
Sep  1 10:40:32.757: INFO: cilium-operator-858666d4b6-t7brd from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.757: INFO: 	Container cilium-operator ready: true, restart count 0
Sep  1 10:40:32.757: INFO: cilium-q8wf6 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.757: INFO: 	Container cilium-agent ready: true, restart count 0
Sep  1 10:40:32.757: INFO: kube-proxy-wdplh from kube-system started at 2023-09-01 10:08:39 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.757: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  1 10:40:32.757: INFO: kyverno-cleanup-reports-28226080-nvx5z from kyverno-system started at 2023-09-01 10:40:00 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.757: INFO: 	Container cleanup ready: false, restart count 0
Sep  1 10:40:32.757: INFO: fluentbit-fluentbit-zzgmw from logging-system started at 2023-09-01 10:23:38 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.757: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  1 10:40:32.757: INFO: logging-fluentd-0 from logging-system started at 2023-09-01 10:23:50 +0000 UTC (2 container statuses recorded)
Sep  1 10:40:32.757: INFO: 	Container config-reloader ready: true, restart count 0
Sep  1 10:40:32.757: INFO: 	Container fluentd ready: true, restart count 1
Sep  1 10:40:32.757: INFO: logging-fluentd-configcheck-de68d16c from logging-system started at 2023-09-01 10:23:13 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.758: INFO: 	Container fluentd ready: false, restart count 0
Sep  1 10:40:32.758: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-09-01 10:23:46 +0000 UTC (2 container statuses recorded)
Sep  1 10:40:32.758: INFO: 	Container alertmanager ready: true, restart count 0
Sep  1 10:40:32.758: INFO: 	Container config-reloader ready: true, restart count 0
Sep  1 10:40:32.758: INFO: node-exporter-zgtk4 from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.758: INFO: 	Container node-exporter ready: true, restart count 0
Sep  1 10:40:32.758: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-09-01 10:23:46 +0000 UTC (2 container statuses recorded)
Sep  1 10:40:32.758: INFO: 	Container config-reloader ready: true, restart count 0
Sep  1 10:40:32.758: INFO: 	Container prometheus ready: true, restart count 0
Sep  1 10:40:32.758: INFO: pod-qos-class-f424b139-61a6-41ad-9a44-d430d64dd81a from pods-1263 started at 2023-09-01 10:40:20 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.758: INFO: 	Container agnhost ready: false, restart count 0
Sep  1 10:40:32.758: INFO: pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30 from pods-2599 started at 2023-09-01 10:40:17 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.758: INFO: 	Container main ready: true, restart count 0
Sep  1 10:40:32.758: INFO: sonobuoy from sonobuoy started at 2023-09-01 10:29:08 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.758: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  1 10:40:32.758: INFO: sonobuoy-e2e-job-2bf224d5c7cf4759 from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
Sep  1 10:40:32.758: INFO: 	Container e2e ready: true, restart count 0
Sep  1 10:40:32.758: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  1 10:40:32.758: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-v249x from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
Sep  1 10:40:32.758: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  1 10:40:32.758: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  1 10:40:32.758: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
Sep  1 10:40:32.779: INFO: cert-manager-66f9685c7f-jlr6s from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container cert-manager ready: true, restart count 0
Sep  1 10:40:32.779: INFO: cert-manager-cainjector-6cfc589789-78hkg from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container cainjector ready: true, restart count 0
Sep  1 10:40:32.779: INFO: cert-manager-webhook-59f6664d4d-cfggr from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container webhook ready: true, restart count 0
Sep  1 10:40:32.779: INFO: minio-6c8d455566-dvvwz from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container minio ready: true, restart count 0
Sep  1 10:40:32.779: INFO: velero-57c7d7c6c4-tlzqn from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container velero ready: true, restart count 0
Sep  1 10:40:32.779: INFO: traefik-7cf5b5b95f-97lpm from ingress-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container traefik ready: true, restart count 0
Sep  1 10:40:32.779: INFO: kube-green-546cd595c4-mjs5l from kube-green-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container kube-green ready: true, restart count 0
Sep  1 10:40:32.779: INFO: cilium-64mb8 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container cilium-agent ready: true, restart count 0
Sep  1 10:40:32.779: INFO: cilium-operator-858666d4b6-mqxc6 from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container cilium-operator ready: true, restart count 0
Sep  1 10:40:32.779: INFO: coredns-787d4945fb-hw6lt from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container coredns ready: true, restart count 0
Sep  1 10:40:32.779: INFO: coredns-787d4945fb-zc797 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container coredns ready: true, restart count 0
Sep  1 10:40:32.779: INFO: hubble-relay-7956d48fc8-dcjfd from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container hubble-relay ready: true, restart count 0
Sep  1 10:40:32.779: INFO: hubble-ui-b86cb9bf7-7bzl2 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (2 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container backend ready: true, restart count 0
Sep  1 10:40:32.779: INFO: 	Container frontend ready: true, restart count 0
Sep  1 10:40:32.779: INFO: kube-proxy-vfc2w from kube-system started at 2023-09-01 10:08:32 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  1 10:40:32.779: INFO: kyverno-5bbbd994c9-kdf4j from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container kyverno ready: true, restart count 0
Sep  1 10:40:32.779: INFO: kyverno-background-54b49bbb5d-wbc6r from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container kyverno-background ready: true, restart count 0
Sep  1 10:40:32.779: INFO: kyverno-cleanup-f5b6cdd5b-jgfw6 from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container kyverno-cleanup ready: true, restart count 0
Sep  1 10:40:32.779: INFO: kyverno-reports-b48cb544f-2sb6h from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container kyverno-reports ready: true, restart count 0
Sep  1 10:40:32.779: INFO: local-path-provisioner-5d8858f87d-gtwb4 from local-path-storage started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container local-path-provisioner ready: true, restart count 0
Sep  1 10:40:32.779: INFO: fluentbit-fluentbit-7986p from logging-system started at 2023-09-01 10:23:38 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  1 10:40:32.779: INFO: logging-operator-5df74f78f5-zzgl8 from logging-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container logging-operator ready: true, restart count 0
Sep  1 10:40:32.779: INFO: kube-state-metrics-8447695667-frzrq from monitoring-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  1 10:40:32.779: INFO: node-exporter-vwctl from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container node-exporter ready: true, restart count 0
Sep  1 10:40:32.779: INFO: prometheus-operator-75f79b8c5d-pjwn9 from monitoring-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  1 10:40:32.779: INFO: rbac-manager-58f6dc584b-xgdd9 from rbac-manager-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container rbac-manager ready: true, restart count 0
Sep  1 10:40:32.779: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-mqksc from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
Sep  1 10:40:32.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  1 10:40:32.779: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node k8s-worker-1.c.operations-lab.internal 09/01/23 10:40:32.818
STEP: verifying the node has the label node k8s-worker-2.c.operations-lab.internal 09/01/23 10:40:32.837
Sep  1 10:40:32.864: INFO: Pod cert-manager-66f9685c7f-jlr6s requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.864: INFO: Pod cert-manager-cainjector-6cfc589789-78hkg requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.865: INFO: Pod cert-manager-webhook-59f6664d4d-cfggr requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.865: INFO: Pod minio-6c8d455566-dvvwz requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.865: INFO: Pod velero-57c7d7c6c4-tlzqn requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.865: INFO: Pod traefik-7cf5b5b95f-97lpm requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.865: INFO: Pod kube-green-546cd595c4-mjs5l requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.865: INFO: Pod cilium-64mb8 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.865: INFO: Pod cilium-operator-858666d4b6-mqxc6 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.866: INFO: Pod cilium-operator-858666d4b6-t7brd requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Sep  1 10:40:32.866: INFO: Pod cilium-q8wf6 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Sep  1 10:40:32.866: INFO: Pod coredns-787d4945fb-hw6lt requesting resource cpu=100m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.866: INFO: Pod coredns-787d4945fb-zc797 requesting resource cpu=100m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.866: INFO: Pod hubble-relay-7956d48fc8-dcjfd requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.866: INFO: Pod hubble-ui-b86cb9bf7-7bzl2 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.866: INFO: Pod kube-proxy-vfc2w requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.867: INFO: Pod kube-proxy-wdplh requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Sep  1 10:40:32.867: INFO: Pod kyverno-5bbbd994c9-kdf4j requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.867: INFO: Pod kyverno-background-54b49bbb5d-wbc6r requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.867: INFO: Pod kyverno-cleanup-f5b6cdd5b-jgfw6 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.867: INFO: Pod kyverno-reports-b48cb544f-2sb6h requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.868: INFO: Pod local-path-provisioner-5d8858f87d-gtwb4 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.868: INFO: Pod fluentbit-fluentbit-7986p requesting resource cpu=100m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.868: INFO: Pod fluentbit-fluentbit-zzgmw requesting resource cpu=100m on Node k8s-worker-1.c.operations-lab.internal
Sep  1 10:40:32.868: INFO: Pod logging-fluentd-0 requesting resource cpu=500m on Node k8s-worker-1.c.operations-lab.internal
Sep  1 10:40:32.869: INFO: Pod logging-operator-5df74f78f5-zzgl8 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.869: INFO: Pod alertmanager-alertmanager-0 requesting resource cpu=10m on Node k8s-worker-1.c.operations-lab.internal
Sep  1 10:40:32.869: INFO: Pod kube-state-metrics-8447695667-frzrq requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.869: INFO: Pod node-exporter-vwctl requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.869: INFO: Pod node-exporter-zgtk4 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Sep  1 10:40:32.870: INFO: Pod prometheus-operator-75f79b8c5d-pjwn9 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.870: INFO: Pod prometheus-prometheus-0 requesting resource cpu=10m on Node k8s-worker-1.c.operations-lab.internal
Sep  1 10:40:32.870: INFO: Pod pod-qos-class-f424b139-61a6-41ad-9a44-d430d64dd81a requesting resource cpu=100m on Node k8s-worker-1.c.operations-lab.internal
Sep  1 10:40:32.870: INFO: Pod pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Sep  1 10:40:32.870: INFO: Pod rbac-manager-58f6dc584b-xgdd9 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.871: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Sep  1 10:40:32.871: INFO: Pod sonobuoy-e2e-job-2bf224d5c7cf4759 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Sep  1 10:40:32.871: INFO: Pod sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-mqksc requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.871: INFO: Pod sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-v249x requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
STEP: Starting Pods to consume most of the cluster CPU. 09/01/23 10:40:32.872
Sep  1 10:40:32.872: INFO: Creating a pod which consumes cpu=896m on Node k8s-worker-1.c.operations-lab.internal
Sep  1 10:40:32.885: INFO: Creating a pod which consumes cpu=1190m on Node k8s-worker-2.c.operations-lab.internal
Sep  1 10:40:32.893: INFO: Waiting up to 5m0s for pod "filler-pod-f036da7f-ce77-4303-ae69-45a35863f704" in namespace "sched-pred-3828" to be "running"
Sep  1 10:40:32.906: INFO: Pod "filler-pod-f036da7f-ce77-4303-ae69-45a35863f704": Phase="Pending", Reason="", readiness=false. Elapsed: 13.183596ms
Sep  1 10:40:34.909: INFO: Pod "filler-pod-f036da7f-ce77-4303-ae69-45a35863f704": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016339338s
Sep  1 10:40:36.910: INFO: Pod "filler-pod-f036da7f-ce77-4303-ae69-45a35863f704": Phase="Running", Reason="", readiness=true. Elapsed: 4.016987557s
Sep  1 10:40:36.910: INFO: Pod "filler-pod-f036da7f-ce77-4303-ae69-45a35863f704" satisfied condition "running"
Sep  1 10:40:36.910: INFO: Waiting up to 5m0s for pod "filler-pod-535f0b7b-548b-4b14-8916-356b90253327" in namespace "sched-pred-3828" to be "running"
Sep  1 10:40:36.914: INFO: Pod "filler-pod-535f0b7b-548b-4b14-8916-356b90253327": Phase="Running", Reason="", readiness=true. Elapsed: 3.662968ms
Sep  1 10:40:36.914: INFO: Pod "filler-pod-535f0b7b-548b-4b14-8916-356b90253327" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 09/01/23 10:40:36.914
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-535f0b7b-548b-4b14-8916-356b90253327.1780c03e1d87cf90], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3828/filler-pod-535f0b7b-548b-4b14-8916-356b90253327 to k8s-worker-2.c.operations-lab.internal] 09/01/23 10:40:36.919
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-535f0b7b-548b-4b14-8916-356b90253327.1780c03e50095813], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/01/23 10:40:36.919
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-535f0b7b-548b-4b14-8916-356b90253327.1780c03e51d7ad65], Reason = [Created], Message = [Created container filler-pod-535f0b7b-548b-4b14-8916-356b90253327] 09/01/23 10:40:36.919
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-535f0b7b-548b-4b14-8916-356b90253327.1780c03e59d255f7], Reason = [Started], Message = [Started container filler-pod-535f0b7b-548b-4b14-8916-356b90253327] 09/01/23 10:40:36.92
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f036da7f-ce77-4303-ae69-45a35863f704.1780c03e1cadc282], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3828/filler-pod-f036da7f-ce77-4303-ae69-45a35863f704 to k8s-worker-1.c.operations-lab.internal] 09/01/23 10:40:36.92
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f036da7f-ce77-4303-ae69-45a35863f704.1780c03e7efff234], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/01/23 10:40:36.92
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f036da7f-ce77-4303-ae69-45a35863f704.1780c03e80765689], Reason = [Created], Message = [Created container filler-pod-f036da7f-ce77-4303-ae69-45a35863f704] 09/01/23 10:40:36.921
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f036da7f-ce77-4303-ae69-45a35863f704.1780c03e88a977ba], Reason = [Started], Message = [Started container filler-pod-f036da7f-ce77-4303-ae69-45a35863f704] 09/01/23 10:40:36.921
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1780c03f0d3ae985], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] 09/01/23 10:40:36.935
STEP: removing the label node off the node k8s-worker-1.c.operations-lab.internal 09/01/23 10:40:37.934
STEP: verifying the node doesn't have the label node 09/01/23 10:40:37.954
STEP: removing the label node off the node k8s-worker-2.c.operations-lab.internal 09/01/23 10:40:37.96
STEP: verifying the node doesn't have the label node 09/01/23 10:40:37.985
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:40:37.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3828" for this suite. 09/01/23 10:40:38.004
------------------------------
• [SLOW TEST] [5.333 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:40:32.684
    Sep  1 10:40:32.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename sched-pred 09/01/23 10:40:32.688
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:32.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:32.725
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  1 10:40:32.728: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  1 10:40:32.738: INFO: Waiting for terminating namespaces to be deleted...
    Sep  1 10:40:32.743: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
    Sep  1 10:40:32.757: INFO: cilium-operator-858666d4b6-t7brd from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.757: INFO: 	Container cilium-operator ready: true, restart count 0
    Sep  1 10:40:32.757: INFO: cilium-q8wf6 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.757: INFO: 	Container cilium-agent ready: true, restart count 0
    Sep  1 10:40:32.757: INFO: kube-proxy-wdplh from kube-system started at 2023-09-01 10:08:39 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.757: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  1 10:40:32.757: INFO: kyverno-cleanup-reports-28226080-nvx5z from kyverno-system started at 2023-09-01 10:40:00 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.757: INFO: 	Container cleanup ready: false, restart count 0
    Sep  1 10:40:32.757: INFO: fluentbit-fluentbit-zzgmw from logging-system started at 2023-09-01 10:23:38 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.757: INFO: 	Container fluent-bit ready: true, restart count 0
    Sep  1 10:40:32.757: INFO: logging-fluentd-0 from logging-system started at 2023-09-01 10:23:50 +0000 UTC (2 container statuses recorded)
    Sep  1 10:40:32.757: INFO: 	Container config-reloader ready: true, restart count 0
    Sep  1 10:40:32.757: INFO: 	Container fluentd ready: true, restart count 1
    Sep  1 10:40:32.757: INFO: logging-fluentd-configcheck-de68d16c from logging-system started at 2023-09-01 10:23:13 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.758: INFO: 	Container fluentd ready: false, restart count 0
    Sep  1 10:40:32.758: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-09-01 10:23:46 +0000 UTC (2 container statuses recorded)
    Sep  1 10:40:32.758: INFO: 	Container alertmanager ready: true, restart count 0
    Sep  1 10:40:32.758: INFO: 	Container config-reloader ready: true, restart count 0
    Sep  1 10:40:32.758: INFO: node-exporter-zgtk4 from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.758: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  1 10:40:32.758: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-09-01 10:23:46 +0000 UTC (2 container statuses recorded)
    Sep  1 10:40:32.758: INFO: 	Container config-reloader ready: true, restart count 0
    Sep  1 10:40:32.758: INFO: 	Container prometheus ready: true, restart count 0
    Sep  1 10:40:32.758: INFO: pod-qos-class-f424b139-61a6-41ad-9a44-d430d64dd81a from pods-1263 started at 2023-09-01 10:40:20 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.758: INFO: 	Container agnhost ready: false, restart count 0
    Sep  1 10:40:32.758: INFO: pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30 from pods-2599 started at 2023-09-01 10:40:17 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.758: INFO: 	Container main ready: true, restart count 0
    Sep  1 10:40:32.758: INFO: sonobuoy from sonobuoy started at 2023-09-01 10:29:08 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.758: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  1 10:40:32.758: INFO: sonobuoy-e2e-job-2bf224d5c7cf4759 from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
    Sep  1 10:40:32.758: INFO: 	Container e2e ready: true, restart count 0
    Sep  1 10:40:32.758: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  1 10:40:32.758: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-v249x from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
    Sep  1 10:40:32.758: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  1 10:40:32.758: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  1 10:40:32.758: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
    Sep  1 10:40:32.779: INFO: cert-manager-66f9685c7f-jlr6s from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container cert-manager ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: cert-manager-cainjector-6cfc589789-78hkg from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container cainjector ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: cert-manager-webhook-59f6664d4d-cfggr from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container webhook ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: minio-6c8d455566-dvvwz from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container minio ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: velero-57c7d7c6c4-tlzqn from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container velero ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: traefik-7cf5b5b95f-97lpm from ingress-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container traefik ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: kube-green-546cd595c4-mjs5l from kube-green-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container kube-green ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: cilium-64mb8 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container cilium-agent ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: cilium-operator-858666d4b6-mqxc6 from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container cilium-operator ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: coredns-787d4945fb-hw6lt from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container coredns ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: coredns-787d4945fb-zc797 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container coredns ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: hubble-relay-7956d48fc8-dcjfd from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container hubble-relay ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: hubble-ui-b86cb9bf7-7bzl2 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (2 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container backend ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: 	Container frontend ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: kube-proxy-vfc2w from kube-system started at 2023-09-01 10:08:32 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: kyverno-5bbbd994c9-kdf4j from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container kyverno ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: kyverno-background-54b49bbb5d-wbc6r from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container kyverno-background ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: kyverno-cleanup-f5b6cdd5b-jgfw6 from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container kyverno-cleanup ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: kyverno-reports-b48cb544f-2sb6h from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container kyverno-reports ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: local-path-provisioner-5d8858f87d-gtwb4 from local-path-storage started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: fluentbit-fluentbit-7986p from logging-system started at 2023-09-01 10:23:38 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container fluent-bit ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: logging-operator-5df74f78f5-zzgl8 from logging-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container logging-operator ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: kube-state-metrics-8447695667-frzrq from monitoring-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: node-exporter-vwctl from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: prometheus-operator-75f79b8c5d-pjwn9 from monitoring-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container prometheus-operator ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: rbac-manager-58f6dc584b-xgdd9 from rbac-manager-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container rbac-manager ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-mqksc from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
    Sep  1 10:40:32.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  1 10:40:32.779: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node k8s-worker-1.c.operations-lab.internal 09/01/23 10:40:32.818
    STEP: verifying the node has the label node k8s-worker-2.c.operations-lab.internal 09/01/23 10:40:32.837
    Sep  1 10:40:32.864: INFO: Pod cert-manager-66f9685c7f-jlr6s requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.864: INFO: Pod cert-manager-cainjector-6cfc589789-78hkg requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.865: INFO: Pod cert-manager-webhook-59f6664d4d-cfggr requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.865: INFO: Pod minio-6c8d455566-dvvwz requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.865: INFO: Pod velero-57c7d7c6c4-tlzqn requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.865: INFO: Pod traefik-7cf5b5b95f-97lpm requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.865: INFO: Pod kube-green-546cd595c4-mjs5l requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.865: INFO: Pod cilium-64mb8 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.865: INFO: Pod cilium-operator-858666d4b6-mqxc6 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.866: INFO: Pod cilium-operator-858666d4b6-t7brd requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Sep  1 10:40:32.866: INFO: Pod cilium-q8wf6 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Sep  1 10:40:32.866: INFO: Pod coredns-787d4945fb-hw6lt requesting resource cpu=100m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.866: INFO: Pod coredns-787d4945fb-zc797 requesting resource cpu=100m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.866: INFO: Pod hubble-relay-7956d48fc8-dcjfd requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.866: INFO: Pod hubble-ui-b86cb9bf7-7bzl2 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.866: INFO: Pod kube-proxy-vfc2w requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.867: INFO: Pod kube-proxy-wdplh requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Sep  1 10:40:32.867: INFO: Pod kyverno-5bbbd994c9-kdf4j requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.867: INFO: Pod kyverno-background-54b49bbb5d-wbc6r requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.867: INFO: Pod kyverno-cleanup-f5b6cdd5b-jgfw6 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.867: INFO: Pod kyverno-reports-b48cb544f-2sb6h requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.868: INFO: Pod local-path-provisioner-5d8858f87d-gtwb4 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.868: INFO: Pod fluentbit-fluentbit-7986p requesting resource cpu=100m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.868: INFO: Pod fluentbit-fluentbit-zzgmw requesting resource cpu=100m on Node k8s-worker-1.c.operations-lab.internal
    Sep  1 10:40:32.868: INFO: Pod logging-fluentd-0 requesting resource cpu=500m on Node k8s-worker-1.c.operations-lab.internal
    Sep  1 10:40:32.869: INFO: Pod logging-operator-5df74f78f5-zzgl8 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.869: INFO: Pod alertmanager-alertmanager-0 requesting resource cpu=10m on Node k8s-worker-1.c.operations-lab.internal
    Sep  1 10:40:32.869: INFO: Pod kube-state-metrics-8447695667-frzrq requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.869: INFO: Pod node-exporter-vwctl requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.869: INFO: Pod node-exporter-zgtk4 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Sep  1 10:40:32.870: INFO: Pod prometheus-operator-75f79b8c5d-pjwn9 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.870: INFO: Pod prometheus-prometheus-0 requesting resource cpu=10m on Node k8s-worker-1.c.operations-lab.internal
    Sep  1 10:40:32.870: INFO: Pod pod-qos-class-f424b139-61a6-41ad-9a44-d430d64dd81a requesting resource cpu=100m on Node k8s-worker-1.c.operations-lab.internal
    Sep  1 10:40:32.870: INFO: Pod pod-exec-websocket-cdb2f5b9-6688-426e-9376-1d3a55624e30 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Sep  1 10:40:32.870: INFO: Pod rbac-manager-58f6dc584b-xgdd9 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.871: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Sep  1 10:40:32.871: INFO: Pod sonobuoy-e2e-job-2bf224d5c7cf4759 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Sep  1 10:40:32.871: INFO: Pod sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-mqksc requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.871: INFO: Pod sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-v249x requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    STEP: Starting Pods to consume most of the cluster CPU. 09/01/23 10:40:32.872
    Sep  1 10:40:32.872: INFO: Creating a pod which consumes cpu=896m on Node k8s-worker-1.c.operations-lab.internal
    Sep  1 10:40:32.885: INFO: Creating a pod which consumes cpu=1190m on Node k8s-worker-2.c.operations-lab.internal
    Sep  1 10:40:32.893: INFO: Waiting up to 5m0s for pod "filler-pod-f036da7f-ce77-4303-ae69-45a35863f704" in namespace "sched-pred-3828" to be "running"
    Sep  1 10:40:32.906: INFO: Pod "filler-pod-f036da7f-ce77-4303-ae69-45a35863f704": Phase="Pending", Reason="", readiness=false. Elapsed: 13.183596ms
    Sep  1 10:40:34.909: INFO: Pod "filler-pod-f036da7f-ce77-4303-ae69-45a35863f704": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016339338s
    Sep  1 10:40:36.910: INFO: Pod "filler-pod-f036da7f-ce77-4303-ae69-45a35863f704": Phase="Running", Reason="", readiness=true. Elapsed: 4.016987557s
    Sep  1 10:40:36.910: INFO: Pod "filler-pod-f036da7f-ce77-4303-ae69-45a35863f704" satisfied condition "running"
    Sep  1 10:40:36.910: INFO: Waiting up to 5m0s for pod "filler-pod-535f0b7b-548b-4b14-8916-356b90253327" in namespace "sched-pred-3828" to be "running"
    Sep  1 10:40:36.914: INFO: Pod "filler-pod-535f0b7b-548b-4b14-8916-356b90253327": Phase="Running", Reason="", readiness=true. Elapsed: 3.662968ms
    Sep  1 10:40:36.914: INFO: Pod "filler-pod-535f0b7b-548b-4b14-8916-356b90253327" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 09/01/23 10:40:36.914
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-535f0b7b-548b-4b14-8916-356b90253327.1780c03e1d87cf90], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3828/filler-pod-535f0b7b-548b-4b14-8916-356b90253327 to k8s-worker-2.c.operations-lab.internal] 09/01/23 10:40:36.919
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-535f0b7b-548b-4b14-8916-356b90253327.1780c03e50095813], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/01/23 10:40:36.919
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-535f0b7b-548b-4b14-8916-356b90253327.1780c03e51d7ad65], Reason = [Created], Message = [Created container filler-pod-535f0b7b-548b-4b14-8916-356b90253327] 09/01/23 10:40:36.919
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-535f0b7b-548b-4b14-8916-356b90253327.1780c03e59d255f7], Reason = [Started], Message = [Started container filler-pod-535f0b7b-548b-4b14-8916-356b90253327] 09/01/23 10:40:36.92
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f036da7f-ce77-4303-ae69-45a35863f704.1780c03e1cadc282], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3828/filler-pod-f036da7f-ce77-4303-ae69-45a35863f704 to k8s-worker-1.c.operations-lab.internal] 09/01/23 10:40:36.92
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f036da7f-ce77-4303-ae69-45a35863f704.1780c03e7efff234], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/01/23 10:40:36.92
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f036da7f-ce77-4303-ae69-45a35863f704.1780c03e80765689], Reason = [Created], Message = [Created container filler-pod-f036da7f-ce77-4303-ae69-45a35863f704] 09/01/23 10:40:36.921
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f036da7f-ce77-4303-ae69-45a35863f704.1780c03e88a977ba], Reason = [Started], Message = [Started container filler-pod-f036da7f-ce77-4303-ae69-45a35863f704] 09/01/23 10:40:36.921
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1780c03f0d3ae985], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] 09/01/23 10:40:36.935
    STEP: removing the label node off the node k8s-worker-1.c.operations-lab.internal 09/01/23 10:40:37.934
    STEP: verifying the node doesn't have the label node 09/01/23 10:40:37.954
    STEP: removing the label node off the node k8s-worker-2.c.operations-lab.internal 09/01/23 10:40:37.96
    STEP: verifying the node doesn't have the label node 09/01/23 10:40:37.985
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:40:37.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3828" for this suite. 09/01/23 10:40:38.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:40:38.041
Sep  1 10:40:38.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 10:40:38.042
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:38.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:38.068
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-9059 09/01/23 10:40:38.073
STEP: creating replication controller nodeport-test in namespace services-9059 09/01/23 10:40:38.097
I0901 10:40:38.109069      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9059, replica count: 2
I0901 10:40:41.160970      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  1 10:40:41.161: INFO: Creating new exec pod
Sep  1 10:40:41.168: INFO: Waiting up to 5m0s for pod "execpoddj84t" in namespace "services-9059" to be "running"
Sep  1 10:40:41.173: INFO: Pod "execpoddj84t": Phase="Pending", Reason="", readiness=false. Elapsed: 5.760565ms
Sep  1 10:40:43.179: INFO: Pod "execpoddj84t": Phase="Running", Reason="", readiness=true. Elapsed: 2.011309486s
Sep  1 10:40:43.179: INFO: Pod "execpoddj84t" satisfied condition "running"
Sep  1 10:40:44.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-9059 exec execpoddj84t -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Sep  1 10:40:44.389: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep  1 10:40:44.389: INFO: stdout: ""
Sep  1 10:40:44.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-9059 exec execpoddj84t -- /bin/sh -x -c nc -v -z -w 2 10.97.6.34 80'
Sep  1 10:40:44.572: INFO: stderr: "+ nc -v -z -w 2 10.97.6.34 80\nConnection to 10.97.6.34 80 port [tcp/http] succeeded!\n"
Sep  1 10:40:44.572: INFO: stdout: ""
Sep  1 10:40:44.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-9059 exec execpoddj84t -- /bin/sh -x -c nc -v -z -w 2 172.16.0.3 31036'
Sep  1 10:40:44.814: INFO: stderr: "+ nc -v -z -w 2 172.16.0.3 31036\nConnection to 172.16.0.3 31036 port [tcp/*] succeeded!\n"
Sep  1 10:40:44.814: INFO: stdout: ""
Sep  1 10:40:44.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-9059 exec execpoddj84t -- /bin/sh -x -c nc -v -z -w 2 172.16.0.4 31036'
Sep  1 10:40:45.016: INFO: stderr: "+ nc -v -z -w 2 172.16.0.4 31036\nConnection to 172.16.0.4 31036 port [tcp/*] succeeded!\n"
Sep  1 10:40:45.016: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 10:40:45.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9059" for this suite. 09/01/23 10:40:45.021
------------------------------
• [SLOW TEST] [6.989 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:40:38.041
    Sep  1 10:40:38.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 10:40:38.042
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:38.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:38.068
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-9059 09/01/23 10:40:38.073
    STEP: creating replication controller nodeport-test in namespace services-9059 09/01/23 10:40:38.097
    I0901 10:40:38.109069      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9059, replica count: 2
    I0901 10:40:41.160970      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  1 10:40:41.161: INFO: Creating new exec pod
    Sep  1 10:40:41.168: INFO: Waiting up to 5m0s for pod "execpoddj84t" in namespace "services-9059" to be "running"
    Sep  1 10:40:41.173: INFO: Pod "execpoddj84t": Phase="Pending", Reason="", readiness=false. Elapsed: 5.760565ms
    Sep  1 10:40:43.179: INFO: Pod "execpoddj84t": Phase="Running", Reason="", readiness=true. Elapsed: 2.011309486s
    Sep  1 10:40:43.179: INFO: Pod "execpoddj84t" satisfied condition "running"
    Sep  1 10:40:44.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-9059 exec execpoddj84t -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Sep  1 10:40:44.389: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Sep  1 10:40:44.389: INFO: stdout: ""
    Sep  1 10:40:44.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-9059 exec execpoddj84t -- /bin/sh -x -c nc -v -z -w 2 10.97.6.34 80'
    Sep  1 10:40:44.572: INFO: stderr: "+ nc -v -z -w 2 10.97.6.34 80\nConnection to 10.97.6.34 80 port [tcp/http] succeeded!\n"
    Sep  1 10:40:44.572: INFO: stdout: ""
    Sep  1 10:40:44.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-9059 exec execpoddj84t -- /bin/sh -x -c nc -v -z -w 2 172.16.0.3 31036'
    Sep  1 10:40:44.814: INFO: stderr: "+ nc -v -z -w 2 172.16.0.3 31036\nConnection to 172.16.0.3 31036 port [tcp/*] succeeded!\n"
    Sep  1 10:40:44.814: INFO: stdout: ""
    Sep  1 10:40:44.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-9059 exec execpoddj84t -- /bin/sh -x -c nc -v -z -w 2 172.16.0.4 31036'
    Sep  1 10:40:45.016: INFO: stderr: "+ nc -v -z -w 2 172.16.0.4 31036\nConnection to 172.16.0.4 31036 port [tcp/*] succeeded!\n"
    Sep  1 10:40:45.016: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:40:45.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9059" for this suite. 09/01/23 10:40:45.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:40:45.031
Sep  1 10:40:45.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename var-expansion 09/01/23 10:40:45.034
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:45.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:45.07
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 09/01/23 10:40:45.074
Sep  1 10:40:45.083: INFO: Waiting up to 5m0s for pod "var-expansion-1c66feee-a743-404a-a755-c7cbf387b035" in namespace "var-expansion-1017" to be "Succeeded or Failed"
Sep  1 10:40:45.089: INFO: Pod "var-expansion-1c66feee-a743-404a-a755-c7cbf387b035": Phase="Pending", Reason="", readiness=false. Elapsed: 5.515114ms
Sep  1 10:40:47.092: INFO: Pod "var-expansion-1c66feee-a743-404a-a755-c7cbf387b035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009125461s
Sep  1 10:40:49.096: INFO: Pod "var-expansion-1c66feee-a743-404a-a755-c7cbf387b035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012438289s
STEP: Saw pod success 09/01/23 10:40:49.096
Sep  1 10:40:49.096: INFO: Pod "var-expansion-1c66feee-a743-404a-a755-c7cbf387b035" satisfied condition "Succeeded or Failed"
Sep  1 10:40:49.099: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod var-expansion-1c66feee-a743-404a-a755-c7cbf387b035 container dapi-container: <nil>
STEP: delete the pod 09/01/23 10:40:49.129
Sep  1 10:40:49.156: INFO: Waiting for pod var-expansion-1c66feee-a743-404a-a755-c7cbf387b035 to disappear
Sep  1 10:40:49.161: INFO: Pod var-expansion-1c66feee-a743-404a-a755-c7cbf387b035 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  1 10:40:49.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1017" for this suite. 09/01/23 10:40:49.169
------------------------------
• [4.149 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:40:45.031
    Sep  1 10:40:45.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename var-expansion 09/01/23 10:40:45.034
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:45.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:45.07
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 09/01/23 10:40:45.074
    Sep  1 10:40:45.083: INFO: Waiting up to 5m0s for pod "var-expansion-1c66feee-a743-404a-a755-c7cbf387b035" in namespace "var-expansion-1017" to be "Succeeded or Failed"
    Sep  1 10:40:45.089: INFO: Pod "var-expansion-1c66feee-a743-404a-a755-c7cbf387b035": Phase="Pending", Reason="", readiness=false. Elapsed: 5.515114ms
    Sep  1 10:40:47.092: INFO: Pod "var-expansion-1c66feee-a743-404a-a755-c7cbf387b035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009125461s
    Sep  1 10:40:49.096: INFO: Pod "var-expansion-1c66feee-a743-404a-a755-c7cbf387b035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012438289s
    STEP: Saw pod success 09/01/23 10:40:49.096
    Sep  1 10:40:49.096: INFO: Pod "var-expansion-1c66feee-a743-404a-a755-c7cbf387b035" satisfied condition "Succeeded or Failed"
    Sep  1 10:40:49.099: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod var-expansion-1c66feee-a743-404a-a755-c7cbf387b035 container dapi-container: <nil>
    STEP: delete the pod 09/01/23 10:40:49.129
    Sep  1 10:40:49.156: INFO: Waiting for pod var-expansion-1c66feee-a743-404a-a755-c7cbf387b035 to disappear
    Sep  1 10:40:49.161: INFO: Pod var-expansion-1c66feee-a743-404a-a755-c7cbf387b035 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:40:49.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1017" for this suite. 09/01/23 10:40:49.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:40:49.186
Sep  1 10:40:49.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename events 09/01/23 10:40:49.187
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:49.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:49.213
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 09/01/23 10:40:49.216
STEP: listing events in all namespaces 09/01/23 10:40:49.229
STEP: listing events in test namespace 09/01/23 10:40:49.246
STEP: listing events with field selection filtering on source 09/01/23 10:40:49.249
STEP: listing events with field selection filtering on reportingController 09/01/23 10:40:49.252
STEP: getting the test event 09/01/23 10:40:49.258
STEP: patching the test event 09/01/23 10:40:49.261
STEP: getting the test event 09/01/23 10:40:49.28
STEP: updating the test event 09/01/23 10:40:49.283
STEP: getting the test event 09/01/23 10:40:49.291
STEP: deleting the test event 09/01/23 10:40:49.295
STEP: listing events in all namespaces 09/01/23 10:40:49.302
STEP: listing events in test namespace 09/01/23 10:40:49.318
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Sep  1 10:40:49.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5627" for this suite. 09/01/23 10:40:49.328
------------------------------
• [0.151 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:40:49.186
    Sep  1 10:40:49.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename events 09/01/23 10:40:49.187
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:49.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:49.213
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 09/01/23 10:40:49.216
    STEP: listing events in all namespaces 09/01/23 10:40:49.229
    STEP: listing events in test namespace 09/01/23 10:40:49.246
    STEP: listing events with field selection filtering on source 09/01/23 10:40:49.249
    STEP: listing events with field selection filtering on reportingController 09/01/23 10:40:49.252
    STEP: getting the test event 09/01/23 10:40:49.258
    STEP: patching the test event 09/01/23 10:40:49.261
    STEP: getting the test event 09/01/23 10:40:49.28
    STEP: updating the test event 09/01/23 10:40:49.283
    STEP: getting the test event 09/01/23 10:40:49.291
    STEP: deleting the test event 09/01/23 10:40:49.295
    STEP: listing events in all namespaces 09/01/23 10:40:49.302
    STEP: listing events in test namespace 09/01/23 10:40:49.318
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:40:49.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5627" for this suite. 09/01/23 10:40:49.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:40:49.34
Sep  1 10:40:49.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pod-network-test 09/01/23 10:40:49.342
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:49.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:49.366
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-3940 09/01/23 10:40:49.369
STEP: creating a selector 09/01/23 10:40:49.37
STEP: Creating the service pods in kubernetes 09/01/23 10:40:49.37
Sep  1 10:40:49.370: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  1 10:40:49.394: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3940" to be "running and ready"
Sep  1 10:40:49.399: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.962818ms
Sep  1 10:40:49.400: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:40:51.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010246432s
Sep  1 10:40:51.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 10:40:53.406: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011267249s
Sep  1 10:40:53.406: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 10:40:55.407: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.012540407s
Sep  1 10:40:55.407: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 10:40:57.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010366408s
Sep  1 10:40:57.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 10:40:59.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009723576s
Sep  1 10:40:59.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 10:41:01.407: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.012819325s
Sep  1 10:41:01.407: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 10:41:03.406: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.012117408s
Sep  1 10:41:03.407: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 10:41:05.407: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012512381s
Sep  1 10:41:05.407: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 10:41:07.408: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.013716081s
Sep  1 10:41:07.408: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 10:41:09.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010034815s
Sep  1 10:41:09.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 10:41:11.406: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011598957s
Sep  1 10:41:11.406: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  1 10:41:11.406: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  1 10:41:11.409: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3940" to be "running and ready"
Sep  1 10:41:11.413: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.457546ms
Sep  1 10:41:11.413: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  1 10:41:11.413: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/01/23 10:41:11.416
Sep  1 10:41:11.423: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3940" to be "running"
Sep  1 10:41:11.428: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.694297ms
Sep  1 10:41:13.434: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010677301s
Sep  1 10:41:13.434: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  1 10:41:13.438: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  1 10:41:13.438: INFO: Breadth first check of 10.10.0.110 on host 172.16.0.3...
Sep  1 10:41:13.441: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.75:9080/dial?request=hostname&protocol=udp&host=10.10.0.110&port=8081&tries=1'] Namespace:pod-network-test-3940 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 10:41:13.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 10:41:13.442: INFO: ExecWithOptions: Clientset creation
Sep  1 10:41:13.442: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3940/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.75%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.0.110%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  1 10:41:13.557: INFO: Waiting for responses: map[]
Sep  1 10:41:13.557: INFO: reached 10.10.0.110 after 0/1 tries
Sep  1 10:41:13.557: INFO: Breadth first check of 10.10.1.100 on host 172.16.0.4...
Sep  1 10:41:13.561: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.75:9080/dial?request=hostname&protocol=udp&host=10.10.1.100&port=8081&tries=1'] Namespace:pod-network-test-3940 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 10:41:13.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 10:41:13.562: INFO: ExecWithOptions: Clientset creation
Sep  1 10:41:13.562: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3940/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.75%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.1.100%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  1 10:41:13.665: INFO: Waiting for responses: map[]
Sep  1 10:41:13.665: INFO: reached 10.10.1.100 after 0/1 tries
Sep  1 10:41:13.666: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  1 10:41:13.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3940" for this suite. 09/01/23 10:41:13.671
------------------------------
• [SLOW TEST] [24.338 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:40:49.34
    Sep  1 10:40:49.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pod-network-test 09/01/23 10:40:49.342
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:40:49.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:40:49.366
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-3940 09/01/23 10:40:49.369
    STEP: creating a selector 09/01/23 10:40:49.37
    STEP: Creating the service pods in kubernetes 09/01/23 10:40:49.37
    Sep  1 10:40:49.370: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  1 10:40:49.394: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3940" to be "running and ready"
    Sep  1 10:40:49.399: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.962818ms
    Sep  1 10:40:49.400: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:40:51.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010246432s
    Sep  1 10:40:51.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 10:40:53.406: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011267249s
    Sep  1 10:40:53.406: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 10:40:55.407: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.012540407s
    Sep  1 10:40:55.407: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 10:40:57.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010366408s
    Sep  1 10:40:57.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 10:40:59.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009723576s
    Sep  1 10:40:59.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 10:41:01.407: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.012819325s
    Sep  1 10:41:01.407: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 10:41:03.406: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.012117408s
    Sep  1 10:41:03.407: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 10:41:05.407: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012512381s
    Sep  1 10:41:05.407: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 10:41:07.408: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.013716081s
    Sep  1 10:41:07.408: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 10:41:09.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010034815s
    Sep  1 10:41:09.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 10:41:11.406: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011598957s
    Sep  1 10:41:11.406: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  1 10:41:11.406: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  1 10:41:11.409: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3940" to be "running and ready"
    Sep  1 10:41:11.413: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.457546ms
    Sep  1 10:41:11.413: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  1 10:41:11.413: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/01/23 10:41:11.416
    Sep  1 10:41:11.423: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3940" to be "running"
    Sep  1 10:41:11.428: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.694297ms
    Sep  1 10:41:13.434: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010677301s
    Sep  1 10:41:13.434: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  1 10:41:13.438: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  1 10:41:13.438: INFO: Breadth first check of 10.10.0.110 on host 172.16.0.3...
    Sep  1 10:41:13.441: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.75:9080/dial?request=hostname&protocol=udp&host=10.10.0.110&port=8081&tries=1'] Namespace:pod-network-test-3940 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 10:41:13.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 10:41:13.442: INFO: ExecWithOptions: Clientset creation
    Sep  1 10:41:13.442: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3940/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.75%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.0.110%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  1 10:41:13.557: INFO: Waiting for responses: map[]
    Sep  1 10:41:13.557: INFO: reached 10.10.0.110 after 0/1 tries
    Sep  1 10:41:13.557: INFO: Breadth first check of 10.10.1.100 on host 172.16.0.4...
    Sep  1 10:41:13.561: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.75:9080/dial?request=hostname&protocol=udp&host=10.10.1.100&port=8081&tries=1'] Namespace:pod-network-test-3940 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 10:41:13.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 10:41:13.562: INFO: ExecWithOptions: Clientset creation
    Sep  1 10:41:13.562: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3940/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.75%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.1.100%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  1 10:41:13.665: INFO: Waiting for responses: map[]
    Sep  1 10:41:13.665: INFO: reached 10.10.1.100 after 0/1 tries
    Sep  1 10:41:13.666: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:41:13.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3940" for this suite. 09/01/23 10:41:13.671
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:41:13.683
Sep  1 10:41:13.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 10:41:13.685
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:13.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:13.708
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 09/01/23 10:41:13.712
Sep  1 10:41:13.724: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-468f3a2c-26ab-41aa-ad46-a5537cba69c4" in namespace "emptydir-1936" to be "running"
Sep  1 10:41:13.728: INFO: Pod "pod-sharedvolume-468f3a2c-26ab-41aa-ad46-a5537cba69c4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.217618ms
Sep  1 10:41:15.732: INFO: Pod "pod-sharedvolume-468f3a2c-26ab-41aa-ad46-a5537cba69c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007136763s
Sep  1 10:41:15.732: INFO: Pod "pod-sharedvolume-468f3a2c-26ab-41aa-ad46-a5537cba69c4" satisfied condition "running"
STEP: Reading file content from the nginx-container 09/01/23 10:41:15.732
Sep  1 10:41:15.732: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1936 PodName:pod-sharedvolume-468f3a2c-26ab-41aa-ad46-a5537cba69c4 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 10:41:15.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 10:41:15.733: INFO: ExecWithOptions: Clientset creation
Sep  1 10:41:15.734: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-1936/pods/pod-sharedvolume-468f3a2c-26ab-41aa-ad46-a5537cba69c4/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Sep  1 10:41:15.845: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 10:41:15.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1936" for this suite. 09/01/23 10:41:15.851
------------------------------
• [2.180 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:41:13.683
    Sep  1 10:41:13.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 10:41:13.685
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:13.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:13.708
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 09/01/23 10:41:13.712
    Sep  1 10:41:13.724: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-468f3a2c-26ab-41aa-ad46-a5537cba69c4" in namespace "emptydir-1936" to be "running"
    Sep  1 10:41:13.728: INFO: Pod "pod-sharedvolume-468f3a2c-26ab-41aa-ad46-a5537cba69c4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.217618ms
    Sep  1 10:41:15.732: INFO: Pod "pod-sharedvolume-468f3a2c-26ab-41aa-ad46-a5537cba69c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007136763s
    Sep  1 10:41:15.732: INFO: Pod "pod-sharedvolume-468f3a2c-26ab-41aa-ad46-a5537cba69c4" satisfied condition "running"
    STEP: Reading file content from the nginx-container 09/01/23 10:41:15.732
    Sep  1 10:41:15.732: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1936 PodName:pod-sharedvolume-468f3a2c-26ab-41aa-ad46-a5537cba69c4 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 10:41:15.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 10:41:15.733: INFO: ExecWithOptions: Clientset creation
    Sep  1 10:41:15.734: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-1936/pods/pod-sharedvolume-468f3a2c-26ab-41aa-ad46-a5537cba69c4/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Sep  1 10:41:15.845: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:41:15.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1936" for this suite. 09/01/23 10:41:15.851
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:41:15.868
Sep  1 10:41:15.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 10:41:15.87
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:15.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:15.889
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Sep  1 10:41:15.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/01/23 10:41:21.122
Sep  1 10:41:21.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-7727 --namespace=crd-publish-openapi-7727 create -f -'
Sep  1 10:41:25.011: INFO: stderr: ""
Sep  1 10:41:25.011: INFO: stdout: "e2e-test-crd-publish-openapi-4371-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  1 10:41:25.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-7727 --namespace=crd-publish-openapi-7727 delete e2e-test-crd-publish-openapi-4371-crds test-cr'
Sep  1 10:41:25.180: INFO: stderr: ""
Sep  1 10:41:25.180: INFO: stdout: "e2e-test-crd-publish-openapi-4371-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep  1 10:41:25.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-7727 --namespace=crd-publish-openapi-7727 apply -f -'
Sep  1 10:41:26.195: INFO: stderr: ""
Sep  1 10:41:26.195: INFO: stdout: "e2e-test-crd-publish-openapi-4371-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  1 10:41:26.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-7727 --namespace=crd-publish-openapi-7727 delete e2e-test-crd-publish-openapi-4371-crds test-cr'
Sep  1 10:41:26.352: INFO: stderr: ""
Sep  1 10:41:26.352: INFO: stdout: "e2e-test-crd-publish-openapi-4371-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 09/01/23 10:41:26.352
Sep  1 10:41:26.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-7727 explain e2e-test-crd-publish-openapi-4371-crds'
Sep  1 10:41:27.032: INFO: stderr: ""
Sep  1 10:41:27.032: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4371-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:41:31.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7727" for this suite. 09/01/23 10:41:31.762
------------------------------
• [SLOW TEST] [15.905 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:41:15.868
    Sep  1 10:41:15.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 10:41:15.87
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:15.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:15.889
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Sep  1 10:41:15.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/01/23 10:41:21.122
    Sep  1 10:41:21.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-7727 --namespace=crd-publish-openapi-7727 create -f -'
    Sep  1 10:41:25.011: INFO: stderr: ""
    Sep  1 10:41:25.011: INFO: stdout: "e2e-test-crd-publish-openapi-4371-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep  1 10:41:25.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-7727 --namespace=crd-publish-openapi-7727 delete e2e-test-crd-publish-openapi-4371-crds test-cr'
    Sep  1 10:41:25.180: INFO: stderr: ""
    Sep  1 10:41:25.180: INFO: stdout: "e2e-test-crd-publish-openapi-4371-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Sep  1 10:41:25.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-7727 --namespace=crd-publish-openapi-7727 apply -f -'
    Sep  1 10:41:26.195: INFO: stderr: ""
    Sep  1 10:41:26.195: INFO: stdout: "e2e-test-crd-publish-openapi-4371-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep  1 10:41:26.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-7727 --namespace=crd-publish-openapi-7727 delete e2e-test-crd-publish-openapi-4371-crds test-cr'
    Sep  1 10:41:26.352: INFO: stderr: ""
    Sep  1 10:41:26.352: INFO: stdout: "e2e-test-crd-publish-openapi-4371-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 09/01/23 10:41:26.352
    Sep  1 10:41:26.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-7727 explain e2e-test-crd-publish-openapi-4371-crds'
    Sep  1 10:41:27.032: INFO: stderr: ""
    Sep  1 10:41:27.032: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4371-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:41:31.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7727" for this suite. 09/01/23 10:41:31.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:41:31.778
Sep  1 10:41:31.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename csiinlinevolumes 09/01/23 10:41:31.781
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:31.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:31.802
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 09/01/23 10:41:31.806
STEP: getting 09/01/23 10:41:31.823
STEP: listing in namespace 09/01/23 10:41:31.834
STEP: patching 09/01/23 10:41:31.837
STEP: deleting 09/01/23 10:41:31.854
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Sep  1 10:41:31.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-8539" for this suite. 09/01/23 10:41:31.876
------------------------------
• [0.105 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:41:31.778
    Sep  1 10:41:31.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename csiinlinevolumes 09/01/23 10:41:31.781
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:31.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:31.802
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 09/01/23 10:41:31.806
    STEP: getting 09/01/23 10:41:31.823
    STEP: listing in namespace 09/01/23 10:41:31.834
    STEP: patching 09/01/23 10:41:31.837
    STEP: deleting 09/01/23 10:41:31.854
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:41:31.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-8539" for this suite. 09/01/23 10:41:31.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:41:31.887
Sep  1 10:41:31.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename svcaccounts 09/01/23 10:41:31.889
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:31.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:31.911
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Sep  1 10:41:31.918: INFO: Got root ca configmap in namespace "svcaccounts-7640"
Sep  1 10:41:31.924: INFO: Deleted root ca configmap in namespace "svcaccounts-7640"
STEP: waiting for a new root ca configmap created 09/01/23 10:41:32.424
Sep  1 10:41:32.428: INFO: Recreated root ca configmap in namespace "svcaccounts-7640"
Sep  1 10:41:32.433: INFO: Updated root ca configmap in namespace "svcaccounts-7640"
STEP: waiting for the root ca configmap reconciled 09/01/23 10:41:32.934
Sep  1 10:41:32.938: INFO: Reconciled root ca configmap in namespace "svcaccounts-7640"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  1 10:41:32.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7640" for this suite. 09/01/23 10:41:32.942
------------------------------
• [1.063 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:41:31.887
    Sep  1 10:41:31.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename svcaccounts 09/01/23 10:41:31.889
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:31.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:31.911
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Sep  1 10:41:31.918: INFO: Got root ca configmap in namespace "svcaccounts-7640"
    Sep  1 10:41:31.924: INFO: Deleted root ca configmap in namespace "svcaccounts-7640"
    STEP: waiting for a new root ca configmap created 09/01/23 10:41:32.424
    Sep  1 10:41:32.428: INFO: Recreated root ca configmap in namespace "svcaccounts-7640"
    Sep  1 10:41:32.433: INFO: Updated root ca configmap in namespace "svcaccounts-7640"
    STEP: waiting for the root ca configmap reconciled 09/01/23 10:41:32.934
    Sep  1 10:41:32.938: INFO: Reconciled root ca configmap in namespace "svcaccounts-7640"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:41:32.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7640" for this suite. 09/01/23 10:41:32.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:41:32.955
Sep  1 10:41:32.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename namespaces 09/01/23 10:41:32.957
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:32.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:32.975
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 09/01/23 10:41:32.978
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:32.996
STEP: Creating a pod in the namespace 09/01/23 10:41:32.999
STEP: Waiting for the pod to have running status 09/01/23 10:41:33.008
Sep  1 10:41:33.008: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7876" to be "running"
Sep  1 10:41:33.012: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.679079ms
Sep  1 10:41:35.016: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00818309s
Sep  1 10:41:35.016: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 09/01/23 10:41:35.016
STEP: Waiting for the namespace to be removed. 09/01/23 10:41:35.023
STEP: Recreating the namespace 09/01/23 10:41:47.027
STEP: Verifying there are no pods in the namespace 09/01/23 10:41:47.043
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:41:47.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4447" for this suite. 09/01/23 10:41:47.053
STEP: Destroying namespace "nsdeletetest-7876" for this suite. 09/01/23 10:41:47.061
Sep  1 10:41:47.064: INFO: Namespace nsdeletetest-7876 was already deleted
STEP: Destroying namespace "nsdeletetest-1631" for this suite. 09/01/23 10:41:47.064
------------------------------
• [SLOW TEST] [14.115 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:41:32.955
    Sep  1 10:41:32.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename namespaces 09/01/23 10:41:32.957
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:32.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:32.975
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 09/01/23 10:41:32.978
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:32.996
    STEP: Creating a pod in the namespace 09/01/23 10:41:32.999
    STEP: Waiting for the pod to have running status 09/01/23 10:41:33.008
    Sep  1 10:41:33.008: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7876" to be "running"
    Sep  1 10:41:33.012: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.679079ms
    Sep  1 10:41:35.016: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00818309s
    Sep  1 10:41:35.016: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 09/01/23 10:41:35.016
    STEP: Waiting for the namespace to be removed. 09/01/23 10:41:35.023
    STEP: Recreating the namespace 09/01/23 10:41:47.027
    STEP: Verifying there are no pods in the namespace 09/01/23 10:41:47.043
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:41:47.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4447" for this suite. 09/01/23 10:41:47.053
    STEP: Destroying namespace "nsdeletetest-7876" for this suite. 09/01/23 10:41:47.061
    Sep  1 10:41:47.064: INFO: Namespace nsdeletetest-7876 was already deleted
    STEP: Destroying namespace "nsdeletetest-1631" for this suite. 09/01/23 10:41:47.064
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:41:47.074
Sep  1 10:41:47.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 10:41:47.076
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:47.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:47.096
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7815 09/01/23 10:41:47.1
STEP: changing the ExternalName service to type=ClusterIP 09/01/23 10:41:47.105
STEP: creating replication controller externalname-service in namespace services-7815 09/01/23 10:41:47.136
I0901 10:41:47.143956      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7815, replica count: 2
I0901 10:41:50.195580      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  1 10:41:50.195: INFO: Creating new exec pod
Sep  1 10:41:50.204: INFO: Waiting up to 5m0s for pod "execpodbb4nc" in namespace "services-7815" to be "running"
Sep  1 10:41:50.207: INFO: Pod "execpodbb4nc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.328251ms
Sep  1 10:41:52.213: INFO: Pod "execpodbb4nc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009084265s
Sep  1 10:41:52.213: INFO: Pod "execpodbb4nc" satisfied condition "running"
Sep  1 10:41:53.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-7815 exec execpodbb4nc -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Sep  1 10:41:53.407: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  1 10:41:53.407: INFO: stdout: ""
Sep  1 10:41:53.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-7815 exec execpodbb4nc -- /bin/sh -x -c nc -v -z -w 2 10.110.130.140 80'
Sep  1 10:41:53.607: INFO: stderr: "+ nc -v -z -w 2 10.110.130.140 80\nConnection to 10.110.130.140 80 port [tcp/http] succeeded!\n"
Sep  1 10:41:53.607: INFO: stdout: ""
Sep  1 10:41:53.607: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 10:41:53.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7815" for this suite. 09/01/23 10:41:53.663
------------------------------
• [SLOW TEST] [6.606 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:41:47.074
    Sep  1 10:41:47.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 10:41:47.076
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:47.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:47.096
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7815 09/01/23 10:41:47.1
    STEP: changing the ExternalName service to type=ClusterIP 09/01/23 10:41:47.105
    STEP: creating replication controller externalname-service in namespace services-7815 09/01/23 10:41:47.136
    I0901 10:41:47.143956      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7815, replica count: 2
    I0901 10:41:50.195580      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  1 10:41:50.195: INFO: Creating new exec pod
    Sep  1 10:41:50.204: INFO: Waiting up to 5m0s for pod "execpodbb4nc" in namespace "services-7815" to be "running"
    Sep  1 10:41:50.207: INFO: Pod "execpodbb4nc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.328251ms
    Sep  1 10:41:52.213: INFO: Pod "execpodbb4nc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009084265s
    Sep  1 10:41:52.213: INFO: Pod "execpodbb4nc" satisfied condition "running"
    Sep  1 10:41:53.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-7815 exec execpodbb4nc -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Sep  1 10:41:53.407: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  1 10:41:53.407: INFO: stdout: ""
    Sep  1 10:41:53.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-7815 exec execpodbb4nc -- /bin/sh -x -c nc -v -z -w 2 10.110.130.140 80'
    Sep  1 10:41:53.607: INFO: stderr: "+ nc -v -z -w 2 10.110.130.140 80\nConnection to 10.110.130.140 80 port [tcp/http] succeeded!\n"
    Sep  1 10:41:53.607: INFO: stdout: ""
    Sep  1 10:41:53.607: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:41:53.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7815" for this suite. 09/01/23 10:41:53.663
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:41:53.681
Sep  1 10:41:53.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename custom-resource-definition 09/01/23 10:41:53.682
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:53.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:53.71
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Sep  1 10:41:53.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:41:56.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7021" for this suite. 09/01/23 10:41:56.901
------------------------------
• [3.229 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:41:53.681
    Sep  1 10:41:53.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename custom-resource-definition 09/01/23 10:41:53.682
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:53.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:53.71
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Sep  1 10:41:53.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:41:56.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7021" for this suite. 09/01/23 10:41:56.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:41:56.91
Sep  1 10:41:56.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 10:41:56.911
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:56.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:56.933
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 09/01/23 10:41:56.937
Sep  1 10:41:56.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8196 cluster-info'
Sep  1 10:41:57.058: INFO: stderr: ""
Sep  1 10:41:57.058: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 10:41:57.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8196" for this suite. 09/01/23 10:41:57.062
------------------------------
• [0.159 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:41:56.91
    Sep  1 10:41:56.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 10:41:56.911
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:56.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:56.933
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 09/01/23 10:41:56.937
    Sep  1 10:41:56.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8196 cluster-info'
    Sep  1 10:41:57.058: INFO: stderr: ""
    Sep  1 10:41:57.058: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:41:57.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8196" for this suite. 09/01/23 10:41:57.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:41:57.07
Sep  1 10:41:57.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename statefulset 09/01/23 10:41:57.073
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:57.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:57.103
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3573 09/01/23 10:41:57.107
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Sep  1 10:41:57.169: INFO: Found 0 stateful pods, waiting for 1
Sep  1 10:42:07.174: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 09/01/23 10:42:07.179
W0901 10:42:07.191165      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  1 10:42:07.199: INFO: Found 1 stateful pods, waiting for 2
Sep  1 10:42:17.204: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  1 10:42:17.204: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 09/01/23 10:42:17.21
STEP: Delete all of the StatefulSets 09/01/23 10:42:17.214
STEP: Verify that StatefulSets have been deleted 09/01/23 10:42:17.222
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  1 10:42:17.232: INFO: Deleting all statefulset in ns statefulset-3573
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  1 10:42:17.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3573" for this suite. 09/01/23 10:42:17.278
------------------------------
• [SLOW TEST] [20.219 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:41:57.07
    Sep  1 10:41:57.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename statefulset 09/01/23 10:41:57.073
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:41:57.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:41:57.103
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3573 09/01/23 10:41:57.107
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Sep  1 10:41:57.169: INFO: Found 0 stateful pods, waiting for 1
    Sep  1 10:42:07.174: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 09/01/23 10:42:07.179
    W0901 10:42:07.191165      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  1 10:42:07.199: INFO: Found 1 stateful pods, waiting for 2
    Sep  1 10:42:17.204: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  1 10:42:17.204: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 09/01/23 10:42:17.21
    STEP: Delete all of the StatefulSets 09/01/23 10:42:17.214
    STEP: Verify that StatefulSets have been deleted 09/01/23 10:42:17.222
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  1 10:42:17.232: INFO: Deleting all statefulset in ns statefulset-3573
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:42:17.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3573" for this suite. 09/01/23 10:42:17.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:42:17.302
Sep  1 10:42:17.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 10:42:17.305
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:42:17.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:42:17.346
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-cdf0c032-39f1-49c9-b15b-dd7efe859641 09/01/23 10:42:17.359
STEP: Creating the pod 09/01/23 10:42:17.365
Sep  1 10:42:17.374: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ef132328-415e-4570-9499-347baece02cb" in namespace "projected-1337" to be "running and ready"
Sep  1 10:42:17.379: INFO: Pod "pod-projected-configmaps-ef132328-415e-4570-9499-347baece02cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.599914ms
Sep  1 10:42:17.379: INFO: The phase of Pod pod-projected-configmaps-ef132328-415e-4570-9499-347baece02cb is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:42:19.384: INFO: Pod "pod-projected-configmaps-ef132328-415e-4570-9499-347baece02cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009167254s
Sep  1 10:42:19.384: INFO: The phase of Pod pod-projected-configmaps-ef132328-415e-4570-9499-347baece02cb is Running (Ready = true)
Sep  1 10:42:19.384: INFO: Pod "pod-projected-configmaps-ef132328-415e-4570-9499-347baece02cb" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-cdf0c032-39f1-49c9-b15b-dd7efe859641 09/01/23 10:42:19.409
STEP: waiting to observe update in volume 09/01/23 10:42:19.415
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  1 10:42:21.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1337" for this suite. 09/01/23 10:42:21.437
------------------------------
• [4.142 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:42:17.302
    Sep  1 10:42:17.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 10:42:17.305
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:42:17.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:42:17.346
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-cdf0c032-39f1-49c9-b15b-dd7efe859641 09/01/23 10:42:17.359
    STEP: Creating the pod 09/01/23 10:42:17.365
    Sep  1 10:42:17.374: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ef132328-415e-4570-9499-347baece02cb" in namespace "projected-1337" to be "running and ready"
    Sep  1 10:42:17.379: INFO: Pod "pod-projected-configmaps-ef132328-415e-4570-9499-347baece02cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.599914ms
    Sep  1 10:42:17.379: INFO: The phase of Pod pod-projected-configmaps-ef132328-415e-4570-9499-347baece02cb is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:42:19.384: INFO: Pod "pod-projected-configmaps-ef132328-415e-4570-9499-347baece02cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009167254s
    Sep  1 10:42:19.384: INFO: The phase of Pod pod-projected-configmaps-ef132328-415e-4570-9499-347baece02cb is Running (Ready = true)
    Sep  1 10:42:19.384: INFO: Pod "pod-projected-configmaps-ef132328-415e-4570-9499-347baece02cb" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-cdf0c032-39f1-49c9-b15b-dd7efe859641 09/01/23 10:42:19.409
    STEP: waiting to observe update in volume 09/01/23 10:42:19.415
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:42:21.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1337" for this suite. 09/01/23 10:42:21.437
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:42:21.447
Sep  1 10:42:21.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 10:42:21.449
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:42:21.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:42:21.473
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-c59c8f7b-a865-4890-ab79-cb83d77b1853 09/01/23 10:42:21.479
STEP: Creating a pod to test consume configMaps 09/01/23 10:42:21.486
Sep  1 10:42:21.495: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839" in namespace "projected-3076" to be "Succeeded or Failed"
Sep  1 10:42:21.499: INFO: Pod "pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839": Phase="Pending", Reason="", readiness=false. Elapsed: 3.194318ms
Sep  1 10:42:23.508: INFO: Pod "pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012450711s
Sep  1 10:42:25.504: INFO: Pod "pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008215136s
STEP: Saw pod success 09/01/23 10:42:25.504
Sep  1 10:42:25.504: INFO: Pod "pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839" satisfied condition "Succeeded or Failed"
Sep  1 10:42:25.507: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839 container agnhost-container: <nil>
STEP: delete the pod 09/01/23 10:42:25.515
Sep  1 10:42:25.531: INFO: Waiting for pod pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839 to disappear
Sep  1 10:42:25.537: INFO: Pod pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  1 10:42:25.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3076" for this suite. 09/01/23 10:42:25.541
------------------------------
• [4.103 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:42:21.447
    Sep  1 10:42:21.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 10:42:21.449
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:42:21.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:42:21.473
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-c59c8f7b-a865-4890-ab79-cb83d77b1853 09/01/23 10:42:21.479
    STEP: Creating a pod to test consume configMaps 09/01/23 10:42:21.486
    Sep  1 10:42:21.495: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839" in namespace "projected-3076" to be "Succeeded or Failed"
    Sep  1 10:42:21.499: INFO: Pod "pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839": Phase="Pending", Reason="", readiness=false. Elapsed: 3.194318ms
    Sep  1 10:42:23.508: INFO: Pod "pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012450711s
    Sep  1 10:42:25.504: INFO: Pod "pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008215136s
    STEP: Saw pod success 09/01/23 10:42:25.504
    Sep  1 10:42:25.504: INFO: Pod "pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839" satisfied condition "Succeeded or Failed"
    Sep  1 10:42:25.507: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839 container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 10:42:25.515
    Sep  1 10:42:25.531: INFO: Waiting for pod pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839 to disappear
    Sep  1 10:42:25.537: INFO: Pod pod-projected-configmaps-27b04ae7-59ed-49d9-8031-390871a0a839 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:42:25.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3076" for this suite. 09/01/23 10:42:25.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:42:25.555
Sep  1 10:42:25.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename gc 09/01/23 10:42:25.557
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:42:25.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:42:25.579
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 09/01/23 10:42:25.582
STEP: Wait for the Deployment to create new ReplicaSet 09/01/23 10:42:25.588
STEP: delete the deployment 09/01/23 10:42:26.107
STEP: wait for all rs to be garbage collected 09/01/23 10:42:26.12
STEP: expected 0 pods, got 2 pods 09/01/23 10:42:26.128
STEP: Gathering metrics 09/01/23 10:42:26.664
Sep  1 10:42:26.746: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
Sep  1 10:42:26.750: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.463871ms
Sep  1 10:42:26.750: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
Sep  1 10:42:26.750: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
Sep  1 10:42:26.883: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  1 10:42:26.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8279" for this suite. 09/01/23 10:42:26.889
------------------------------
• [1.346 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:42:25.555
    Sep  1 10:42:25.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename gc 09/01/23 10:42:25.557
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:42:25.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:42:25.579
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 09/01/23 10:42:25.582
    STEP: Wait for the Deployment to create new ReplicaSet 09/01/23 10:42:25.588
    STEP: delete the deployment 09/01/23 10:42:26.107
    STEP: wait for all rs to be garbage collected 09/01/23 10:42:26.12
    STEP: expected 0 pods, got 2 pods 09/01/23 10:42:26.128
    STEP: Gathering metrics 09/01/23 10:42:26.664
    Sep  1 10:42:26.746: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
    Sep  1 10:42:26.750: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.463871ms
    Sep  1 10:42:26.750: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
    Sep  1 10:42:26.750: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
    Sep  1 10:42:26.883: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:42:26.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8279" for this suite. 09/01/23 10:42:26.889
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:42:26.901
Sep  1 10:42:26.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename dns 09/01/23 10:42:26.903
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:42:26.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:42:26.936
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 09/01/23 10:42:26.94
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1279.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1279.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 09/01/23 10:42:26.956
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1279.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1279.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 09/01/23 10:42:26.956
STEP: creating a pod to probe DNS 09/01/23 10:42:26.956
STEP: submitting the pod to kubernetes 09/01/23 10:42:26.956
Sep  1 10:42:26.969: INFO: Waiting up to 15m0s for pod "dns-test-7d9d5c91-a9e3-4aee-a980-af3d7fd8c22a" in namespace "dns-1279" to be "running"
Sep  1 10:42:26.976: INFO: Pod "dns-test-7d9d5c91-a9e3-4aee-a980-af3d7fd8c22a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.140518ms
Sep  1 10:42:28.980: INFO: Pod "dns-test-7d9d5c91-a9e3-4aee-a980-af3d7fd8c22a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010535921s
Sep  1 10:42:30.981: INFO: Pod "dns-test-7d9d5c91-a9e3-4aee-a980-af3d7fd8c22a": Phase="Running", Reason="", readiness=true. Elapsed: 4.010923554s
Sep  1 10:42:30.981: INFO: Pod "dns-test-7d9d5c91-a9e3-4aee-a980-af3d7fd8c22a" satisfied condition "running"
STEP: retrieving the pod 09/01/23 10:42:30.981
STEP: looking for the results for each expected name from probers 09/01/23 10:42:30.983
Sep  1 10:42:31.007: INFO: DNS probes using dns-1279/dns-test-7d9d5c91-a9e3-4aee-a980-af3d7fd8c22a succeeded

STEP: deleting the pod 09/01/23 10:42:31.007
STEP: deleting the test headless service 09/01/23 10:42:31.035
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  1 10:42:31.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1279" for this suite. 09/01/23 10:42:31.118
------------------------------
• [4.231 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:42:26.901
    Sep  1 10:42:26.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename dns 09/01/23 10:42:26.903
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:42:26.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:42:26.936
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 09/01/23 10:42:26.94
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1279.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1279.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     09/01/23 10:42:26.956
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1279.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1279.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     09/01/23 10:42:26.956
    STEP: creating a pod to probe DNS 09/01/23 10:42:26.956
    STEP: submitting the pod to kubernetes 09/01/23 10:42:26.956
    Sep  1 10:42:26.969: INFO: Waiting up to 15m0s for pod "dns-test-7d9d5c91-a9e3-4aee-a980-af3d7fd8c22a" in namespace "dns-1279" to be "running"
    Sep  1 10:42:26.976: INFO: Pod "dns-test-7d9d5c91-a9e3-4aee-a980-af3d7fd8c22a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.140518ms
    Sep  1 10:42:28.980: INFO: Pod "dns-test-7d9d5c91-a9e3-4aee-a980-af3d7fd8c22a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010535921s
    Sep  1 10:42:30.981: INFO: Pod "dns-test-7d9d5c91-a9e3-4aee-a980-af3d7fd8c22a": Phase="Running", Reason="", readiness=true. Elapsed: 4.010923554s
    Sep  1 10:42:30.981: INFO: Pod "dns-test-7d9d5c91-a9e3-4aee-a980-af3d7fd8c22a" satisfied condition "running"
    STEP: retrieving the pod 09/01/23 10:42:30.981
    STEP: looking for the results for each expected name from probers 09/01/23 10:42:30.983
    Sep  1 10:42:31.007: INFO: DNS probes using dns-1279/dns-test-7d9d5c91-a9e3-4aee-a980-af3d7fd8c22a succeeded

    STEP: deleting the pod 09/01/23 10:42:31.007
    STEP: deleting the test headless service 09/01/23 10:42:31.035
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:42:31.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1279" for this suite. 09/01/23 10:42:31.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:42:31.138
Sep  1 10:42:31.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-probe 09/01/23 10:42:31.14
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:42:31.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:42:31.169
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-b830dc4a-0d4c-426d-b0c9-b113450beaa3 in namespace container-probe-877 09/01/23 10:42:31.173
Sep  1 10:42:31.184: INFO: Waiting up to 5m0s for pod "liveness-b830dc4a-0d4c-426d-b0c9-b113450beaa3" in namespace "container-probe-877" to be "not pending"
Sep  1 10:42:31.190: INFO: Pod "liveness-b830dc4a-0d4c-426d-b0c9-b113450beaa3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.722395ms
Sep  1 10:42:33.196: INFO: Pod "liveness-b830dc4a-0d4c-426d-b0c9-b113450beaa3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011986932s
Sep  1 10:42:33.196: INFO: Pod "liveness-b830dc4a-0d4c-426d-b0c9-b113450beaa3" satisfied condition "not pending"
Sep  1 10:42:33.196: INFO: Started pod liveness-b830dc4a-0d4c-426d-b0c9-b113450beaa3 in namespace container-probe-877
STEP: checking the pod's current state and verifying that restartCount is present 09/01/23 10:42:33.196
Sep  1 10:42:33.203: INFO: Initial restart count of pod liveness-b830dc4a-0d4c-426d-b0c9-b113450beaa3 is 0
STEP: deleting the pod 09/01/23 10:46:33.869
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  1 10:46:33.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-877" for this suite. 09/01/23 10:46:33.925
------------------------------
• [SLOW TEST] [242.796 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:42:31.138
    Sep  1 10:42:31.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-probe 09/01/23 10:42:31.14
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:42:31.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:42:31.169
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-b830dc4a-0d4c-426d-b0c9-b113450beaa3 in namespace container-probe-877 09/01/23 10:42:31.173
    Sep  1 10:42:31.184: INFO: Waiting up to 5m0s for pod "liveness-b830dc4a-0d4c-426d-b0c9-b113450beaa3" in namespace "container-probe-877" to be "not pending"
    Sep  1 10:42:31.190: INFO: Pod "liveness-b830dc4a-0d4c-426d-b0c9-b113450beaa3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.722395ms
    Sep  1 10:42:33.196: INFO: Pod "liveness-b830dc4a-0d4c-426d-b0c9-b113450beaa3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011986932s
    Sep  1 10:42:33.196: INFO: Pod "liveness-b830dc4a-0d4c-426d-b0c9-b113450beaa3" satisfied condition "not pending"
    Sep  1 10:42:33.196: INFO: Started pod liveness-b830dc4a-0d4c-426d-b0c9-b113450beaa3 in namespace container-probe-877
    STEP: checking the pod's current state and verifying that restartCount is present 09/01/23 10:42:33.196
    Sep  1 10:42:33.203: INFO: Initial restart count of pod liveness-b830dc4a-0d4c-426d-b0c9-b113450beaa3 is 0
    STEP: deleting the pod 09/01/23 10:46:33.869
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:46:33.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-877" for this suite. 09/01/23 10:46:33.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:46:33.942
Sep  1 10:46:33.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 10:46:33.945
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:46:33.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:46:33.971
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 10:46:33.99
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 10:46:34.655
STEP: Deploying the webhook pod 09/01/23 10:46:34.663
STEP: Wait for the deployment to be ready 09/01/23 10:46:34.678
Sep  1 10:46:34.689: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/01/23 10:46:36.703
STEP: Verifying the service has paired with the endpoint 09/01/23 10:46:36.721
Sep  1 10:46:37.721: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Sep  1 10:46:37.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/01/23 10:46:38.24
STEP: Creating a custom resource that should be denied by the webhook 09/01/23 10:46:38.263
STEP: Creating a custom resource whose deletion would be denied by the webhook 09/01/23 10:46:40.405
STEP: Updating the custom resource with disallowed data should be denied 09/01/23 10:46:40.417
STEP: Deleting the custom resource should be denied 09/01/23 10:46:40.426
STEP: Remove the offending key and value from the custom resource data 09/01/23 10:46:40.434
STEP: Deleting the updated custom resource should be successful 09/01/23 10:46:40.446
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:46:40.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6354" for this suite. 09/01/23 10:46:41.054
STEP: Destroying namespace "webhook-6354-markers" for this suite. 09/01/23 10:46:41.07
------------------------------
• [SLOW TEST] [7.157 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:46:33.942
    Sep  1 10:46:33.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 10:46:33.945
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:46:33.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:46:33.971
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 10:46:33.99
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 10:46:34.655
    STEP: Deploying the webhook pod 09/01/23 10:46:34.663
    STEP: Wait for the deployment to be ready 09/01/23 10:46:34.678
    Sep  1 10:46:34.689: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/01/23 10:46:36.703
    STEP: Verifying the service has paired with the endpoint 09/01/23 10:46:36.721
    Sep  1 10:46:37.721: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Sep  1 10:46:37.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/01/23 10:46:38.24
    STEP: Creating a custom resource that should be denied by the webhook 09/01/23 10:46:38.263
    STEP: Creating a custom resource whose deletion would be denied by the webhook 09/01/23 10:46:40.405
    STEP: Updating the custom resource with disallowed data should be denied 09/01/23 10:46:40.417
    STEP: Deleting the custom resource should be denied 09/01/23 10:46:40.426
    STEP: Remove the offending key and value from the custom resource data 09/01/23 10:46:40.434
    STEP: Deleting the updated custom resource should be successful 09/01/23 10:46:40.446
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:46:40.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6354" for this suite. 09/01/23 10:46:41.054
    STEP: Destroying namespace "webhook-6354-markers" for this suite. 09/01/23 10:46:41.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:46:41.116
Sep  1 10:46:41.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubelet-test 09/01/23 10:46:41.155
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:46:41.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:46:41.206
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Sep  1 10:46:41.229: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsbb4b545e-1a71-4814-a692-98caa173f627" in namespace "kubelet-test-4656" to be "running and ready"
Sep  1 10:46:41.235: INFO: Pod "busybox-readonly-fsbb4b545e-1a71-4814-a692-98caa173f627": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02236ms
Sep  1 10:46:41.235: INFO: The phase of Pod busybox-readonly-fsbb4b545e-1a71-4814-a692-98caa173f627 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:46:43.241: INFO: Pod "busybox-readonly-fsbb4b545e-1a71-4814-a692-98caa173f627": Phase="Running", Reason="", readiness=true. Elapsed: 2.011736315s
Sep  1 10:46:43.241: INFO: The phase of Pod busybox-readonly-fsbb4b545e-1a71-4814-a692-98caa173f627 is Running (Ready = true)
Sep  1 10:46:43.241: INFO: Pod "busybox-readonly-fsbb4b545e-1a71-4814-a692-98caa173f627" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  1 10:46:43.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4656" for this suite. 09/01/23 10:46:43.273
------------------------------
• [2.169 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:46:41.116
    Sep  1 10:46:41.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubelet-test 09/01/23 10:46:41.155
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:46:41.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:46:41.206
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Sep  1 10:46:41.229: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsbb4b545e-1a71-4814-a692-98caa173f627" in namespace "kubelet-test-4656" to be "running and ready"
    Sep  1 10:46:41.235: INFO: Pod "busybox-readonly-fsbb4b545e-1a71-4814-a692-98caa173f627": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02236ms
    Sep  1 10:46:41.235: INFO: The phase of Pod busybox-readonly-fsbb4b545e-1a71-4814-a692-98caa173f627 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:46:43.241: INFO: Pod "busybox-readonly-fsbb4b545e-1a71-4814-a692-98caa173f627": Phase="Running", Reason="", readiness=true. Elapsed: 2.011736315s
    Sep  1 10:46:43.241: INFO: The phase of Pod busybox-readonly-fsbb4b545e-1a71-4814-a692-98caa173f627 is Running (Ready = true)
    Sep  1 10:46:43.241: INFO: Pod "busybox-readonly-fsbb4b545e-1a71-4814-a692-98caa173f627" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:46:43.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4656" for this suite. 09/01/23 10:46:43.273
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:46:43.286
Sep  1 10:46:43.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-runtime 09/01/23 10:46:43.288
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:46:43.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:46:43.307
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 09/01/23 10:46:43.31
STEP: wait for the container to reach Succeeded 09/01/23 10:46:43.324
STEP: get the container status 09/01/23 10:46:47.345
STEP: the container should be terminated 09/01/23 10:46:47.359
STEP: the termination message should be set 09/01/23 10:46:47.359
Sep  1 10:46:47.359: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/01/23 10:46:47.36
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  1 10:46:47.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2353" for this suite. 09/01/23 10:46:47.388
------------------------------
• [4.109 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:46:43.286
    Sep  1 10:46:43.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-runtime 09/01/23 10:46:43.288
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:46:43.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:46:43.307
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 09/01/23 10:46:43.31
    STEP: wait for the container to reach Succeeded 09/01/23 10:46:43.324
    STEP: get the container status 09/01/23 10:46:47.345
    STEP: the container should be terminated 09/01/23 10:46:47.359
    STEP: the termination message should be set 09/01/23 10:46:47.359
    Sep  1 10:46:47.359: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/01/23 10:46:47.36
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:46:47.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2353" for this suite. 09/01/23 10:46:47.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:46:47.398
Sep  1 10:46:47.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 10:46:47.4
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:46:47.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:46:47.425
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Sep  1 10:46:47.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/01/23 10:46:54.343
Sep  1 10:46:54.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-2565 --namespace=crd-publish-openapi-2565 create -f -'
Sep  1 10:46:55.976: INFO: stderr: ""
Sep  1 10:46:55.976: INFO: stdout: "e2e-test-crd-publish-openapi-8236-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  1 10:46:55.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-2565 --namespace=crd-publish-openapi-2565 delete e2e-test-crd-publish-openapi-8236-crds test-cr'
Sep  1 10:46:56.077: INFO: stderr: ""
Sep  1 10:46:56.077: INFO: stdout: "e2e-test-crd-publish-openapi-8236-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep  1 10:46:56.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-2565 --namespace=crd-publish-openapi-2565 apply -f -'
Sep  1 10:46:56.750: INFO: stderr: ""
Sep  1 10:46:56.750: INFO: stdout: "e2e-test-crd-publish-openapi-8236-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  1 10:46:56.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-2565 --namespace=crd-publish-openapi-2565 delete e2e-test-crd-publish-openapi-8236-crds test-cr'
Sep  1 10:46:56.900: INFO: stderr: ""
Sep  1 10:46:56.900: INFO: stdout: "e2e-test-crd-publish-openapi-8236-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/01/23 10:46:56.9
Sep  1 10:46:56.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-2565 explain e2e-test-crd-publish-openapi-8236-crds'
Sep  1 10:46:57.469: INFO: stderr: ""
Sep  1 10:46:57.469: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8236-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:47:02.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2565" for this suite. 09/01/23 10:47:02.251
------------------------------
• [SLOW TEST] [14.863 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:46:47.398
    Sep  1 10:46:47.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 10:46:47.4
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:46:47.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:46:47.425
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Sep  1 10:46:47.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/01/23 10:46:54.343
    Sep  1 10:46:54.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-2565 --namespace=crd-publish-openapi-2565 create -f -'
    Sep  1 10:46:55.976: INFO: stderr: ""
    Sep  1 10:46:55.976: INFO: stdout: "e2e-test-crd-publish-openapi-8236-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep  1 10:46:55.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-2565 --namespace=crd-publish-openapi-2565 delete e2e-test-crd-publish-openapi-8236-crds test-cr'
    Sep  1 10:46:56.077: INFO: stderr: ""
    Sep  1 10:46:56.077: INFO: stdout: "e2e-test-crd-publish-openapi-8236-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Sep  1 10:46:56.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-2565 --namespace=crd-publish-openapi-2565 apply -f -'
    Sep  1 10:46:56.750: INFO: stderr: ""
    Sep  1 10:46:56.750: INFO: stdout: "e2e-test-crd-publish-openapi-8236-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep  1 10:46:56.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-2565 --namespace=crd-publish-openapi-2565 delete e2e-test-crd-publish-openapi-8236-crds test-cr'
    Sep  1 10:46:56.900: INFO: stderr: ""
    Sep  1 10:46:56.900: INFO: stdout: "e2e-test-crd-publish-openapi-8236-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/01/23 10:46:56.9
    Sep  1 10:46:56.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-2565 explain e2e-test-crd-publish-openapi-8236-crds'
    Sep  1 10:46:57.469: INFO: stderr: ""
    Sep  1 10:46:57.469: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8236-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:47:02.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2565" for this suite. 09/01/23 10:47:02.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:47:02.265
Sep  1 10:47:02.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename var-expansion 09/01/23 10:47:02.267
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:02.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:02.291
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Sep  1 10:47:02.306: INFO: Waiting up to 2m0s for pod "var-expansion-88b3cad5-93bb-456f-81fd-24ee5d9a2104" in namespace "var-expansion-6713" to be "container 0 failed with reason CreateContainerConfigError"
Sep  1 10:47:02.313: INFO: Pod "var-expansion-88b3cad5-93bb-456f-81fd-24ee5d9a2104": Phase="Pending", Reason="", readiness=false. Elapsed: 6.961228ms
Sep  1 10:47:04.317: INFO: Pod "var-expansion-88b3cad5-93bb-456f-81fd-24ee5d9a2104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011179948s
Sep  1 10:47:04.317: INFO: Pod "var-expansion-88b3cad5-93bb-456f-81fd-24ee5d9a2104" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep  1 10:47:04.318: INFO: Deleting pod "var-expansion-88b3cad5-93bb-456f-81fd-24ee5d9a2104" in namespace "var-expansion-6713"
Sep  1 10:47:04.328: INFO: Wait up to 5m0s for pod "var-expansion-88b3cad5-93bb-456f-81fd-24ee5d9a2104" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  1 10:47:06.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6713" for this suite. 09/01/23 10:47:06.339
------------------------------
• [4.081 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:47:02.265
    Sep  1 10:47:02.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename var-expansion 09/01/23 10:47:02.267
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:02.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:02.291
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Sep  1 10:47:02.306: INFO: Waiting up to 2m0s for pod "var-expansion-88b3cad5-93bb-456f-81fd-24ee5d9a2104" in namespace "var-expansion-6713" to be "container 0 failed with reason CreateContainerConfigError"
    Sep  1 10:47:02.313: INFO: Pod "var-expansion-88b3cad5-93bb-456f-81fd-24ee5d9a2104": Phase="Pending", Reason="", readiness=false. Elapsed: 6.961228ms
    Sep  1 10:47:04.317: INFO: Pod "var-expansion-88b3cad5-93bb-456f-81fd-24ee5d9a2104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011179948s
    Sep  1 10:47:04.317: INFO: Pod "var-expansion-88b3cad5-93bb-456f-81fd-24ee5d9a2104" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep  1 10:47:04.318: INFO: Deleting pod "var-expansion-88b3cad5-93bb-456f-81fd-24ee5d9a2104" in namespace "var-expansion-6713"
    Sep  1 10:47:04.328: INFO: Wait up to 5m0s for pod "var-expansion-88b3cad5-93bb-456f-81fd-24ee5d9a2104" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:47:06.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6713" for this suite. 09/01/23 10:47:06.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:47:06.347
Sep  1 10:47:06.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename job 09/01/23 10:47:06.35
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:06.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:06.374
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 09/01/23 10:47:06.422
STEP: Ensure pods equal to parallelism count is attached to the job 09/01/23 10:47:06.43
STEP: patching /status 09/01/23 10:47:08.435
STEP: updating /status 09/01/23 10:47:08.445
STEP: get /status 09/01/23 10:47:08.48
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  1 10:47:08.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6097" for this suite. 09/01/23 10:47:08.488
------------------------------
• [2.148 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:47:06.347
    Sep  1 10:47:06.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename job 09/01/23 10:47:06.35
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:06.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:06.374
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 09/01/23 10:47:06.422
    STEP: Ensure pods equal to parallelism count is attached to the job 09/01/23 10:47:06.43
    STEP: patching /status 09/01/23 10:47:08.435
    STEP: updating /status 09/01/23 10:47:08.445
    STEP: get /status 09/01/23 10:47:08.48
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:47:08.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6097" for this suite. 09/01/23 10:47:08.488
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:47:08.501
Sep  1 10:47:08.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 10:47:08.503
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:08.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:08.527
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 09/01/23 10:47:08.532
Sep  1 10:47:08.541: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8" in namespace "downward-api-3327" to be "Succeeded or Failed"
Sep  1 10:47:08.545: INFO: Pod "downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.632237ms
Sep  1 10:47:10.549: INFO: Pod "downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007343257s
Sep  1 10:47:12.553: INFO: Pod "downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011510192s
STEP: Saw pod success 09/01/23 10:47:12.553
Sep  1 10:47:12.554: INFO: Pod "downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8" satisfied condition "Succeeded or Failed"
Sep  1 10:47:12.561: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8 container client-container: <nil>
STEP: delete the pod 09/01/23 10:47:12.58
Sep  1 10:47:12.647: INFO: Waiting for pod downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8 to disappear
Sep  1 10:47:12.653: INFO: Pod downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  1 10:47:12.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3327" for this suite. 09/01/23 10:47:12.659
------------------------------
• [4.172 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:47:08.501
    Sep  1 10:47:08.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 10:47:08.503
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:08.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:08.527
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 09/01/23 10:47:08.532
    Sep  1 10:47:08.541: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8" in namespace "downward-api-3327" to be "Succeeded or Failed"
    Sep  1 10:47:08.545: INFO: Pod "downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.632237ms
    Sep  1 10:47:10.549: INFO: Pod "downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007343257s
    Sep  1 10:47:12.553: INFO: Pod "downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011510192s
    STEP: Saw pod success 09/01/23 10:47:12.553
    Sep  1 10:47:12.554: INFO: Pod "downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8" satisfied condition "Succeeded or Failed"
    Sep  1 10:47:12.561: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8 container client-container: <nil>
    STEP: delete the pod 09/01/23 10:47:12.58
    Sep  1 10:47:12.647: INFO: Waiting for pod downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8 to disappear
    Sep  1 10:47:12.653: INFO: Pod downwardapi-volume-b9fa0b5e-c6ac-47d7-b3fb-ba11f196ddd8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:47:12.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3327" for this suite. 09/01/23 10:47:12.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:47:12.679
Sep  1 10:47:12.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 10:47:12.681
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:12.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:12.719
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 09/01/23 10:47:12.723
Sep  1 10:47:12.723: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9839 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 09/01/23 10:47:12.829
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 10:47:12.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9839" for this suite. 09/01/23 10:47:12.844
------------------------------
• [0.173 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:47:12.679
    Sep  1 10:47:12.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 10:47:12.681
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:12.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:12.719
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 09/01/23 10:47:12.723
    Sep  1 10:47:12.723: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9839 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 09/01/23 10:47:12.829
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:47:12.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9839" for this suite. 09/01/23 10:47:12.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:47:12.853
Sep  1 10:47:12.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 10:47:12.855
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:12.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:12.874
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-06710122-334a-4470-824c-a6bc919b72c6 09/01/23 10:47:12.878
STEP: Creating a pod to test consume configMaps 09/01/23 10:47:12.885
Sep  1 10:47:12.893: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388" in namespace "projected-1868" to be "Succeeded or Failed"
Sep  1 10:47:12.898: INFO: Pod "pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388": Phase="Pending", Reason="", readiness=false. Elapsed: 4.983383ms
Sep  1 10:47:14.904: INFO: Pod "pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010455389s
Sep  1 10:47:16.908: INFO: Pod "pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014191893s
STEP: Saw pod success 09/01/23 10:47:16.908
Sep  1 10:47:16.908: INFO: Pod "pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388" satisfied condition "Succeeded or Failed"
Sep  1 10:47:16.912: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388 container agnhost-container: <nil>
STEP: delete the pod 09/01/23 10:47:16.92
Sep  1 10:47:16.937: INFO: Waiting for pod pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388 to disappear
Sep  1 10:47:16.942: INFO: Pod pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  1 10:47:16.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1868" for this suite. 09/01/23 10:47:16.946
------------------------------
• [4.101 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:47:12.853
    Sep  1 10:47:12.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 10:47:12.855
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:12.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:12.874
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-06710122-334a-4470-824c-a6bc919b72c6 09/01/23 10:47:12.878
    STEP: Creating a pod to test consume configMaps 09/01/23 10:47:12.885
    Sep  1 10:47:12.893: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388" in namespace "projected-1868" to be "Succeeded or Failed"
    Sep  1 10:47:12.898: INFO: Pod "pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388": Phase="Pending", Reason="", readiness=false. Elapsed: 4.983383ms
    Sep  1 10:47:14.904: INFO: Pod "pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010455389s
    Sep  1 10:47:16.908: INFO: Pod "pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014191893s
    STEP: Saw pod success 09/01/23 10:47:16.908
    Sep  1 10:47:16.908: INFO: Pod "pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388" satisfied condition "Succeeded or Failed"
    Sep  1 10:47:16.912: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388 container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 10:47:16.92
    Sep  1 10:47:16.937: INFO: Waiting for pod pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388 to disappear
    Sep  1 10:47:16.942: INFO: Pod pod-projected-configmaps-9c06debc-c178-4f43-9947-2c26712d9388 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:47:16.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1868" for this suite. 09/01/23 10:47:16.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:47:16.954
Sep  1 10:47:16.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename deployment 09/01/23 10:47:16.955
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:16.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:16.977
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 09/01/23 10:47:16.996
Sep  1 10:47:16.996: INFO: Creating simple deployment test-deployment-8bvmm
Sep  1 10:47:17.016: INFO: deployment "test-deployment-8bvmm" doesn't have the required revision set
STEP: Getting /status 09/01/23 10:47:19.034
Sep  1 10:47:19.038: INFO: Deployment test-deployment-8bvmm has Conditions: [{Available True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bvmm-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 09/01/23 10:47:19.039
Sep  1 10:47:19.050: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 47, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 47, 17, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-8bvmm-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 09/01/23 10:47:19.05
Sep  1 10:47:19.053: INFO: Observed &Deployment event: ADDED
Sep  1 10:47:19.053: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8bvmm-54bc444df"}
Sep  1 10:47:19.053: INFO: Observed &Deployment event: MODIFIED
Sep  1 10:47:19.054: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8bvmm-54bc444df"}
Sep  1 10:47:19.054: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  1 10:47:19.054: INFO: Observed &Deployment event: MODIFIED
Sep  1 10:47:19.054: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  1 10:47:19.054: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8bvmm-54bc444df" is progressing.}
Sep  1 10:47:19.055: INFO: Observed &Deployment event: MODIFIED
Sep  1 10:47:19.055: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  1 10:47:19.055: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bvmm-54bc444df" has successfully progressed.}
Sep  1 10:47:19.055: INFO: Observed &Deployment event: MODIFIED
Sep  1 10:47:19.055: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  1 10:47:19.055: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bvmm-54bc444df" has successfully progressed.}
Sep  1 10:47:19.055: INFO: Found Deployment test-deployment-8bvmm in namespace deployment-6311 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  1 10:47:19.055: INFO: Deployment test-deployment-8bvmm has an updated status
STEP: patching the Statefulset Status 09/01/23 10:47:19.055
Sep  1 10:47:19.055: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  1 10:47:19.066: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 09/01/23 10:47:19.066
Sep  1 10:47:19.070: INFO: Observed &Deployment event: ADDED
Sep  1 10:47:19.070: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8bvmm-54bc444df"}
Sep  1 10:47:19.071: INFO: Observed &Deployment event: MODIFIED
Sep  1 10:47:19.071: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8bvmm-54bc444df"}
Sep  1 10:47:19.071: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  1 10:47:19.072: INFO: Observed &Deployment event: MODIFIED
Sep  1 10:47:19.072: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  1 10:47:19.072: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8bvmm-54bc444df" is progressing.}
Sep  1 10:47:19.072: INFO: Observed &Deployment event: MODIFIED
Sep  1 10:47:19.072: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  1 10:47:19.072: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bvmm-54bc444df" has successfully progressed.}
Sep  1 10:47:19.073: INFO: Observed &Deployment event: MODIFIED
Sep  1 10:47:19.073: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  1 10:47:19.073: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bvmm-54bc444df" has successfully progressed.}
Sep  1 10:47:19.073: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  1 10:47:19.073: INFO: Observed &Deployment event: MODIFIED
Sep  1 10:47:19.073: INFO: Found deployment test-deployment-8bvmm in namespace deployment-6311 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Sep  1 10:47:19.073: INFO: Deployment test-deployment-8bvmm has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  1 10:47:19.080: INFO: Deployment "test-deployment-8bvmm":
&Deployment{ObjectMeta:{test-deployment-8bvmm  deployment-6311  9a5dd29a-637b-4290-bad5-3b2397656a7f 15627 1 2023-09-01 10:47:17 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-01 10:47:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-09-01 10:47:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-09-01 10:47:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007b3adc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-8bvmm-54bc444df",LastUpdateTime:2023-09-01 10:47:19 +0000 UTC,LastTransitionTime:2023-09-01 10:47:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  1 10:47:19.085: INFO: New ReplicaSet "test-deployment-8bvmm-54bc444df" of Deployment "test-deployment-8bvmm":
&ReplicaSet{ObjectMeta:{test-deployment-8bvmm-54bc444df  deployment-6311  fbf2193e-ae19-4e54-b379-439615306722 15617 1 2023-09-01 10:47:17 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-8bvmm 9a5dd29a-637b-4290-bad5-3b2397656a7f 0xc007b3b1b0 0xc007b3b1b1}] [] [{kube-controller-manager Update apps/v1 2023-09-01 10:47:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a5dd29a-637b-4290-bad5-3b2397656a7f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:47:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007b3b258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  1 10:47:19.096: INFO: Pod "test-deployment-8bvmm-54bc444df-v5psf" is available:
&Pod{ObjectMeta:{test-deployment-8bvmm-54bc444df-v5psf test-deployment-8bvmm-54bc444df- deployment-6311  cdbf94db-ec61-464c-9ad7-52d5af722e2a 15616 0 2023-09-01 10:47:17 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-8bvmm-54bc444df fbf2193e-ae19-4e54-b379-439615306722 0xc007b3b600 0xc007b3b601}] [] [{kube-controller-manager Update v1 2023-09-01 10:47:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fbf2193e-ae19-4e54-b379-439615306722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 10:47:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:47:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:47:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:47:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:47:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.248,StartTime:2023-09-01 10:47:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 10:47:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c8408ecdcb572884bf33e7a4132ac92f24e295ead6494b6412a4371bec165815,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  1 10:47:19.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6311" for this suite. 09/01/23 10:47:19.102
------------------------------
• [2.158 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:47:16.954
    Sep  1 10:47:16.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename deployment 09/01/23 10:47:16.955
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:16.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:16.977
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 09/01/23 10:47:16.996
    Sep  1 10:47:16.996: INFO: Creating simple deployment test-deployment-8bvmm
    Sep  1 10:47:17.016: INFO: deployment "test-deployment-8bvmm" doesn't have the required revision set
    STEP: Getting /status 09/01/23 10:47:19.034
    Sep  1 10:47:19.038: INFO: Deployment test-deployment-8bvmm has Conditions: [{Available True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bvmm-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 09/01/23 10:47:19.039
    Sep  1 10:47:19.050: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 47, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 47, 17, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-8bvmm-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 09/01/23 10:47:19.05
    Sep  1 10:47:19.053: INFO: Observed &Deployment event: ADDED
    Sep  1 10:47:19.053: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8bvmm-54bc444df"}
    Sep  1 10:47:19.053: INFO: Observed &Deployment event: MODIFIED
    Sep  1 10:47:19.054: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8bvmm-54bc444df"}
    Sep  1 10:47:19.054: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  1 10:47:19.054: INFO: Observed &Deployment event: MODIFIED
    Sep  1 10:47:19.054: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  1 10:47:19.054: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8bvmm-54bc444df" is progressing.}
    Sep  1 10:47:19.055: INFO: Observed &Deployment event: MODIFIED
    Sep  1 10:47:19.055: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  1 10:47:19.055: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bvmm-54bc444df" has successfully progressed.}
    Sep  1 10:47:19.055: INFO: Observed &Deployment event: MODIFIED
    Sep  1 10:47:19.055: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  1 10:47:19.055: INFO: Observed Deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bvmm-54bc444df" has successfully progressed.}
    Sep  1 10:47:19.055: INFO: Found Deployment test-deployment-8bvmm in namespace deployment-6311 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  1 10:47:19.055: INFO: Deployment test-deployment-8bvmm has an updated status
    STEP: patching the Statefulset Status 09/01/23 10:47:19.055
    Sep  1 10:47:19.055: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  1 10:47:19.066: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 09/01/23 10:47:19.066
    Sep  1 10:47:19.070: INFO: Observed &Deployment event: ADDED
    Sep  1 10:47:19.070: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8bvmm-54bc444df"}
    Sep  1 10:47:19.071: INFO: Observed &Deployment event: MODIFIED
    Sep  1 10:47:19.071: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8bvmm-54bc444df"}
    Sep  1 10:47:19.071: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  1 10:47:19.072: INFO: Observed &Deployment event: MODIFIED
    Sep  1 10:47:19.072: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  1 10:47:19.072: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:17 +0000 UTC 2023-09-01 10:47:17 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8bvmm-54bc444df" is progressing.}
    Sep  1 10:47:19.072: INFO: Observed &Deployment event: MODIFIED
    Sep  1 10:47:19.072: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  1 10:47:19.072: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bvmm-54bc444df" has successfully progressed.}
    Sep  1 10:47:19.073: INFO: Observed &Deployment event: MODIFIED
    Sep  1 10:47:19.073: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  1 10:47:19.073: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-01 10:47:18 +0000 UTC 2023-09-01 10:47:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bvmm-54bc444df" has successfully progressed.}
    Sep  1 10:47:19.073: INFO: Observed deployment test-deployment-8bvmm in namespace deployment-6311 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  1 10:47:19.073: INFO: Observed &Deployment event: MODIFIED
    Sep  1 10:47:19.073: INFO: Found deployment test-deployment-8bvmm in namespace deployment-6311 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Sep  1 10:47:19.073: INFO: Deployment test-deployment-8bvmm has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  1 10:47:19.080: INFO: Deployment "test-deployment-8bvmm":
    &Deployment{ObjectMeta:{test-deployment-8bvmm  deployment-6311  9a5dd29a-637b-4290-bad5-3b2397656a7f 15627 1 2023-09-01 10:47:17 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-01 10:47:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-09-01 10:47:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-09-01 10:47:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007b3adc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-8bvmm-54bc444df",LastUpdateTime:2023-09-01 10:47:19 +0000 UTC,LastTransitionTime:2023-09-01 10:47:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  1 10:47:19.085: INFO: New ReplicaSet "test-deployment-8bvmm-54bc444df" of Deployment "test-deployment-8bvmm":
    &ReplicaSet{ObjectMeta:{test-deployment-8bvmm-54bc444df  deployment-6311  fbf2193e-ae19-4e54-b379-439615306722 15617 1 2023-09-01 10:47:17 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-8bvmm 9a5dd29a-637b-4290-bad5-3b2397656a7f 0xc007b3b1b0 0xc007b3b1b1}] [] [{kube-controller-manager Update apps/v1 2023-09-01 10:47:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a5dd29a-637b-4290-bad5-3b2397656a7f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:47:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007b3b258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  1 10:47:19.096: INFO: Pod "test-deployment-8bvmm-54bc444df-v5psf" is available:
    &Pod{ObjectMeta:{test-deployment-8bvmm-54bc444df-v5psf test-deployment-8bvmm-54bc444df- deployment-6311  cdbf94db-ec61-464c-9ad7-52d5af722e2a 15616 0 2023-09-01 10:47:17 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-8bvmm-54bc444df fbf2193e-ae19-4e54-b379-439615306722 0xc007b3b600 0xc007b3b601}] [] [{kube-controller-manager Update v1 2023-09-01 10:47:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fbf2193e-ae19-4e54-b379-439615306722\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 10:47:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:47:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:47:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:47:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:47:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.248,StartTime:2023-09-01 10:47:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 10:47:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c8408ecdcb572884bf33e7a4132ac92f24e295ead6494b6412a4371bec165815,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:47:19.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6311" for this suite. 09/01/23 10:47:19.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:47:19.119
Sep  1 10:47:19.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename gc 09/01/23 10:47:19.121
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:19.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:19.151
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 09/01/23 10:47:19.16
STEP: create the rc2 09/01/23 10:47:19.167
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/01/23 10:47:24.189
STEP: delete the rc simpletest-rc-to-be-deleted 09/01/23 10:47:25.369
STEP: wait for the rc to be deleted 09/01/23 10:47:25.385
Sep  1 10:47:30.457: INFO: 76 pods remaining
Sep  1 10:47:30.457: INFO: 68 pods has nil DeletionTimestamp
Sep  1 10:47:30.457: INFO: 
Sep  1 10:47:35.410: INFO: 57 pods remaining
Sep  1 10:47:35.411: INFO: 50 pods has nil DeletionTimestamp
Sep  1 10:47:35.411: INFO: 
STEP: Gathering metrics 09/01/23 10:47:40.429
Sep  1 10:47:40.505: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
Sep  1 10:47:40.522: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 16.32421ms
Sep  1 10:47:40.522: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
Sep  1 10:47:40.522: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
Sep  1 10:47:40.907: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  1 10:47:40.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fsbl" in namespace "gc-2978"
Sep  1 10:47:40.960: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hfb5" in namespace "gc-2978"
Sep  1 10:47:41.026: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lnk5" in namespace "gc-2978"
Sep  1 10:47:41.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-2r8g2" in namespace "gc-2978"
Sep  1 10:47:41.205: INFO: Deleting pod "simpletest-rc-to-be-deleted-422jc" in namespace "gc-2978"
Sep  1 10:47:41.273: INFO: Deleting pod "simpletest-rc-to-be-deleted-44q4k" in namespace "gc-2978"
Sep  1 10:47:41.345: INFO: Deleting pod "simpletest-rc-to-be-deleted-4h9dp" in namespace "gc-2978"
Sep  1 10:47:41.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fg6g" in namespace "gc-2978"
Sep  1 10:47:41.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-5j86h" in namespace "gc-2978"
Sep  1 10:47:41.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mnq2" in namespace "gc-2978"
Sep  1 10:47:41.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-5sdhb" in namespace "gc-2978"
Sep  1 10:47:41.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-5z5qz" in namespace "gc-2978"
Sep  1 10:47:41.692: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zlzq" in namespace "gc-2978"
Sep  1 10:47:41.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-6m2zt" in namespace "gc-2978"
Sep  1 10:47:41.803: INFO: Deleting pod "simpletest-rc-to-be-deleted-76qvh" in namespace "gc-2978"
Sep  1 10:47:41.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jx99" in namespace "gc-2978"
Sep  1 10:47:41.933: INFO: Deleting pod "simpletest-rc-to-be-deleted-7r64z" in namespace "gc-2978"
Sep  1 10:47:41.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-8pdrk" in namespace "gc-2978"
Sep  1 10:47:42.058: INFO: Deleting pod "simpletest-rc-to-be-deleted-8r667" in namespace "gc-2978"
Sep  1 10:47:42.106: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tvmc" in namespace "gc-2978"
Sep  1 10:47:42.158: INFO: Deleting pod "simpletest-rc-to-be-deleted-96x5z" in namespace "gc-2978"
Sep  1 10:47:42.190: INFO: Deleting pod "simpletest-rc-to-be-deleted-9h8q4" in namespace "gc-2978"
Sep  1 10:47:42.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lxq7" in namespace "gc-2978"
Sep  1 10:47:42.315: INFO: Deleting pod "simpletest-rc-to-be-deleted-9w27n" in namespace "gc-2978"
Sep  1 10:47:42.354: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xfbc" in namespace "gc-2978"
Sep  1 10:47:42.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8t5p" in namespace "gc-2978"
Sep  1 10:47:42.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbpz8" in namespace "gc-2978"
Sep  1 10:47:42.643: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhgqd" in namespace "gc-2978"
Sep  1 10:47:42.700: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmcwk" in namespace "gc-2978"
Sep  1 10:47:42.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqwl6" in namespace "gc-2978"
Sep  1 10:47:42.905: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5qrv" in namespace "gc-2978"
Sep  1 10:47:43.001: INFO: Deleting pod "simpletest-rc-to-be-deleted-chhw9" in namespace "gc-2978"
Sep  1 10:47:43.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-csmfj" in namespace "gc-2978"
Sep  1 10:47:43.201: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxb2b" in namespace "gc-2978"
Sep  1 10:47:43.384: INFO: Deleting pod "simpletest-rc-to-be-deleted-cz42h" in namespace "gc-2978"
Sep  1 10:47:43.511: INFO: Deleting pod "simpletest-rc-to-be-deleted-d28t5" in namespace "gc-2978"
Sep  1 10:47:43.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7ttm" in namespace "gc-2978"
Sep  1 10:47:43.831: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl2sl" in namespace "gc-2978"
Sep  1 10:47:44.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-frzwg" in namespace "gc-2978"
Sep  1 10:47:44.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvgvs" in namespace "gc-2978"
Sep  1 10:47:44.552: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8gg5" in namespace "gc-2978"
Sep  1 10:47:44.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-gb7mp" in namespace "gc-2978"
Sep  1 10:47:44.768: INFO: Deleting pod "simpletest-rc-to-be-deleted-gc57n" in namespace "gc-2978"
Sep  1 10:47:44.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-grlsv" in namespace "gc-2978"
Sep  1 10:47:45.006: INFO: Deleting pod "simpletest-rc-to-be-deleted-hm5fg" in namespace "gc-2978"
Sep  1 10:47:45.366: INFO: Deleting pod "simpletest-rc-to-be-deleted-jb7qz" in namespace "gc-2978"
Sep  1 10:47:45.451: INFO: Deleting pod "simpletest-rc-to-be-deleted-jdxd7" in namespace "gc-2978"
Sep  1 10:47:45.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-jxb4d" in namespace "gc-2978"
Sep  1 10:47:45.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-jxbs8" in namespace "gc-2978"
Sep  1 10:47:46.042: INFO: Deleting pod "simpletest-rc-to-be-deleted-jxkf6" in namespace "gc-2978"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  1 10:47:46.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2978" for this suite. 09/01/23 10:47:46.396
------------------------------
• [SLOW TEST] [27.526 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:47:19.119
    Sep  1 10:47:19.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename gc 09/01/23 10:47:19.121
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:19.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:19.151
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 09/01/23 10:47:19.16
    STEP: create the rc2 09/01/23 10:47:19.167
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/01/23 10:47:24.189
    STEP: delete the rc simpletest-rc-to-be-deleted 09/01/23 10:47:25.369
    STEP: wait for the rc to be deleted 09/01/23 10:47:25.385
    Sep  1 10:47:30.457: INFO: 76 pods remaining
    Sep  1 10:47:30.457: INFO: 68 pods has nil DeletionTimestamp
    Sep  1 10:47:30.457: INFO: 
    Sep  1 10:47:35.410: INFO: 57 pods remaining
    Sep  1 10:47:35.411: INFO: 50 pods has nil DeletionTimestamp
    Sep  1 10:47:35.411: INFO: 
    STEP: Gathering metrics 09/01/23 10:47:40.429
    Sep  1 10:47:40.505: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
    Sep  1 10:47:40.522: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 16.32421ms
    Sep  1 10:47:40.522: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
    Sep  1 10:47:40.522: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
    Sep  1 10:47:40.907: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep  1 10:47:40.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fsbl" in namespace "gc-2978"
    Sep  1 10:47:40.960: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hfb5" in namespace "gc-2978"
    Sep  1 10:47:41.026: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lnk5" in namespace "gc-2978"
    Sep  1 10:47:41.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-2r8g2" in namespace "gc-2978"
    Sep  1 10:47:41.205: INFO: Deleting pod "simpletest-rc-to-be-deleted-422jc" in namespace "gc-2978"
    Sep  1 10:47:41.273: INFO: Deleting pod "simpletest-rc-to-be-deleted-44q4k" in namespace "gc-2978"
    Sep  1 10:47:41.345: INFO: Deleting pod "simpletest-rc-to-be-deleted-4h9dp" in namespace "gc-2978"
    Sep  1 10:47:41.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fg6g" in namespace "gc-2978"
    Sep  1 10:47:41.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-5j86h" in namespace "gc-2978"
    Sep  1 10:47:41.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mnq2" in namespace "gc-2978"
    Sep  1 10:47:41.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-5sdhb" in namespace "gc-2978"
    Sep  1 10:47:41.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-5z5qz" in namespace "gc-2978"
    Sep  1 10:47:41.692: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zlzq" in namespace "gc-2978"
    Sep  1 10:47:41.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-6m2zt" in namespace "gc-2978"
    Sep  1 10:47:41.803: INFO: Deleting pod "simpletest-rc-to-be-deleted-76qvh" in namespace "gc-2978"
    Sep  1 10:47:41.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jx99" in namespace "gc-2978"
    Sep  1 10:47:41.933: INFO: Deleting pod "simpletest-rc-to-be-deleted-7r64z" in namespace "gc-2978"
    Sep  1 10:47:41.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-8pdrk" in namespace "gc-2978"
    Sep  1 10:47:42.058: INFO: Deleting pod "simpletest-rc-to-be-deleted-8r667" in namespace "gc-2978"
    Sep  1 10:47:42.106: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tvmc" in namespace "gc-2978"
    Sep  1 10:47:42.158: INFO: Deleting pod "simpletest-rc-to-be-deleted-96x5z" in namespace "gc-2978"
    Sep  1 10:47:42.190: INFO: Deleting pod "simpletest-rc-to-be-deleted-9h8q4" in namespace "gc-2978"
    Sep  1 10:47:42.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lxq7" in namespace "gc-2978"
    Sep  1 10:47:42.315: INFO: Deleting pod "simpletest-rc-to-be-deleted-9w27n" in namespace "gc-2978"
    Sep  1 10:47:42.354: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xfbc" in namespace "gc-2978"
    Sep  1 10:47:42.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8t5p" in namespace "gc-2978"
    Sep  1 10:47:42.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbpz8" in namespace "gc-2978"
    Sep  1 10:47:42.643: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhgqd" in namespace "gc-2978"
    Sep  1 10:47:42.700: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmcwk" in namespace "gc-2978"
    Sep  1 10:47:42.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqwl6" in namespace "gc-2978"
    Sep  1 10:47:42.905: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5qrv" in namespace "gc-2978"
    Sep  1 10:47:43.001: INFO: Deleting pod "simpletest-rc-to-be-deleted-chhw9" in namespace "gc-2978"
    Sep  1 10:47:43.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-csmfj" in namespace "gc-2978"
    Sep  1 10:47:43.201: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxb2b" in namespace "gc-2978"
    Sep  1 10:47:43.384: INFO: Deleting pod "simpletest-rc-to-be-deleted-cz42h" in namespace "gc-2978"
    Sep  1 10:47:43.511: INFO: Deleting pod "simpletest-rc-to-be-deleted-d28t5" in namespace "gc-2978"
    Sep  1 10:47:43.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7ttm" in namespace "gc-2978"
    Sep  1 10:47:43.831: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl2sl" in namespace "gc-2978"
    Sep  1 10:47:44.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-frzwg" in namespace "gc-2978"
    Sep  1 10:47:44.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvgvs" in namespace "gc-2978"
    Sep  1 10:47:44.552: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8gg5" in namespace "gc-2978"
    Sep  1 10:47:44.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-gb7mp" in namespace "gc-2978"
    Sep  1 10:47:44.768: INFO: Deleting pod "simpletest-rc-to-be-deleted-gc57n" in namespace "gc-2978"
    Sep  1 10:47:44.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-grlsv" in namespace "gc-2978"
    Sep  1 10:47:45.006: INFO: Deleting pod "simpletest-rc-to-be-deleted-hm5fg" in namespace "gc-2978"
    Sep  1 10:47:45.366: INFO: Deleting pod "simpletest-rc-to-be-deleted-jb7qz" in namespace "gc-2978"
    Sep  1 10:47:45.451: INFO: Deleting pod "simpletest-rc-to-be-deleted-jdxd7" in namespace "gc-2978"
    Sep  1 10:47:45.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-jxb4d" in namespace "gc-2978"
    Sep  1 10:47:45.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-jxbs8" in namespace "gc-2978"
    Sep  1 10:47:46.042: INFO: Deleting pod "simpletest-rc-to-be-deleted-jxkf6" in namespace "gc-2978"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:47:46.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2978" for this suite. 09/01/23 10:47:46.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:47:46.677
Sep  1 10:47:46.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename daemonsets 09/01/23 10:47:46.709
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:47.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:47.222
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
Sep  1 10:47:47.573: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 09/01/23 10:47:47.612
Sep  1 10:47:47.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:47:47.649: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 09/01/23 10:47:47.649
Sep  1 10:47:47.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:47:47.831: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:47:48.853: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:47:48.853: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:47:49.845: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:47:49.845: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:47:50.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:47:50.838: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:47:51.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:47:51.839: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:47:52.879: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:47:52.879: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:47:54.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:47:54.076: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:47:54.907: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:47:54.908: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:47:55.849: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:47:55.850: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:47:56.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:47:56.838: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:47:57.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:47:57.836: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:47:58.836: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:47:58.837: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:47:59.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 10:47:59.836: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 09/01/23 10:47:59.839
Sep  1 10:47:59.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 10:47:59.861: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Sep  1 10:48:00.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:48:00.866: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/01/23 10:48:00.866
Sep  1 10:48:00.882: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:48:00.882: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:48:01.887: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:48:01.887: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:48:02.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 10:48:02.886: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/01/23 10:48:02.893
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3544, will wait for the garbage collector to delete the pods 09/01/23 10:48:02.893
Sep  1 10:48:02.954: INFO: Deleting DaemonSet.extensions daemon-set took: 7.332551ms
Sep  1 10:48:03.055: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.680374ms
Sep  1 10:48:05.360: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:48:05.360: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  1 10:48:05.363: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17414"},"items":null}

Sep  1 10:48:05.366: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17414"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:48:05.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3544" for this suite. 09/01/23 10:48:05.406
------------------------------
• [SLOW TEST] [18.737 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:47:46.677
    Sep  1 10:47:46.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename daemonsets 09/01/23 10:47:46.709
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:47:47.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:47:47.222
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:205
    Sep  1 10:47:47.573: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 09/01/23 10:47:47.612
    Sep  1 10:47:47.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:47:47.649: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 09/01/23 10:47:47.649
    Sep  1 10:47:47.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:47:47.831: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:47:48.853: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:47:48.853: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:47:49.845: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:47:49.845: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:47:50.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:47:50.838: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:47:51.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:47:51.839: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:47:52.879: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:47:52.879: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:47:54.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:47:54.076: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:47:54.907: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:47:54.908: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:47:55.849: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:47:55.850: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:47:56.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:47:56.838: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:47:57.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:47:57.836: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:47:58.836: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:47:58.837: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:47:59.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 10:47:59.836: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 09/01/23 10:47:59.839
    Sep  1 10:47:59.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 10:47:59.861: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Sep  1 10:48:00.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:48:00.866: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/01/23 10:48:00.866
    Sep  1 10:48:00.882: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:48:00.882: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:48:01.887: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:48:01.887: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:48:02.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 10:48:02.886: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/01/23 10:48:02.893
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3544, will wait for the garbage collector to delete the pods 09/01/23 10:48:02.893
    Sep  1 10:48:02.954: INFO: Deleting DaemonSet.extensions daemon-set took: 7.332551ms
    Sep  1 10:48:03.055: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.680374ms
    Sep  1 10:48:05.360: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:48:05.360: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  1 10:48:05.363: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17414"},"items":null}

    Sep  1 10:48:05.366: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17414"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:48:05.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3544" for this suite. 09/01/23 10:48:05.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:48:05.423
Sep  1 10:48:05.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 10:48:05.428
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:48:05.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:48:05.465
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 10:48:05.489
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 10:48:05.846
STEP: Deploying the webhook pod 09/01/23 10:48:05.853
STEP: Wait for the deployment to be ready 09/01/23 10:48:05.869
Sep  1 10:48:05.875: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 09/01/23 10:48:07.886
STEP: Verifying the service has paired with the endpoint 09/01/23 10:48:07.902
Sep  1 10:48:08.903: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/01/23 10:48:08.908
STEP: create a pod that should be updated by the webhook 09/01/23 10:48:08.927
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:48:08.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7385" for this suite. 09/01/23 10:48:09.042
STEP: Destroying namespace "webhook-7385-markers" for this suite. 09/01/23 10:48:09.06
------------------------------
• [3.654 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:48:05.423
    Sep  1 10:48:05.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 10:48:05.428
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:48:05.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:48:05.465
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 10:48:05.489
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 10:48:05.846
    STEP: Deploying the webhook pod 09/01/23 10:48:05.853
    STEP: Wait for the deployment to be ready 09/01/23 10:48:05.869
    Sep  1 10:48:05.875: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 09/01/23 10:48:07.886
    STEP: Verifying the service has paired with the endpoint 09/01/23 10:48:07.902
    Sep  1 10:48:08.903: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/01/23 10:48:08.908
    STEP: create a pod that should be updated by the webhook 09/01/23 10:48:08.927
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:48:08.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7385" for this suite. 09/01/23 10:48:09.042
    STEP: Destroying namespace "webhook-7385-markers" for this suite. 09/01/23 10:48:09.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:48:09.079
Sep  1 10:48:09.079: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename disruption 09/01/23 10:48:09.081
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:48:09.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:48:09.118
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 09/01/23 10:48:09.124
STEP: Waiting for the pdb to be processed 09/01/23 10:48:09.13
STEP: First trying to evict a pod which shouldn't be evictable 09/01/23 10:48:09.165
STEP: Waiting for all pods to be running 09/01/23 10:48:09.165
Sep  1 10:48:09.169: INFO: pods: 0 < 3
Sep  1 10:48:11.173: INFO: running pods: 0 < 3
STEP: locating a running pod 09/01/23 10:48:13.174
STEP: Updating the pdb to allow a pod to be evicted 09/01/23 10:48:13.184
STEP: Waiting for the pdb to be processed 09/01/23 10:48:13.194
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/01/23 10:48:13.201
STEP: Waiting for all pods to be running 09/01/23 10:48:13.201
STEP: Waiting for the pdb to observed all healthy pods 09/01/23 10:48:13.205
STEP: Patching the pdb to disallow a pod to be evicted 09/01/23 10:48:13.237
STEP: Waiting for the pdb to be processed 09/01/23 10:48:13.258
STEP: Waiting for all pods to be running 09/01/23 10:48:15.266
STEP: locating a running pod 09/01/23 10:48:15.269
STEP: Deleting the pdb to allow a pod to be evicted 09/01/23 10:48:15.281
STEP: Waiting for the pdb to be deleted 09/01/23 10:48:15.286
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/01/23 10:48:15.289
STEP: Waiting for all pods to be running 09/01/23 10:48:15.289
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  1 10:48:15.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9340" for this suite. 09/01/23 10:48:15.325
------------------------------
• [SLOW TEST] [6.271 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:48:09.079
    Sep  1 10:48:09.079: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename disruption 09/01/23 10:48:09.081
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:48:09.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:48:09.118
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 09/01/23 10:48:09.124
    STEP: Waiting for the pdb to be processed 09/01/23 10:48:09.13
    STEP: First trying to evict a pod which shouldn't be evictable 09/01/23 10:48:09.165
    STEP: Waiting for all pods to be running 09/01/23 10:48:09.165
    Sep  1 10:48:09.169: INFO: pods: 0 < 3
    Sep  1 10:48:11.173: INFO: running pods: 0 < 3
    STEP: locating a running pod 09/01/23 10:48:13.174
    STEP: Updating the pdb to allow a pod to be evicted 09/01/23 10:48:13.184
    STEP: Waiting for the pdb to be processed 09/01/23 10:48:13.194
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/01/23 10:48:13.201
    STEP: Waiting for all pods to be running 09/01/23 10:48:13.201
    STEP: Waiting for the pdb to observed all healthy pods 09/01/23 10:48:13.205
    STEP: Patching the pdb to disallow a pod to be evicted 09/01/23 10:48:13.237
    STEP: Waiting for the pdb to be processed 09/01/23 10:48:13.258
    STEP: Waiting for all pods to be running 09/01/23 10:48:15.266
    STEP: locating a running pod 09/01/23 10:48:15.269
    STEP: Deleting the pdb to allow a pod to be evicted 09/01/23 10:48:15.281
    STEP: Waiting for the pdb to be deleted 09/01/23 10:48:15.286
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/01/23 10:48:15.289
    STEP: Waiting for all pods to be running 09/01/23 10:48:15.289
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:48:15.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9340" for this suite. 09/01/23 10:48:15.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:48:15.357
Sep  1 10:48:15.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename taint-multiple-pods 09/01/23 10:48:15.359
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:48:15.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:48:15.394
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Sep  1 10:48:15.397: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  1 10:49:15.506: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Sep  1 10:49:15.543: INFO: Starting informer...
STEP: Starting pods... 09/01/23 10:49:15.543
Sep  1 10:49:15.808: INFO: Pod1 is running on k8s-worker-1.c.operations-lab.internal. Tainting Node
Sep  1 10:49:16.018: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7741" to be "running"
Sep  1 10:49:16.022: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.328405ms
Sep  1 10:49:18.028: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009968251s
Sep  1 10:49:20.026: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.00752711s
Sep  1 10:49:20.026: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Sep  1 10:49:20.026: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7741" to be "running"
Sep  1 10:49:20.029: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.763965ms
Sep  1 10:49:20.029: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Sep  1 10:49:20.029: INFO: Pod2 is running on k8s-worker-1.c.operations-lab.internal. Tainting Node
STEP: Trying to apply a taint on the Node 09/01/23 10:49:20.029
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/01/23 10:49:20.043
STEP: Waiting for Pod1 and Pod2 to be deleted 09/01/23 10:49:20.047
Sep  1 10:49:25.919: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep  1 10:49:45.960: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/01/23 10:49:45.975
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:49:45.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-7741" for this suite. 09/01/23 10:49:46.004
------------------------------
• [SLOW TEST] [90.663 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:48:15.357
    Sep  1 10:48:15.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename taint-multiple-pods 09/01/23 10:48:15.359
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:48:15.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:48:15.394
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Sep  1 10:48:15.397: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  1 10:49:15.506: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Sep  1 10:49:15.543: INFO: Starting informer...
    STEP: Starting pods... 09/01/23 10:49:15.543
    Sep  1 10:49:15.808: INFO: Pod1 is running on k8s-worker-1.c.operations-lab.internal. Tainting Node
    Sep  1 10:49:16.018: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7741" to be "running"
    Sep  1 10:49:16.022: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.328405ms
    Sep  1 10:49:18.028: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009968251s
    Sep  1 10:49:20.026: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.00752711s
    Sep  1 10:49:20.026: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Sep  1 10:49:20.026: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7741" to be "running"
    Sep  1 10:49:20.029: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.763965ms
    Sep  1 10:49:20.029: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Sep  1 10:49:20.029: INFO: Pod2 is running on k8s-worker-1.c.operations-lab.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 09/01/23 10:49:20.029
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/01/23 10:49:20.043
    STEP: Waiting for Pod1 and Pod2 to be deleted 09/01/23 10:49:20.047
    Sep  1 10:49:25.919: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Sep  1 10:49:45.960: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/01/23 10:49:45.975
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:49:45.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-7741" for this suite. 09/01/23 10:49:46.004
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:49:46.023
Sep  1 10:49:46.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename containers 09/01/23 10:49:46.025
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:49:46.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:49:46.054
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 09/01/23 10:49:46.058
Sep  1 10:49:46.065: INFO: Waiting up to 5m0s for pod "client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2" in namespace "containers-6095" to be "Succeeded or Failed"
Sep  1 10:49:46.074: INFO: Pod "client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.723316ms
Sep  1 10:49:48.079: INFO: Pod "client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013949238s
Sep  1 10:49:50.086: INFO: Pod "client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020603217s
Sep  1 10:49:52.078: INFO: Pod "client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012864722s
STEP: Saw pod success 09/01/23 10:49:52.079
Sep  1 10:49:52.080: INFO: Pod "client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2" satisfied condition "Succeeded or Failed"
Sep  1 10:49:52.083: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2 container agnhost-container: <nil>
STEP: delete the pod 09/01/23 10:49:52.112
Sep  1 10:49:52.127: INFO: Waiting for pod client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2 to disappear
Sep  1 10:49:52.131: INFO: Pod client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  1 10:49:52.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6095" for this suite. 09/01/23 10:49:52.138
------------------------------
• [SLOW TEST] [6.124 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:49:46.023
    Sep  1 10:49:46.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename containers 09/01/23 10:49:46.025
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:49:46.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:49:46.054
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 09/01/23 10:49:46.058
    Sep  1 10:49:46.065: INFO: Waiting up to 5m0s for pod "client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2" in namespace "containers-6095" to be "Succeeded or Failed"
    Sep  1 10:49:46.074: INFO: Pod "client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.723316ms
    Sep  1 10:49:48.079: INFO: Pod "client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013949238s
    Sep  1 10:49:50.086: INFO: Pod "client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020603217s
    Sep  1 10:49:52.078: INFO: Pod "client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012864722s
    STEP: Saw pod success 09/01/23 10:49:52.079
    Sep  1 10:49:52.080: INFO: Pod "client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2" satisfied condition "Succeeded or Failed"
    Sep  1 10:49:52.083: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2 container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 10:49:52.112
    Sep  1 10:49:52.127: INFO: Waiting for pod client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2 to disappear
    Sep  1 10:49:52.131: INFO: Pod client-containers-f1024f75-ee7f-4cce-a2f4-0f625e2ebda2 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:49:52.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6095" for this suite. 09/01/23 10:49:52.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:49:52.16
Sep  1 10:49:52.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename namespaces 09/01/23 10:49:52.163
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:49:52.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:49:52.187
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-tlddf" 09/01/23 10:49:52.192
Sep  1 10:49:52.206: INFO: Namespace "e2e-ns-tlddf-3729" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-tlddf-3729" 09/01/23 10:49:52.207
Sep  1 10:49:52.217: INFO: Namespace "e2e-ns-tlddf-3729" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-tlddf-3729" 09/01/23 10:49:52.217
Sep  1 10:49:52.226: INFO: Namespace "e2e-ns-tlddf-3729" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:49:52.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1870" for this suite. 09/01/23 10:49:52.234
STEP: Destroying namespace "e2e-ns-tlddf-3729" for this suite. 09/01/23 10:49:52.24
------------------------------
• [0.088 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:49:52.16
    Sep  1 10:49:52.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename namespaces 09/01/23 10:49:52.163
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:49:52.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:49:52.187
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-tlddf" 09/01/23 10:49:52.192
    Sep  1 10:49:52.206: INFO: Namespace "e2e-ns-tlddf-3729" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-tlddf-3729" 09/01/23 10:49:52.207
    Sep  1 10:49:52.217: INFO: Namespace "e2e-ns-tlddf-3729" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-tlddf-3729" 09/01/23 10:49:52.217
    Sep  1 10:49:52.226: INFO: Namespace "e2e-ns-tlddf-3729" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:49:52.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1870" for this suite. 09/01/23 10:49:52.234
    STEP: Destroying namespace "e2e-ns-tlddf-3729" for this suite. 09/01/23 10:49:52.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:49:52.256
Sep  1 10:49:52.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 10:49:52.258
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:49:52.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:49:52.286
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 09/01/23 10:49:52.295
Sep  1 10:49:52.305: INFO: Waiting up to 5m0s for pod "pod-3c3191a1-f080-43a6-a238-4de9072f58c1" in namespace "emptydir-9084" to be "Succeeded or Failed"
Sep  1 10:49:52.309: INFO: Pod "pod-3c3191a1-f080-43a6-a238-4de9072f58c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.355303ms
Sep  1 10:49:54.313: INFO: Pod "pod-3c3191a1-f080-43a6-a238-4de9072f58c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007375874s
Sep  1 10:49:56.314: INFO: Pod "pod-3c3191a1-f080-43a6-a238-4de9072f58c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008791255s
STEP: Saw pod success 09/01/23 10:49:56.314
Sep  1 10:49:56.314: INFO: Pod "pod-3c3191a1-f080-43a6-a238-4de9072f58c1" satisfied condition "Succeeded or Failed"
Sep  1 10:49:56.318: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-3c3191a1-f080-43a6-a238-4de9072f58c1 container test-container: <nil>
STEP: delete the pod 09/01/23 10:49:56.323
Sep  1 10:49:56.334: INFO: Waiting for pod pod-3c3191a1-f080-43a6-a238-4de9072f58c1 to disappear
Sep  1 10:49:56.338: INFO: Pod pod-3c3191a1-f080-43a6-a238-4de9072f58c1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 10:49:56.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9084" for this suite. 09/01/23 10:49:56.342
------------------------------
• [4.094 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:49:52.256
    Sep  1 10:49:52.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 10:49:52.258
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:49:52.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:49:52.286
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 09/01/23 10:49:52.295
    Sep  1 10:49:52.305: INFO: Waiting up to 5m0s for pod "pod-3c3191a1-f080-43a6-a238-4de9072f58c1" in namespace "emptydir-9084" to be "Succeeded or Failed"
    Sep  1 10:49:52.309: INFO: Pod "pod-3c3191a1-f080-43a6-a238-4de9072f58c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.355303ms
    Sep  1 10:49:54.313: INFO: Pod "pod-3c3191a1-f080-43a6-a238-4de9072f58c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007375874s
    Sep  1 10:49:56.314: INFO: Pod "pod-3c3191a1-f080-43a6-a238-4de9072f58c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008791255s
    STEP: Saw pod success 09/01/23 10:49:56.314
    Sep  1 10:49:56.314: INFO: Pod "pod-3c3191a1-f080-43a6-a238-4de9072f58c1" satisfied condition "Succeeded or Failed"
    Sep  1 10:49:56.318: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-3c3191a1-f080-43a6-a238-4de9072f58c1 container test-container: <nil>
    STEP: delete the pod 09/01/23 10:49:56.323
    Sep  1 10:49:56.334: INFO: Waiting for pod pod-3c3191a1-f080-43a6-a238-4de9072f58c1 to disappear
    Sep  1 10:49:56.338: INFO: Pod pod-3c3191a1-f080-43a6-a238-4de9072f58c1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:49:56.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9084" for this suite. 09/01/23 10:49:56.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:49:56.351
Sep  1 10:49:56.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename security-context-test 09/01/23 10:49:56.353
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:49:56.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:49:56.372
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Sep  1 10:49:56.383: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5" in namespace "security-context-test-3165" to be "Succeeded or Failed"
Sep  1 10:49:56.386: INFO: Pod "alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.923238ms
Sep  1 10:49:58.392: INFO: Pod "alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008417698s
Sep  1 10:50:00.391: INFO: Pod "alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008162287s
Sep  1 10:50:02.390: INFO: Pod "alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006957975s
Sep  1 10:50:02.390: INFO: Pod "alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  1 10:50:02.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3165" for this suite. 09/01/23 10:50:02.401
------------------------------
• [SLOW TEST] [6.057 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:49:56.351
    Sep  1 10:49:56.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename security-context-test 09/01/23 10:49:56.353
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:49:56.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:49:56.372
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Sep  1 10:49:56.383: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5" in namespace "security-context-test-3165" to be "Succeeded or Failed"
    Sep  1 10:49:56.386: INFO: Pod "alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.923238ms
    Sep  1 10:49:58.392: INFO: Pod "alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008417698s
    Sep  1 10:50:00.391: INFO: Pod "alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008162287s
    Sep  1 10:50:02.390: INFO: Pod "alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006957975s
    Sep  1 10:50:02.390: INFO: Pod "alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:50:02.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3165" for this suite. 09/01/23 10:50:02.401
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:50:02.415
Sep  1 10:50:02.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename sched-pred 09/01/23 10:50:02.417
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:50:02.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:50:02.435
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  1 10:50:02.439: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  1 10:50:02.446: INFO: Waiting for terminating namespaces to be deleted...
Sep  1 10:50:02.450: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
Sep  1 10:50:02.460: INFO: cilium-operator-858666d4b6-t7brd from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.460: INFO: 	Container cilium-operator ready: true, restart count 0
Sep  1 10:50:02.460: INFO: cilium-q8wf6 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.460: INFO: 	Container cilium-agent ready: true, restart count 0
Sep  1 10:50:02.460: INFO: kube-proxy-wdplh from kube-system started at 2023-09-01 10:08:39 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.460: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  1 10:50:02.460: INFO: kyverno-cleanup-reports-28226090-9hzwf from kyverno-system started at 2023-09-01 10:50:00 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.460: INFO: 	Container cleanup ready: false, restart count 0
Sep  1 10:50:02.460: INFO: fluentbit-fluentbit-z77m2 from logging-system started at 2023-09-01 10:49:46 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.460: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  1 10:50:02.460: INFO: logging-fluentd-0 from logging-system started at 2023-09-01 10:49:45 +0000 UTC (2 container statuses recorded)
Sep  1 10:50:02.460: INFO: 	Container config-reloader ready: true, restart count 0
Sep  1 10:50:02.460: INFO: 	Container fluentd ready: true, restart count 0
Sep  1 10:50:02.460: INFO: node-exporter-zgtk4 from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.460: INFO: 	Container node-exporter ready: true, restart count 0
Sep  1 10:50:02.460: INFO: alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5 from security-context-test-3165 started at 2023-09-01 10:49:56 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.460: INFO: 	Container alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5 ready: false, restart count 0
Sep  1 10:50:02.460: INFO: sonobuoy from sonobuoy started at 2023-09-01 10:29:08 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.460: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  1 10:50:02.460: INFO: sonobuoy-e2e-job-2bf224d5c7cf4759 from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
Sep  1 10:50:02.460: INFO: 	Container e2e ready: true, restart count 0
Sep  1 10:50:02.460: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  1 10:50:02.460: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-v249x from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
Sep  1 10:50:02.461: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  1 10:50:02.461: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  1 10:50:02.461: INFO: webhook-to-be-mutated from webhook-7385 started at 2023-09-01 10:48:08 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.461: INFO: 	Container example ready: false, restart count 0
Sep  1 10:50:02.461: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
Sep  1 10:50:02.475: INFO: cert-manager-66f9685c7f-jlr6s from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.475: INFO: 	Container cert-manager ready: true, restart count 0
Sep  1 10:50:02.475: INFO: cert-manager-cainjector-6cfc589789-78hkg from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.475: INFO: 	Container cainjector ready: true, restart count 0
Sep  1 10:50:02.475: INFO: cert-manager-webhook-59f6664d4d-cfggr from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.475: INFO: 	Container webhook ready: true, restart count 0
Sep  1 10:50:02.475: INFO: rs-8b59k from disruption-9340 started at 2023-09-01 10:48:15 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.476: INFO: 	Container donothing ready: false, restart count 0
Sep  1 10:50:02.476: INFO: minio-6c8d455566-dvvwz from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.476: INFO: 	Container minio ready: true, restart count 0
Sep  1 10:50:02.476: INFO: velero-57c7d7c6c4-tlzqn from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.476: INFO: 	Container velero ready: true, restart count 0
Sep  1 10:50:02.477: INFO: traefik-7cf5b5b95f-97lpm from ingress-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.477: INFO: 	Container traefik ready: true, restart count 0
Sep  1 10:50:02.477: INFO: kube-green-546cd595c4-mjs5l from kube-green-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.477: INFO: 	Container kube-green ready: true, restart count 0
Sep  1 10:50:02.477: INFO: cilium-64mb8 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.477: INFO: 	Container cilium-agent ready: true, restart count 0
Sep  1 10:50:02.478: INFO: cilium-operator-858666d4b6-mqxc6 from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.478: INFO: 	Container cilium-operator ready: true, restart count 0
Sep  1 10:50:02.478: INFO: coredns-787d4945fb-hw6lt from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.478: INFO: 	Container coredns ready: true, restart count 0
Sep  1 10:50:02.478: INFO: coredns-787d4945fb-zc797 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.478: INFO: 	Container coredns ready: true, restart count 0
Sep  1 10:50:02.479: INFO: hubble-relay-7956d48fc8-dcjfd from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.479: INFO: 	Container hubble-relay ready: true, restart count 0
Sep  1 10:50:02.479: INFO: hubble-ui-b86cb9bf7-7bzl2 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (2 container statuses recorded)
Sep  1 10:50:02.479: INFO: 	Container backend ready: true, restart count 0
Sep  1 10:50:02.479: INFO: 	Container frontend ready: true, restart count 0
Sep  1 10:50:02.479: INFO: kube-proxy-vfc2w from kube-system started at 2023-09-01 10:08:32 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.480: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  1 10:50:02.480: INFO: kyverno-5bbbd994c9-kdf4j from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.480: INFO: 	Container kyverno ready: true, restart count 0
Sep  1 10:50:02.480: INFO: kyverno-background-54b49bbb5d-wbc6r from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.480: INFO: 	Container kyverno-background ready: true, restart count 0
Sep  1 10:50:02.480: INFO: kyverno-cleanup-f5b6cdd5b-jgfw6 from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.481: INFO: 	Container kyverno-cleanup ready: true, restart count 0
Sep  1 10:50:02.481: INFO: kyverno-reports-b48cb544f-2sb6h from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.481: INFO: 	Container kyverno-reports ready: true, restart count 0
Sep  1 10:50:02.481: INFO: local-path-provisioner-5d8858f87d-gtwb4 from local-path-storage started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.481: INFO: 	Container local-path-provisioner ready: true, restart count 0
Sep  1 10:50:02.481: INFO: fluentbit-fluentbit-7986p from logging-system started at 2023-09-01 10:23:38 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.481: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  1 10:50:02.482: INFO: logging-operator-5df74f78f5-zzgl8 from logging-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.482: INFO: 	Container logging-operator ready: true, restart count 0
Sep  1 10:50:02.482: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-09-01 10:49:23 +0000 UTC (2 container statuses recorded)
Sep  1 10:50:02.482: INFO: 	Container alertmanager ready: true, restart count 0
Sep  1 10:50:02.482: INFO: 	Container config-reloader ready: true, restart count 0
Sep  1 10:50:02.482: INFO: kube-state-metrics-8447695667-frzrq from monitoring-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.483: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  1 10:50:02.483: INFO: node-exporter-vwctl from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.483: INFO: 	Container node-exporter ready: true, restart count 0
Sep  1 10:50:02.483: INFO: prometheus-operator-75f79b8c5d-pjwn9 from monitoring-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.483: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  1 10:50:02.483: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-09-01 10:49:22 +0000 UTC (2 container statuses recorded)
Sep  1 10:50:02.484: INFO: 	Container config-reloader ready: true, restart count 0
Sep  1 10:50:02.484: INFO: 	Container prometheus ready: true, restart count 0
Sep  1 10:50:02.484: INFO: rbac-manager-58f6dc584b-xgdd9 from rbac-manager-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:50:02.484: INFO: 	Container rbac-manager ready: true, restart count 0
Sep  1 10:50:02.484: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-mqksc from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
Sep  1 10:50:02.485: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  1 10:50:02.485: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/01/23 10:50:02.485
Sep  1 10:50:02.498: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9154" to be "running"
Sep  1 10:50:02.501: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.112323ms
Sep  1 10:50:04.505: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007006922s
Sep  1 10:50:04.505: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/01/23 10:50:04.508
STEP: Trying to apply a random label on the found node. 09/01/23 10:50:04.522
STEP: verifying the node has the label kubernetes.io/e2e-9ebb6527-4923-4a43-9944-b5b0e9b31ca8 95 09/01/23 10:50:04.557
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/01/23 10:50:04.564
Sep  1 10:50:04.570: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-9154" to be "not pending"
Sep  1 10:50:04.584: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.392605ms
Sep  1 10:50:06.590: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018724488s
Sep  1 10:50:06.590: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.16.0.3 on the node which pod4 resides and expect not scheduled 09/01/23 10:50:06.59
Sep  1 10:50:06.596: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-9154" to be "not pending"
Sep  1 10:50:06.599: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.672954ms
Sep  1 10:50:08.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007470682s
Sep  1 10:50:10.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007645002s
Sep  1 10:50:12.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00710533s
Sep  1 10:50:14.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006426789s
Sep  1 10:50:16.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007728856s
Sep  1 10:50:18.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00675796s
Sep  1 10:50:20.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006790025s
Sep  1 10:50:22.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008266896s
Sep  1 10:50:24.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006630737s
Sep  1 10:50:26.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007233761s
Sep  1 10:50:28.606: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009822545s
Sep  1 10:50:30.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006647145s
Sep  1 10:50:32.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007162627s
Sep  1 10:50:34.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006195596s
Sep  1 10:50:36.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006336194s
Sep  1 10:50:38.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00708935s
Sep  1 10:50:40.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008519725s
Sep  1 10:50:42.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00825034s
Sep  1 10:50:44.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006424478s
Sep  1 10:50:46.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006385488s
Sep  1 10:50:48.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006866994s
Sep  1 10:50:50.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.00753136s
Sep  1 10:50:52.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008256221s
Sep  1 10:50:54.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006572441s
Sep  1 10:50:56.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00786439s
Sep  1 10:50:58.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006362526s
Sep  1 10:51:00.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.006964201s
Sep  1 10:51:02.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007479043s
Sep  1 10:51:04.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0081766s
Sep  1 10:51:06.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007547964s
Sep  1 10:51:08.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008216891s
Sep  1 10:51:10.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007328359s
Sep  1 10:51:12.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.00870228s
Sep  1 10:51:14.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006153677s
Sep  1 10:51:16.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007319309s
Sep  1 10:51:18.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007236752s
Sep  1 10:51:20.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006560652s
Sep  1 10:51:22.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008101179s
Sep  1 10:51:24.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006627448s
Sep  1 10:51:26.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007996315s
Sep  1 10:51:28.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007873665s
Sep  1 10:51:30.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007723738s
Sep  1 10:51:32.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007344099s
Sep  1 10:51:34.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.00767766s
Sep  1 10:51:36.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008077844s
Sep  1 10:51:38.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007379579s
Sep  1 10:51:40.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006000095s
Sep  1 10:51:42.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005935195s
Sep  1 10:51:44.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.007208737s
Sep  1 10:51:46.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008195049s
Sep  1 10:51:48.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008099227s
Sep  1 10:51:50.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006637569s
Sep  1 10:51:52.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.00789492s
Sep  1 10:51:54.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006198203s
Sep  1 10:51:56.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007444716s
Sep  1 10:51:58.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006717827s
Sep  1 10:52:00.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006784374s
Sep  1 10:52:02.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.007460272s
Sep  1 10:52:04.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006971629s
Sep  1 10:52:06.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007460513s
Sep  1 10:52:08.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.006348848s
Sep  1 10:52:10.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006358423s
Sep  1 10:52:12.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.005894405s
Sep  1 10:52:14.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.00625707s
Sep  1 10:52:16.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.006880217s
Sep  1 10:52:18.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.006306458s
Sep  1 10:52:20.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.007512451s
Sep  1 10:52:22.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.006865393s
Sep  1 10:52:24.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.006091515s
Sep  1 10:52:26.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.006171094s
Sep  1 10:52:28.608: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.012126329s
Sep  1 10:52:30.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.005966757s
Sep  1 10:52:32.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.007759151s
Sep  1 10:52:34.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.00647619s
Sep  1 10:52:36.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007158383s
Sep  1 10:52:38.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.006914418s
Sep  1 10:52:40.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.006319871s
Sep  1 10:52:42.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.006735145s
Sep  1 10:52:44.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.007161957s
Sep  1 10:52:46.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008200701s
Sep  1 10:52:48.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.006095788s
Sep  1 10:52:50.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.006246759s
Sep  1 10:52:52.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007873091s
Sep  1 10:52:54.685: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.088604836s
Sep  1 10:52:56.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006487351s
Sep  1 10:52:58.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.009288562s
Sep  1 10:53:00.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.006291735s
Sep  1 10:53:02.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007342316s
Sep  1 10:53:04.607: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.011220821s
Sep  1 10:53:06.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.006523436s
Sep  1 10:53:08.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.007445546s
Sep  1 10:53:10.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.007720128s
Sep  1 10:53:12.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008599591s
Sep  1 10:53:14.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.00717114s
Sep  1 10:53:16.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.007248367s
Sep  1 10:53:18.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.00649564s
Sep  1 10:53:20.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.006733881s
Sep  1 10:53:22.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.006604495s
Sep  1 10:53:24.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.006202513s
Sep  1 10:53:26.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.007038743s
Sep  1 10:53:28.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.006946348s
Sep  1 10:53:30.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.006581207s
Sep  1 10:53:32.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.006466997s
Sep  1 10:53:34.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.00762699s
Sep  1 10:53:36.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007686666s
Sep  1 10:53:38.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008758881s
Sep  1 10:53:40.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008067589s
Sep  1 10:53:42.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.006259634s
Sep  1 10:53:44.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006330404s
Sep  1 10:53:46.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.007908384s
Sep  1 10:53:48.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.009145518s
Sep  1 10:53:50.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.006608164s
Sep  1 10:53:52.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.007882774s
Sep  1 10:53:54.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.006242974s
Sep  1 10:53:56.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.006167158s
Sep  1 10:53:58.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.006827345s
Sep  1 10:54:00.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008737862s
Sep  1 10:54:02.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.006598117s
Sep  1 10:54:04.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.006423732s
Sep  1 10:54:06.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007807655s
Sep  1 10:54:08.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008030932s
Sep  1 10:54:10.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.006751997s
Sep  1 10:54:12.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.008946977s
Sep  1 10:54:14.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.006810906s
Sep  1 10:54:16.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.007811609s
Sep  1 10:54:18.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.006484164s
Sep  1 10:54:20.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.006348039s
Sep  1 10:54:22.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.006598957s
Sep  1 10:54:24.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.007824578s
Sep  1 10:54:26.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.007548643s
Sep  1 10:54:28.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007432841s
Sep  1 10:54:30.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.006241171s
Sep  1 10:54:32.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.00793954s
Sep  1 10:54:34.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007606528s
Sep  1 10:54:36.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007911014s
Sep  1 10:54:38.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.006731519s
Sep  1 10:54:40.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.006288169s
Sep  1 10:54:42.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007464084s
Sep  1 10:54:44.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.007053996s
Sep  1 10:54:46.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008501074s
Sep  1 10:54:48.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008432247s
Sep  1 10:54:50.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.00808644s
Sep  1 10:54:52.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009279142s
Sep  1 10:54:54.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.00655307s
Sep  1 10:54:56.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.006845745s
Sep  1 10:54:58.607: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.011551739s
Sep  1 10:55:00.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007199445s
Sep  1 10:55:02.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.007773391s
Sep  1 10:55:04.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.006611509s
Sep  1 10:55:06.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.006544093s
Sep  1 10:55:06.607: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.0108742s
STEP: removing the label kubernetes.io/e2e-9ebb6527-4923-4a43-9944-b5b0e9b31ca8 off the node k8s-worker-1.c.operations-lab.internal 09/01/23 10:55:06.607
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9ebb6527-4923-4a43-9944-b5b0e9b31ca8 09/01/23 10:55:06.629
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:55:06.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9154" for this suite. 09/01/23 10:55:06.645
------------------------------
• [SLOW TEST] [304.235 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:50:02.415
    Sep  1 10:50:02.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename sched-pred 09/01/23 10:50:02.417
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:50:02.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:50:02.435
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  1 10:50:02.439: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  1 10:50:02.446: INFO: Waiting for terminating namespaces to be deleted...
    Sep  1 10:50:02.450: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
    Sep  1 10:50:02.460: INFO: cilium-operator-858666d4b6-t7brd from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.460: INFO: 	Container cilium-operator ready: true, restart count 0
    Sep  1 10:50:02.460: INFO: cilium-q8wf6 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.460: INFO: 	Container cilium-agent ready: true, restart count 0
    Sep  1 10:50:02.460: INFO: kube-proxy-wdplh from kube-system started at 2023-09-01 10:08:39 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.460: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  1 10:50:02.460: INFO: kyverno-cleanup-reports-28226090-9hzwf from kyverno-system started at 2023-09-01 10:50:00 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.460: INFO: 	Container cleanup ready: false, restart count 0
    Sep  1 10:50:02.460: INFO: fluentbit-fluentbit-z77m2 from logging-system started at 2023-09-01 10:49:46 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.460: INFO: 	Container fluent-bit ready: true, restart count 0
    Sep  1 10:50:02.460: INFO: logging-fluentd-0 from logging-system started at 2023-09-01 10:49:45 +0000 UTC (2 container statuses recorded)
    Sep  1 10:50:02.460: INFO: 	Container config-reloader ready: true, restart count 0
    Sep  1 10:50:02.460: INFO: 	Container fluentd ready: true, restart count 0
    Sep  1 10:50:02.460: INFO: node-exporter-zgtk4 from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.460: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  1 10:50:02.460: INFO: alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5 from security-context-test-3165 started at 2023-09-01 10:49:56 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.460: INFO: 	Container alpine-nnp-false-d802852b-3ca5-446b-8859-8a3379fefdf5 ready: false, restart count 0
    Sep  1 10:50:02.460: INFO: sonobuoy from sonobuoy started at 2023-09-01 10:29:08 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.460: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  1 10:50:02.460: INFO: sonobuoy-e2e-job-2bf224d5c7cf4759 from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
    Sep  1 10:50:02.460: INFO: 	Container e2e ready: true, restart count 0
    Sep  1 10:50:02.460: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  1 10:50:02.460: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-v249x from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
    Sep  1 10:50:02.461: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  1 10:50:02.461: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  1 10:50:02.461: INFO: webhook-to-be-mutated from webhook-7385 started at 2023-09-01 10:48:08 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.461: INFO: 	Container example ready: false, restart count 0
    Sep  1 10:50:02.461: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
    Sep  1 10:50:02.475: INFO: cert-manager-66f9685c7f-jlr6s from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.475: INFO: 	Container cert-manager ready: true, restart count 0
    Sep  1 10:50:02.475: INFO: cert-manager-cainjector-6cfc589789-78hkg from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.475: INFO: 	Container cainjector ready: true, restart count 0
    Sep  1 10:50:02.475: INFO: cert-manager-webhook-59f6664d4d-cfggr from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.475: INFO: 	Container webhook ready: true, restart count 0
    Sep  1 10:50:02.475: INFO: rs-8b59k from disruption-9340 started at 2023-09-01 10:48:15 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.476: INFO: 	Container donothing ready: false, restart count 0
    Sep  1 10:50:02.476: INFO: minio-6c8d455566-dvvwz from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.476: INFO: 	Container minio ready: true, restart count 0
    Sep  1 10:50:02.476: INFO: velero-57c7d7c6c4-tlzqn from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.476: INFO: 	Container velero ready: true, restart count 0
    Sep  1 10:50:02.477: INFO: traefik-7cf5b5b95f-97lpm from ingress-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.477: INFO: 	Container traefik ready: true, restart count 0
    Sep  1 10:50:02.477: INFO: kube-green-546cd595c4-mjs5l from kube-green-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.477: INFO: 	Container kube-green ready: true, restart count 0
    Sep  1 10:50:02.477: INFO: cilium-64mb8 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.477: INFO: 	Container cilium-agent ready: true, restart count 0
    Sep  1 10:50:02.478: INFO: cilium-operator-858666d4b6-mqxc6 from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.478: INFO: 	Container cilium-operator ready: true, restart count 0
    Sep  1 10:50:02.478: INFO: coredns-787d4945fb-hw6lt from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.478: INFO: 	Container coredns ready: true, restart count 0
    Sep  1 10:50:02.478: INFO: coredns-787d4945fb-zc797 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.478: INFO: 	Container coredns ready: true, restart count 0
    Sep  1 10:50:02.479: INFO: hubble-relay-7956d48fc8-dcjfd from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.479: INFO: 	Container hubble-relay ready: true, restart count 0
    Sep  1 10:50:02.479: INFO: hubble-ui-b86cb9bf7-7bzl2 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (2 container statuses recorded)
    Sep  1 10:50:02.479: INFO: 	Container backend ready: true, restart count 0
    Sep  1 10:50:02.479: INFO: 	Container frontend ready: true, restart count 0
    Sep  1 10:50:02.479: INFO: kube-proxy-vfc2w from kube-system started at 2023-09-01 10:08:32 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.480: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  1 10:50:02.480: INFO: kyverno-5bbbd994c9-kdf4j from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.480: INFO: 	Container kyverno ready: true, restart count 0
    Sep  1 10:50:02.480: INFO: kyverno-background-54b49bbb5d-wbc6r from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.480: INFO: 	Container kyverno-background ready: true, restart count 0
    Sep  1 10:50:02.480: INFO: kyverno-cleanup-f5b6cdd5b-jgfw6 from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.481: INFO: 	Container kyverno-cleanup ready: true, restart count 0
    Sep  1 10:50:02.481: INFO: kyverno-reports-b48cb544f-2sb6h from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.481: INFO: 	Container kyverno-reports ready: true, restart count 0
    Sep  1 10:50:02.481: INFO: local-path-provisioner-5d8858f87d-gtwb4 from local-path-storage started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.481: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Sep  1 10:50:02.481: INFO: fluentbit-fluentbit-7986p from logging-system started at 2023-09-01 10:23:38 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.481: INFO: 	Container fluent-bit ready: true, restart count 0
    Sep  1 10:50:02.482: INFO: logging-operator-5df74f78f5-zzgl8 from logging-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.482: INFO: 	Container logging-operator ready: true, restart count 0
    Sep  1 10:50:02.482: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-09-01 10:49:23 +0000 UTC (2 container statuses recorded)
    Sep  1 10:50:02.482: INFO: 	Container alertmanager ready: true, restart count 0
    Sep  1 10:50:02.482: INFO: 	Container config-reloader ready: true, restart count 0
    Sep  1 10:50:02.482: INFO: kube-state-metrics-8447695667-frzrq from monitoring-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.483: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Sep  1 10:50:02.483: INFO: node-exporter-vwctl from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.483: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  1 10:50:02.483: INFO: prometheus-operator-75f79b8c5d-pjwn9 from monitoring-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.483: INFO: 	Container prometheus-operator ready: true, restart count 0
    Sep  1 10:50:02.483: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-09-01 10:49:22 +0000 UTC (2 container statuses recorded)
    Sep  1 10:50:02.484: INFO: 	Container config-reloader ready: true, restart count 0
    Sep  1 10:50:02.484: INFO: 	Container prometheus ready: true, restart count 0
    Sep  1 10:50:02.484: INFO: rbac-manager-58f6dc584b-xgdd9 from rbac-manager-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:50:02.484: INFO: 	Container rbac-manager ready: true, restart count 0
    Sep  1 10:50:02.484: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-mqksc from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
    Sep  1 10:50:02.485: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  1 10:50:02.485: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/01/23 10:50:02.485
    Sep  1 10:50:02.498: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9154" to be "running"
    Sep  1 10:50:02.501: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.112323ms
    Sep  1 10:50:04.505: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007006922s
    Sep  1 10:50:04.505: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/01/23 10:50:04.508
    STEP: Trying to apply a random label on the found node. 09/01/23 10:50:04.522
    STEP: verifying the node has the label kubernetes.io/e2e-9ebb6527-4923-4a43-9944-b5b0e9b31ca8 95 09/01/23 10:50:04.557
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/01/23 10:50:04.564
    Sep  1 10:50:04.570: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-9154" to be "not pending"
    Sep  1 10:50:04.584: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.392605ms
    Sep  1 10:50:06.590: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018724488s
    Sep  1 10:50:06.590: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.16.0.3 on the node which pod4 resides and expect not scheduled 09/01/23 10:50:06.59
    Sep  1 10:50:06.596: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-9154" to be "not pending"
    Sep  1 10:50:06.599: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.672954ms
    Sep  1 10:50:08.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007470682s
    Sep  1 10:50:10.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007645002s
    Sep  1 10:50:12.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00710533s
    Sep  1 10:50:14.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006426789s
    Sep  1 10:50:16.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007728856s
    Sep  1 10:50:18.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00675796s
    Sep  1 10:50:20.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006790025s
    Sep  1 10:50:22.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008266896s
    Sep  1 10:50:24.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006630737s
    Sep  1 10:50:26.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007233761s
    Sep  1 10:50:28.606: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009822545s
    Sep  1 10:50:30.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006647145s
    Sep  1 10:50:32.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007162627s
    Sep  1 10:50:34.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006195596s
    Sep  1 10:50:36.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006336194s
    Sep  1 10:50:38.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00708935s
    Sep  1 10:50:40.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008519725s
    Sep  1 10:50:42.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00825034s
    Sep  1 10:50:44.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006424478s
    Sep  1 10:50:46.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006385488s
    Sep  1 10:50:48.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006866994s
    Sep  1 10:50:50.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.00753136s
    Sep  1 10:50:52.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008256221s
    Sep  1 10:50:54.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006572441s
    Sep  1 10:50:56.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00786439s
    Sep  1 10:50:58.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006362526s
    Sep  1 10:51:00.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.006964201s
    Sep  1 10:51:02.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007479043s
    Sep  1 10:51:04.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0081766s
    Sep  1 10:51:06.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007547964s
    Sep  1 10:51:08.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008216891s
    Sep  1 10:51:10.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007328359s
    Sep  1 10:51:12.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.00870228s
    Sep  1 10:51:14.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006153677s
    Sep  1 10:51:16.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007319309s
    Sep  1 10:51:18.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007236752s
    Sep  1 10:51:20.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006560652s
    Sep  1 10:51:22.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008101179s
    Sep  1 10:51:24.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006627448s
    Sep  1 10:51:26.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007996315s
    Sep  1 10:51:28.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007873665s
    Sep  1 10:51:30.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007723738s
    Sep  1 10:51:32.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007344099s
    Sep  1 10:51:34.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.00767766s
    Sep  1 10:51:36.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008077844s
    Sep  1 10:51:38.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007379579s
    Sep  1 10:51:40.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006000095s
    Sep  1 10:51:42.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005935195s
    Sep  1 10:51:44.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.007208737s
    Sep  1 10:51:46.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008195049s
    Sep  1 10:51:48.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008099227s
    Sep  1 10:51:50.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006637569s
    Sep  1 10:51:52.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.00789492s
    Sep  1 10:51:54.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006198203s
    Sep  1 10:51:56.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007444716s
    Sep  1 10:51:58.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006717827s
    Sep  1 10:52:00.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006784374s
    Sep  1 10:52:02.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.007460272s
    Sep  1 10:52:04.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006971629s
    Sep  1 10:52:06.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007460513s
    Sep  1 10:52:08.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.006348848s
    Sep  1 10:52:10.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006358423s
    Sep  1 10:52:12.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.005894405s
    Sep  1 10:52:14.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.00625707s
    Sep  1 10:52:16.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.006880217s
    Sep  1 10:52:18.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.006306458s
    Sep  1 10:52:20.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.007512451s
    Sep  1 10:52:22.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.006865393s
    Sep  1 10:52:24.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.006091515s
    Sep  1 10:52:26.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.006171094s
    Sep  1 10:52:28.608: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.012126329s
    Sep  1 10:52:30.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.005966757s
    Sep  1 10:52:32.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.007759151s
    Sep  1 10:52:34.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.00647619s
    Sep  1 10:52:36.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007158383s
    Sep  1 10:52:38.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.006914418s
    Sep  1 10:52:40.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.006319871s
    Sep  1 10:52:42.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.006735145s
    Sep  1 10:52:44.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.007161957s
    Sep  1 10:52:46.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008200701s
    Sep  1 10:52:48.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.006095788s
    Sep  1 10:52:50.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.006246759s
    Sep  1 10:52:52.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007873091s
    Sep  1 10:52:54.685: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.088604836s
    Sep  1 10:52:56.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006487351s
    Sep  1 10:52:58.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.009288562s
    Sep  1 10:53:00.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.006291735s
    Sep  1 10:53:02.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007342316s
    Sep  1 10:53:04.607: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.011220821s
    Sep  1 10:53:06.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.006523436s
    Sep  1 10:53:08.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.007445546s
    Sep  1 10:53:10.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.007720128s
    Sep  1 10:53:12.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008599591s
    Sep  1 10:53:14.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.00717114s
    Sep  1 10:53:16.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.007248367s
    Sep  1 10:53:18.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.00649564s
    Sep  1 10:53:20.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.006733881s
    Sep  1 10:53:22.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.006604495s
    Sep  1 10:53:24.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.006202513s
    Sep  1 10:53:26.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.007038743s
    Sep  1 10:53:28.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.006946348s
    Sep  1 10:53:30.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.006581207s
    Sep  1 10:53:32.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.006466997s
    Sep  1 10:53:34.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.00762699s
    Sep  1 10:53:36.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007686666s
    Sep  1 10:53:38.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008758881s
    Sep  1 10:53:40.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008067589s
    Sep  1 10:53:42.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.006259634s
    Sep  1 10:53:44.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006330404s
    Sep  1 10:53:46.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.007908384s
    Sep  1 10:53:48.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.009145518s
    Sep  1 10:53:50.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.006608164s
    Sep  1 10:53:52.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.007882774s
    Sep  1 10:53:54.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.006242974s
    Sep  1 10:53:56.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.006167158s
    Sep  1 10:53:58.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.006827345s
    Sep  1 10:54:00.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008737862s
    Sep  1 10:54:02.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.006598117s
    Sep  1 10:54:04.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.006423732s
    Sep  1 10:54:06.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007807655s
    Sep  1 10:54:08.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008030932s
    Sep  1 10:54:10.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.006751997s
    Sep  1 10:54:12.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.008946977s
    Sep  1 10:54:14.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.006810906s
    Sep  1 10:54:16.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.007811609s
    Sep  1 10:54:18.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.006484164s
    Sep  1 10:54:20.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.006348039s
    Sep  1 10:54:22.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.006598957s
    Sep  1 10:54:24.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.007824578s
    Sep  1 10:54:26.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.007548643s
    Sep  1 10:54:28.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007432841s
    Sep  1 10:54:30.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.006241171s
    Sep  1 10:54:32.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.00793954s
    Sep  1 10:54:34.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007606528s
    Sep  1 10:54:36.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007911014s
    Sep  1 10:54:38.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.006731519s
    Sep  1 10:54:40.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.006288169s
    Sep  1 10:54:42.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007464084s
    Sep  1 10:54:44.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.007053996s
    Sep  1 10:54:46.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008501074s
    Sep  1 10:54:48.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008432247s
    Sep  1 10:54:50.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.00808644s
    Sep  1 10:54:52.605: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009279142s
    Sep  1 10:54:54.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.00655307s
    Sep  1 10:54:56.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.006845745s
    Sep  1 10:54:58.607: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.011551739s
    Sep  1 10:55:00.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007199445s
    Sep  1 10:55:02.604: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.007773391s
    Sep  1 10:55:04.603: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.006611509s
    Sep  1 10:55:06.602: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.006544093s
    Sep  1 10:55:06.607: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.0108742s
    STEP: removing the label kubernetes.io/e2e-9ebb6527-4923-4a43-9944-b5b0e9b31ca8 off the node k8s-worker-1.c.operations-lab.internal 09/01/23 10:55:06.607
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-9ebb6527-4923-4a43-9944-b5b0e9b31ca8 09/01/23 10:55:06.629
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:55:06.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9154" for this suite. 09/01/23 10:55:06.645
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:55:06.653
Sep  1 10:55:06.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename svcaccounts 09/01/23 10:55:06.654
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:06.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:06.68
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  09/01/23 10:55:06.684
Sep  1 10:55:06.692: INFO: Waiting up to 5m0s for pod "test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b" in namespace "svcaccounts-2952" to be "Succeeded or Failed"
Sep  1 10:55:06.698: INFO: Pod "test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.075922ms
Sep  1 10:55:08.702: INFO: Pod "test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009745155s
Sep  1 10:55:10.703: INFO: Pod "test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010030515s
STEP: Saw pod success 09/01/23 10:55:10.703
Sep  1 10:55:10.703: INFO: Pod "test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b" satisfied condition "Succeeded or Failed"
Sep  1 10:55:10.708: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b container agnhost-container: <nil>
STEP: delete the pod 09/01/23 10:55:10.728
Sep  1 10:55:10.743: INFO: Waiting for pod test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b to disappear
Sep  1 10:55:10.746: INFO: Pod test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  1 10:55:10.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2952" for this suite. 09/01/23 10:55:10.751
------------------------------
• [4.103 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:55:06.653
    Sep  1 10:55:06.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename svcaccounts 09/01/23 10:55:06.654
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:06.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:06.68
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  09/01/23 10:55:06.684
    Sep  1 10:55:06.692: INFO: Waiting up to 5m0s for pod "test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b" in namespace "svcaccounts-2952" to be "Succeeded or Failed"
    Sep  1 10:55:06.698: INFO: Pod "test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.075922ms
    Sep  1 10:55:08.702: INFO: Pod "test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009745155s
    Sep  1 10:55:10.703: INFO: Pod "test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010030515s
    STEP: Saw pod success 09/01/23 10:55:10.703
    Sep  1 10:55:10.703: INFO: Pod "test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b" satisfied condition "Succeeded or Failed"
    Sep  1 10:55:10.708: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 10:55:10.728
    Sep  1 10:55:10.743: INFO: Waiting for pod test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b to disappear
    Sep  1 10:55:10.746: INFO: Pod test-pod-d64d22ee-056d-4436-b05d-3f64ca6b0d5b no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:55:10.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2952" for this suite. 09/01/23 10:55:10.751
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:55:10.762
Sep  1 10:55:10.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename secrets 09/01/23 10:55:10.764
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:10.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:10.786
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  1 10:55:10.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7623" for this suite. 09/01/23 10:55:10.857
------------------------------
• [0.102 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:55:10.762
    Sep  1 10:55:10.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename secrets 09/01/23 10:55:10.764
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:10.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:10.786
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:55:10.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7623" for this suite. 09/01/23 10:55:10.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:55:10.871
Sep  1 10:55:10.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename replicaset 09/01/23 10:55:10.873
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:10.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:10.893
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/01/23 10:55:10.899
Sep  1 10:55:10.907: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-3135" to be "running and ready"
Sep  1 10:55:10.911: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 3.811002ms
Sep  1 10:55:10.911: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:55:12.915: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.007569881s
Sep  1 10:55:12.915: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Sep  1 10:55:12.915: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 09/01/23 10:55:12.918
STEP: Then the orphan pod is adopted 09/01/23 10:55:12.923
STEP: When the matched label of one of its pods change 09/01/23 10:55:13.931
Sep  1 10:55:13.934: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 09/01/23 10:55:13.949
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  1 10:55:14.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3135" for this suite. 09/01/23 10:55:14.97
------------------------------
• [4.106 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:55:10.871
    Sep  1 10:55:10.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename replicaset 09/01/23 10:55:10.873
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:10.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:10.893
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/01/23 10:55:10.899
    Sep  1 10:55:10.907: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-3135" to be "running and ready"
    Sep  1 10:55:10.911: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 3.811002ms
    Sep  1 10:55:10.911: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:55:12.915: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.007569881s
    Sep  1 10:55:12.915: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Sep  1 10:55:12.915: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 09/01/23 10:55:12.918
    STEP: Then the orphan pod is adopted 09/01/23 10:55:12.923
    STEP: When the matched label of one of its pods change 09/01/23 10:55:13.931
    Sep  1 10:55:13.934: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/01/23 10:55:13.949
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:55:14.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3135" for this suite. 09/01/23 10:55:14.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:55:14.99
Sep  1 10:55:14.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 10:55:14.992
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:15.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:15.029
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 09/01/23 10:55:15.035
Sep  1 10:55:15.035: INFO: Creating e2e-svc-a-m2lxf
Sep  1 10:55:15.078: INFO: Creating e2e-svc-b-xdcwj
Sep  1 10:55:15.115: INFO: Creating e2e-svc-c-876x6
STEP: deleting service collection 09/01/23 10:55:15.138
Sep  1 10:55:15.201: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 10:55:15.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1837" for this suite. 09/01/23 10:55:15.209
------------------------------
• [0.226 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:55:14.99
    Sep  1 10:55:14.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 10:55:14.992
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:15.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:15.029
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 09/01/23 10:55:15.035
    Sep  1 10:55:15.035: INFO: Creating e2e-svc-a-m2lxf
    Sep  1 10:55:15.078: INFO: Creating e2e-svc-b-xdcwj
    Sep  1 10:55:15.115: INFO: Creating e2e-svc-c-876x6
    STEP: deleting service collection 09/01/23 10:55:15.138
    Sep  1 10:55:15.201: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:55:15.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1837" for this suite. 09/01/23 10:55:15.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:55:15.226
Sep  1 10:55:15.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 10:55:15.228
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:15.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:15.255
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 10:55:15.286
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 10:55:15.608
STEP: Deploying the webhook pod 09/01/23 10:55:15.616
STEP: Wait for the deployment to be ready 09/01/23 10:55:15.63
Sep  1 10:55:15.638: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 09/01/23 10:55:17.669
STEP: Verifying the service has paired with the endpoint 09/01/23 10:55:17.71
Sep  1 10:55:18.711: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/01/23 10:55:18.717
STEP: create a configmap that should be updated by the webhook 09/01/23 10:55:18.736
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:55:18.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6881" for this suite. 09/01/23 10:55:18.829
STEP: Destroying namespace "webhook-6881-markers" for this suite. 09/01/23 10:55:18.845
------------------------------
• [3.648 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:55:15.226
    Sep  1 10:55:15.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 10:55:15.228
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:15.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:15.255
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 10:55:15.286
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 10:55:15.608
    STEP: Deploying the webhook pod 09/01/23 10:55:15.616
    STEP: Wait for the deployment to be ready 09/01/23 10:55:15.63
    Sep  1 10:55:15.638: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 09/01/23 10:55:17.669
    STEP: Verifying the service has paired with the endpoint 09/01/23 10:55:17.71
    Sep  1 10:55:18.711: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/01/23 10:55:18.717
    STEP: create a configmap that should be updated by the webhook 09/01/23 10:55:18.736
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:55:18.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6881" for this suite. 09/01/23 10:55:18.829
    STEP: Destroying namespace "webhook-6881-markers" for this suite. 09/01/23 10:55:18.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:55:18.876
Sep  1 10:55:18.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 10:55:18.886
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:18.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:18.923
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-ade7699b-2de4-4a81-83c8-eea274a6e1fb 09/01/23 10:55:18.927
STEP: Creating a pod to test consume configMaps 09/01/23 10:55:18.933
Sep  1 10:55:18.941: INFO: Waiting up to 5m0s for pod "pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de" in namespace "configmap-4225" to be "Succeeded or Failed"
Sep  1 10:55:18.945: INFO: Pod "pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.893221ms
Sep  1 10:55:20.950: INFO: Pod "pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007797736s
Sep  1 10:55:22.949: INFO: Pod "pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007381685s
STEP: Saw pod success 09/01/23 10:55:22.949
Sep  1 10:55:22.950: INFO: Pod "pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de" satisfied condition "Succeeded or Failed"
Sep  1 10:55:22.953: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de container agnhost-container: <nil>
STEP: delete the pod 09/01/23 10:55:22.96
Sep  1 10:55:22.974: INFO: Waiting for pod pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de to disappear
Sep  1 10:55:22.978: INFO: Pod pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 10:55:22.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4225" for this suite. 09/01/23 10:55:22.984
------------------------------
• [4.114 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:55:18.876
    Sep  1 10:55:18.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 10:55:18.886
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:18.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:18.923
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-ade7699b-2de4-4a81-83c8-eea274a6e1fb 09/01/23 10:55:18.927
    STEP: Creating a pod to test consume configMaps 09/01/23 10:55:18.933
    Sep  1 10:55:18.941: INFO: Waiting up to 5m0s for pod "pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de" in namespace "configmap-4225" to be "Succeeded or Failed"
    Sep  1 10:55:18.945: INFO: Pod "pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.893221ms
    Sep  1 10:55:20.950: INFO: Pod "pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007797736s
    Sep  1 10:55:22.949: INFO: Pod "pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007381685s
    STEP: Saw pod success 09/01/23 10:55:22.949
    Sep  1 10:55:22.950: INFO: Pod "pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de" satisfied condition "Succeeded or Failed"
    Sep  1 10:55:22.953: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 10:55:22.96
    Sep  1 10:55:22.974: INFO: Waiting for pod pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de to disappear
    Sep  1 10:55:22.978: INFO: Pod pod-configmaps-68756836-0e03-40fb-9e3b-39cf661216de no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:55:22.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4225" for this suite. 09/01/23 10:55:22.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:55:22.993
Sep  1 10:55:22.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 10:55:22.994
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:23.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:23.012
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 09/01/23 10:55:23.018
Sep  1 10:55:23.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 create -f -'
Sep  1 10:55:24.079: INFO: stderr: ""
Sep  1 10:55:24.079: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/01/23 10:55:24.079
Sep  1 10:55:24.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  1 10:55:24.209: INFO: stderr: ""
Sep  1 10:55:24.209: INFO: stdout: "update-demo-nautilus-79xgv update-demo-nautilus-pr2bk "
Sep  1 10:55:24.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-79xgv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  1 10:55:24.320: INFO: stderr: ""
Sep  1 10:55:24.320: INFO: stdout: ""
Sep  1 10:55:24.320: INFO: update-demo-nautilus-79xgv is created but not running
Sep  1 10:55:29.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  1 10:55:29.430: INFO: stderr: ""
Sep  1 10:55:29.430: INFO: stdout: "update-demo-nautilus-79xgv update-demo-nautilus-pr2bk "
Sep  1 10:55:29.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-79xgv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  1 10:55:29.533: INFO: stderr: ""
Sep  1 10:55:29.533: INFO: stdout: "true"
Sep  1 10:55:29.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-79xgv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  1 10:55:29.641: INFO: stderr: ""
Sep  1 10:55:29.641: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  1 10:55:29.641: INFO: validating pod update-demo-nautilus-79xgv
Sep  1 10:55:29.648: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  1 10:55:29.648: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  1 10:55:29.648: INFO: update-demo-nautilus-79xgv is verified up and running
Sep  1 10:55:29.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-pr2bk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  1 10:55:29.761: INFO: stderr: ""
Sep  1 10:55:29.761: INFO: stdout: "true"
Sep  1 10:55:29.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-pr2bk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  1 10:55:29.854: INFO: stderr: ""
Sep  1 10:55:29.854: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  1 10:55:29.854: INFO: validating pod update-demo-nautilus-pr2bk
Sep  1 10:55:29.859: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  1 10:55:29.860: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  1 10:55:29.860: INFO: update-demo-nautilus-pr2bk is verified up and running
STEP: scaling down the replication controller 09/01/23 10:55:29.86
Sep  1 10:55:29.865: INFO: scanned /root for discovery docs: <nil>
Sep  1 10:55:29.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Sep  1 10:55:31.133: INFO: stderr: ""
Sep  1 10:55:31.133: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/01/23 10:55:31.133
Sep  1 10:55:31.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  1 10:55:31.232: INFO: stderr: ""
Sep  1 10:55:31.232: INFO: stdout: "update-demo-nautilus-79xgv update-demo-nautilus-pr2bk "
STEP: Replicas for name=update-demo: expected=1 actual=2 09/01/23 10:55:31.232
Sep  1 10:55:36.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  1 10:55:36.342: INFO: stderr: ""
Sep  1 10:55:36.342: INFO: stdout: "update-demo-nautilus-pr2bk "
Sep  1 10:55:36.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-pr2bk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  1 10:55:36.444: INFO: stderr: ""
Sep  1 10:55:36.444: INFO: stdout: "true"
Sep  1 10:55:36.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-pr2bk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  1 10:55:36.540: INFO: stderr: ""
Sep  1 10:55:36.540: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  1 10:55:36.540: INFO: validating pod update-demo-nautilus-pr2bk
Sep  1 10:55:36.545: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  1 10:55:36.545: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  1 10:55:36.545: INFO: update-demo-nautilus-pr2bk is verified up and running
STEP: scaling up the replication controller 09/01/23 10:55:36.545
Sep  1 10:55:36.549: INFO: scanned /root for discovery docs: <nil>
Sep  1 10:55:36.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Sep  1 10:55:37.703: INFO: stderr: ""
Sep  1 10:55:37.703: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/01/23 10:55:37.703
Sep  1 10:55:37.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  1 10:55:37.808: INFO: stderr: ""
Sep  1 10:55:37.808: INFO: stdout: "update-demo-nautilus-ht9cx update-demo-nautilus-pr2bk "
Sep  1 10:55:37.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-ht9cx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  1 10:55:37.902: INFO: stderr: ""
Sep  1 10:55:37.902: INFO: stdout: ""
Sep  1 10:55:37.902: INFO: update-demo-nautilus-ht9cx is created but not running
Sep  1 10:55:42.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  1 10:55:43.013: INFO: stderr: ""
Sep  1 10:55:43.013: INFO: stdout: "update-demo-nautilus-ht9cx update-demo-nautilus-pr2bk "
Sep  1 10:55:43.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-ht9cx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  1 10:55:43.113: INFO: stderr: ""
Sep  1 10:55:43.113: INFO: stdout: "true"
Sep  1 10:55:43.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-ht9cx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  1 10:55:43.214: INFO: stderr: ""
Sep  1 10:55:43.214: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  1 10:55:43.214: INFO: validating pod update-demo-nautilus-ht9cx
Sep  1 10:55:43.219: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  1 10:55:43.219: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  1 10:55:43.219: INFO: update-demo-nautilus-ht9cx is verified up and running
Sep  1 10:55:43.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-pr2bk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  1 10:55:43.317: INFO: stderr: ""
Sep  1 10:55:43.317: INFO: stdout: "true"
Sep  1 10:55:43.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-pr2bk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  1 10:55:43.419: INFO: stderr: ""
Sep  1 10:55:43.420: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  1 10:55:43.420: INFO: validating pod update-demo-nautilus-pr2bk
Sep  1 10:55:43.425: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  1 10:55:43.427: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  1 10:55:43.427: INFO: update-demo-nautilus-pr2bk is verified up and running
STEP: using delete to clean up resources 09/01/23 10:55:43.427
Sep  1 10:55:43.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 delete --grace-period=0 --force -f -'
Sep  1 10:55:43.520: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  1 10:55:43.520: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  1 10:55:43.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get rc,svc -l name=update-demo --no-headers'
Sep  1 10:55:43.744: INFO: stderr: "No resources found in kubectl-7434 namespace.\n"
Sep  1 10:55:43.745: INFO: stdout: ""
Sep  1 10:55:43.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  1 10:55:43.951: INFO: stderr: ""
Sep  1 10:55:43.951: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 10:55:43.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7434" for this suite. 09/01/23 10:55:43.955
------------------------------
• [SLOW TEST] [20.969 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:55:22.993
    Sep  1 10:55:22.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 10:55:22.994
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:23.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:23.012
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 09/01/23 10:55:23.018
    Sep  1 10:55:23.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 create -f -'
    Sep  1 10:55:24.079: INFO: stderr: ""
    Sep  1 10:55:24.079: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/01/23 10:55:24.079
    Sep  1 10:55:24.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  1 10:55:24.209: INFO: stderr: ""
    Sep  1 10:55:24.209: INFO: stdout: "update-demo-nautilus-79xgv update-demo-nautilus-pr2bk "
    Sep  1 10:55:24.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-79xgv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  1 10:55:24.320: INFO: stderr: ""
    Sep  1 10:55:24.320: INFO: stdout: ""
    Sep  1 10:55:24.320: INFO: update-demo-nautilus-79xgv is created but not running
    Sep  1 10:55:29.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  1 10:55:29.430: INFO: stderr: ""
    Sep  1 10:55:29.430: INFO: stdout: "update-demo-nautilus-79xgv update-demo-nautilus-pr2bk "
    Sep  1 10:55:29.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-79xgv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  1 10:55:29.533: INFO: stderr: ""
    Sep  1 10:55:29.533: INFO: stdout: "true"
    Sep  1 10:55:29.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-79xgv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  1 10:55:29.641: INFO: stderr: ""
    Sep  1 10:55:29.641: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  1 10:55:29.641: INFO: validating pod update-demo-nautilus-79xgv
    Sep  1 10:55:29.648: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  1 10:55:29.648: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  1 10:55:29.648: INFO: update-demo-nautilus-79xgv is verified up and running
    Sep  1 10:55:29.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-pr2bk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  1 10:55:29.761: INFO: stderr: ""
    Sep  1 10:55:29.761: INFO: stdout: "true"
    Sep  1 10:55:29.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-pr2bk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  1 10:55:29.854: INFO: stderr: ""
    Sep  1 10:55:29.854: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  1 10:55:29.854: INFO: validating pod update-demo-nautilus-pr2bk
    Sep  1 10:55:29.859: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  1 10:55:29.860: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  1 10:55:29.860: INFO: update-demo-nautilus-pr2bk is verified up and running
    STEP: scaling down the replication controller 09/01/23 10:55:29.86
    Sep  1 10:55:29.865: INFO: scanned /root for discovery docs: <nil>
    Sep  1 10:55:29.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Sep  1 10:55:31.133: INFO: stderr: ""
    Sep  1 10:55:31.133: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/01/23 10:55:31.133
    Sep  1 10:55:31.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  1 10:55:31.232: INFO: stderr: ""
    Sep  1 10:55:31.232: INFO: stdout: "update-demo-nautilus-79xgv update-demo-nautilus-pr2bk "
    STEP: Replicas for name=update-demo: expected=1 actual=2 09/01/23 10:55:31.232
    Sep  1 10:55:36.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  1 10:55:36.342: INFO: stderr: ""
    Sep  1 10:55:36.342: INFO: stdout: "update-demo-nautilus-pr2bk "
    Sep  1 10:55:36.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-pr2bk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  1 10:55:36.444: INFO: stderr: ""
    Sep  1 10:55:36.444: INFO: stdout: "true"
    Sep  1 10:55:36.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-pr2bk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  1 10:55:36.540: INFO: stderr: ""
    Sep  1 10:55:36.540: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  1 10:55:36.540: INFO: validating pod update-demo-nautilus-pr2bk
    Sep  1 10:55:36.545: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  1 10:55:36.545: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  1 10:55:36.545: INFO: update-demo-nautilus-pr2bk is verified up and running
    STEP: scaling up the replication controller 09/01/23 10:55:36.545
    Sep  1 10:55:36.549: INFO: scanned /root for discovery docs: <nil>
    Sep  1 10:55:36.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Sep  1 10:55:37.703: INFO: stderr: ""
    Sep  1 10:55:37.703: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/01/23 10:55:37.703
    Sep  1 10:55:37.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  1 10:55:37.808: INFO: stderr: ""
    Sep  1 10:55:37.808: INFO: stdout: "update-demo-nautilus-ht9cx update-demo-nautilus-pr2bk "
    Sep  1 10:55:37.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-ht9cx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  1 10:55:37.902: INFO: stderr: ""
    Sep  1 10:55:37.902: INFO: stdout: ""
    Sep  1 10:55:37.902: INFO: update-demo-nautilus-ht9cx is created but not running
    Sep  1 10:55:42.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  1 10:55:43.013: INFO: stderr: ""
    Sep  1 10:55:43.013: INFO: stdout: "update-demo-nautilus-ht9cx update-demo-nautilus-pr2bk "
    Sep  1 10:55:43.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-ht9cx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  1 10:55:43.113: INFO: stderr: ""
    Sep  1 10:55:43.113: INFO: stdout: "true"
    Sep  1 10:55:43.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-ht9cx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  1 10:55:43.214: INFO: stderr: ""
    Sep  1 10:55:43.214: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  1 10:55:43.214: INFO: validating pod update-demo-nautilus-ht9cx
    Sep  1 10:55:43.219: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  1 10:55:43.219: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  1 10:55:43.219: INFO: update-demo-nautilus-ht9cx is verified up and running
    Sep  1 10:55:43.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-pr2bk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  1 10:55:43.317: INFO: stderr: ""
    Sep  1 10:55:43.317: INFO: stdout: "true"
    Sep  1 10:55:43.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods update-demo-nautilus-pr2bk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  1 10:55:43.419: INFO: stderr: ""
    Sep  1 10:55:43.420: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  1 10:55:43.420: INFO: validating pod update-demo-nautilus-pr2bk
    Sep  1 10:55:43.425: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  1 10:55:43.427: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  1 10:55:43.427: INFO: update-demo-nautilus-pr2bk is verified up and running
    STEP: using delete to clean up resources 09/01/23 10:55:43.427
    Sep  1 10:55:43.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 delete --grace-period=0 --force -f -'
    Sep  1 10:55:43.520: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  1 10:55:43.520: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep  1 10:55:43.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get rc,svc -l name=update-demo --no-headers'
    Sep  1 10:55:43.744: INFO: stderr: "No resources found in kubectl-7434 namespace.\n"
    Sep  1 10:55:43.745: INFO: stdout: ""
    Sep  1 10:55:43.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7434 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  1 10:55:43.951: INFO: stderr: ""
    Sep  1 10:55:43.951: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:55:43.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7434" for this suite. 09/01/23 10:55:43.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:55:43.963
Sep  1 10:55:43.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pods 09/01/23 10:55:43.964
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:43.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:43.992
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 09/01/23 10:55:44.004
STEP: watching for Pod to be ready 09/01/23 10:55:44.013
Sep  1 10:55:44.015: INFO: observed Pod pod-test in namespace pods-7066 in phase Pending with labels: map[test-pod-static:true] & conditions []
Sep  1 10:55:44.022: INFO: observed Pod pod-test in namespace pods-7066 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:44 +0000 UTC  }]
Sep  1 10:55:44.032: INFO: observed Pod pod-test in namespace pods-7066 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:44 +0000 UTC  }]
Sep  1 10:55:46.178: INFO: Found Pod pod-test in namespace pods-7066 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:44 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 09/01/23 10:55:46.181
STEP: getting the Pod and ensuring that it's patched 09/01/23 10:55:46.196
STEP: replacing the Pod's status Ready condition to False 09/01/23 10:55:46.202
STEP: check the Pod again to ensure its Ready conditions are False 09/01/23 10:55:46.222
STEP: deleting the Pod via a Collection with a LabelSelector 09/01/23 10:55:46.224
STEP: watching for the Pod to be deleted 09/01/23 10:55:46.233
Sep  1 10:55:46.236: INFO: observed event type MODIFIED
Sep  1 10:55:48.191: INFO: observed event type MODIFIED
Sep  1 10:55:49.200: INFO: observed event type MODIFIED
Sep  1 10:55:49.210: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  1 10:55:49.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7066" for this suite. 09/01/23 10:55:49.232
------------------------------
• [SLOW TEST] [5.278 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:55:43.963
    Sep  1 10:55:43.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pods 09/01/23 10:55:43.964
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:43.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:43.992
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 09/01/23 10:55:44.004
    STEP: watching for Pod to be ready 09/01/23 10:55:44.013
    Sep  1 10:55:44.015: INFO: observed Pod pod-test in namespace pods-7066 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Sep  1 10:55:44.022: INFO: observed Pod pod-test in namespace pods-7066 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:44 +0000 UTC  }]
    Sep  1 10:55:44.032: INFO: observed Pod pod-test in namespace pods-7066 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:44 +0000 UTC  }]
    Sep  1 10:55:46.178: INFO: Found Pod pod-test in namespace pods-7066 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:55:44 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 09/01/23 10:55:46.181
    STEP: getting the Pod and ensuring that it's patched 09/01/23 10:55:46.196
    STEP: replacing the Pod's status Ready condition to False 09/01/23 10:55:46.202
    STEP: check the Pod again to ensure its Ready conditions are False 09/01/23 10:55:46.222
    STEP: deleting the Pod via a Collection with a LabelSelector 09/01/23 10:55:46.224
    STEP: watching for the Pod to be deleted 09/01/23 10:55:46.233
    Sep  1 10:55:46.236: INFO: observed event type MODIFIED
    Sep  1 10:55:48.191: INFO: observed event type MODIFIED
    Sep  1 10:55:49.200: INFO: observed event type MODIFIED
    Sep  1 10:55:49.210: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:55:49.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7066" for this suite. 09/01/23 10:55:49.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:55:49.243
Sep  1 10:55:49.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename daemonsets 09/01/23 10:55:49.246
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:49.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:49.273
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
STEP: Creating simple DaemonSet "daemon-set" 09/01/23 10:55:49.299
STEP: Check that daemon pods launch on every node of the cluster. 09/01/23 10:55:49.307
Sep  1 10:55:49.313: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 10:55:49.317: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:55:49.318: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:55:50.323: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 10:55:50.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:55:50.327: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 10:55:51.323: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 10:55:51.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  1 10:55:51.326: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 09/01/23 10:55:51.331
Sep  1 10:55:51.335: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 09/01/23 10:55:51.335
Sep  1 10:55:51.348: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 09/01/23 10:55:51.348
Sep  1 10:55:51.350: INFO: Observed &DaemonSet event: ADDED
Sep  1 10:55:51.351: INFO: Observed &DaemonSet event: MODIFIED
Sep  1 10:55:51.352: INFO: Observed &DaemonSet event: MODIFIED
Sep  1 10:55:51.352: INFO: Observed &DaemonSet event: MODIFIED
Sep  1 10:55:51.353: INFO: Observed &DaemonSet event: MODIFIED
Sep  1 10:55:51.353: INFO: Found daemon set daemon-set in namespace daemonsets-1709 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  1 10:55:51.353: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 09/01/23 10:55:51.353
STEP: watching for the daemon set status to be patched 09/01/23 10:55:51.365
Sep  1 10:55:51.369: INFO: Observed &DaemonSet event: ADDED
Sep  1 10:55:51.370: INFO: Observed &DaemonSet event: MODIFIED
Sep  1 10:55:51.370: INFO: Observed &DaemonSet event: MODIFIED
Sep  1 10:55:51.371: INFO: Observed &DaemonSet event: MODIFIED
Sep  1 10:55:51.371: INFO: Observed &DaemonSet event: MODIFIED
Sep  1 10:55:51.371: INFO: Observed daemon set daemon-set in namespace daemonsets-1709 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  1 10:55:51.372: INFO: Observed &DaemonSet event: MODIFIED
Sep  1 10:55:51.372: INFO: Found daemon set daemon-set in namespace daemonsets-1709 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Sep  1 10:55:51.372: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/01/23 10:55:51.375
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1709, will wait for the garbage collector to delete the pods 09/01/23 10:55:51.375
Sep  1 10:55:51.436: INFO: Deleting DaemonSet.extensions daemon-set took: 8.081498ms
Sep  1 10:55:51.538: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.337023ms
Sep  1 10:55:54.243: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 10:55:54.243: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  1 10:55:54.246: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20941"},"items":null}

Sep  1 10:55:54.249: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20941"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:55:54.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1709" for this suite. 09/01/23 10:55:54.279
------------------------------
• [SLOW TEST] [5.044 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:55:49.243
    Sep  1 10:55:49.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename daemonsets 09/01/23 10:55:49.246
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:49.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:49.273
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:873
    STEP: Creating simple DaemonSet "daemon-set" 09/01/23 10:55:49.299
    STEP: Check that daemon pods launch on every node of the cluster. 09/01/23 10:55:49.307
    Sep  1 10:55:49.313: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 10:55:49.317: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:55:49.318: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:55:50.323: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 10:55:50.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:55:50.327: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 10:55:51.323: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 10:55:51.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  1 10:55:51.326: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 09/01/23 10:55:51.331
    Sep  1 10:55:51.335: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 09/01/23 10:55:51.335
    Sep  1 10:55:51.348: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 09/01/23 10:55:51.348
    Sep  1 10:55:51.350: INFO: Observed &DaemonSet event: ADDED
    Sep  1 10:55:51.351: INFO: Observed &DaemonSet event: MODIFIED
    Sep  1 10:55:51.352: INFO: Observed &DaemonSet event: MODIFIED
    Sep  1 10:55:51.352: INFO: Observed &DaemonSet event: MODIFIED
    Sep  1 10:55:51.353: INFO: Observed &DaemonSet event: MODIFIED
    Sep  1 10:55:51.353: INFO: Found daemon set daemon-set in namespace daemonsets-1709 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  1 10:55:51.353: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 09/01/23 10:55:51.353
    STEP: watching for the daemon set status to be patched 09/01/23 10:55:51.365
    Sep  1 10:55:51.369: INFO: Observed &DaemonSet event: ADDED
    Sep  1 10:55:51.370: INFO: Observed &DaemonSet event: MODIFIED
    Sep  1 10:55:51.370: INFO: Observed &DaemonSet event: MODIFIED
    Sep  1 10:55:51.371: INFO: Observed &DaemonSet event: MODIFIED
    Sep  1 10:55:51.371: INFO: Observed &DaemonSet event: MODIFIED
    Sep  1 10:55:51.371: INFO: Observed daemon set daemon-set in namespace daemonsets-1709 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  1 10:55:51.372: INFO: Observed &DaemonSet event: MODIFIED
    Sep  1 10:55:51.372: INFO: Found daemon set daemon-set in namespace daemonsets-1709 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Sep  1 10:55:51.372: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/01/23 10:55:51.375
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1709, will wait for the garbage collector to delete the pods 09/01/23 10:55:51.375
    Sep  1 10:55:51.436: INFO: Deleting DaemonSet.extensions daemon-set took: 8.081498ms
    Sep  1 10:55:51.538: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.337023ms
    Sep  1 10:55:54.243: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 10:55:54.243: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  1 10:55:54.246: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20941"},"items":null}

    Sep  1 10:55:54.249: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20941"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:55:54.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1709" for this suite. 09/01/23 10:55:54.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:55:54.302
Sep  1 10:55:54.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename containers 09/01/23 10:55:54.303
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:54.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:54.325
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 09/01/23 10:55:54.328
Sep  1 10:55:54.340: INFO: Waiting up to 5m0s for pod "client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55" in namespace "containers-9125" to be "Succeeded or Failed"
Sep  1 10:55:54.345: INFO: Pod "client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.527141ms
Sep  1 10:55:56.349: INFO: Pod "client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008573577s
Sep  1 10:55:58.350: INFO: Pod "client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009131777s
STEP: Saw pod success 09/01/23 10:55:58.35
Sep  1 10:55:58.350: INFO: Pod "client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55" satisfied condition "Succeeded or Failed"
Sep  1 10:55:58.353: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55 container agnhost-container: <nil>
STEP: delete the pod 09/01/23 10:55:58.359
Sep  1 10:55:58.371: INFO: Waiting for pod client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55 to disappear
Sep  1 10:55:58.374: INFO: Pod client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  1 10:55:58.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9125" for this suite. 09/01/23 10:55:58.378
------------------------------
• [4.081 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:55:54.302
    Sep  1 10:55:54.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename containers 09/01/23 10:55:54.303
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:54.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:54.325
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 09/01/23 10:55:54.328
    Sep  1 10:55:54.340: INFO: Waiting up to 5m0s for pod "client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55" in namespace "containers-9125" to be "Succeeded or Failed"
    Sep  1 10:55:54.345: INFO: Pod "client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.527141ms
    Sep  1 10:55:56.349: INFO: Pod "client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008573577s
    Sep  1 10:55:58.350: INFO: Pod "client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009131777s
    STEP: Saw pod success 09/01/23 10:55:58.35
    Sep  1 10:55:58.350: INFO: Pod "client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55" satisfied condition "Succeeded or Failed"
    Sep  1 10:55:58.353: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55 container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 10:55:58.359
    Sep  1 10:55:58.371: INFO: Waiting for pod client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55 to disappear
    Sep  1 10:55:58.374: INFO: Pod client-containers-86e36add-7715-4868-80a9-3e0f13b0ab55 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:55:58.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9125" for this suite. 09/01/23 10:55:58.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:55:58.384
Sep  1 10:55:58.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename sched-pred 09/01/23 10:55:58.385
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:58.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:58.405
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  1 10:55:58.409: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  1 10:55:58.416: INFO: Waiting for terminating namespaces to be deleted...
Sep  1 10:55:58.419: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
Sep  1 10:55:58.430: INFO: cilium-operator-858666d4b6-t7brd from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.430: INFO: 	Container cilium-operator ready: true, restart count 0
Sep  1 10:55:58.430: INFO: cilium-q8wf6 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.430: INFO: 	Container cilium-agent ready: true, restart count 0
Sep  1 10:55:58.430: INFO: kube-proxy-wdplh from kube-system started at 2023-09-01 10:08:39 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.430: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  1 10:55:58.430: INFO: fluentbit-fluentbit-z77m2 from logging-system started at 2023-09-01 10:49:46 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.430: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  1 10:55:58.430: INFO: logging-fluentd-0 from logging-system started at 2023-09-01 10:49:45 +0000 UTC (2 container statuses recorded)
Sep  1 10:55:58.431: INFO: 	Container config-reloader ready: true, restart count 0
Sep  1 10:55:58.431: INFO: 	Container fluentd ready: true, restart count 0
Sep  1 10:55:58.431: INFO: node-exporter-zgtk4 from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.432: INFO: 	Container node-exporter ready: true, restart count 0
Sep  1 10:55:58.432: INFO: sonobuoy from sonobuoy started at 2023-09-01 10:29:08 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.432: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  1 10:55:58.432: INFO: sonobuoy-e2e-job-2bf224d5c7cf4759 from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
Sep  1 10:55:58.433: INFO: 	Container e2e ready: true, restart count 0
Sep  1 10:55:58.433: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  1 10:55:58.433: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-v249x from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
Sep  1 10:55:58.433: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  1 10:55:58.433: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  1 10:55:58.433: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
Sep  1 10:55:58.449: INFO: cert-manager-66f9685c7f-jlr6s from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container cert-manager ready: true, restart count 0
Sep  1 10:55:58.449: INFO: cert-manager-cainjector-6cfc589789-78hkg from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container cainjector ready: true, restart count 0
Sep  1 10:55:58.449: INFO: cert-manager-webhook-59f6664d4d-cfggr from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container webhook ready: true, restart count 0
Sep  1 10:55:58.449: INFO: minio-6c8d455566-dvvwz from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container minio ready: true, restart count 0
Sep  1 10:55:58.449: INFO: velero-57c7d7c6c4-tlzqn from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container velero ready: true, restart count 0
Sep  1 10:55:58.449: INFO: traefik-7cf5b5b95f-97lpm from ingress-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container traefik ready: true, restart count 0
Sep  1 10:55:58.449: INFO: kube-green-546cd595c4-mjs5l from kube-green-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container kube-green ready: true, restart count 0
Sep  1 10:55:58.449: INFO: cilium-64mb8 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container cilium-agent ready: true, restart count 0
Sep  1 10:55:58.449: INFO: cilium-operator-858666d4b6-mqxc6 from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container cilium-operator ready: true, restart count 0
Sep  1 10:55:58.449: INFO: coredns-787d4945fb-hw6lt from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container coredns ready: true, restart count 0
Sep  1 10:55:58.449: INFO: coredns-787d4945fb-zc797 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container coredns ready: true, restart count 0
Sep  1 10:55:58.449: INFO: hubble-relay-7956d48fc8-dcjfd from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container hubble-relay ready: true, restart count 0
Sep  1 10:55:58.449: INFO: hubble-ui-b86cb9bf7-7bzl2 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (2 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container backend ready: true, restart count 0
Sep  1 10:55:58.449: INFO: 	Container frontend ready: true, restart count 0
Sep  1 10:55:58.449: INFO: kube-proxy-vfc2w from kube-system started at 2023-09-01 10:08:32 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  1 10:55:58.449: INFO: kyverno-5bbbd994c9-kdf4j from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container kyverno ready: true, restart count 0
Sep  1 10:55:58.449: INFO: kyverno-background-54b49bbb5d-wbc6r from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container kyverno-background ready: true, restart count 0
Sep  1 10:55:58.449: INFO: kyverno-cleanup-f5b6cdd5b-jgfw6 from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container kyverno-cleanup ready: true, restart count 0
Sep  1 10:55:58.449: INFO: kyverno-reports-b48cb544f-2sb6h from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container kyverno-reports ready: true, restart count 0
Sep  1 10:55:58.449: INFO: local-path-provisioner-5d8858f87d-gtwb4 from local-path-storage started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container local-path-provisioner ready: true, restart count 0
Sep  1 10:55:58.449: INFO: fluentbit-fluentbit-7986p from logging-system started at 2023-09-01 10:23:38 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  1 10:55:58.449: INFO: logging-operator-5df74f78f5-zzgl8 from logging-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container logging-operator ready: true, restart count 0
Sep  1 10:55:58.449: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-09-01 10:49:23 +0000 UTC (2 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container alertmanager ready: true, restart count 0
Sep  1 10:55:58.449: INFO: 	Container config-reloader ready: true, restart count 0
Sep  1 10:55:58.449: INFO: kube-state-metrics-8447695667-frzrq from monitoring-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  1 10:55:58.449: INFO: node-exporter-vwctl from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container node-exporter ready: true, restart count 0
Sep  1 10:55:58.449: INFO: prometheus-operator-75f79b8c5d-pjwn9 from monitoring-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  1 10:55:58.449: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-09-01 10:49:22 +0000 UTC (2 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container config-reloader ready: true, restart count 0
Sep  1 10:55:58.449: INFO: 	Container prometheus ready: true, restart count 0
Sep  1 10:55:58.449: INFO: rbac-manager-58f6dc584b-xgdd9 from rbac-manager-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container rbac-manager ready: true, restart count 0
Sep  1 10:55:58.449: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-mqksc from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
Sep  1 10:55:58.449: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  1 10:55:58.449: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 09/01/23 10:55:58.449
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1780c1159de4ed47], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 09/01/23 10:55:58.483
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:55:59.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5310" for this suite. 09/01/23 10:55:59.487
------------------------------
• [1.112 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:55:58.384
    Sep  1 10:55:58.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename sched-pred 09/01/23 10:55:58.385
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:58.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:58.405
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  1 10:55:58.409: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  1 10:55:58.416: INFO: Waiting for terminating namespaces to be deleted...
    Sep  1 10:55:58.419: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
    Sep  1 10:55:58.430: INFO: cilium-operator-858666d4b6-t7brd from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.430: INFO: 	Container cilium-operator ready: true, restart count 0
    Sep  1 10:55:58.430: INFO: cilium-q8wf6 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.430: INFO: 	Container cilium-agent ready: true, restart count 0
    Sep  1 10:55:58.430: INFO: kube-proxy-wdplh from kube-system started at 2023-09-01 10:08:39 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.430: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  1 10:55:58.430: INFO: fluentbit-fluentbit-z77m2 from logging-system started at 2023-09-01 10:49:46 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.430: INFO: 	Container fluent-bit ready: true, restart count 0
    Sep  1 10:55:58.430: INFO: logging-fluentd-0 from logging-system started at 2023-09-01 10:49:45 +0000 UTC (2 container statuses recorded)
    Sep  1 10:55:58.431: INFO: 	Container config-reloader ready: true, restart count 0
    Sep  1 10:55:58.431: INFO: 	Container fluentd ready: true, restart count 0
    Sep  1 10:55:58.431: INFO: node-exporter-zgtk4 from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.432: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  1 10:55:58.432: INFO: sonobuoy from sonobuoy started at 2023-09-01 10:29:08 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.432: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  1 10:55:58.432: INFO: sonobuoy-e2e-job-2bf224d5c7cf4759 from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
    Sep  1 10:55:58.433: INFO: 	Container e2e ready: true, restart count 0
    Sep  1 10:55:58.433: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  1 10:55:58.433: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-v249x from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
    Sep  1 10:55:58.433: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  1 10:55:58.433: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  1 10:55:58.433: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
    Sep  1 10:55:58.449: INFO: cert-manager-66f9685c7f-jlr6s from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container cert-manager ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: cert-manager-cainjector-6cfc589789-78hkg from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container cainjector ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: cert-manager-webhook-59f6664d4d-cfggr from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container webhook ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: minio-6c8d455566-dvvwz from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container minio ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: velero-57c7d7c6c4-tlzqn from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container velero ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: traefik-7cf5b5b95f-97lpm from ingress-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container traefik ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: kube-green-546cd595c4-mjs5l from kube-green-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container kube-green ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: cilium-64mb8 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container cilium-agent ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: cilium-operator-858666d4b6-mqxc6 from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container cilium-operator ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: coredns-787d4945fb-hw6lt from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container coredns ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: coredns-787d4945fb-zc797 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container coredns ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: hubble-relay-7956d48fc8-dcjfd from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container hubble-relay ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: hubble-ui-b86cb9bf7-7bzl2 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (2 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container backend ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: 	Container frontend ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: kube-proxy-vfc2w from kube-system started at 2023-09-01 10:08:32 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: kyverno-5bbbd994c9-kdf4j from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container kyverno ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: kyverno-background-54b49bbb5d-wbc6r from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container kyverno-background ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: kyverno-cleanup-f5b6cdd5b-jgfw6 from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container kyverno-cleanup ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: kyverno-reports-b48cb544f-2sb6h from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container kyverno-reports ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: local-path-provisioner-5d8858f87d-gtwb4 from local-path-storage started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: fluentbit-fluentbit-7986p from logging-system started at 2023-09-01 10:23:38 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container fluent-bit ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: logging-operator-5df74f78f5-zzgl8 from logging-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container logging-operator ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-09-01 10:49:23 +0000 UTC (2 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container alertmanager ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: 	Container config-reloader ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: kube-state-metrics-8447695667-frzrq from monitoring-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: node-exporter-vwctl from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: prometheus-operator-75f79b8c5d-pjwn9 from monitoring-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container prometheus-operator ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-09-01 10:49:22 +0000 UTC (2 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container config-reloader ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: 	Container prometheus ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: rbac-manager-58f6dc584b-xgdd9 from rbac-manager-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container rbac-manager ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-mqksc from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
    Sep  1 10:55:58.449: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  1 10:55:58.449: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 09/01/23 10:55:58.449
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1780c1159de4ed47], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 09/01/23 10:55:58.483
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:55:59.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5310" for this suite. 09/01/23 10:55:59.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:55:59.504
Sep  1 10:55:59.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename resourcequota 09/01/23 10:55:59.507
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:59.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:59.529
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 09/01/23 10:55:59.559
STEP: Creating a ResourceQuota 09/01/23 10:56:04.571
STEP: Ensuring resource quota status is calculated 09/01/23 10:56:04.576
STEP: Creating a ReplicationController 09/01/23 10:56:06.581
STEP: Ensuring resource quota status captures replication controller creation 09/01/23 10:56:06.597
STEP: Deleting a ReplicationController 09/01/23 10:56:08.601
STEP: Ensuring resource quota status released usage 09/01/23 10:56:08.607
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  1 10:56:10.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3520" for this suite. 09/01/23 10:56:10.615
------------------------------
• [SLOW TEST] [11.116 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:55:59.504
    Sep  1 10:55:59.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename resourcequota 09/01/23 10:55:59.507
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:55:59.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:55:59.529
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 09/01/23 10:55:59.559
    STEP: Creating a ResourceQuota 09/01/23 10:56:04.571
    STEP: Ensuring resource quota status is calculated 09/01/23 10:56:04.576
    STEP: Creating a ReplicationController 09/01/23 10:56:06.581
    STEP: Ensuring resource quota status captures replication controller creation 09/01/23 10:56:06.597
    STEP: Deleting a ReplicationController 09/01/23 10:56:08.601
    STEP: Ensuring resource quota status released usage 09/01/23 10:56:08.607
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:56:10.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3520" for this suite. 09/01/23 10:56:10.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:56:10.626
Sep  1 10:56:10.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 10:56:10.628
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:10.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:10.649
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 09/01/23 10:56:10.653
Sep  1 10:56:10.663: INFO: Waiting up to 5m0s for pod "downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0" in namespace "downward-api-981" to be "Succeeded or Failed"
Sep  1 10:56:10.672: INFO: Pod "downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.551266ms
Sep  1 10:56:12.676: INFO: Pod "downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012579984s
Sep  1 10:56:14.678: INFO: Pod "downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01455675s
STEP: Saw pod success 09/01/23 10:56:14.678
Sep  1 10:56:14.678: INFO: Pod "downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0" satisfied condition "Succeeded or Failed"
Sep  1 10:56:14.682: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0 container dapi-container: <nil>
STEP: delete the pod 09/01/23 10:56:14.691
Sep  1 10:56:14.706: INFO: Waiting for pod downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0 to disappear
Sep  1 10:56:14.710: INFO: Pod downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  1 10:56:14.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-981" for this suite. 09/01/23 10:56:14.714
------------------------------
• [4.094 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:56:10.626
    Sep  1 10:56:10.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 10:56:10.628
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:10.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:10.649
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 09/01/23 10:56:10.653
    Sep  1 10:56:10.663: INFO: Waiting up to 5m0s for pod "downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0" in namespace "downward-api-981" to be "Succeeded or Failed"
    Sep  1 10:56:10.672: INFO: Pod "downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.551266ms
    Sep  1 10:56:12.676: INFO: Pod "downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012579984s
    Sep  1 10:56:14.678: INFO: Pod "downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01455675s
    STEP: Saw pod success 09/01/23 10:56:14.678
    Sep  1 10:56:14.678: INFO: Pod "downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0" satisfied condition "Succeeded or Failed"
    Sep  1 10:56:14.682: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0 container dapi-container: <nil>
    STEP: delete the pod 09/01/23 10:56:14.691
    Sep  1 10:56:14.706: INFO: Waiting for pod downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0 to disappear
    Sep  1 10:56:14.710: INFO: Pod downward-api-da9b552d-51d1-4588-8ee0-46bdcc040cf0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:56:14.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-981" for this suite. 09/01/23 10:56:14.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:56:14.724
Sep  1 10:56:14.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename events 09/01/23 10:56:14.726
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:14.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:14.758
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 09/01/23 10:56:14.763
STEP: listing all events in all namespaces 09/01/23 10:56:14.769
STEP: patching the test event 09/01/23 10:56:14.787
STEP: fetching the test event 09/01/23 10:56:14.793
STEP: updating the test event 09/01/23 10:56:14.796
STEP: getting the test event 09/01/23 10:56:14.805
STEP: deleting the test event 09/01/23 10:56:14.808
STEP: listing all events in all namespaces 09/01/23 10:56:14.815
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Sep  1 10:56:14.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8057" for this suite. 09/01/23 10:56:14.839
------------------------------
• [0.123 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:56:14.724
    Sep  1 10:56:14.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename events 09/01/23 10:56:14.726
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:14.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:14.758
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 09/01/23 10:56:14.763
    STEP: listing all events in all namespaces 09/01/23 10:56:14.769
    STEP: patching the test event 09/01/23 10:56:14.787
    STEP: fetching the test event 09/01/23 10:56:14.793
    STEP: updating the test event 09/01/23 10:56:14.796
    STEP: getting the test event 09/01/23 10:56:14.805
    STEP: deleting the test event 09/01/23 10:56:14.808
    STEP: listing all events in all namespaces 09/01/23 10:56:14.815
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:56:14.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8057" for this suite. 09/01/23 10:56:14.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:56:14.849
Sep  1 10:56:14.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename disruption 09/01/23 10:56:14.852
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:14.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:14.87
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:56:14.873
Sep  1 10:56:14.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename disruption-2 09/01/23 10:56:14.875
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:14.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:14.895
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 09/01/23 10:56:14.918
STEP: Waiting for the pdb to be processed 09/01/23 10:56:16.932
STEP: Waiting for the pdb to be processed 09/01/23 10:56:18.946
STEP: listing a collection of PDBs across all namespaces 09/01/23 10:56:20.957
STEP: listing a collection of PDBs in namespace disruption-905 09/01/23 10:56:20.961
STEP: deleting a collection of PDBs 09/01/23 10:56:20.964
STEP: Waiting for the PDB collection to be deleted 09/01/23 10:56:20.978
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Sep  1 10:56:20.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  1 10:56:20.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-3660" for this suite. 09/01/23 10:56:20.99
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-905" for this suite. 09/01/23 10:56:20.998
------------------------------
• [SLOW TEST] [6.156 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:56:14.849
    Sep  1 10:56:14.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename disruption 09/01/23 10:56:14.852
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:14.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:14.87
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:56:14.873
    Sep  1 10:56:14.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename disruption-2 09/01/23 10:56:14.875
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:14.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:14.895
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 09/01/23 10:56:14.918
    STEP: Waiting for the pdb to be processed 09/01/23 10:56:16.932
    STEP: Waiting for the pdb to be processed 09/01/23 10:56:18.946
    STEP: listing a collection of PDBs across all namespaces 09/01/23 10:56:20.957
    STEP: listing a collection of PDBs in namespace disruption-905 09/01/23 10:56:20.961
    STEP: deleting a collection of PDBs 09/01/23 10:56:20.964
    STEP: Waiting for the PDB collection to be deleted 09/01/23 10:56:20.978
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:56:20.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:56:20.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-3660" for this suite. 09/01/23 10:56:20.99
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-905" for this suite. 09/01/23 10:56:20.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:56:21.007
Sep  1 10:56:21.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename podtemplate 09/01/23 10:56:21.01
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:21.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:21.035
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  1 10:56:21.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8757" for this suite. 09/01/23 10:56:21.793
------------------------------
• [0.796 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:56:21.007
    Sep  1 10:56:21.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename podtemplate 09/01/23 10:56:21.01
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:21.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:21.035
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:56:21.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8757" for this suite. 09/01/23 10:56:21.793
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:56:21.808
Sep  1 10:56:21.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename job 09/01/23 10:56:21.81
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:21.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:21.828
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 09/01/23 10:56:21.834
STEP: Patching the Job 09/01/23 10:56:21.842
STEP: Watching for Job to be patched 09/01/23 10:56:21.857
Sep  1 10:56:21.966: INFO: Event ADDED observed for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking:]
Sep  1 10:56:21.967: INFO: Event MODIFIED found for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 09/01/23 10:56:21.967
STEP: Watching for Job to be updated 09/01/23 10:56:21.979
Sep  1 10:56:21.981: INFO: Event MODIFIED found for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  1 10:56:21.981: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 09/01/23 10:56:21.981
Sep  1 10:56:21.985: INFO: Job: e2e-fsj2g as labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g]
STEP: Waiting for job to complete 09/01/23 10:56:21.985
STEP: Delete a job collection with a labelselector 09/01/23 10:56:31.989
STEP: Watching for Job to be deleted 09/01/23 10:56:31.997
Sep  1 10:56:32.000: INFO: Event MODIFIED observed for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  1 10:56:32.000: INFO: Event MODIFIED observed for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  1 10:56:32.000: INFO: Event MODIFIED observed for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  1 10:56:32.001: INFO: Event MODIFIED observed for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  1 10:56:32.001: INFO: Event MODIFIED observed for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  1 10:56:32.001: INFO: Event DELETED found for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 09/01/23 10:56:32.001
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  1 10:56:32.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1978" for this suite. 09/01/23 10:56:32.023
------------------------------
• [SLOW TEST] [10.228 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:56:21.808
    Sep  1 10:56:21.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename job 09/01/23 10:56:21.81
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:21.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:21.828
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 09/01/23 10:56:21.834
    STEP: Patching the Job 09/01/23 10:56:21.842
    STEP: Watching for Job to be patched 09/01/23 10:56:21.857
    Sep  1 10:56:21.966: INFO: Event ADDED observed for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking:]
    Sep  1 10:56:21.967: INFO: Event MODIFIED found for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 09/01/23 10:56:21.967
    STEP: Watching for Job to be updated 09/01/23 10:56:21.979
    Sep  1 10:56:21.981: INFO: Event MODIFIED found for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  1 10:56:21.981: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 09/01/23 10:56:21.981
    Sep  1 10:56:21.985: INFO: Job: e2e-fsj2g as labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g]
    STEP: Waiting for job to complete 09/01/23 10:56:21.985
    STEP: Delete a job collection with a labelselector 09/01/23 10:56:31.989
    STEP: Watching for Job to be deleted 09/01/23 10:56:31.997
    Sep  1 10:56:32.000: INFO: Event MODIFIED observed for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  1 10:56:32.000: INFO: Event MODIFIED observed for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  1 10:56:32.000: INFO: Event MODIFIED observed for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  1 10:56:32.001: INFO: Event MODIFIED observed for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  1 10:56:32.001: INFO: Event MODIFIED observed for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  1 10:56:32.001: INFO: Event DELETED found for Job e2e-fsj2g in namespace job-1978 with labels: map[e2e-fsj2g:patched e2e-job-label:e2e-fsj2g] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 09/01/23 10:56:32.001
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:56:32.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1978" for this suite. 09/01/23 10:56:32.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:56:32.047
Sep  1 10:56:32.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 10:56:32.048
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:32.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:32.08
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-f8454666-50da-4946-9261-cc387f5c2440 09/01/23 10:56:32.084
STEP: Creating a pod to test consume secrets 09/01/23 10:56:32.091
Sep  1 10:56:32.102: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540" in namespace "projected-3071" to be "Succeeded or Failed"
Sep  1 10:56:32.105: INFO: Pod "pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540": Phase="Pending", Reason="", readiness=false. Elapsed: 2.758027ms
Sep  1 10:56:34.109: INFO: Pod "pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006803215s
Sep  1 10:56:36.109: INFO: Pod "pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007336916s
STEP: Saw pod success 09/01/23 10:56:36.109
Sep  1 10:56:36.110: INFO: Pod "pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540" satisfied condition "Succeeded or Failed"
Sep  1 10:56:36.112: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/01/23 10:56:36.119
Sep  1 10:56:36.133: INFO: Waiting for pod pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540 to disappear
Sep  1 10:56:36.136: INFO: Pod pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  1 10:56:36.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3071" for this suite. 09/01/23 10:56:36.14
------------------------------
• [4.100 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:56:32.047
    Sep  1 10:56:32.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 10:56:32.048
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:32.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:32.08
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-f8454666-50da-4946-9261-cc387f5c2440 09/01/23 10:56:32.084
    STEP: Creating a pod to test consume secrets 09/01/23 10:56:32.091
    Sep  1 10:56:32.102: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540" in namespace "projected-3071" to be "Succeeded or Failed"
    Sep  1 10:56:32.105: INFO: Pod "pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540": Phase="Pending", Reason="", readiness=false. Elapsed: 2.758027ms
    Sep  1 10:56:34.109: INFO: Pod "pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006803215s
    Sep  1 10:56:36.109: INFO: Pod "pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007336916s
    STEP: Saw pod success 09/01/23 10:56:36.109
    Sep  1 10:56:36.110: INFO: Pod "pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540" satisfied condition "Succeeded or Failed"
    Sep  1 10:56:36.112: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/01/23 10:56:36.119
    Sep  1 10:56:36.133: INFO: Waiting for pod pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540 to disappear
    Sep  1 10:56:36.136: INFO: Pod pod-projected-secrets-22f0d2a7-995d-4b5b-bbb9-b5bcc0888540 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:56:36.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3071" for this suite. 09/01/23 10:56:36.14
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:56:36.146
Sep  1 10:56:36.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 10:56:36.149
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:36.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:36.17
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/01/23 10:56:36.175
Sep  1 10:56:36.185: INFO: Waiting up to 5m0s for pod "pod-c4be7b3a-eace-4594-a06e-68445fc19395" in namespace "emptydir-7931" to be "Succeeded or Failed"
Sep  1 10:56:36.190: INFO: Pod "pod-c4be7b3a-eace-4594-a06e-68445fc19395": Phase="Pending", Reason="", readiness=false. Elapsed: 4.518444ms
Sep  1 10:56:38.193: INFO: Pod "pod-c4be7b3a-eace-4594-a06e-68445fc19395": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008172787s
Sep  1 10:56:40.194: INFO: Pod "pod-c4be7b3a-eace-4594-a06e-68445fc19395": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009180748s
STEP: Saw pod success 09/01/23 10:56:40.194
Sep  1 10:56:40.195: INFO: Pod "pod-c4be7b3a-eace-4594-a06e-68445fc19395" satisfied condition "Succeeded or Failed"
Sep  1 10:56:40.198: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-c4be7b3a-eace-4594-a06e-68445fc19395 container test-container: <nil>
STEP: delete the pod 09/01/23 10:56:40.204
Sep  1 10:56:40.218: INFO: Waiting for pod pod-c4be7b3a-eace-4594-a06e-68445fc19395 to disappear
Sep  1 10:56:40.222: INFO: Pod pod-c4be7b3a-eace-4594-a06e-68445fc19395 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 10:56:40.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7931" for this suite. 09/01/23 10:56:40.228
------------------------------
• [4.089 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:56:36.146
    Sep  1 10:56:36.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 10:56:36.149
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:36.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:36.17
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/01/23 10:56:36.175
    Sep  1 10:56:36.185: INFO: Waiting up to 5m0s for pod "pod-c4be7b3a-eace-4594-a06e-68445fc19395" in namespace "emptydir-7931" to be "Succeeded or Failed"
    Sep  1 10:56:36.190: INFO: Pod "pod-c4be7b3a-eace-4594-a06e-68445fc19395": Phase="Pending", Reason="", readiness=false. Elapsed: 4.518444ms
    Sep  1 10:56:38.193: INFO: Pod "pod-c4be7b3a-eace-4594-a06e-68445fc19395": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008172787s
    Sep  1 10:56:40.194: INFO: Pod "pod-c4be7b3a-eace-4594-a06e-68445fc19395": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009180748s
    STEP: Saw pod success 09/01/23 10:56:40.194
    Sep  1 10:56:40.195: INFO: Pod "pod-c4be7b3a-eace-4594-a06e-68445fc19395" satisfied condition "Succeeded or Failed"
    Sep  1 10:56:40.198: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-c4be7b3a-eace-4594-a06e-68445fc19395 container test-container: <nil>
    STEP: delete the pod 09/01/23 10:56:40.204
    Sep  1 10:56:40.218: INFO: Waiting for pod pod-c4be7b3a-eace-4594-a06e-68445fc19395 to disappear
    Sep  1 10:56:40.222: INFO: Pod pod-c4be7b3a-eace-4594-a06e-68445fc19395 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:56:40.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7931" for this suite. 09/01/23 10:56:40.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:56:40.24
Sep  1 10:56:40.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 10:56:40.241
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:40.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:40.262
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/01/23 10:56:40.267
Sep  1 10:56:40.280: INFO: Waiting up to 5m0s for pod "pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97" in namespace "emptydir-8060" to be "Succeeded or Failed"
Sep  1 10:56:40.297: INFO: Pod "pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97": Phase="Pending", Reason="", readiness=false. Elapsed: 17.568829ms
Sep  1 10:56:42.302: INFO: Pod "pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02182317s
Sep  1 10:56:44.301: INFO: Pod "pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021624759s
STEP: Saw pod success 09/01/23 10:56:44.302
Sep  1 10:56:44.302: INFO: Pod "pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97" satisfied condition "Succeeded or Failed"
Sep  1 10:56:44.305: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97 container test-container: <nil>
STEP: delete the pod 09/01/23 10:56:44.311
Sep  1 10:56:44.325: INFO: Waiting for pod pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97 to disappear
Sep  1 10:56:44.329: INFO: Pod pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 10:56:44.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8060" for this suite. 09/01/23 10:56:44.333
------------------------------
• [4.102 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:56:40.24
    Sep  1 10:56:40.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 10:56:40.241
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:40.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:40.262
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/01/23 10:56:40.267
    Sep  1 10:56:40.280: INFO: Waiting up to 5m0s for pod "pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97" in namespace "emptydir-8060" to be "Succeeded or Failed"
    Sep  1 10:56:40.297: INFO: Pod "pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97": Phase="Pending", Reason="", readiness=false. Elapsed: 17.568829ms
    Sep  1 10:56:42.302: INFO: Pod "pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02182317s
    Sep  1 10:56:44.301: INFO: Pod "pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021624759s
    STEP: Saw pod success 09/01/23 10:56:44.302
    Sep  1 10:56:44.302: INFO: Pod "pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97" satisfied condition "Succeeded or Failed"
    Sep  1 10:56:44.305: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97 container test-container: <nil>
    STEP: delete the pod 09/01/23 10:56:44.311
    Sep  1 10:56:44.325: INFO: Waiting for pod pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97 to disappear
    Sep  1 10:56:44.329: INFO: Pod pod-d574ae0f-9ddf-4a3e-9fc9-235bb7d49c97 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:56:44.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8060" for this suite. 09/01/23 10:56:44.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:56:44.347
Sep  1 10:56:44.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename security-context 09/01/23 10:56:44.349
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:44.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:44.37
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/01/23 10:56:44.373
Sep  1 10:56:44.381: INFO: Waiting up to 5m0s for pod "security-context-6999cdd2-bad9-4947-9815-aad50c048ef2" in namespace "security-context-4674" to be "Succeeded or Failed"
Sep  1 10:56:44.385: INFO: Pod "security-context-6999cdd2-bad9-4947-9815-aad50c048ef2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.722078ms
Sep  1 10:56:46.391: INFO: Pod "security-context-6999cdd2-bad9-4947-9815-aad50c048ef2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009005169s
Sep  1 10:56:48.390: INFO: Pod "security-context-6999cdd2-bad9-4947-9815-aad50c048ef2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008332258s
STEP: Saw pod success 09/01/23 10:56:48.39
Sep  1 10:56:48.390: INFO: Pod "security-context-6999cdd2-bad9-4947-9815-aad50c048ef2" satisfied condition "Succeeded or Failed"
Sep  1 10:56:48.393: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod security-context-6999cdd2-bad9-4947-9815-aad50c048ef2 container test-container: <nil>
STEP: delete the pod 09/01/23 10:56:48.399
Sep  1 10:56:48.414: INFO: Waiting for pod security-context-6999cdd2-bad9-4947-9815-aad50c048ef2 to disappear
Sep  1 10:56:48.417: INFO: Pod security-context-6999cdd2-bad9-4947-9815-aad50c048ef2 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  1 10:56:48.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4674" for this suite. 09/01/23 10:56:48.422
------------------------------
• [4.085 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:56:44.347
    Sep  1 10:56:44.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename security-context 09/01/23 10:56:44.349
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:44.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:44.37
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/01/23 10:56:44.373
    Sep  1 10:56:44.381: INFO: Waiting up to 5m0s for pod "security-context-6999cdd2-bad9-4947-9815-aad50c048ef2" in namespace "security-context-4674" to be "Succeeded or Failed"
    Sep  1 10:56:44.385: INFO: Pod "security-context-6999cdd2-bad9-4947-9815-aad50c048ef2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.722078ms
    Sep  1 10:56:46.391: INFO: Pod "security-context-6999cdd2-bad9-4947-9815-aad50c048ef2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009005169s
    Sep  1 10:56:48.390: INFO: Pod "security-context-6999cdd2-bad9-4947-9815-aad50c048ef2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008332258s
    STEP: Saw pod success 09/01/23 10:56:48.39
    Sep  1 10:56:48.390: INFO: Pod "security-context-6999cdd2-bad9-4947-9815-aad50c048ef2" satisfied condition "Succeeded or Failed"
    Sep  1 10:56:48.393: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod security-context-6999cdd2-bad9-4947-9815-aad50c048ef2 container test-container: <nil>
    STEP: delete the pod 09/01/23 10:56:48.399
    Sep  1 10:56:48.414: INFO: Waiting for pod security-context-6999cdd2-bad9-4947-9815-aad50c048ef2 to disappear
    Sep  1 10:56:48.417: INFO: Pod security-context-6999cdd2-bad9-4947-9815-aad50c048ef2 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:56:48.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4674" for this suite. 09/01/23 10:56:48.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:56:48.44
Sep  1 10:56:48.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename job 09/01/23 10:56:48.441
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:48.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:48.466
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 09/01/23 10:56:48.469
STEP: Ensuring job reaches completions 09/01/23 10:56:48.475
STEP: Ensuring pods with index for job exist 09/01/23 10:57:00.479
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  1 10:57:00.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8144" for this suite. 09/01/23 10:57:00.489
------------------------------
• [SLOW TEST] [12.055 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:56:48.44
    Sep  1 10:56:48.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename job 09/01/23 10:56:48.441
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:56:48.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:56:48.466
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 09/01/23 10:56:48.469
    STEP: Ensuring job reaches completions 09/01/23 10:56:48.475
    STEP: Ensuring pods with index for job exist 09/01/23 10:57:00.479
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:57:00.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8144" for this suite. 09/01/23 10:57:00.489
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:57:00.496
Sep  1 10:57:00.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 10:57:00.498
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:57:00.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:57:00.523
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 09/01/23 10:57:00.527
Sep  1 10:57:00.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad" in namespace "projected-3662" to be "Succeeded or Failed"
Sep  1 10:57:00.538: INFO: Pod "downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.81214ms
Sep  1 10:57:02.543: INFO: Pod "downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007141655s
Sep  1 10:57:04.542: INFO: Pod "downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006754923s
STEP: Saw pod success 09/01/23 10:57:04.542
Sep  1 10:57:04.542: INFO: Pod "downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad" satisfied condition "Succeeded or Failed"
Sep  1 10:57:04.545: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad container client-container: <nil>
STEP: delete the pod 09/01/23 10:57:04.55
Sep  1 10:57:04.565: INFO: Waiting for pod downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad to disappear
Sep  1 10:57:04.569: INFO: Pod downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  1 10:57:04.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3662" for this suite. 09/01/23 10:57:04.575
------------------------------
• [4.085 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:57:00.496
    Sep  1 10:57:00.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 10:57:00.498
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:57:00.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:57:00.523
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 09/01/23 10:57:00.527
    Sep  1 10:57:00.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad" in namespace "projected-3662" to be "Succeeded or Failed"
    Sep  1 10:57:00.538: INFO: Pod "downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.81214ms
    Sep  1 10:57:02.543: INFO: Pod "downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007141655s
    Sep  1 10:57:04.542: INFO: Pod "downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006754923s
    STEP: Saw pod success 09/01/23 10:57:04.542
    Sep  1 10:57:04.542: INFO: Pod "downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad" satisfied condition "Succeeded or Failed"
    Sep  1 10:57:04.545: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad container client-container: <nil>
    STEP: delete the pod 09/01/23 10:57:04.55
    Sep  1 10:57:04.565: INFO: Waiting for pod downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad to disappear
    Sep  1 10:57:04.569: INFO: Pod downwardapi-volume-92e6cbc8-5528-4687-89e1-99c3ddf888ad no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:57:04.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3662" for this suite. 09/01/23 10:57:04.575
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:57:04.583
Sep  1 10:57:04.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 10:57:04.586
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:57:04.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:57:04.623
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/01/23 10:57:04.627
Sep  1 10:57:04.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/01/23 10:57:18.872
Sep  1 10:57:18.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 10:57:23.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:57:39.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6973" for this suite. 09/01/23 10:57:39.319
------------------------------
• [SLOW TEST] [34.743 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:57:04.583
    Sep  1 10:57:04.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 10:57:04.586
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:57:04.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:57:04.623
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/01/23 10:57:04.627
    Sep  1 10:57:04.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/01/23 10:57:18.872
    Sep  1 10:57:18.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 10:57:23.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:57:39.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6973" for this suite. 09/01/23 10:57:39.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:57:39.333
Sep  1 10:57:39.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 10:57:39.335
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:57:39.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:57:39.357
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 09/01/23 10:57:39.407
Sep  1 10:57:39.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6234 create -f -'
Sep  1 10:57:41.305: INFO: stderr: ""
Sep  1 10:57:41.305: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 09/01/23 10:57:41.305
Sep  1 10:57:41.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6234 diff -f -'
Sep  1 10:57:42.400: INFO: rc: 1
Sep  1 10:57:42.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6234 delete -f -'
Sep  1 10:57:42.528: INFO: stderr: ""
Sep  1 10:57:42.528: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 10:57:42.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6234" for this suite. 09/01/23 10:57:42.532
------------------------------
• [3.208 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:57:39.333
    Sep  1 10:57:39.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 10:57:39.335
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:57:39.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:57:39.357
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 09/01/23 10:57:39.407
    Sep  1 10:57:39.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6234 create -f -'
    Sep  1 10:57:41.305: INFO: stderr: ""
    Sep  1 10:57:41.305: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 09/01/23 10:57:41.305
    Sep  1 10:57:41.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6234 diff -f -'
    Sep  1 10:57:42.400: INFO: rc: 1
    Sep  1 10:57:42.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6234 delete -f -'
    Sep  1 10:57:42.528: INFO: stderr: ""
    Sep  1 10:57:42.528: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:57:42.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6234" for this suite. 09/01/23 10:57:42.532
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:57:42.542
Sep  1 10:57:42.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename subpath 09/01/23 10:57:42.545
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:57:42.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:57:42.562
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/01/23 10:57:42.566
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-hrx9 09/01/23 10:57:42.577
STEP: Creating a pod to test atomic-volume-subpath 09/01/23 10:57:42.577
Sep  1 10:57:42.588: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hrx9" in namespace "subpath-9684" to be "Succeeded or Failed"
Sep  1 10:57:42.593: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699697ms
Sep  1 10:57:44.598: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009303819s
Sep  1 10:57:46.597: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 4.008618824s
Sep  1 10:57:48.597: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 6.008719857s
Sep  1 10:57:50.598: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 8.009541301s
Sep  1 10:57:52.598: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 10.009273184s
Sep  1 10:57:54.674: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 12.085536565s
Sep  1 10:57:56.597: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 14.00877719s
Sep  1 10:57:58.598: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 16.009771354s
Sep  1 10:58:00.599: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 18.0107236s
Sep  1 10:58:02.598: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 20.009662501s
Sep  1 10:58:04.597: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=false. Elapsed: 22.008709194s
Sep  1 10:58:06.597: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008492075s
STEP: Saw pod success 09/01/23 10:58:06.597
Sep  1 10:58:06.597: INFO: Pod "pod-subpath-test-projected-hrx9" satisfied condition "Succeeded or Failed"
Sep  1 10:58:06.600: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-subpath-test-projected-hrx9 container test-container-subpath-projected-hrx9: <nil>
STEP: delete the pod 09/01/23 10:58:06.607
Sep  1 10:58:06.624: INFO: Waiting for pod pod-subpath-test-projected-hrx9 to disappear
Sep  1 10:58:06.627: INFO: Pod pod-subpath-test-projected-hrx9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-hrx9 09/01/23 10:58:06.627
Sep  1 10:58:06.627: INFO: Deleting pod "pod-subpath-test-projected-hrx9" in namespace "subpath-9684"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  1 10:58:06.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9684" for this suite. 09/01/23 10:58:06.634
------------------------------
• [SLOW TEST] [24.099 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:57:42.542
    Sep  1 10:57:42.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename subpath 09/01/23 10:57:42.545
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:57:42.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:57:42.562
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/01/23 10:57:42.566
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-hrx9 09/01/23 10:57:42.577
    STEP: Creating a pod to test atomic-volume-subpath 09/01/23 10:57:42.577
    Sep  1 10:57:42.588: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hrx9" in namespace "subpath-9684" to be "Succeeded or Failed"
    Sep  1 10:57:42.593: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699697ms
    Sep  1 10:57:44.598: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009303819s
    Sep  1 10:57:46.597: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 4.008618824s
    Sep  1 10:57:48.597: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 6.008719857s
    Sep  1 10:57:50.598: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 8.009541301s
    Sep  1 10:57:52.598: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 10.009273184s
    Sep  1 10:57:54.674: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 12.085536565s
    Sep  1 10:57:56.597: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 14.00877719s
    Sep  1 10:57:58.598: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 16.009771354s
    Sep  1 10:58:00.599: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 18.0107236s
    Sep  1 10:58:02.598: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=true. Elapsed: 20.009662501s
    Sep  1 10:58:04.597: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Running", Reason="", readiness=false. Elapsed: 22.008709194s
    Sep  1 10:58:06.597: INFO: Pod "pod-subpath-test-projected-hrx9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008492075s
    STEP: Saw pod success 09/01/23 10:58:06.597
    Sep  1 10:58:06.597: INFO: Pod "pod-subpath-test-projected-hrx9" satisfied condition "Succeeded or Failed"
    Sep  1 10:58:06.600: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-subpath-test-projected-hrx9 container test-container-subpath-projected-hrx9: <nil>
    STEP: delete the pod 09/01/23 10:58:06.607
    Sep  1 10:58:06.624: INFO: Waiting for pod pod-subpath-test-projected-hrx9 to disappear
    Sep  1 10:58:06.627: INFO: Pod pod-subpath-test-projected-hrx9 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-hrx9 09/01/23 10:58:06.627
    Sep  1 10:58:06.627: INFO: Deleting pod "pod-subpath-test-projected-hrx9" in namespace "subpath-9684"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:58:06.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9684" for this suite. 09/01/23 10:58:06.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:58:06.646
Sep  1 10:58:06.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename cronjob 09/01/23 10:58:06.647
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:58:06.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:58:06.673
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 09/01/23 10:58:06.677
STEP: creating 09/01/23 10:58:06.677
STEP: getting 09/01/23 10:58:06.684
STEP: listing 09/01/23 10:58:06.687
STEP: watching 09/01/23 10:58:06.69
Sep  1 10:58:06.690: INFO: starting watch
STEP: cluster-wide listing 09/01/23 10:58:06.693
STEP: cluster-wide watching 09/01/23 10:58:06.698
Sep  1 10:58:06.699: INFO: starting watch
STEP: patching 09/01/23 10:58:06.7
STEP: updating 09/01/23 10:58:06.707
Sep  1 10:58:06.718: INFO: waiting for watch events with expected annotations
Sep  1 10:58:06.719: INFO: saw patched and updated annotations
STEP: patching /status 09/01/23 10:58:06.72
STEP: updating /status 09/01/23 10:58:06.726
STEP: get /status 09/01/23 10:58:06.734
STEP: deleting 09/01/23 10:58:06.738
STEP: deleting a collection 09/01/23 10:58:06.756
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  1 10:58:06.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3399" for this suite. 09/01/23 10:58:06.77
------------------------------
• [0.131 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:58:06.646
    Sep  1 10:58:06.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename cronjob 09/01/23 10:58:06.647
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:58:06.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:58:06.673
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 09/01/23 10:58:06.677
    STEP: creating 09/01/23 10:58:06.677
    STEP: getting 09/01/23 10:58:06.684
    STEP: listing 09/01/23 10:58:06.687
    STEP: watching 09/01/23 10:58:06.69
    Sep  1 10:58:06.690: INFO: starting watch
    STEP: cluster-wide listing 09/01/23 10:58:06.693
    STEP: cluster-wide watching 09/01/23 10:58:06.698
    Sep  1 10:58:06.699: INFO: starting watch
    STEP: patching 09/01/23 10:58:06.7
    STEP: updating 09/01/23 10:58:06.707
    Sep  1 10:58:06.718: INFO: waiting for watch events with expected annotations
    Sep  1 10:58:06.719: INFO: saw patched and updated annotations
    STEP: patching /status 09/01/23 10:58:06.72
    STEP: updating /status 09/01/23 10:58:06.726
    STEP: get /status 09/01/23 10:58:06.734
    STEP: deleting 09/01/23 10:58:06.738
    STEP: deleting a collection 09/01/23 10:58:06.756
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:58:06.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3399" for this suite. 09/01/23 10:58:06.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:58:06.778
Sep  1 10:58:06.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename secrets 09/01/23 10:58:06.781
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:58:06.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:58:06.805
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 09/01/23 10:58:06.808
STEP: listing secrets in all namespaces to ensure that there are more than zero 09/01/23 10:58:06.814
STEP: patching the secret 09/01/23 10:58:06.818
STEP: deleting the secret using a LabelSelector 09/01/23 10:58:06.827
STEP: listing secrets in all namespaces, searching for label name and value in patch 09/01/23 10:58:06.837
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  1 10:58:06.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-523" for this suite. 09/01/23 10:58:06.847
------------------------------
• [0.076 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:58:06.778
    Sep  1 10:58:06.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename secrets 09/01/23 10:58:06.781
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:58:06.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:58:06.805
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 09/01/23 10:58:06.808
    STEP: listing secrets in all namespaces to ensure that there are more than zero 09/01/23 10:58:06.814
    STEP: patching the secret 09/01/23 10:58:06.818
    STEP: deleting the secret using a LabelSelector 09/01/23 10:58:06.827
    STEP: listing secrets in all namespaces, searching for label name and value in patch 09/01/23 10:58:06.837
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:58:06.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-523" for this suite. 09/01/23 10:58:06.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:58:06.855
Sep  1 10:58:06.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename certificates 09/01/23 10:58:06.856
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:58:06.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:58:06.929
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 09/01/23 10:58:07.396
STEP: getting /apis/certificates.k8s.io 09/01/23 10:58:07.4
STEP: getting /apis/certificates.k8s.io/v1 09/01/23 10:58:07.401
STEP: creating 09/01/23 10:58:07.402
STEP: getting 09/01/23 10:58:07.461
STEP: listing 09/01/23 10:58:07.463
STEP: watching 09/01/23 10:58:07.467
Sep  1 10:58:07.467: INFO: starting watch
STEP: patching 09/01/23 10:58:07.468
STEP: updating 09/01/23 10:58:07.477
Sep  1 10:58:07.494: INFO: waiting for watch events with expected annotations
Sep  1 10:58:07.494: INFO: saw patched and updated annotations
STEP: getting /approval 09/01/23 10:58:07.495
STEP: patching /approval 09/01/23 10:58:07.601
STEP: updating /approval 09/01/23 10:58:07.608
STEP: getting /status 09/01/23 10:58:07.651
STEP: patching /status 09/01/23 10:58:07.655
STEP: updating /status 09/01/23 10:58:07.68
STEP: deleting 09/01/23 10:58:07.718
STEP: deleting a collection 09/01/23 10:58:07.731
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:58:07.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-3522" for this suite. 09/01/23 10:58:07.751
------------------------------
• [0.906 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:58:06.855
    Sep  1 10:58:06.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename certificates 09/01/23 10:58:06.856
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:58:06.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:58:06.929
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 09/01/23 10:58:07.396
    STEP: getting /apis/certificates.k8s.io 09/01/23 10:58:07.4
    STEP: getting /apis/certificates.k8s.io/v1 09/01/23 10:58:07.401
    STEP: creating 09/01/23 10:58:07.402
    STEP: getting 09/01/23 10:58:07.461
    STEP: listing 09/01/23 10:58:07.463
    STEP: watching 09/01/23 10:58:07.467
    Sep  1 10:58:07.467: INFO: starting watch
    STEP: patching 09/01/23 10:58:07.468
    STEP: updating 09/01/23 10:58:07.477
    Sep  1 10:58:07.494: INFO: waiting for watch events with expected annotations
    Sep  1 10:58:07.494: INFO: saw patched and updated annotations
    STEP: getting /approval 09/01/23 10:58:07.495
    STEP: patching /approval 09/01/23 10:58:07.601
    STEP: updating /approval 09/01/23 10:58:07.608
    STEP: getting /status 09/01/23 10:58:07.651
    STEP: patching /status 09/01/23 10:58:07.655
    STEP: updating /status 09/01/23 10:58:07.68
    STEP: deleting 09/01/23 10:58:07.718
    STEP: deleting a collection 09/01/23 10:58:07.731
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:58:07.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-3522" for this suite. 09/01/23 10:58:07.751
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:58:07.762
Sep  1 10:58:07.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename replication-controller 09/01/23 10:58:07.764
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:58:07.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:58:07.824
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-xssgq" 09/01/23 10:58:07.828
Sep  1 10:58:07.834: INFO: Get Replication Controller "e2e-rc-xssgq" to confirm replicas
Sep  1 10:58:08.838: INFO: Get Replication Controller "e2e-rc-xssgq" to confirm replicas
Sep  1 10:58:08.842: INFO: Found 1 replicas for "e2e-rc-xssgq" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-xssgq" 09/01/23 10:58:08.842
STEP: Updating a scale subresource 09/01/23 10:58:08.957
STEP: Verifying replicas where modified for replication controller "e2e-rc-xssgq" 09/01/23 10:58:08.967
Sep  1 10:58:08.967: INFO: Get Replication Controller "e2e-rc-xssgq" to confirm replicas
Sep  1 10:58:09.971: INFO: Get Replication Controller "e2e-rc-xssgq" to confirm replicas
Sep  1 10:58:09.974: INFO: Found 2 replicas for "e2e-rc-xssgq" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  1 10:58:09.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9344" for this suite. 09/01/23 10:58:09.978
------------------------------
• [2.225 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:58:07.762
    Sep  1 10:58:07.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename replication-controller 09/01/23 10:58:07.764
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:58:07.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:58:07.824
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-xssgq" 09/01/23 10:58:07.828
    Sep  1 10:58:07.834: INFO: Get Replication Controller "e2e-rc-xssgq" to confirm replicas
    Sep  1 10:58:08.838: INFO: Get Replication Controller "e2e-rc-xssgq" to confirm replicas
    Sep  1 10:58:08.842: INFO: Found 1 replicas for "e2e-rc-xssgq" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-xssgq" 09/01/23 10:58:08.842
    STEP: Updating a scale subresource 09/01/23 10:58:08.957
    STEP: Verifying replicas where modified for replication controller "e2e-rc-xssgq" 09/01/23 10:58:08.967
    Sep  1 10:58:08.967: INFO: Get Replication Controller "e2e-rc-xssgq" to confirm replicas
    Sep  1 10:58:09.971: INFO: Get Replication Controller "e2e-rc-xssgq" to confirm replicas
    Sep  1 10:58:09.974: INFO: Found 2 replicas for "e2e-rc-xssgq" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:58:09.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9344" for this suite. 09/01/23 10:58:09.978
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:58:09.994
Sep  1 10:58:09.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename statefulset 09/01/23 10:58:09.996
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:58:10.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:58:10.014
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6542 09/01/23 10:58:10.017
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-6542 09/01/23 10:58:10.026
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6542 09/01/23 10:58:10.034
Sep  1 10:58:10.046: INFO: Found 0 stateful pods, waiting for 1
Sep  1 10:58:20.051: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/01/23 10:58:20.052
Sep  1 10:58:20.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-6542 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  1 10:58:20.281: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  1 10:58:20.281: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  1 10:58:20.281: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  1 10:58:20.286: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  1 10:58:30.291: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  1 10:58:30.291: INFO: Waiting for statefulset status.replicas updated to 0
Sep  1 10:58:30.328: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Sep  1 10:58:30.328: INFO: ss-0  k8s-worker-1.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:10 +0000 UTC  }]
Sep  1 10:58:30.328: INFO: ss-2                                          Pending         []
Sep  1 10:58:30.328: INFO: 
Sep  1 10:58:30.328: INFO: StatefulSet ss has not reached scale 3, at 2
Sep  1 10:58:31.332: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.983915939s
Sep  1 10:58:32.336: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979802982s
Sep  1 10:58:33.340: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975756207s
Sep  1 10:58:34.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970839349s
Sep  1 10:58:35.350: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.966618122s
Sep  1 10:58:36.354: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.961837952s
Sep  1 10:58:37.359: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.956957771s
Sep  1 10:58:38.364: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952306786s
Sep  1 10:58:39.369: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.549626ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6542 09/01/23 10:58:40.369
Sep  1 10:58:40.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-6542 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  1 10:58:40.561: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  1 10:58:40.561: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  1 10:58:40.561: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  1 10:58:40.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-6542 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  1 10:58:40.780: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  1 10:58:40.781: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  1 10:58:40.781: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  1 10:58:40.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-6542 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  1 10:58:41.016: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  1 10:58:41.016: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  1 10:58:41.016: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  1 10:58:41.021: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Sep  1 10:58:51.026: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  1 10:58:51.026: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  1 10:58:51.026: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 09/01/23 10:58:51.026
Sep  1 10:58:51.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-6542 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  1 10:58:51.217: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  1 10:58:51.217: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  1 10:58:51.217: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  1 10:58:51.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-6542 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  1 10:58:51.421: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  1 10:58:51.421: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  1 10:58:51.421: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  1 10:58:51.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-6542 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  1 10:58:51.629: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  1 10:58:51.629: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  1 10:58:51.629: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  1 10:58:51.629: INFO: Waiting for statefulset status.replicas updated to 0
Sep  1 10:58:51.631: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep  1 10:59:01.639: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  1 10:59:01.639: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  1 10:59:01.639: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  1 10:59:01.655: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Sep  1 10:59:01.655: INFO: ss-0  k8s-worker-1.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:10 +0000 UTC  }]
Sep  1 10:59:01.655: INFO: ss-1  k8s-worker-1.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  }]
Sep  1 10:59:01.655: INFO: ss-2  k8s-worker-2.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  }]
Sep  1 10:59:01.655: INFO: 
Sep  1 10:59:01.655: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  1 10:59:02.660: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Sep  1 10:59:02.660: INFO: ss-0  k8s-worker-1.c.operations-lab.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:10 +0000 UTC  }]
Sep  1 10:59:02.661: INFO: ss-1  k8s-worker-1.c.operations-lab.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  }]
Sep  1 10:59:02.661: INFO: ss-2  k8s-worker-2.c.operations-lab.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  }]
Sep  1 10:59:02.661: INFO: 
Sep  1 10:59:02.661: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  1 10:59:03.665: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988001881s
Sep  1 10:59:04.669: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.983382304s
Sep  1 10:59:05.673: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.980202925s
Sep  1 10:59:06.678: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.976495057s
Sep  1 10:59:07.682: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.971242762s
Sep  1 10:59:08.686: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.967579261s
Sep  1 10:59:09.689: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.963882842s
Sep  1 10:59:10.694: INFO: Verifying statefulset ss doesn't scale past 0 for another 960.206522ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6542 09/01/23 10:59:11.694
Sep  1 10:59:11.699: INFO: Scaling statefulset ss to 0
Sep  1 10:59:11.709: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  1 10:59:11.715: INFO: Deleting all statefulset in ns statefulset-6542
Sep  1 10:59:11.718: INFO: Scaling statefulset ss to 0
Sep  1 10:59:11.738: INFO: Waiting for statefulset status.replicas updated to 0
Sep  1 10:59:11.740: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  1 10:59:11.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6542" for this suite. 09/01/23 10:59:11.797
------------------------------
• [SLOW TEST] [61.810 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:58:09.994
    Sep  1 10:58:09.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename statefulset 09/01/23 10:58:09.996
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:58:10.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:58:10.014
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6542 09/01/23 10:58:10.017
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-6542 09/01/23 10:58:10.026
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6542 09/01/23 10:58:10.034
    Sep  1 10:58:10.046: INFO: Found 0 stateful pods, waiting for 1
    Sep  1 10:58:20.051: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/01/23 10:58:20.052
    Sep  1 10:58:20.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-6542 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  1 10:58:20.281: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  1 10:58:20.281: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  1 10:58:20.281: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  1 10:58:20.286: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep  1 10:58:30.291: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  1 10:58:30.291: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  1 10:58:30.328: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
    Sep  1 10:58:30.328: INFO: ss-0  k8s-worker-1.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:10 +0000 UTC  }]
    Sep  1 10:58:30.328: INFO: ss-2                                          Pending         []
    Sep  1 10:58:30.328: INFO: 
    Sep  1 10:58:30.328: INFO: StatefulSet ss has not reached scale 3, at 2
    Sep  1 10:58:31.332: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.983915939s
    Sep  1 10:58:32.336: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979802982s
    Sep  1 10:58:33.340: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975756207s
    Sep  1 10:58:34.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970839349s
    Sep  1 10:58:35.350: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.966618122s
    Sep  1 10:58:36.354: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.961837952s
    Sep  1 10:58:37.359: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.956957771s
    Sep  1 10:58:38.364: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952306786s
    Sep  1 10:58:39.369: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.549626ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6542 09/01/23 10:58:40.369
    Sep  1 10:58:40.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-6542 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  1 10:58:40.561: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  1 10:58:40.561: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  1 10:58:40.561: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  1 10:58:40.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-6542 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  1 10:58:40.780: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep  1 10:58:40.781: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  1 10:58:40.781: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  1 10:58:40.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-6542 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  1 10:58:41.016: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep  1 10:58:41.016: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  1 10:58:41.016: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  1 10:58:41.021: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Sep  1 10:58:51.026: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  1 10:58:51.026: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  1 10:58:51.026: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 09/01/23 10:58:51.026
    Sep  1 10:58:51.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-6542 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  1 10:58:51.217: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  1 10:58:51.217: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  1 10:58:51.217: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  1 10:58:51.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-6542 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  1 10:58:51.421: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  1 10:58:51.421: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  1 10:58:51.421: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  1 10:58:51.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-6542 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  1 10:58:51.629: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  1 10:58:51.629: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  1 10:58:51.629: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  1 10:58:51.629: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  1 10:58:51.631: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Sep  1 10:59:01.639: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  1 10:59:01.639: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep  1 10:59:01.639: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep  1 10:59:01.655: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
    Sep  1 10:59:01.655: INFO: ss-0  k8s-worker-1.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:10 +0000 UTC  }]
    Sep  1 10:59:01.655: INFO: ss-1  k8s-worker-1.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  }]
    Sep  1 10:59:01.655: INFO: ss-2  k8s-worker-2.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  }]
    Sep  1 10:59:01.655: INFO: 
    Sep  1 10:59:01.655: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep  1 10:59:02.660: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
    Sep  1 10:59:02.660: INFO: ss-0  k8s-worker-1.c.operations-lab.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:10 +0000 UTC  }]
    Sep  1 10:59:02.661: INFO: ss-1  k8s-worker-1.c.operations-lab.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  }]
    Sep  1 10:59:02.661: INFO: ss-2  k8s-worker-2.c.operations-lab.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-01 10:58:30 +0000 UTC  }]
    Sep  1 10:59:02.661: INFO: 
    Sep  1 10:59:02.661: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep  1 10:59:03.665: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988001881s
    Sep  1 10:59:04.669: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.983382304s
    Sep  1 10:59:05.673: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.980202925s
    Sep  1 10:59:06.678: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.976495057s
    Sep  1 10:59:07.682: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.971242762s
    Sep  1 10:59:08.686: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.967579261s
    Sep  1 10:59:09.689: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.963882842s
    Sep  1 10:59:10.694: INFO: Verifying statefulset ss doesn't scale past 0 for another 960.206522ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6542 09/01/23 10:59:11.694
    Sep  1 10:59:11.699: INFO: Scaling statefulset ss to 0
    Sep  1 10:59:11.709: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  1 10:59:11.715: INFO: Deleting all statefulset in ns statefulset-6542
    Sep  1 10:59:11.718: INFO: Scaling statefulset ss to 0
    Sep  1 10:59:11.738: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  1 10:59:11.740: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:59:11.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6542" for this suite. 09/01/23 10:59:11.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:59:11.818
Sep  1 10:59:11.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename deployment 09/01/23 10:59:11.82
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:59:11.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:59:11.847
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Sep  1 10:59:11.863: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  1 10:59:16.868: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/01/23 10:59:16.868
Sep  1 10:59:16.868: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  1 10:59:18.873: INFO: Creating deployment "test-rollover-deployment"
Sep  1 10:59:18.881: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  1 10:59:20.889: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  1 10:59:20.895: INFO: Ensure that both replica sets have 1 created replica
Sep  1 10:59:20.901: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  1 10:59:20.911: INFO: Updating deployment test-rollover-deployment
Sep  1 10:59:20.911: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  1 10:59:22.918: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  1 10:59:22.923: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  1 10:59:22.929: INFO: all replica sets need to contain the pod-template-hash label
Sep  1 10:59:22.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 10:59:24.936: INFO: all replica sets need to contain the pod-template-hash label
Sep  1 10:59:24.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 10:59:26.936: INFO: all replica sets need to contain the pod-template-hash label
Sep  1 10:59:26.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 10:59:28.936: INFO: all replica sets need to contain the pod-template-hash label
Sep  1 10:59:28.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 10:59:30.939: INFO: all replica sets need to contain the pod-template-hash label
Sep  1 10:59:30.939: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 10:59:32.938: INFO: 
Sep  1 10:59:32.938: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  1 10:59:32.950: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8983  2b689fc9-41d8-44c0-9bd9-436c81c234b4 23083 2 2023-09-01 10:59:18 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-01 10:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:59:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0057bd928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-01 10:59:18 +0000 UTC,LastTransitionTime:2023-09-01 10:59:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-09-01 10:59:32 +0000 UTC,LastTransitionTime:2023-09-01 10:59:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  1 10:59:32.954: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8983  b570fe97-6641-4f65-9179-0f3f7dc91b6e 23072 2 2023-09-01 10:59:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 2b689fc9-41d8-44c0-9bd9-436c81c234b4 0xc0057bddf7 0xc0057bddf8}] [] [{kube-controller-manager Update apps/v1 2023-09-01 10:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b689fc9-41d8-44c0-9bd9-436c81c234b4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:59:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0057bdea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  1 10:59:32.954: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  1 10:59:32.955: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8983  c4d9b356-63f7-47dc-9b44-4399b7a1b2c8 23081 2 2023-09-01 10:59:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 2b689fc9-41d8-44c0-9bd9-436c81c234b4 0xc0057bdcc7 0xc0057bdcc8}] [] [{e2e.test Update apps/v1 2023-09-01 10:59:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:59:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b689fc9-41d8-44c0-9bd9-436c81c234b4\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:59:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0057bdd88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  1 10:59:32.955: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8983  eef8677c-9d70-4c29-b0d1-266a5cbf42c4 22999 2 2023-09-01 10:59:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 2b689fc9-41d8-44c0-9bd9-436c81c234b4 0xc0057bdf17 0xc0057bdf18}] [] [{kube-controller-manager Update apps/v1 2023-09-01 10:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b689fc9-41d8-44c0-9bd9-436c81c234b4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:59:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0057bdfc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  1 10:59:32.959: INFO: Pod "test-rollover-deployment-6c6df9974f-rc4fr" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-rc4fr test-rollover-deployment-6c6df9974f- deployment-8983  ff199496-5a2e-4dce-99b6-3b0c2e86c5b8 23017 0 2023-09-01 10:59:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f b570fe97-6641-4f65-9179-0f3f7dc91b6e 0xc005a0c517 0xc005a0c518}] [] [{kube-controller-manager Update v1 2023-09-01 10:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b570fe97-6641-4f65-9179-0f3f7dc91b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 10:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.240\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-24f9k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-24f9k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.240,StartTime:2023-09-01 10:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 10:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://bf590bc39ee64d0fde1249306d822d7893470ff7c2d6ee5d5717e8117bb1534e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.240,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  1 10:59:32.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8983" for this suite. 09/01/23 10:59:32.966
------------------------------
• [SLOW TEST] [21.154 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:59:11.818
    Sep  1 10:59:11.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename deployment 09/01/23 10:59:11.82
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:59:11.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:59:11.847
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Sep  1 10:59:11.863: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Sep  1 10:59:16.868: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/01/23 10:59:16.868
    Sep  1 10:59:16.868: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Sep  1 10:59:18.873: INFO: Creating deployment "test-rollover-deployment"
    Sep  1 10:59:18.881: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Sep  1 10:59:20.889: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Sep  1 10:59:20.895: INFO: Ensure that both replica sets have 1 created replica
    Sep  1 10:59:20.901: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Sep  1 10:59:20.911: INFO: Updating deployment test-rollover-deployment
    Sep  1 10:59:20.911: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Sep  1 10:59:22.918: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Sep  1 10:59:22.923: INFO: Make sure deployment "test-rollover-deployment" is complete
    Sep  1 10:59:22.929: INFO: all replica sets need to contain the pod-template-hash label
    Sep  1 10:59:22.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 10:59:24.936: INFO: all replica sets need to contain the pod-template-hash label
    Sep  1 10:59:24.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 10:59:26.936: INFO: all replica sets need to contain the pod-template-hash label
    Sep  1 10:59:26.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 10:59:28.936: INFO: all replica sets need to contain the pod-template-hash label
    Sep  1 10:59:28.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 10:59:30.939: INFO: all replica sets need to contain the pod-template-hash label
    Sep  1 10:59:30.939: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 10, 59, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 10, 59, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 10:59:32.938: INFO: 
    Sep  1 10:59:32.938: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  1 10:59:32.950: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-8983  2b689fc9-41d8-44c0-9bd9-436c81c234b4 23083 2 2023-09-01 10:59:18 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-01 10:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:59:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0057bd928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-01 10:59:18 +0000 UTC,LastTransitionTime:2023-09-01 10:59:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-09-01 10:59:32 +0000 UTC,LastTransitionTime:2023-09-01 10:59:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  1 10:59:32.954: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8983  b570fe97-6641-4f65-9179-0f3f7dc91b6e 23072 2 2023-09-01 10:59:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 2b689fc9-41d8-44c0-9bd9-436c81c234b4 0xc0057bddf7 0xc0057bddf8}] [] [{kube-controller-manager Update apps/v1 2023-09-01 10:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b689fc9-41d8-44c0-9bd9-436c81c234b4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:59:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0057bdea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  1 10:59:32.954: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Sep  1 10:59:32.955: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8983  c4d9b356-63f7-47dc-9b44-4399b7a1b2c8 23081 2 2023-09-01 10:59:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 2b689fc9-41d8-44c0-9bd9-436c81c234b4 0xc0057bdcc7 0xc0057bdcc8}] [] [{e2e.test Update apps/v1 2023-09-01 10:59:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:59:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b689fc9-41d8-44c0-9bd9-436c81c234b4\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:59:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0057bdd88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  1 10:59:32.955: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8983  eef8677c-9d70-4c29-b0d1-266a5cbf42c4 22999 2 2023-09-01 10:59:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 2b689fc9-41d8-44c0-9bd9-436c81c234b4 0xc0057bdf17 0xc0057bdf18}] [] [{kube-controller-manager Update apps/v1 2023-09-01 10:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b689fc9-41d8-44c0-9bd9-436c81c234b4\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 10:59:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0057bdfc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  1 10:59:32.959: INFO: Pod "test-rollover-deployment-6c6df9974f-rc4fr" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-rc4fr test-rollover-deployment-6c6df9974f- deployment-8983  ff199496-5a2e-4dce-99b6-3b0c2e86c5b8 23017 0 2023-09-01 10:59:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f b570fe97-6641-4f65-9179-0f3f7dc91b6e 0xc005a0c517 0xc005a0c518}] [] [{kube-controller-manager Update v1 2023-09-01 10:59:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b570fe97-6641-4f65-9179-0f3f7dc91b6e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 10:59:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.240\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-24f9k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-24f9k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:59:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:59:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 10:59:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.240,StartTime:2023-09-01 10:59:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 10:59:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://bf590bc39ee64d0fde1249306d822d7893470ff7c2d6ee5d5717e8117bb1534e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.240,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:59:32.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8983" for this suite. 09/01/23 10:59:32.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:59:32.975
Sep  1 10:59:32.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 10:59:32.978
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:59:32.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:59:32.997
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 10:59:33.015
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 10:59:33.464
STEP: Deploying the webhook pod 09/01/23 10:59:33.471
STEP: Wait for the deployment to be ready 09/01/23 10:59:33.485
Sep  1 10:59:33.494: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/01/23 10:59:35.503
STEP: Verifying the service has paired with the endpoint 09/01/23 10:59:35.518
Sep  1 10:59:36.519: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 09/01/23 10:59:36.522
STEP: create a pod that should be denied by the webhook 09/01/23 10:59:36.543
STEP: create a pod that causes the webhook to hang 09/01/23 10:59:36.589
STEP: create a configmap that should be denied by the webhook 09/01/23 10:59:46.596
STEP: create a configmap that should be admitted by the webhook 09/01/23 10:59:46.783
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/01/23 10:59:46.796
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/01/23 10:59:46.803
STEP: create a namespace that bypass the webhook 09/01/23 10:59:46.809
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/01/23 10:59:46.816
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 10:59:46.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1615" for this suite. 09/01/23 10:59:46.914
STEP: Destroying namespace "webhook-1615-markers" for this suite. 09/01/23 10:59:46.937
------------------------------
• [SLOW TEST] [13.982 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:59:32.975
    Sep  1 10:59:32.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 10:59:32.978
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:59:32.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:59:32.997
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 10:59:33.015
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 10:59:33.464
    STEP: Deploying the webhook pod 09/01/23 10:59:33.471
    STEP: Wait for the deployment to be ready 09/01/23 10:59:33.485
    Sep  1 10:59:33.494: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/01/23 10:59:35.503
    STEP: Verifying the service has paired with the endpoint 09/01/23 10:59:35.518
    Sep  1 10:59:36.519: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 09/01/23 10:59:36.522
    STEP: create a pod that should be denied by the webhook 09/01/23 10:59:36.543
    STEP: create a pod that causes the webhook to hang 09/01/23 10:59:36.589
    STEP: create a configmap that should be denied by the webhook 09/01/23 10:59:46.596
    STEP: create a configmap that should be admitted by the webhook 09/01/23 10:59:46.783
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/01/23 10:59:46.796
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/01/23 10:59:46.803
    STEP: create a namespace that bypass the webhook 09/01/23 10:59:46.809
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/01/23 10:59:46.816
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 10:59:46.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1615" for this suite. 09/01/23 10:59:46.914
    STEP: Destroying namespace "webhook-1615-markers" for this suite. 09/01/23 10:59:46.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 10:59:46.966
Sep  1 10:59:46.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-probe 09/01/23 10:59:46.969
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:59:47.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:59:47.007
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Sep  1 10:59:47.030: INFO: Waiting up to 5m0s for pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100" in namespace "container-probe-3345" to be "running and ready"
Sep  1 10:59:47.037: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Pending", Reason="", readiness=false. Elapsed: 6.230685ms
Sep  1 10:59:47.037: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 10:59:49.041: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 2.01031519s
Sep  1 10:59:49.042: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
Sep  1 10:59:51.041: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 4.009994035s
Sep  1 10:59:51.041: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
Sep  1 10:59:53.042: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 6.010850702s
Sep  1 10:59:53.042: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
Sep  1 10:59:55.042: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 8.011119655s
Sep  1 10:59:55.042: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
Sep  1 10:59:57.041: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 10.010186778s
Sep  1 10:59:57.041: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
Sep  1 10:59:59.042: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 12.010905453s
Sep  1 10:59:59.042: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
Sep  1 11:00:01.041: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 14.010052979s
Sep  1 11:00:01.042: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
Sep  1 11:00:03.041: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 16.010098472s
Sep  1 11:00:03.041: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
Sep  1 11:00:05.041: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 18.010179512s
Sep  1 11:00:05.041: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
Sep  1 11:00:07.044: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 20.012956463s
Sep  1 11:00:07.044: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
Sep  1 11:00:09.041: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=true. Elapsed: 22.010351466s
Sep  1 11:00:09.041: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = true)
Sep  1 11:00:09.042: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100" satisfied condition "running and ready"
Sep  1 11:00:09.046: INFO: Container started at 2023-09-01 10:59:47 +0000 UTC, pod became ready at 2023-09-01 11:00:07 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  1 11:00:09.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3345" for this suite. 09/01/23 11:00:09.051
------------------------------
• [SLOW TEST] [22.091 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 10:59:46.966
    Sep  1 10:59:46.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-probe 09/01/23 10:59:46.969
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 10:59:47.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 10:59:47.007
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Sep  1 10:59:47.030: INFO: Waiting up to 5m0s for pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100" in namespace "container-probe-3345" to be "running and ready"
    Sep  1 10:59:47.037: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Pending", Reason="", readiness=false. Elapsed: 6.230685ms
    Sep  1 10:59:47.037: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 10:59:49.041: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 2.01031519s
    Sep  1 10:59:49.042: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
    Sep  1 10:59:51.041: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 4.009994035s
    Sep  1 10:59:51.041: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
    Sep  1 10:59:53.042: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 6.010850702s
    Sep  1 10:59:53.042: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
    Sep  1 10:59:55.042: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 8.011119655s
    Sep  1 10:59:55.042: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
    Sep  1 10:59:57.041: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 10.010186778s
    Sep  1 10:59:57.041: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
    Sep  1 10:59:59.042: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 12.010905453s
    Sep  1 10:59:59.042: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
    Sep  1 11:00:01.041: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 14.010052979s
    Sep  1 11:00:01.042: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
    Sep  1 11:00:03.041: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 16.010098472s
    Sep  1 11:00:03.041: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
    Sep  1 11:00:05.041: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 18.010179512s
    Sep  1 11:00:05.041: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
    Sep  1 11:00:07.044: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=false. Elapsed: 20.012956463s
    Sep  1 11:00:07.044: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = false)
    Sep  1 11:00:09.041: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100": Phase="Running", Reason="", readiness=true. Elapsed: 22.010351466s
    Sep  1 11:00:09.041: INFO: The phase of Pod test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100 is Running (Ready = true)
    Sep  1 11:00:09.042: INFO: Pod "test-webserver-587a4488-29e7-4e44-8fca-339ea3c23100" satisfied condition "running and ready"
    Sep  1 11:00:09.046: INFO: Container started at 2023-09-01 10:59:47 +0000 UTC, pod became ready at 2023-09-01 11:00:07 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:00:09.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3345" for this suite. 09/01/23 11:00:09.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:00:09.067
Sep  1 11:00:09.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename endpointslice 09/01/23 11:00:09.069
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:00:09.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:00:09.096
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 09/01/23 11:00:09.101
STEP: getting /apis/discovery.k8s.io 09/01/23 11:00:09.104
STEP: getting /apis/discovery.k8s.iov1 09/01/23 11:00:09.105
STEP: creating 09/01/23 11:00:09.106
STEP: getting 09/01/23 11:00:09.138
STEP: listing 09/01/23 11:00:09.141
STEP: watching 09/01/23 11:00:09.145
Sep  1 11:00:09.145: INFO: starting watch
STEP: cluster-wide listing 09/01/23 11:00:09.147
STEP: cluster-wide watching 09/01/23 11:00:09.15
Sep  1 11:00:09.151: INFO: starting watch
STEP: patching 09/01/23 11:00:09.153
STEP: updating 09/01/23 11:00:09.199
Sep  1 11:00:09.207: INFO: waiting for watch events with expected annotations
Sep  1 11:00:09.208: INFO: saw patched and updated annotations
STEP: deleting 09/01/23 11:00:09.208
STEP: deleting a collection 09/01/23 11:00:09.219
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  1 11:00:09.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4267" for this suite. 09/01/23 11:00:09.24
------------------------------
• [0.182 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:00:09.067
    Sep  1 11:00:09.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename endpointslice 09/01/23 11:00:09.069
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:00:09.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:00:09.096
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 09/01/23 11:00:09.101
    STEP: getting /apis/discovery.k8s.io 09/01/23 11:00:09.104
    STEP: getting /apis/discovery.k8s.iov1 09/01/23 11:00:09.105
    STEP: creating 09/01/23 11:00:09.106
    STEP: getting 09/01/23 11:00:09.138
    STEP: listing 09/01/23 11:00:09.141
    STEP: watching 09/01/23 11:00:09.145
    Sep  1 11:00:09.145: INFO: starting watch
    STEP: cluster-wide listing 09/01/23 11:00:09.147
    STEP: cluster-wide watching 09/01/23 11:00:09.15
    Sep  1 11:00:09.151: INFO: starting watch
    STEP: patching 09/01/23 11:00:09.153
    STEP: updating 09/01/23 11:00:09.199
    Sep  1 11:00:09.207: INFO: waiting for watch events with expected annotations
    Sep  1 11:00:09.208: INFO: saw patched and updated annotations
    STEP: deleting 09/01/23 11:00:09.208
    STEP: deleting a collection 09/01/23 11:00:09.219
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:00:09.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4267" for this suite. 09/01/23 11:00:09.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:00:09.26
Sep  1 11:00:09.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pod-network-test 09/01/23 11:00:09.261
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:00:09.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:00:09.285
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-8494 09/01/23 11:00:09.314
STEP: creating a selector 09/01/23 11:00:09.315
STEP: Creating the service pods in kubernetes 09/01/23 11:00:09.315
Sep  1 11:00:09.316: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  1 11:00:09.352: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8494" to be "running and ready"
Sep  1 11:00:09.361: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.562231ms
Sep  1 11:00:09.362: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:00:11.370: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.018346061s
Sep  1 11:00:11.370: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:00:13.368: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016305029s
Sep  1 11:00:13.368: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:00:15.365: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.013673529s
Sep  1 11:00:15.365: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:00:17.367: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015279045s
Sep  1 11:00:17.367: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:00:19.366: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014252885s
Sep  1 11:00:19.366: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:00:21.367: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015712298s
Sep  1 11:00:21.367: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:00:23.366: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014720305s
Sep  1 11:00:23.366: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:00:25.366: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014277129s
Sep  1 11:00:25.366: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:00:27.368: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.01594075s
Sep  1 11:00:27.368: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:00:29.366: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014417764s
Sep  1 11:00:29.366: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:00:31.368: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.016213222s
Sep  1 11:00:31.368: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  1 11:00:31.368: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  1 11:00:31.371: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8494" to be "running and ready"
Sep  1 11:00:31.374: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.917863ms
Sep  1 11:00:31.374: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  1 11:00:31.374: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/01/23 11:00:31.377
Sep  1 11:00:31.395: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8494" to be "running"
Sep  1 11:00:31.405: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.351152ms
Sep  1 11:00:33.410: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015554573s
Sep  1 11:00:33.411: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  1 11:00:33.414: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8494" to be "running"
Sep  1 11:00:33.417: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.309573ms
Sep  1 11:00:33.417: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep  1 11:00:33.420: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  1 11:00:33.420: INFO: Going to poll 10.10.0.218 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep  1 11:00:33.423: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.0.218 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8494 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:00:33.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:00:33.424: INFO: ExecWithOptions: Clientset creation
Sep  1 11:00:33.424: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8494/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.0.218+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  1 11:00:34.620: INFO: Found all 1 expected endpoints: [netserver-0]
Sep  1 11:00:34.620: INFO: Going to poll 10.10.1.225 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep  1 11:00:34.623: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.1.225 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8494 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:00:34.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:00:34.624: INFO: ExecWithOptions: Clientset creation
Sep  1 11:00:34.624: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8494/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.1.225+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  1 11:00:35.716: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  1 11:00:35.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-8494" for this suite. 09/01/23 11:00:35.721
------------------------------
• [SLOW TEST] [26.469 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:00:09.26
    Sep  1 11:00:09.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pod-network-test 09/01/23 11:00:09.261
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:00:09.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:00:09.285
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-8494 09/01/23 11:00:09.314
    STEP: creating a selector 09/01/23 11:00:09.315
    STEP: Creating the service pods in kubernetes 09/01/23 11:00:09.315
    Sep  1 11:00:09.316: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  1 11:00:09.352: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8494" to be "running and ready"
    Sep  1 11:00:09.361: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.562231ms
    Sep  1 11:00:09.362: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:00:11.370: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.018346061s
    Sep  1 11:00:11.370: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:00:13.368: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016305029s
    Sep  1 11:00:13.368: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:00:15.365: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.013673529s
    Sep  1 11:00:15.365: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:00:17.367: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015279045s
    Sep  1 11:00:17.367: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:00:19.366: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014252885s
    Sep  1 11:00:19.366: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:00:21.367: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015712298s
    Sep  1 11:00:21.367: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:00:23.366: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014720305s
    Sep  1 11:00:23.366: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:00:25.366: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014277129s
    Sep  1 11:00:25.366: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:00:27.368: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.01594075s
    Sep  1 11:00:27.368: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:00:29.366: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014417764s
    Sep  1 11:00:29.366: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:00:31.368: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.016213222s
    Sep  1 11:00:31.368: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  1 11:00:31.368: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  1 11:00:31.371: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8494" to be "running and ready"
    Sep  1 11:00:31.374: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.917863ms
    Sep  1 11:00:31.374: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  1 11:00:31.374: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/01/23 11:00:31.377
    Sep  1 11:00:31.395: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8494" to be "running"
    Sep  1 11:00:31.405: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.351152ms
    Sep  1 11:00:33.410: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015554573s
    Sep  1 11:00:33.411: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  1 11:00:33.414: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8494" to be "running"
    Sep  1 11:00:33.417: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.309573ms
    Sep  1 11:00:33.417: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep  1 11:00:33.420: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  1 11:00:33.420: INFO: Going to poll 10.10.0.218 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Sep  1 11:00:33.423: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.0.218 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8494 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:00:33.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:00:33.424: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:00:33.424: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8494/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.0.218+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  1 11:00:34.620: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep  1 11:00:34.620: INFO: Going to poll 10.10.1.225 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Sep  1 11:00:34.623: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.1.225 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8494 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:00:34.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:00:34.624: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:00:34.624: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8494/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.1.225+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  1 11:00:35.716: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:00:35.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-8494" for this suite. 09/01/23 11:00:35.721
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:00:35.74
Sep  1 11:00:35.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename init-container 09/01/23 11:00:35.741
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:00:35.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:00:35.766
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 09/01/23 11:00:35.77
Sep  1 11:00:35.770: INFO: PodSpec: initContainers in spec.initContainers
Sep  1 11:01:20.459: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ed95553f-afca-4ec6-a4b8-b01a31536630", GenerateName:"", Namespace:"init-container-1754", SelfLink:"", UID:"e29219cf-3e69-43d3-912e-2c5cffda7baf", ResourceVersion:"23950", Generation:0, CreationTimestamp:time.Date(2023, time.September, 1, 11, 0, 35, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"770503104"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 1, 11, 0, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0016ba030), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 1, 11, 1, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0016ba078), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-tsbz6", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003150c60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tsbz6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tsbz6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tsbz6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004bbff30), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-worker-1.c.operations-lab.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00028ff10), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004bbffb0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004bbffd0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004bbffd8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004bbffdc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0065a11c0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 1, 11, 0, 35, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 1, 11, 0, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 1, 11, 0, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 1, 11, 0, 35, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.0.3", PodIP:"10.10.0.163", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.10.0.163"}}, StartTime:time.Date(2023, time.September, 1, 11, 0, 35, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0001f0070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0001f00e0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://ac27dadb9f7a90be47b7031219d81af3e736d9cb31ba4f923033976bccf0232a", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003150ce0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003150cc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00c16e2df)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:01:20.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1754" for this suite. 09/01/23 11:01:20.465
------------------------------
• [SLOW TEST] [44.732 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:00:35.74
    Sep  1 11:00:35.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename init-container 09/01/23 11:00:35.741
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:00:35.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:00:35.766
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 09/01/23 11:00:35.77
    Sep  1 11:00:35.770: INFO: PodSpec: initContainers in spec.initContainers
    Sep  1 11:01:20.459: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ed95553f-afca-4ec6-a4b8-b01a31536630", GenerateName:"", Namespace:"init-container-1754", SelfLink:"", UID:"e29219cf-3e69-43d3-912e-2c5cffda7baf", ResourceVersion:"23950", Generation:0, CreationTimestamp:time.Date(2023, time.September, 1, 11, 0, 35, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"770503104"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 1, 11, 0, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0016ba030), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 1, 11, 1, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0016ba078), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-tsbz6", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003150c60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tsbz6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tsbz6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tsbz6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004bbff30), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-worker-1.c.operations-lab.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00028ff10), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004bbffb0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004bbffd0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004bbffd8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004bbffdc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0065a11c0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 1, 11, 0, 35, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 1, 11, 0, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 1, 11, 0, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 1, 11, 0, 35, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.0.3", PodIP:"10.10.0.163", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.10.0.163"}}, StartTime:time.Date(2023, time.September, 1, 11, 0, 35, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0001f0070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0001f00e0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://ac27dadb9f7a90be47b7031219d81af3e736d9cb31ba4f923033976bccf0232a", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003150ce0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003150cc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00c16e2df)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:01:20.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1754" for this suite. 09/01/23 11:01:20.465
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:01:20.478
Sep  1 11:01:20.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 11:01:20.481
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:01:20.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:01:20.503
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/01/23 11:01:20.508
Sep  1 11:01:20.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:01:25.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:01:42.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-156" for this suite. 09/01/23 11:01:42.361
------------------------------
• [SLOW TEST] [21.891 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:01:20.478
    Sep  1 11:01:20.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 11:01:20.481
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:01:20.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:01:20.503
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/01/23 11:01:20.508
    Sep  1 11:01:20.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:01:25.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:01:42.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-156" for this suite. 09/01/23 11:01:42.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:01:42.382
Sep  1 11:01:42.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 11:01:42.384
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:01:42.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:01:42.408
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 09/01/23 11:01:42.412
Sep  1 11:01:42.437: INFO: Waiting up to 5m0s for pod "pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c" in namespace "emptydir-1220" to be "Succeeded or Failed"
Sep  1 11:01:42.444: INFO: Pod "pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.206114ms
Sep  1 11:01:44.448: INFO: Pod "pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010240939s
Sep  1 11:01:46.449: INFO: Pod "pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011857695s
STEP: Saw pod success 09/01/23 11:01:46.449
Sep  1 11:01:46.450: INFO: Pod "pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c" satisfied condition "Succeeded or Failed"
Sep  1 11:01:46.453: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c container test-container: <nil>
STEP: delete the pod 09/01/23 11:01:46.475
Sep  1 11:01:46.492: INFO: Waiting for pod pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c to disappear
Sep  1 11:01:46.497: INFO: Pod pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 11:01:46.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1220" for this suite. 09/01/23 11:01:46.502
------------------------------
• [4.130 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:01:42.382
    Sep  1 11:01:42.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 11:01:42.384
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:01:42.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:01:42.408
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/01/23 11:01:42.412
    Sep  1 11:01:42.437: INFO: Waiting up to 5m0s for pod "pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c" in namespace "emptydir-1220" to be "Succeeded or Failed"
    Sep  1 11:01:42.444: INFO: Pod "pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.206114ms
    Sep  1 11:01:44.448: INFO: Pod "pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010240939s
    Sep  1 11:01:46.449: INFO: Pod "pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011857695s
    STEP: Saw pod success 09/01/23 11:01:46.449
    Sep  1 11:01:46.450: INFO: Pod "pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c" satisfied condition "Succeeded or Failed"
    Sep  1 11:01:46.453: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c container test-container: <nil>
    STEP: delete the pod 09/01/23 11:01:46.475
    Sep  1 11:01:46.492: INFO: Waiting for pod pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c to disappear
    Sep  1 11:01:46.497: INFO: Pod pod-3dd74c5a-1722-47dd-adaf-5db1b548f04c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:01:46.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1220" for this suite. 09/01/23 11:01:46.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:01:46.519
Sep  1 11:01:46.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 11:01:46.521
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:01:46.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:01:46.54
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 11:01:46.561
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:01:47.272
STEP: Deploying the webhook pod 09/01/23 11:01:47.282
STEP: Wait for the deployment to be ready 09/01/23 11:01:47.3
Sep  1 11:01:47.317: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/01/23 11:01:49.327
STEP: Verifying the service has paired with the endpoint 09/01/23 11:01:49.347
Sep  1 11:01:50.348: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Sep  1 11:01:50.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5887-crds.webhook.example.com via the AdmissionRegistration API 09/01/23 11:01:50.867
STEP: Creating a custom resource while v1 is storage version 09/01/23 11:01:50.892
STEP: Patching Custom Resource Definition to set v2 as storage 09/01/23 11:01:52.978
STEP: Patching the custom resource while v2 is storage version 09/01/23 11:01:53.001
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:01:53.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5922" for this suite. 09/01/23 11:01:53.76
STEP: Destroying namespace "webhook-5922-markers" for this suite. 09/01/23 11:01:53.798
------------------------------
• [SLOW TEST] [7.345 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:01:46.519
    Sep  1 11:01:46.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 11:01:46.521
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:01:46.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:01:46.54
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 11:01:46.561
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:01:47.272
    STEP: Deploying the webhook pod 09/01/23 11:01:47.282
    STEP: Wait for the deployment to be ready 09/01/23 11:01:47.3
    Sep  1 11:01:47.317: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/01/23 11:01:49.327
    STEP: Verifying the service has paired with the endpoint 09/01/23 11:01:49.347
    Sep  1 11:01:50.348: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Sep  1 11:01:50.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5887-crds.webhook.example.com via the AdmissionRegistration API 09/01/23 11:01:50.867
    STEP: Creating a custom resource while v1 is storage version 09/01/23 11:01:50.892
    STEP: Patching Custom Resource Definition to set v2 as storage 09/01/23 11:01:52.978
    STEP: Patching the custom resource while v2 is storage version 09/01/23 11:01:53.001
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:01:53.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5922" for this suite. 09/01/23 11:01:53.76
    STEP: Destroying namespace "webhook-5922-markers" for this suite. 09/01/23 11:01:53.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:01:53.88
Sep  1 11:01:53.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename daemonsets 09/01/23 11:01:53.885
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:01:53.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:01:53.937
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
Sep  1 11:01:54.198: INFO: Create a RollingUpdate DaemonSet
Sep  1 11:01:54.220: INFO: Check that daemon pods launch on every node of the cluster
Sep  1 11:01:54.369: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:01:54.407: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 11:01:54.408: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:01:55.451: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:01:55.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 11:01:55.500: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:01:56.413: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:01:56.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 11:01:56.417: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:01:57.413: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:01:57.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  1 11:01:57.417: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Sep  1 11:01:57.417: INFO: Update the DaemonSet to trigger a rollout
Sep  1 11:01:57.428: INFO: Updating DaemonSet daemon-set
Sep  1 11:02:00.443: INFO: Roll back the DaemonSet before rollout is complete
Sep  1 11:02:00.453: INFO: Updating DaemonSet daemon-set
Sep  1 11:02:00.453: INFO: Make sure DaemonSet rollback is complete
Sep  1 11:02:00.463: INFO: Wrong image for pod: daemon-set-zl57d. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Sep  1 11:02:00.463: INFO: Pod daemon-set-zl57d is not available
Sep  1 11:02:00.467: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:02:01.476: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:02:02.477: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:02:03.474: INFO: Pod daemon-set-czlcp is not available
Sep  1 11:02:03.479: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/01/23 11:02:03.487
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3848, will wait for the garbage collector to delete the pods 09/01/23 11:02:03.487
Sep  1 11:02:03.548: INFO: Deleting DaemonSet.extensions daemon-set took: 6.807138ms
Sep  1 11:02:03.651: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.624288ms
Sep  1 11:02:34.957: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 11:02:34.957: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  1 11:02:34.960: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24586"},"items":null}

Sep  1 11:02:34.963: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24586"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:02:34.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3848" for this suite. 09/01/23 11:02:34.978
------------------------------
• [SLOW TEST] [41.109 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:01:53.88
    Sep  1 11:01:53.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename daemonsets 09/01/23 11:01:53.885
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:01:53.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:01:53.937
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:443
    Sep  1 11:01:54.198: INFO: Create a RollingUpdate DaemonSet
    Sep  1 11:01:54.220: INFO: Check that daemon pods launch on every node of the cluster
    Sep  1 11:01:54.369: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:01:54.407: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 11:01:54.408: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:01:55.451: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:01:55.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 11:01:55.500: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:01:56.413: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:01:56.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 11:01:56.417: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:01:57.413: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:01:57.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  1 11:01:57.417: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Sep  1 11:01:57.417: INFO: Update the DaemonSet to trigger a rollout
    Sep  1 11:01:57.428: INFO: Updating DaemonSet daemon-set
    Sep  1 11:02:00.443: INFO: Roll back the DaemonSet before rollout is complete
    Sep  1 11:02:00.453: INFO: Updating DaemonSet daemon-set
    Sep  1 11:02:00.453: INFO: Make sure DaemonSet rollback is complete
    Sep  1 11:02:00.463: INFO: Wrong image for pod: daemon-set-zl57d. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Sep  1 11:02:00.463: INFO: Pod daemon-set-zl57d is not available
    Sep  1 11:02:00.467: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:02:01.476: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:02:02.477: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:02:03.474: INFO: Pod daemon-set-czlcp is not available
    Sep  1 11:02:03.479: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/01/23 11:02:03.487
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3848, will wait for the garbage collector to delete the pods 09/01/23 11:02:03.487
    Sep  1 11:02:03.548: INFO: Deleting DaemonSet.extensions daemon-set took: 6.807138ms
    Sep  1 11:02:03.651: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.624288ms
    Sep  1 11:02:34.957: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 11:02:34.957: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  1 11:02:34.960: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24586"},"items":null}

    Sep  1 11:02:34.963: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24586"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:02:34.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3848" for this suite. 09/01/23 11:02:34.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:02:34.994
Sep  1 11:02:34.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename controllerrevisions 09/01/23 11:02:34.996
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:02:35.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:02:35.018
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-2x6xk-daemon-set" 09/01/23 11:02:35.042
STEP: Check that daemon pods launch on every node of the cluster. 09/01/23 11:02:35.048
Sep  1 11:02:35.053: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:02:35.058: INFO: Number of nodes with available pods controlled by daemonset e2e-2x6xk-daemon-set: 0
Sep  1 11:02:35.058: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:02:36.079: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:02:36.094: INFO: Number of nodes with available pods controlled by daemonset e2e-2x6xk-daemon-set: 0
Sep  1 11:02:36.094: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:02:37.063: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:02:37.066: INFO: Number of nodes with available pods controlled by daemonset e2e-2x6xk-daemon-set: 2
Sep  1 11:02:37.067: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-2x6xk-daemon-set
STEP: Confirm DaemonSet "e2e-2x6xk-daemon-set" successfully created with "daemonset-name=e2e-2x6xk-daemon-set" label 09/01/23 11:02:37.069
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-2x6xk-daemon-set" 09/01/23 11:02:37.077
Sep  1 11:02:37.081: INFO: Located ControllerRevision: "e2e-2x6xk-daemon-set-66d5f5fbf8"
STEP: Patching ControllerRevision "e2e-2x6xk-daemon-set-66d5f5fbf8" 09/01/23 11:02:37.085
Sep  1 11:02:37.095: INFO: e2e-2x6xk-daemon-set-66d5f5fbf8 has been patched
STEP: Create a new ControllerRevision 09/01/23 11:02:37.095
Sep  1 11:02:37.101: INFO: Created ControllerRevision: e2e-2x6xk-daemon-set-75bdd6495b
STEP: Confirm that there are two ControllerRevisions 09/01/23 11:02:37.101
Sep  1 11:02:37.102: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  1 11:02:37.106: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-2x6xk-daemon-set-66d5f5fbf8" 09/01/23 11:02:37.106
STEP: Confirm that there is only one ControllerRevision 09/01/23 11:02:37.112
Sep  1 11:02:37.112: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  1 11:02:37.115: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-2x6xk-daemon-set-75bdd6495b" 09/01/23 11:02:37.118
Sep  1 11:02:37.130: INFO: e2e-2x6xk-daemon-set-75bdd6495b has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 09/01/23 11:02:37.13
W0901 11:02:37.139771      19 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 09/01/23 11:02:37.139
Sep  1 11:02:37.140: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  1 11:02:38.144: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  1 11:02:38.148: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-2x6xk-daemon-set-75bdd6495b=updated" 09/01/23 11:02:38.148
STEP: Confirm that there is only one ControllerRevision 09/01/23 11:02:38.157
Sep  1 11:02:38.157: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  1 11:02:38.160: INFO: Found 1 ControllerRevisions
Sep  1 11:02:38.163: INFO: ControllerRevision "e2e-2x6xk-daemon-set-58bd5646b5" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-2x6xk-daemon-set" 09/01/23 11:02:38.166
STEP: deleting DaemonSet.extensions e2e-2x6xk-daemon-set in namespace controllerrevisions-751, will wait for the garbage collector to delete the pods 09/01/23 11:02:38.167
Sep  1 11:02:38.227: INFO: Deleting DaemonSet.extensions e2e-2x6xk-daemon-set took: 6.810337ms
Sep  1 11:02:38.328: INFO: Terminating DaemonSet.extensions e2e-2x6xk-daemon-set pods took: 101.132382ms
Sep  1 11:02:40.033: INFO: Number of nodes with available pods controlled by daemonset e2e-2x6xk-daemon-set: 0
Sep  1 11:02:40.033: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-2x6xk-daemon-set
Sep  1 11:02:40.037: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24670"},"items":null}

Sep  1 11:02:40.040: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24670"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:02:40.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-751" for this suite. 09/01/23 11:02:40.059
------------------------------
• [SLOW TEST] [5.074 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:02:34.994
    Sep  1 11:02:34.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename controllerrevisions 09/01/23 11:02:34.996
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:02:35.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:02:35.018
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-2x6xk-daemon-set" 09/01/23 11:02:35.042
    STEP: Check that daemon pods launch on every node of the cluster. 09/01/23 11:02:35.048
    Sep  1 11:02:35.053: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:02:35.058: INFO: Number of nodes with available pods controlled by daemonset e2e-2x6xk-daemon-set: 0
    Sep  1 11:02:35.058: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:02:36.079: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:02:36.094: INFO: Number of nodes with available pods controlled by daemonset e2e-2x6xk-daemon-set: 0
    Sep  1 11:02:36.094: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:02:37.063: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:02:37.066: INFO: Number of nodes with available pods controlled by daemonset e2e-2x6xk-daemon-set: 2
    Sep  1 11:02:37.067: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-2x6xk-daemon-set
    STEP: Confirm DaemonSet "e2e-2x6xk-daemon-set" successfully created with "daemonset-name=e2e-2x6xk-daemon-set" label 09/01/23 11:02:37.069
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-2x6xk-daemon-set" 09/01/23 11:02:37.077
    Sep  1 11:02:37.081: INFO: Located ControllerRevision: "e2e-2x6xk-daemon-set-66d5f5fbf8"
    STEP: Patching ControllerRevision "e2e-2x6xk-daemon-set-66d5f5fbf8" 09/01/23 11:02:37.085
    Sep  1 11:02:37.095: INFO: e2e-2x6xk-daemon-set-66d5f5fbf8 has been patched
    STEP: Create a new ControllerRevision 09/01/23 11:02:37.095
    Sep  1 11:02:37.101: INFO: Created ControllerRevision: e2e-2x6xk-daemon-set-75bdd6495b
    STEP: Confirm that there are two ControllerRevisions 09/01/23 11:02:37.101
    Sep  1 11:02:37.102: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  1 11:02:37.106: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-2x6xk-daemon-set-66d5f5fbf8" 09/01/23 11:02:37.106
    STEP: Confirm that there is only one ControllerRevision 09/01/23 11:02:37.112
    Sep  1 11:02:37.112: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  1 11:02:37.115: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-2x6xk-daemon-set-75bdd6495b" 09/01/23 11:02:37.118
    Sep  1 11:02:37.130: INFO: e2e-2x6xk-daemon-set-75bdd6495b has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 09/01/23 11:02:37.13
    W0901 11:02:37.139771      19 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 09/01/23 11:02:37.139
    Sep  1 11:02:37.140: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  1 11:02:38.144: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  1 11:02:38.148: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-2x6xk-daemon-set-75bdd6495b=updated" 09/01/23 11:02:38.148
    STEP: Confirm that there is only one ControllerRevision 09/01/23 11:02:38.157
    Sep  1 11:02:38.157: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  1 11:02:38.160: INFO: Found 1 ControllerRevisions
    Sep  1 11:02:38.163: INFO: ControllerRevision "e2e-2x6xk-daemon-set-58bd5646b5" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-2x6xk-daemon-set" 09/01/23 11:02:38.166
    STEP: deleting DaemonSet.extensions e2e-2x6xk-daemon-set in namespace controllerrevisions-751, will wait for the garbage collector to delete the pods 09/01/23 11:02:38.167
    Sep  1 11:02:38.227: INFO: Deleting DaemonSet.extensions e2e-2x6xk-daemon-set took: 6.810337ms
    Sep  1 11:02:38.328: INFO: Terminating DaemonSet.extensions e2e-2x6xk-daemon-set pods took: 101.132382ms
    Sep  1 11:02:40.033: INFO: Number of nodes with available pods controlled by daemonset e2e-2x6xk-daemon-set: 0
    Sep  1 11:02:40.033: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-2x6xk-daemon-set
    Sep  1 11:02:40.037: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24670"},"items":null}

    Sep  1 11:02:40.040: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24670"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:02:40.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-751" for this suite. 09/01/23 11:02:40.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:02:40.078
Sep  1 11:02:40.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename secrets 09/01/23 11:02:40.08
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:02:40.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:02:40.106
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-19e1ffe7-8582-4ef8-aa06-85256cdbca53 09/01/23 11:02:40.115
STEP: Creating secret with name s-test-opt-upd-8439cdbd-e3bd-4ebc-a704-f64e165dcd95 09/01/23 11:02:40.121
STEP: Creating the pod 09/01/23 11:02:40.129
Sep  1 11:02:40.141: INFO: Waiting up to 5m0s for pod "pod-secrets-dc966655-1dff-485a-a36b-ebffb062f636" in namespace "secrets-4288" to be "running and ready"
Sep  1 11:02:40.153: INFO: Pod "pod-secrets-dc966655-1dff-485a-a36b-ebffb062f636": Phase="Pending", Reason="", readiness=false. Elapsed: 11.902878ms
Sep  1 11:02:40.154: INFO: The phase of Pod pod-secrets-dc966655-1dff-485a-a36b-ebffb062f636 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:02:42.160: INFO: Pod "pod-secrets-dc966655-1dff-485a-a36b-ebffb062f636": Phase="Running", Reason="", readiness=true. Elapsed: 2.018718431s
Sep  1 11:02:42.160: INFO: The phase of Pod pod-secrets-dc966655-1dff-485a-a36b-ebffb062f636 is Running (Ready = true)
Sep  1 11:02:42.160: INFO: Pod "pod-secrets-dc966655-1dff-485a-a36b-ebffb062f636" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-19e1ffe7-8582-4ef8-aa06-85256cdbca53 09/01/23 11:02:42.182
STEP: Updating secret s-test-opt-upd-8439cdbd-e3bd-4ebc-a704-f64e165dcd95 09/01/23 11:02:42.189
STEP: Creating secret with name s-test-opt-create-5d1da6ce-c444-4268-b09f-683db2d27d6c 09/01/23 11:02:42.196
STEP: waiting to observe update in volume 09/01/23 11:02:42.204
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  1 11:02:44.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4288" for this suite. 09/01/23 11:02:44.242
------------------------------
• [4.173 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:02:40.078
    Sep  1 11:02:40.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename secrets 09/01/23 11:02:40.08
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:02:40.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:02:40.106
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-19e1ffe7-8582-4ef8-aa06-85256cdbca53 09/01/23 11:02:40.115
    STEP: Creating secret with name s-test-opt-upd-8439cdbd-e3bd-4ebc-a704-f64e165dcd95 09/01/23 11:02:40.121
    STEP: Creating the pod 09/01/23 11:02:40.129
    Sep  1 11:02:40.141: INFO: Waiting up to 5m0s for pod "pod-secrets-dc966655-1dff-485a-a36b-ebffb062f636" in namespace "secrets-4288" to be "running and ready"
    Sep  1 11:02:40.153: INFO: Pod "pod-secrets-dc966655-1dff-485a-a36b-ebffb062f636": Phase="Pending", Reason="", readiness=false. Elapsed: 11.902878ms
    Sep  1 11:02:40.154: INFO: The phase of Pod pod-secrets-dc966655-1dff-485a-a36b-ebffb062f636 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:02:42.160: INFO: Pod "pod-secrets-dc966655-1dff-485a-a36b-ebffb062f636": Phase="Running", Reason="", readiness=true. Elapsed: 2.018718431s
    Sep  1 11:02:42.160: INFO: The phase of Pod pod-secrets-dc966655-1dff-485a-a36b-ebffb062f636 is Running (Ready = true)
    Sep  1 11:02:42.160: INFO: Pod "pod-secrets-dc966655-1dff-485a-a36b-ebffb062f636" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-19e1ffe7-8582-4ef8-aa06-85256cdbca53 09/01/23 11:02:42.182
    STEP: Updating secret s-test-opt-upd-8439cdbd-e3bd-4ebc-a704-f64e165dcd95 09/01/23 11:02:42.189
    STEP: Creating secret with name s-test-opt-create-5d1da6ce-c444-4268-b09f-683db2d27d6c 09/01/23 11:02:42.196
    STEP: waiting to observe update in volume 09/01/23 11:02:42.204
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:02:44.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4288" for this suite. 09/01/23 11:02:44.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:02:44.262
Sep  1 11:02:44.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 11:02:44.264
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:02:44.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:02:44.286
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 11:02:44.309
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:02:44.596
STEP: Deploying the webhook pod 09/01/23 11:02:44.605
STEP: Wait for the deployment to be ready 09/01/23 11:02:44.621
Sep  1 11:02:44.632: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/01/23 11:02:46.644
STEP: Verifying the service has paired with the endpoint 09/01/23 11:02:46.669
Sep  1 11:02:47.672: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 09/01/23 11:02:47.677
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/01/23 11:02:47.68
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/01/23 11:02:47.681
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/01/23 11:02:47.682
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/01/23 11:02:47.684
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/01/23 11:02:47.684
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/01/23 11:02:47.686
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:02:47.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7499" for this suite. 09/01/23 11:02:47.79
STEP: Destroying namespace "webhook-7499-markers" for this suite. 09/01/23 11:02:47.809
------------------------------
• [3.569 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:02:44.262
    Sep  1 11:02:44.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 11:02:44.264
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:02:44.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:02:44.286
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 11:02:44.309
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:02:44.596
    STEP: Deploying the webhook pod 09/01/23 11:02:44.605
    STEP: Wait for the deployment to be ready 09/01/23 11:02:44.621
    Sep  1 11:02:44.632: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/01/23 11:02:46.644
    STEP: Verifying the service has paired with the endpoint 09/01/23 11:02:46.669
    Sep  1 11:02:47.672: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 09/01/23 11:02:47.677
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/01/23 11:02:47.68
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/01/23 11:02:47.681
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/01/23 11:02:47.682
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/01/23 11:02:47.684
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/01/23 11:02:47.684
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/01/23 11:02:47.686
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:02:47.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7499" for this suite. 09/01/23 11:02:47.79
    STEP: Destroying namespace "webhook-7499-markers" for this suite. 09/01/23 11:02:47.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:02:47.838
Sep  1 11:02:47.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 11:02:47.84
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:02:47.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:02:47.886
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 09/01/23 11:02:47.891
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 11:02:47.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9070" for this suite. 09/01/23 11:02:47.905
------------------------------
• [0.079 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:02:47.838
    Sep  1 11:02:47.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 11:02:47.84
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:02:47.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:02:47.886
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 09/01/23 11:02:47.891
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:02:47.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9070" for this suite. 09/01/23 11:02:47.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:02:47.924
Sep  1 11:02:47.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename prestop 09/01/23 11:02:47.927
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:02:47.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:02:47.949
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-845 09/01/23 11:02:47.953
STEP: Waiting for pods to come up. 09/01/23 11:02:47.966
Sep  1 11:02:47.966: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-845" to be "running"
Sep  1 11:02:47.972: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.853657ms
Sep  1 11:02:49.977: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.010813424s
Sep  1 11:02:49.977: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-845 09/01/23 11:02:49.981
Sep  1 11:02:49.993: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-845" to be "running"
Sep  1 11:02:49.998: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039758ms
Sep  1 11:02:52.004: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.010094725s
Sep  1 11:02:52.004: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 09/01/23 11:02:52.004
Sep  1 11:02:57.024: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 09/01/23 11:02:57.025
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Sep  1 11:02:57.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-845" for this suite. 09/01/23 11:02:57.049
------------------------------
• [SLOW TEST] [9.135 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:02:47.924
    Sep  1 11:02:47.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename prestop 09/01/23 11:02:47.927
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:02:47.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:02:47.949
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-845 09/01/23 11:02:47.953
    STEP: Waiting for pods to come up. 09/01/23 11:02:47.966
    Sep  1 11:02:47.966: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-845" to be "running"
    Sep  1 11:02:47.972: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.853657ms
    Sep  1 11:02:49.977: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.010813424s
    Sep  1 11:02:49.977: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-845 09/01/23 11:02:49.981
    Sep  1 11:02:49.993: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-845" to be "running"
    Sep  1 11:02:49.998: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039758ms
    Sep  1 11:02:52.004: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.010094725s
    Sep  1 11:02:52.004: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 09/01/23 11:02:52.004
    Sep  1 11:02:57.024: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 09/01/23 11:02:57.025
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:02:57.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-845" for this suite. 09/01/23 11:02:57.049
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:02:57.059
Sep  1 11:02:57.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 11:02:57.065
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:02:57.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:02:57.087
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 09/01/23 11:02:57.093
Sep  1 11:02:57.105: INFO: Waiting up to 5m0s for pod "downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092" in namespace "downward-api-3059" to be "Succeeded or Failed"
Sep  1 11:02:57.110: INFO: Pod "downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092": Phase="Pending", Reason="", readiness=false. Elapsed: 5.298262ms
Sep  1 11:02:59.117: INFO: Pod "downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011753252s
Sep  1 11:03:01.114: INFO: Pod "downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009330765s
STEP: Saw pod success 09/01/23 11:03:01.114
Sep  1 11:03:01.115: INFO: Pod "downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092" satisfied condition "Succeeded or Failed"
Sep  1 11:03:01.118: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092 container dapi-container: <nil>
STEP: delete the pod 09/01/23 11:03:01.126
Sep  1 11:03:01.136: INFO: Waiting for pod downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092 to disappear
Sep  1 11:03:01.139: INFO: Pod downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  1 11:03:01.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3059" for this suite. 09/01/23 11:03:01.145
------------------------------
• [4.095 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:02:57.059
    Sep  1 11:02:57.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 11:02:57.065
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:02:57.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:02:57.087
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 09/01/23 11:02:57.093
    Sep  1 11:02:57.105: INFO: Waiting up to 5m0s for pod "downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092" in namespace "downward-api-3059" to be "Succeeded or Failed"
    Sep  1 11:02:57.110: INFO: Pod "downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092": Phase="Pending", Reason="", readiness=false. Elapsed: 5.298262ms
    Sep  1 11:02:59.117: INFO: Pod "downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011753252s
    Sep  1 11:03:01.114: INFO: Pod "downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009330765s
    STEP: Saw pod success 09/01/23 11:03:01.114
    Sep  1 11:03:01.115: INFO: Pod "downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092" satisfied condition "Succeeded or Failed"
    Sep  1 11:03:01.118: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092 container dapi-container: <nil>
    STEP: delete the pod 09/01/23 11:03:01.126
    Sep  1 11:03:01.136: INFO: Waiting for pod downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092 to disappear
    Sep  1 11:03:01.139: INFO: Pod downward-api-c5c142fb-3794-4f8d-a882-ae8cd4214092 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:03:01.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3059" for this suite. 09/01/23 11:03:01.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:03:01.157
Sep  1 11:03:01.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-runtime 09/01/23 11:03:01.159
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:03:01.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:03:01.183
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 09/01/23 11:03:01.187
STEP: wait for the container to reach Succeeded 09/01/23 11:03:01.197
STEP: get the container status 09/01/23 11:03:05.222
STEP: the container should be terminated 09/01/23 11:03:05.225
STEP: the termination message should be set 09/01/23 11:03:05.225
Sep  1 11:03:05.226: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 09/01/23 11:03:05.226
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  1 11:03:05.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9097" for this suite. 09/01/23 11:03:05.249
------------------------------
• [4.101 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:03:01.157
    Sep  1 11:03:01.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-runtime 09/01/23 11:03:01.159
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:03:01.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:03:01.183
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 09/01/23 11:03:01.187
    STEP: wait for the container to reach Succeeded 09/01/23 11:03:01.197
    STEP: get the container status 09/01/23 11:03:05.222
    STEP: the container should be terminated 09/01/23 11:03:05.225
    STEP: the termination message should be set 09/01/23 11:03:05.225
    Sep  1 11:03:05.226: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 09/01/23 11:03:05.226
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:03:05.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9097" for this suite. 09/01/23 11:03:05.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:03:05.267
Sep  1 11:03:05.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename namespaces 09/01/23 11:03:05.269
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:03:05.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:03:05.29
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 09/01/23 11:03:05.293
Sep  1 11:03:05.296: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 09/01/23 11:03:05.296
Sep  1 11:03:05.304: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 09/01/23 11:03:05.304
Sep  1 11:03:05.313: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:03:05.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-268" for this suite. 09/01/23 11:03:05.32
------------------------------
• [0.060 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:03:05.267
    Sep  1 11:03:05.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename namespaces 09/01/23 11:03:05.269
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:03:05.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:03:05.29
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 09/01/23 11:03:05.293
    Sep  1 11:03:05.296: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 09/01/23 11:03:05.296
    Sep  1 11:03:05.304: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 09/01/23 11:03:05.304
    Sep  1 11:03:05.313: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:03:05.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-268" for this suite. 09/01/23 11:03:05.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:03:05.331
Sep  1 11:03:05.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 11:03:05.333
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:03:05.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:03:05.35
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 09/01/23 11:03:05.354
Sep  1 11:03:05.362: INFO: Waiting up to 5m0s for pod "pod-baa0e1fb-2e4b-4377-8782-27b209e56174" in namespace "emptydir-2049" to be "Succeeded or Failed"
Sep  1 11:03:05.369: INFO: Pod "pod-baa0e1fb-2e4b-4377-8782-27b209e56174": Phase="Pending", Reason="", readiness=false. Elapsed: 7.20804ms
Sep  1 11:03:07.374: INFO: Pod "pod-baa0e1fb-2e4b-4377-8782-27b209e56174": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012590348s
Sep  1 11:03:09.374: INFO: Pod "pod-baa0e1fb-2e4b-4377-8782-27b209e56174": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012295061s
STEP: Saw pod success 09/01/23 11:03:09.374
Sep  1 11:03:09.375: INFO: Pod "pod-baa0e1fb-2e4b-4377-8782-27b209e56174" satisfied condition "Succeeded or Failed"
Sep  1 11:03:09.377: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-baa0e1fb-2e4b-4377-8782-27b209e56174 container test-container: <nil>
STEP: delete the pod 09/01/23 11:03:09.384
Sep  1 11:03:09.398: INFO: Waiting for pod pod-baa0e1fb-2e4b-4377-8782-27b209e56174 to disappear
Sep  1 11:03:09.402: INFO: Pod pod-baa0e1fb-2e4b-4377-8782-27b209e56174 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 11:03:09.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2049" for this suite. 09/01/23 11:03:09.406
------------------------------
• [4.081 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:03:05.331
    Sep  1 11:03:05.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 11:03:05.333
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:03:05.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:03:05.35
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/01/23 11:03:05.354
    Sep  1 11:03:05.362: INFO: Waiting up to 5m0s for pod "pod-baa0e1fb-2e4b-4377-8782-27b209e56174" in namespace "emptydir-2049" to be "Succeeded or Failed"
    Sep  1 11:03:05.369: INFO: Pod "pod-baa0e1fb-2e4b-4377-8782-27b209e56174": Phase="Pending", Reason="", readiness=false. Elapsed: 7.20804ms
    Sep  1 11:03:07.374: INFO: Pod "pod-baa0e1fb-2e4b-4377-8782-27b209e56174": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012590348s
    Sep  1 11:03:09.374: INFO: Pod "pod-baa0e1fb-2e4b-4377-8782-27b209e56174": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012295061s
    STEP: Saw pod success 09/01/23 11:03:09.374
    Sep  1 11:03:09.375: INFO: Pod "pod-baa0e1fb-2e4b-4377-8782-27b209e56174" satisfied condition "Succeeded or Failed"
    Sep  1 11:03:09.377: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-baa0e1fb-2e4b-4377-8782-27b209e56174 container test-container: <nil>
    STEP: delete the pod 09/01/23 11:03:09.384
    Sep  1 11:03:09.398: INFO: Waiting for pod pod-baa0e1fb-2e4b-4377-8782-27b209e56174 to disappear
    Sep  1 11:03:09.402: INFO: Pod pod-baa0e1fb-2e4b-4377-8782-27b209e56174 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:03:09.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2049" for this suite. 09/01/23 11:03:09.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:03:09.417
Sep  1 11:03:09.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename sched-preemption 09/01/23 11:03:09.419
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:03:09.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:03:09.438
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  1 11:03:09.457: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  1 11:04:09.504: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:04:09.508
Sep  1 11:04:09.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename sched-preemption-path 09/01/23 11:04:09.51
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:09.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:09.533
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Sep  1 11:04:09.555: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Sep  1 11:04:09.559: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Sep  1 11:04:09.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:04:09.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-6061" for this suite. 09/01/23 11:04:09.648
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8970" for this suite. 09/01/23 11:04:09.655
------------------------------
• [SLOW TEST] [60.244 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:03:09.417
    Sep  1 11:03:09.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename sched-preemption 09/01/23 11:03:09.419
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:03:09.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:03:09.438
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  1 11:03:09.457: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  1 11:04:09.504: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:04:09.508
    Sep  1 11:04:09.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename sched-preemption-path 09/01/23 11:04:09.51
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:09.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:09.533
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Sep  1 11:04:09.555: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Sep  1 11:04:09.559: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:04:09.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:04:09.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-6061" for this suite. 09/01/23 11:04:09.648
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8970" for this suite. 09/01/23 11:04:09.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:04:09.669
Sep  1 11:04:09.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-lifecycle-hook 09/01/23 11:04:09.671
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:09.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:09.693
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/01/23 11:04:09.701
Sep  1 11:04:09.712: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4404" to be "running and ready"
Sep  1 11:04:09.716: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.173138ms
Sep  1 11:04:09.716: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:04:11.720: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008074342s
Sep  1 11:04:11.720: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  1 11:04:11.720: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 09/01/23 11:04:11.724
Sep  1 11:04:11.729: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4404" to be "running and ready"
Sep  1 11:04:11.732: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.993947ms
Sep  1 11:04:11.732: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:04:13.736: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006810692s
Sep  1 11:04:13.736: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Sep  1 11:04:13.736: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/01/23 11:04:13.739
STEP: delete the pod with lifecycle hook 09/01/23 11:04:13.746
Sep  1 11:04:13.753: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  1 11:04:13.757: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  1 11:04:15.759: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  1 11:04:15.763: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  1 11:04:17.758: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  1 11:04:17.765: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  1 11:04:17.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4404" for this suite. 09/01/23 11:04:17.77
------------------------------
• [SLOW TEST] [8.108 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:04:09.669
    Sep  1 11:04:09.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/01/23 11:04:09.671
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:09.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:09.693
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/01/23 11:04:09.701
    Sep  1 11:04:09.712: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4404" to be "running and ready"
    Sep  1 11:04:09.716: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.173138ms
    Sep  1 11:04:09.716: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:04:11.720: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008074342s
    Sep  1 11:04:11.720: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  1 11:04:11.720: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 09/01/23 11:04:11.724
    Sep  1 11:04:11.729: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4404" to be "running and ready"
    Sep  1 11:04:11.732: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.993947ms
    Sep  1 11:04:11.732: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:04:13.736: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006810692s
    Sep  1 11:04:13.736: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Sep  1 11:04:13.736: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/01/23 11:04:13.739
    STEP: delete the pod with lifecycle hook 09/01/23 11:04:13.746
    Sep  1 11:04:13.753: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  1 11:04:13.757: INFO: Pod pod-with-poststart-exec-hook still exists
    Sep  1 11:04:15.759: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  1 11:04:15.763: INFO: Pod pod-with-poststart-exec-hook still exists
    Sep  1 11:04:17.758: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  1 11:04:17.765: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:04:17.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4404" for this suite. 09/01/23 11:04:17.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:04:17.783
Sep  1 11:04:17.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pods 09/01/23 11:04:17.786
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:17.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:17.803
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Sep  1 11:04:17.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: creating the pod 09/01/23 11:04:17.808
STEP: submitting the pod to kubernetes 09/01/23 11:04:17.808
Sep  1 11:04:17.818: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-3f50a3f2-8ce4-48d3-8a5f-16cf9fc34223" in namespace "pods-2415" to be "running and ready"
Sep  1 11:04:17.824: INFO: Pod "pod-logs-websocket-3f50a3f2-8ce4-48d3-8a5f-16cf9fc34223": Phase="Pending", Reason="", readiness=false. Elapsed: 5.067011ms
Sep  1 11:04:17.824: INFO: The phase of Pod pod-logs-websocket-3f50a3f2-8ce4-48d3-8a5f-16cf9fc34223 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:04:19.828: INFO: Pod "pod-logs-websocket-3f50a3f2-8ce4-48d3-8a5f-16cf9fc34223": Phase="Running", Reason="", readiness=true. Elapsed: 2.009234569s
Sep  1 11:04:19.828: INFO: The phase of Pod pod-logs-websocket-3f50a3f2-8ce4-48d3-8a5f-16cf9fc34223 is Running (Ready = true)
Sep  1 11:04:19.828: INFO: Pod "pod-logs-websocket-3f50a3f2-8ce4-48d3-8a5f-16cf9fc34223" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  1 11:04:19.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2415" for this suite. 09/01/23 11:04:19.898
------------------------------
• [2.121 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:04:17.783
    Sep  1 11:04:17.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pods 09/01/23 11:04:17.786
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:17.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:17.803
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Sep  1 11:04:17.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: creating the pod 09/01/23 11:04:17.808
    STEP: submitting the pod to kubernetes 09/01/23 11:04:17.808
    Sep  1 11:04:17.818: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-3f50a3f2-8ce4-48d3-8a5f-16cf9fc34223" in namespace "pods-2415" to be "running and ready"
    Sep  1 11:04:17.824: INFO: Pod "pod-logs-websocket-3f50a3f2-8ce4-48d3-8a5f-16cf9fc34223": Phase="Pending", Reason="", readiness=false. Elapsed: 5.067011ms
    Sep  1 11:04:17.824: INFO: The phase of Pod pod-logs-websocket-3f50a3f2-8ce4-48d3-8a5f-16cf9fc34223 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:04:19.828: INFO: Pod "pod-logs-websocket-3f50a3f2-8ce4-48d3-8a5f-16cf9fc34223": Phase="Running", Reason="", readiness=true. Elapsed: 2.009234569s
    Sep  1 11:04:19.828: INFO: The phase of Pod pod-logs-websocket-3f50a3f2-8ce4-48d3-8a5f-16cf9fc34223 is Running (Ready = true)
    Sep  1 11:04:19.828: INFO: Pod "pod-logs-websocket-3f50a3f2-8ce4-48d3-8a5f-16cf9fc34223" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:04:19.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2415" for this suite. 09/01/23 11:04:19.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:04:19.91
Sep  1 11:04:19.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:04:19.912
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:19.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:19.933
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-fe5f1ce1-b3f4-4746-ac2f-7c8fa768dc1e 09/01/23 11:04:19.94
STEP: Creating configMap with name cm-test-opt-upd-ab9ae642-2390-426d-85d8-baf21ca08c03 09/01/23 11:04:19.944
STEP: Creating the pod 09/01/23 11:04:19.949
Sep  1 11:04:19.959: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ae22956d-1e61-4b4f-a217-e5694f49ab76" in namespace "projected-3512" to be "running and ready"
Sep  1 11:04:19.962: INFO: Pod "pod-projected-configmaps-ae22956d-1e61-4b4f-a217-e5694f49ab76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.863468ms
Sep  1 11:04:19.962: INFO: The phase of Pod pod-projected-configmaps-ae22956d-1e61-4b4f-a217-e5694f49ab76 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:04:21.966: INFO: Pod "pod-projected-configmaps-ae22956d-1e61-4b4f-a217-e5694f49ab76": Phase="Running", Reason="", readiness=true. Elapsed: 2.006920509s
Sep  1 11:04:21.966: INFO: The phase of Pod pod-projected-configmaps-ae22956d-1e61-4b4f-a217-e5694f49ab76 is Running (Ready = true)
Sep  1 11:04:21.966: INFO: Pod "pod-projected-configmaps-ae22956d-1e61-4b4f-a217-e5694f49ab76" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-fe5f1ce1-b3f4-4746-ac2f-7c8fa768dc1e 09/01/23 11:04:21.987
STEP: Updating configmap cm-test-opt-upd-ab9ae642-2390-426d-85d8-baf21ca08c03 09/01/23 11:04:21.996
STEP: Creating configMap with name cm-test-opt-create-9ca68e00-1a30-44a7-83a6-72d3876ec14a 09/01/23 11:04:22.001
STEP: waiting to observe update in volume 09/01/23 11:04:22.006
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:04:24.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3512" for this suite. 09/01/23 11:04:24.038
------------------------------
• [4.135 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:04:19.91
    Sep  1 11:04:19.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:04:19.912
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:19.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:19.933
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-fe5f1ce1-b3f4-4746-ac2f-7c8fa768dc1e 09/01/23 11:04:19.94
    STEP: Creating configMap with name cm-test-opt-upd-ab9ae642-2390-426d-85d8-baf21ca08c03 09/01/23 11:04:19.944
    STEP: Creating the pod 09/01/23 11:04:19.949
    Sep  1 11:04:19.959: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ae22956d-1e61-4b4f-a217-e5694f49ab76" in namespace "projected-3512" to be "running and ready"
    Sep  1 11:04:19.962: INFO: Pod "pod-projected-configmaps-ae22956d-1e61-4b4f-a217-e5694f49ab76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.863468ms
    Sep  1 11:04:19.962: INFO: The phase of Pod pod-projected-configmaps-ae22956d-1e61-4b4f-a217-e5694f49ab76 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:04:21.966: INFO: Pod "pod-projected-configmaps-ae22956d-1e61-4b4f-a217-e5694f49ab76": Phase="Running", Reason="", readiness=true. Elapsed: 2.006920509s
    Sep  1 11:04:21.966: INFO: The phase of Pod pod-projected-configmaps-ae22956d-1e61-4b4f-a217-e5694f49ab76 is Running (Ready = true)
    Sep  1 11:04:21.966: INFO: Pod "pod-projected-configmaps-ae22956d-1e61-4b4f-a217-e5694f49ab76" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-fe5f1ce1-b3f4-4746-ac2f-7c8fa768dc1e 09/01/23 11:04:21.987
    STEP: Updating configmap cm-test-opt-upd-ab9ae642-2390-426d-85d8-baf21ca08c03 09/01/23 11:04:21.996
    STEP: Creating configMap with name cm-test-opt-create-9ca68e00-1a30-44a7-83a6-72d3876ec14a 09/01/23 11:04:22.001
    STEP: waiting to observe update in volume 09/01/23 11:04:22.006
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:04:24.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3512" for this suite. 09/01/23 11:04:24.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:04:24.05
Sep  1 11:04:24.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename watch 09/01/23 11:04:24.052
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:24.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:24.076
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 09/01/23 11:04:24.08
STEP: starting a background goroutine to produce watch events 09/01/23 11:04:24.083
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/01/23 11:04:24.083
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  1 11:04:26.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6269" for this suite. 09/01/23 11:04:26.909
------------------------------
• [2.910 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:04:24.05
    Sep  1 11:04:24.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename watch 09/01/23 11:04:24.052
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:24.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:24.076
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 09/01/23 11:04:24.08
    STEP: starting a background goroutine to produce watch events 09/01/23 11:04:24.083
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/01/23 11:04:24.083
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:04:26.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6269" for this suite. 09/01/23 11:04:26.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:04:26.966
Sep  1 11:04:26.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename limitrange 09/01/23 11:04:26.969
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:26.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:26.992
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-69jpx" in namespace "limitrange-1293" 09/01/23 11:04:26.996
STEP: Creating another limitRange in another namespace 09/01/23 11:04:27.064
Sep  1 11:04:27.078: INFO: Namespace "e2e-limitrange-69jpx-6627" created
Sep  1 11:04:27.078: INFO: Creating LimitRange "e2e-limitrange-69jpx" in namespace "e2e-limitrange-69jpx-6627"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-69jpx" 09/01/23 11:04:27.083
Sep  1 11:04:27.086: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-69jpx" in "limitrange-1293" namespace 09/01/23 11:04:27.086
Sep  1 11:04:27.095: INFO: LimitRange "e2e-limitrange-69jpx" has been patched
STEP: Delete LimitRange "e2e-limitrange-69jpx" by Collection with labelSelector: "e2e-limitrange-69jpx=patched" 09/01/23 11:04:27.095
STEP: Confirm that the limitRange "e2e-limitrange-69jpx" has been deleted 09/01/23 11:04:27.102
Sep  1 11:04:27.102: INFO: Requesting list of LimitRange to confirm quantity
Sep  1 11:04:27.105: INFO: Found 0 LimitRange with label "e2e-limitrange-69jpx=patched"
Sep  1 11:04:27.105: INFO: LimitRange "e2e-limitrange-69jpx" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-69jpx" 09/01/23 11:04:27.105
Sep  1 11:04:27.109: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Sep  1 11:04:27.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-1293" for this suite. 09/01/23 11:04:27.113
STEP: Destroying namespace "e2e-limitrange-69jpx-6627" for this suite. 09/01/23 11:04:27.122
------------------------------
• [0.163 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:04:26.966
    Sep  1 11:04:26.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename limitrange 09/01/23 11:04:26.969
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:26.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:26.992
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-69jpx" in namespace "limitrange-1293" 09/01/23 11:04:26.996
    STEP: Creating another limitRange in another namespace 09/01/23 11:04:27.064
    Sep  1 11:04:27.078: INFO: Namespace "e2e-limitrange-69jpx-6627" created
    Sep  1 11:04:27.078: INFO: Creating LimitRange "e2e-limitrange-69jpx" in namespace "e2e-limitrange-69jpx-6627"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-69jpx" 09/01/23 11:04:27.083
    Sep  1 11:04:27.086: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-69jpx" in "limitrange-1293" namespace 09/01/23 11:04:27.086
    Sep  1 11:04:27.095: INFO: LimitRange "e2e-limitrange-69jpx" has been patched
    STEP: Delete LimitRange "e2e-limitrange-69jpx" by Collection with labelSelector: "e2e-limitrange-69jpx=patched" 09/01/23 11:04:27.095
    STEP: Confirm that the limitRange "e2e-limitrange-69jpx" has been deleted 09/01/23 11:04:27.102
    Sep  1 11:04:27.102: INFO: Requesting list of LimitRange to confirm quantity
    Sep  1 11:04:27.105: INFO: Found 0 LimitRange with label "e2e-limitrange-69jpx=patched"
    Sep  1 11:04:27.105: INFO: LimitRange "e2e-limitrange-69jpx" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-69jpx" 09/01/23 11:04:27.105
    Sep  1 11:04:27.109: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:04:27.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-1293" for this suite. 09/01/23 11:04:27.113
    STEP: Destroying namespace "e2e-limitrange-69jpx-6627" for this suite. 09/01/23 11:04:27.122
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:04:27.132
Sep  1 11:04:27.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 11:04:27.134
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:27.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:27.155
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 09/01/23 11:04:27.159
Sep  1 11:04:27.159: INFO: namespace kubectl-4519
Sep  1 11:04:27.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-4519 create -f -'
Sep  1 11:04:31.588: INFO: stderr: ""
Sep  1 11:04:31.588: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/01/23 11:04:31.588
Sep  1 11:04:32.594: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  1 11:04:32.594: INFO: Found 0 / 1
Sep  1 11:04:33.592: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  1 11:04:33.592: INFO: Found 1 / 1
Sep  1 11:04:33.592: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  1 11:04:33.596: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  1 11:04:33.596: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  1 11:04:33.596: INFO: wait on agnhost-primary startup in kubectl-4519 
Sep  1 11:04:33.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-4519 logs agnhost-primary-sn5ss agnhost-primary'
Sep  1 11:04:33.777: INFO: stderr: ""
Sep  1 11:04:33.778: INFO: stdout: "Paused\n"
STEP: exposing RC 09/01/23 11:04:33.778
Sep  1 11:04:33.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-4519 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Sep  1 11:04:33.917: INFO: stderr: ""
Sep  1 11:04:33.917: INFO: stdout: "service/rm2 exposed\n"
Sep  1 11:04:33.926: INFO: Service rm2 in namespace kubectl-4519 found.
STEP: exposing service 09/01/23 11:04:35.931
Sep  1 11:04:35.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-4519 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Sep  1 11:04:36.052: INFO: stderr: ""
Sep  1 11:04:36.052: INFO: stdout: "service/rm3 exposed\n"
Sep  1 11:04:36.059: INFO: Service rm3 in namespace kubectl-4519 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 11:04:38.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4519" for this suite. 09/01/23 11:04:38.07
------------------------------
• [SLOW TEST] [10.944 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:04:27.132
    Sep  1 11:04:27.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 11:04:27.134
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:27.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:27.155
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 09/01/23 11:04:27.159
    Sep  1 11:04:27.159: INFO: namespace kubectl-4519
    Sep  1 11:04:27.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-4519 create -f -'
    Sep  1 11:04:31.588: INFO: stderr: ""
    Sep  1 11:04:31.588: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/01/23 11:04:31.588
    Sep  1 11:04:32.594: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  1 11:04:32.594: INFO: Found 0 / 1
    Sep  1 11:04:33.592: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  1 11:04:33.592: INFO: Found 1 / 1
    Sep  1 11:04:33.592: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep  1 11:04:33.596: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  1 11:04:33.596: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  1 11:04:33.596: INFO: wait on agnhost-primary startup in kubectl-4519 
    Sep  1 11:04:33.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-4519 logs agnhost-primary-sn5ss agnhost-primary'
    Sep  1 11:04:33.777: INFO: stderr: ""
    Sep  1 11:04:33.778: INFO: stdout: "Paused\n"
    STEP: exposing RC 09/01/23 11:04:33.778
    Sep  1 11:04:33.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-4519 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Sep  1 11:04:33.917: INFO: stderr: ""
    Sep  1 11:04:33.917: INFO: stdout: "service/rm2 exposed\n"
    Sep  1 11:04:33.926: INFO: Service rm2 in namespace kubectl-4519 found.
    STEP: exposing service 09/01/23 11:04:35.931
    Sep  1 11:04:35.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-4519 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Sep  1 11:04:36.052: INFO: stderr: ""
    Sep  1 11:04:36.052: INFO: stdout: "service/rm3 exposed\n"
    Sep  1 11:04:36.059: INFO: Service rm3 in namespace kubectl-4519 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:04:38.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4519" for this suite. 09/01/23 11:04:38.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:04:38.078
Sep  1 11:04:38.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename statefulset 09/01/23 11:04:38.079
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:38.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:38.099
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3436 09/01/23 11:04:38.103
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-3436 09/01/23 11:04:38.108
Sep  1 11:04:38.123: INFO: Found 0 stateful pods, waiting for 1
Sep  1 11:04:48.126: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 09/01/23 11:04:48.132
STEP: updating a scale subresource 09/01/23 11:04:48.172
STEP: verifying the statefulset Spec.Replicas was modified 09/01/23 11:04:48.177
STEP: Patch a scale subresource 09/01/23 11:04:48.181
STEP: verifying the statefulset Spec.Replicas was modified 09/01/23 11:04:48.189
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  1 11:04:48.201: INFO: Deleting all statefulset in ns statefulset-3436
Sep  1 11:04:48.206: INFO: Scaling statefulset ss to 0
Sep  1 11:04:58.231: INFO: Waiting for statefulset status.replicas updated to 0
Sep  1 11:04:58.234: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  1 11:04:58.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3436" for this suite. 09/01/23 11:04:58.277
------------------------------
• [SLOW TEST] [20.211 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:04:38.078
    Sep  1 11:04:38.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename statefulset 09/01/23 11:04:38.079
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:38.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:38.099
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3436 09/01/23 11:04:38.103
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-3436 09/01/23 11:04:38.108
    Sep  1 11:04:38.123: INFO: Found 0 stateful pods, waiting for 1
    Sep  1 11:04:48.126: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 09/01/23 11:04:48.132
    STEP: updating a scale subresource 09/01/23 11:04:48.172
    STEP: verifying the statefulset Spec.Replicas was modified 09/01/23 11:04:48.177
    STEP: Patch a scale subresource 09/01/23 11:04:48.181
    STEP: verifying the statefulset Spec.Replicas was modified 09/01/23 11:04:48.189
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  1 11:04:48.201: INFO: Deleting all statefulset in ns statefulset-3436
    Sep  1 11:04:48.206: INFO: Scaling statefulset ss to 0
    Sep  1 11:04:58.231: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  1 11:04:58.234: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:04:58.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3436" for this suite. 09/01/23 11:04:58.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:04:58.295
Sep  1 11:04:58.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 11:04:58.297
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:58.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:58.316
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 09/01/23 11:04:58.318
Sep  1 11:04:58.325: INFO: Waiting up to 5m0s for pod "downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126" in namespace "downward-api-5478" to be "Succeeded or Failed"
Sep  1 11:04:58.329: INFO: Pod "downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126": Phase="Pending", Reason="", readiness=false. Elapsed: 4.586434ms
Sep  1 11:05:00.334: INFO: Pod "downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126": Phase="Running", Reason="", readiness=false. Elapsed: 2.008843533s
Sep  1 11:05:02.335: INFO: Pod "downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009749955s
STEP: Saw pod success 09/01/23 11:05:02.335
Sep  1 11:05:02.335: INFO: Pod "downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126" satisfied condition "Succeeded or Failed"
Sep  1 11:05:02.342: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126 container dapi-container: <nil>
STEP: delete the pod 09/01/23 11:05:02.35
Sep  1 11:05:02.364: INFO: Waiting for pod downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126 to disappear
Sep  1 11:05:02.367: INFO: Pod downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  1 11:05:02.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5478" for this suite. 09/01/23 11:05:02.374
------------------------------
• [4.088 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:04:58.295
    Sep  1 11:04:58.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 11:04:58.297
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:04:58.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:04:58.316
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 09/01/23 11:04:58.318
    Sep  1 11:04:58.325: INFO: Waiting up to 5m0s for pod "downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126" in namespace "downward-api-5478" to be "Succeeded or Failed"
    Sep  1 11:04:58.329: INFO: Pod "downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126": Phase="Pending", Reason="", readiness=false. Elapsed: 4.586434ms
    Sep  1 11:05:00.334: INFO: Pod "downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126": Phase="Running", Reason="", readiness=false. Elapsed: 2.008843533s
    Sep  1 11:05:02.335: INFO: Pod "downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009749955s
    STEP: Saw pod success 09/01/23 11:05:02.335
    Sep  1 11:05:02.335: INFO: Pod "downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126" satisfied condition "Succeeded or Failed"
    Sep  1 11:05:02.342: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126 container dapi-container: <nil>
    STEP: delete the pod 09/01/23 11:05:02.35
    Sep  1 11:05:02.364: INFO: Waiting for pod downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126 to disappear
    Sep  1 11:05:02.367: INFO: Pod downward-api-ff1de7bf-d901-4bd8-ab5e-77ed79eb3126 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:05:02.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5478" for this suite. 09/01/23 11:05:02.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:05:02.397
Sep  1 11:05:02.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 11:05:02.401
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:05:02.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:05:02.426
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-57120412-7cc5-4442-9abf-0fb7ba86cee7 09/01/23 11:05:02.431
STEP: Creating a pod to test consume configMaps 09/01/23 11:05:02.438
Sep  1 11:05:02.450: INFO: Waiting up to 5m0s for pod "pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2" in namespace "configmap-1123" to be "Succeeded or Failed"
Sep  1 11:05:02.457: INFO: Pod "pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.026613ms
Sep  1 11:05:04.464: INFO: Pod "pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013481911s
Sep  1 11:05:06.463: INFO: Pod "pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012969253s
STEP: Saw pod success 09/01/23 11:05:06.463
Sep  1 11:05:06.465: INFO: Pod "pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2" satisfied condition "Succeeded or Failed"
Sep  1 11:05:06.468: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2 container agnhost-container: <nil>
STEP: delete the pod 09/01/23 11:05:06.478
Sep  1 11:05:06.497: INFO: Waiting for pod pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2 to disappear
Sep  1 11:05:06.502: INFO: Pod pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:05:06.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1123" for this suite. 09/01/23 11:05:06.515
------------------------------
• [4.128 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:05:02.397
    Sep  1 11:05:02.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 11:05:02.401
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:05:02.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:05:02.426
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-57120412-7cc5-4442-9abf-0fb7ba86cee7 09/01/23 11:05:02.431
    STEP: Creating a pod to test consume configMaps 09/01/23 11:05:02.438
    Sep  1 11:05:02.450: INFO: Waiting up to 5m0s for pod "pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2" in namespace "configmap-1123" to be "Succeeded or Failed"
    Sep  1 11:05:02.457: INFO: Pod "pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.026613ms
    Sep  1 11:05:04.464: INFO: Pod "pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013481911s
    Sep  1 11:05:06.463: INFO: Pod "pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012969253s
    STEP: Saw pod success 09/01/23 11:05:06.463
    Sep  1 11:05:06.465: INFO: Pod "pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2" satisfied condition "Succeeded or Failed"
    Sep  1 11:05:06.468: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2 container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 11:05:06.478
    Sep  1 11:05:06.497: INFO: Waiting for pod pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2 to disappear
    Sep  1 11:05:06.502: INFO: Pod pod-configmaps-c36643b5-e705-4476-aaee-7c5535ebead2 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:05:06.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1123" for this suite. 09/01/23 11:05:06.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:05:06.529
Sep  1 11:05:06.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename sched-preemption 09/01/23 11:05:06.535
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:05:06.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:05:06.562
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  1 11:05:06.587: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  1 11:06:06.639: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 09/01/23 11:06:06.643
Sep  1 11:06:06.671: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep  1 11:06:06.693: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep  1 11:06:06.716: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep  1 11:06:06.726: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/01/23 11:06:06.726
Sep  1 11:06:06.726: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4230" to be "running"
Sep  1 11:06:06.729: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.939053ms
Sep  1 11:06:08.738: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.012139244s
Sep  1 11:06:08.738: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep  1 11:06:08.738: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4230" to be "running"
Sep  1 11:06:08.742: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.00079ms
Sep  1 11:06:08.743: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  1 11:06:08.743: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4230" to be "running"
Sep  1 11:06:08.748: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.04624ms
Sep  1 11:06:10.753: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009787607s
Sep  1 11:06:10.753: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  1 11:06:10.753: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4230" to be "running"
Sep  1 11:06:10.757: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.063635ms
Sep  1 11:06:10.757: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 09/01/23 11:06:10.757
Sep  1 11:06:10.766: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Sep  1 11:06:10.770: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.81062ms
Sep  1 11:06:12.775: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008795938s
Sep  1 11:06:14.775: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008295819s
Sep  1 11:06:14.775: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:06:14.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4230" for this suite. 09/01/23 11:06:14.89
------------------------------
• [SLOW TEST] [68.367 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:05:06.529
    Sep  1 11:05:06.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename sched-preemption 09/01/23 11:05:06.535
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:05:06.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:05:06.562
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  1 11:05:06.587: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  1 11:06:06.639: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 09/01/23 11:06:06.643
    Sep  1 11:06:06.671: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep  1 11:06:06.693: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep  1 11:06:06.716: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep  1 11:06:06.726: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/01/23 11:06:06.726
    Sep  1 11:06:06.726: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4230" to be "running"
    Sep  1 11:06:06.729: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.939053ms
    Sep  1 11:06:08.738: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.012139244s
    Sep  1 11:06:08.738: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep  1 11:06:08.738: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4230" to be "running"
    Sep  1 11:06:08.742: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.00079ms
    Sep  1 11:06:08.743: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  1 11:06:08.743: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4230" to be "running"
    Sep  1 11:06:08.748: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.04624ms
    Sep  1 11:06:10.753: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009787607s
    Sep  1 11:06:10.753: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  1 11:06:10.753: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4230" to be "running"
    Sep  1 11:06:10.757: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.063635ms
    Sep  1 11:06:10.757: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 09/01/23 11:06:10.757
    Sep  1 11:06:10.766: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Sep  1 11:06:10.770: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.81062ms
    Sep  1 11:06:12.775: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008795938s
    Sep  1 11:06:14.775: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008295819s
    Sep  1 11:06:14.775: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:06:14.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4230" for this suite. 09/01/23 11:06:14.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:06:14.904
Sep  1 11:06:14.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 11:06:14.905
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:14.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:14.925
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Sep  1 11:06:14.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/01/23 11:06:18.77
Sep  1 11:06:18.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-9136 --namespace=crd-publish-openapi-9136 create -f -'
Sep  1 11:06:21.885: INFO: stderr: ""
Sep  1 11:06:21.885: INFO: stdout: "e2e-test-crd-publish-openapi-9217-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  1 11:06:21.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-9136 --namespace=crd-publish-openapi-9136 delete e2e-test-crd-publish-openapi-9217-crds test-cr'
Sep  1 11:06:21.993: INFO: stderr: ""
Sep  1 11:06:21.993: INFO: stdout: "e2e-test-crd-publish-openapi-9217-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep  1 11:06:21.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-9136 --namespace=crd-publish-openapi-9136 apply -f -'
Sep  1 11:06:22.624: INFO: stderr: ""
Sep  1 11:06:22.624: INFO: stdout: "e2e-test-crd-publish-openapi-9217-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  1 11:06:22.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-9136 --namespace=crd-publish-openapi-9136 delete e2e-test-crd-publish-openapi-9217-crds test-cr'
Sep  1 11:06:22.754: INFO: stderr: ""
Sep  1 11:06:22.754: INFO: stdout: "e2e-test-crd-publish-openapi-9217-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/01/23 11:06:22.754
Sep  1 11:06:22.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-9136 explain e2e-test-crd-publish-openapi-9217-crds'
Sep  1 11:06:23.388: INFO: stderr: ""
Sep  1 11:06:23.388: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9217-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:06:27.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9136" for this suite. 09/01/23 11:06:27.648
------------------------------
• [SLOW TEST] [12.751 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:06:14.904
    Sep  1 11:06:14.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 11:06:14.905
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:14.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:14.925
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Sep  1 11:06:14.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/01/23 11:06:18.77
    Sep  1 11:06:18.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-9136 --namespace=crd-publish-openapi-9136 create -f -'
    Sep  1 11:06:21.885: INFO: stderr: ""
    Sep  1 11:06:21.885: INFO: stdout: "e2e-test-crd-publish-openapi-9217-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep  1 11:06:21.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-9136 --namespace=crd-publish-openapi-9136 delete e2e-test-crd-publish-openapi-9217-crds test-cr'
    Sep  1 11:06:21.993: INFO: stderr: ""
    Sep  1 11:06:21.993: INFO: stdout: "e2e-test-crd-publish-openapi-9217-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Sep  1 11:06:21.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-9136 --namespace=crd-publish-openapi-9136 apply -f -'
    Sep  1 11:06:22.624: INFO: stderr: ""
    Sep  1 11:06:22.624: INFO: stdout: "e2e-test-crd-publish-openapi-9217-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep  1 11:06:22.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-9136 --namespace=crd-publish-openapi-9136 delete e2e-test-crd-publish-openapi-9217-crds test-cr'
    Sep  1 11:06:22.754: INFO: stderr: ""
    Sep  1 11:06:22.754: INFO: stdout: "e2e-test-crd-publish-openapi-9217-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/01/23 11:06:22.754
    Sep  1 11:06:22.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=crd-publish-openapi-9136 explain e2e-test-crd-publish-openapi-9217-crds'
    Sep  1 11:06:23.388: INFO: stderr: ""
    Sep  1 11:06:23.388: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9217-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:06:27.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9136" for this suite. 09/01/23 11:06:27.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:06:27.658
Sep  1 11:06:27.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename podtemplate 09/01/23 11:06:27.66
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:27.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:27.69
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 09/01/23 11:06:27.695
STEP: Replace a pod template 09/01/23 11:06:27.7
Sep  1 11:06:27.710: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  1 11:06:27.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-3023" for this suite. 09/01/23 11:06:27.717
------------------------------
• [0.066 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:06:27.658
    Sep  1 11:06:27.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename podtemplate 09/01/23 11:06:27.66
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:27.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:27.69
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 09/01/23 11:06:27.695
    STEP: Replace a pod template 09/01/23 11:06:27.7
    Sep  1 11:06:27.710: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:06:27.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-3023" for this suite. 09/01/23 11:06:27.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:06:27.725
Sep  1 11:06:27.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename replication-controller 09/01/23 11:06:27.727
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:27.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:27.75
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 09/01/23 11:06:27.753
Sep  1 11:06:27.764: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8640" to be "running and ready"
Sep  1 11:06:27.768: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.842646ms
Sep  1 11:06:27.768: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:06:29.772: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.007805011s
Sep  1 11:06:29.772: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Sep  1 11:06:29.772: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 09/01/23 11:06:29.777
STEP: Then the orphan pod is adopted 09/01/23 11:06:29.804
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  1 11:06:30.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8640" for this suite. 09/01/23 11:06:30.817
------------------------------
• [3.099 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:06:27.725
    Sep  1 11:06:27.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename replication-controller 09/01/23 11:06:27.727
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:27.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:27.75
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 09/01/23 11:06:27.753
    Sep  1 11:06:27.764: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8640" to be "running and ready"
    Sep  1 11:06:27.768: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.842646ms
    Sep  1 11:06:27.768: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:06:29.772: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.007805011s
    Sep  1 11:06:29.772: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Sep  1 11:06:29.772: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 09/01/23 11:06:29.777
    STEP: Then the orphan pod is adopted 09/01/23 11:06:29.804
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:06:30.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8640" for this suite. 09/01/23 11:06:30.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:06:30.829
Sep  1 11:06:30.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 11:06:30.83
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:30.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:30.856
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 11:06:30.878
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:06:31.396
STEP: Deploying the webhook pod 09/01/23 11:06:31.406
STEP: Wait for the deployment to be ready 09/01/23 11:06:31.423
Sep  1 11:06:31.433: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/01/23 11:06:33.444
STEP: Verifying the service has paired with the endpoint 09/01/23 11:06:33.468
Sep  1 11:06:34.469: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 09/01/23 11:06:34.473
STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/01/23 11:06:34.495
STEP: Creating a configMap that should not be mutated 09/01/23 11:06:34.505
STEP: Patching a mutating webhook configuration's rules to include the create operation 09/01/23 11:06:34.518
STEP: Creating a configMap that should be mutated 09/01/23 11:06:34.528
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:06:34.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2244" for this suite. 09/01/23 11:06:34.708
STEP: Destroying namespace "webhook-2244-markers" for this suite. 09/01/23 11:06:34.722
------------------------------
• [3.910 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:06:30.829
    Sep  1 11:06:30.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 11:06:30.83
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:30.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:30.856
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 11:06:30.878
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:06:31.396
    STEP: Deploying the webhook pod 09/01/23 11:06:31.406
    STEP: Wait for the deployment to be ready 09/01/23 11:06:31.423
    Sep  1 11:06:31.433: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/01/23 11:06:33.444
    STEP: Verifying the service has paired with the endpoint 09/01/23 11:06:33.468
    Sep  1 11:06:34.469: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 09/01/23 11:06:34.473
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/01/23 11:06:34.495
    STEP: Creating a configMap that should not be mutated 09/01/23 11:06:34.505
    STEP: Patching a mutating webhook configuration's rules to include the create operation 09/01/23 11:06:34.518
    STEP: Creating a configMap that should be mutated 09/01/23 11:06:34.528
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:06:34.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2244" for this suite. 09/01/23 11:06:34.708
    STEP: Destroying namespace "webhook-2244-markers" for this suite. 09/01/23 11:06:34.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:06:34.749
Sep  1 11:06:34.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 11:06:34.751
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:34.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:34.783
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-6881 09/01/23 11:06:34.789
STEP: creating service affinity-clusterip-transition in namespace services-6881 09/01/23 11:06:34.79
STEP: creating replication controller affinity-clusterip-transition in namespace services-6881 09/01/23 11:06:34.804
I0901 11:06:34.817091      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-6881, replica count: 3
I0901 11:06:37.868697      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  1 11:06:37.879: INFO: Creating new exec pod
Sep  1 11:06:37.889: INFO: Waiting up to 5m0s for pod "execpod-affinitydh2w5" in namespace "services-6881" to be "running"
Sep  1 11:06:37.897: INFO: Pod "execpod-affinitydh2w5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.502229ms
Sep  1 11:06:39.902: INFO: Pod "execpod-affinitydh2w5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012570297s
Sep  1 11:06:39.902: INFO: Pod "execpod-affinitydh2w5" satisfied condition "running"
Sep  1 11:06:40.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6881 exec execpod-affinitydh2w5 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Sep  1 11:06:41.299: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep  1 11:06:41.299: INFO: stdout: ""
Sep  1 11:06:41.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6881 exec execpod-affinitydh2w5 -- /bin/sh -x -c nc -v -z -w 2 10.106.98.25 80'
Sep  1 11:06:41.498: INFO: stderr: "+ nc -v -z -w 2 10.106.98.25 80\nConnection to 10.106.98.25 80 port [tcp/http] succeeded!\n"
Sep  1 11:06:41.498: INFO: stdout: ""
Sep  1 11:06:41.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6881 exec execpod-affinitydh2w5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.98.25:80/ ; done'
Sep  1 11:06:42.090: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n"
Sep  1 11:06:42.090: INFO: stdout: "\naffinity-clusterip-transition-mqmmf\naffinity-clusterip-transition-x7gpb\naffinity-clusterip-transition-mqmmf\naffinity-clusterip-transition-mqmmf\naffinity-clusterip-transition-x7gpb\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-x7gpb\naffinity-clusterip-transition-x7gpb\naffinity-clusterip-transition-mqmmf\naffinity-clusterip-transition-mqmmf\naffinity-clusterip-transition-mqmmf\naffinity-clusterip-transition-x7gpb\naffinity-clusterip-transition-mqmmf\naffinity-clusterip-transition-x7gpb\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m"
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-mqmmf
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-x7gpb
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-mqmmf
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-mqmmf
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-x7gpb
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-x7gpb
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-x7gpb
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-mqmmf
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-mqmmf
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-mqmmf
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-x7gpb
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-mqmmf
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-x7gpb
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6881 exec execpod-affinitydh2w5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.98.25:80/ ; done'
Sep  1 11:06:42.479: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n"
Sep  1 11:06:42.480: INFO: stdout: "\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m"
Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.481: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.481: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.481: INFO: Received response from host: affinity-clusterip-transition-4tq9m
Sep  1 11:06:42.481: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6881, will wait for the garbage collector to delete the pods 09/01/23 11:06:42.497
Sep  1 11:06:42.560: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.867162ms
Sep  1 11:06:42.661: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.552584ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 11:06:45.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6881" for this suite. 09/01/23 11:06:45.072
------------------------------
• [SLOW TEST] [10.338 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:06:34.749
    Sep  1 11:06:34.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 11:06:34.751
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:34.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:34.783
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-6881 09/01/23 11:06:34.789
    STEP: creating service affinity-clusterip-transition in namespace services-6881 09/01/23 11:06:34.79
    STEP: creating replication controller affinity-clusterip-transition in namespace services-6881 09/01/23 11:06:34.804
    I0901 11:06:34.817091      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-6881, replica count: 3
    I0901 11:06:37.868697      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  1 11:06:37.879: INFO: Creating new exec pod
    Sep  1 11:06:37.889: INFO: Waiting up to 5m0s for pod "execpod-affinitydh2w5" in namespace "services-6881" to be "running"
    Sep  1 11:06:37.897: INFO: Pod "execpod-affinitydh2w5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.502229ms
    Sep  1 11:06:39.902: INFO: Pod "execpod-affinitydh2w5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012570297s
    Sep  1 11:06:39.902: INFO: Pod "execpod-affinitydh2w5" satisfied condition "running"
    Sep  1 11:06:40.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6881 exec execpod-affinitydh2w5 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Sep  1 11:06:41.299: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Sep  1 11:06:41.299: INFO: stdout: ""
    Sep  1 11:06:41.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6881 exec execpod-affinitydh2w5 -- /bin/sh -x -c nc -v -z -w 2 10.106.98.25 80'
    Sep  1 11:06:41.498: INFO: stderr: "+ nc -v -z -w 2 10.106.98.25 80\nConnection to 10.106.98.25 80 port [tcp/http] succeeded!\n"
    Sep  1 11:06:41.498: INFO: stdout: ""
    Sep  1 11:06:41.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6881 exec execpod-affinitydh2w5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.98.25:80/ ; done'
    Sep  1 11:06:42.090: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n"
    Sep  1 11:06:42.090: INFO: stdout: "\naffinity-clusterip-transition-mqmmf\naffinity-clusterip-transition-x7gpb\naffinity-clusterip-transition-mqmmf\naffinity-clusterip-transition-mqmmf\naffinity-clusterip-transition-x7gpb\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-x7gpb\naffinity-clusterip-transition-x7gpb\naffinity-clusterip-transition-mqmmf\naffinity-clusterip-transition-mqmmf\naffinity-clusterip-transition-mqmmf\naffinity-clusterip-transition-x7gpb\naffinity-clusterip-transition-mqmmf\naffinity-clusterip-transition-x7gpb\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m"
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-mqmmf
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-x7gpb
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-mqmmf
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-mqmmf
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-x7gpb
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-x7gpb
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-x7gpb
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-mqmmf
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-mqmmf
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-mqmmf
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-x7gpb
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-mqmmf
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-x7gpb
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.090: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6881 exec execpod-affinitydh2w5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.98.25:80/ ; done'
    Sep  1 11:06:42.479: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.98.25:80/\n"
    Sep  1 11:06:42.480: INFO: stdout: "\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m\naffinity-clusterip-transition-4tq9m"
    Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.480: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.481: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.481: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.481: INFO: Received response from host: affinity-clusterip-transition-4tq9m
    Sep  1 11:06:42.481: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6881, will wait for the garbage collector to delete the pods 09/01/23 11:06:42.497
    Sep  1 11:06:42.560: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.867162ms
    Sep  1 11:06:42.661: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.552584ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:06:45.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6881" for this suite. 09/01/23 11:06:45.072
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:06:45.091
Sep  1 11:06:45.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubelet-test 09/01/23 11:06:45.093
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:45.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:45.122
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  1 11:06:45.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2358" for this suite. 09/01/23 11:06:45.18
------------------------------
• [0.097 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:06:45.091
    Sep  1 11:06:45.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubelet-test 09/01/23 11:06:45.093
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:45.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:45.122
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:06:45.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2358" for this suite. 09/01/23 11:06:45.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:06:45.196
Sep  1 11:06:45.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename resourcequota 09/01/23 11:06:45.198
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:45.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:45.226
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 09/01/23 11:06:45.23
STEP: Getting a ResourceQuota 09/01/23 11:06:45.235
STEP: Updating a ResourceQuota 09/01/23 11:06:45.242
STEP: Verifying a ResourceQuota was modified 09/01/23 11:06:45.253
STEP: Deleting a ResourceQuota 09/01/23 11:06:45.26
STEP: Verifying the deleted ResourceQuota 09/01/23 11:06:45.269
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  1 11:06:45.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7248" for this suite. 09/01/23 11:06:45.302
------------------------------
• [0.121 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:06:45.196
    Sep  1 11:06:45.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename resourcequota 09/01/23 11:06:45.198
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:45.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:45.226
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 09/01/23 11:06:45.23
    STEP: Getting a ResourceQuota 09/01/23 11:06:45.235
    STEP: Updating a ResourceQuota 09/01/23 11:06:45.242
    STEP: Verifying a ResourceQuota was modified 09/01/23 11:06:45.253
    STEP: Deleting a ResourceQuota 09/01/23 11:06:45.26
    STEP: Verifying the deleted ResourceQuota 09/01/23 11:06:45.269
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:06:45.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7248" for this suite. 09/01/23 11:06:45.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:06:45.327
Sep  1 11:06:45.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename sched-pred 09/01/23 11:06:45.34
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:45.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:45.383
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  1 11:06:45.388: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  1 11:06:45.398: INFO: Waiting for terminating namespaces to be deleted...
Sep  1 11:06:45.403: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
Sep  1 11:06:45.416: INFO: cilium-operator-858666d4b6-t7brd from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.417: INFO: 	Container cilium-operator ready: true, restart count 0
Sep  1 11:06:45.417: INFO: cilium-q8wf6 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.418: INFO: 	Container cilium-agent ready: true, restart count 0
Sep  1 11:06:45.418: INFO: kube-proxy-wdplh from kube-system started at 2023-09-01 10:08:39 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.419: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  1 11:06:45.419: INFO: fluentbit-fluentbit-z77m2 from logging-system started at 2023-09-01 10:49:46 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.420: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  1 11:06:45.420: INFO: logging-fluentd-0 from logging-system started at 2023-09-01 10:49:45 +0000 UTC (2 container statuses recorded)
Sep  1 11:06:45.421: INFO: 	Container config-reloader ready: true, restart count 0
Sep  1 11:06:45.421: INFO: 	Container fluentd ready: true, restart count 1
Sep  1 11:06:45.422: INFO: node-exporter-zgtk4 from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.422: INFO: 	Container node-exporter ready: true, restart count 0
Sep  1 11:06:45.423: INFO: sonobuoy from sonobuoy started at 2023-09-01 10:29:08 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.423: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  1 11:06:45.424: INFO: sonobuoy-e2e-job-2bf224d5c7cf4759 from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
Sep  1 11:06:45.424: INFO: 	Container e2e ready: true, restart count 0
Sep  1 11:06:45.425: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  1 11:06:45.425: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-v249x from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
Sep  1 11:06:45.426: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  1 11:06:45.427: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  1 11:06:45.427: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
Sep  1 11:06:45.448: INFO: cert-manager-66f9685c7f-jlr6s from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.449: INFO: 	Container cert-manager ready: true, restart count 0
Sep  1 11:06:45.449: INFO: cert-manager-cainjector-6cfc589789-78hkg from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.450: INFO: 	Container cainjector ready: true, restart count 0
Sep  1 11:06:45.450: INFO: cert-manager-webhook-59f6664d4d-cfggr from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.451: INFO: 	Container webhook ready: true, restart count 0
Sep  1 11:06:45.451: INFO: minio-6c8d455566-dvvwz from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.452: INFO: 	Container minio ready: true, restart count 0
Sep  1 11:06:45.452: INFO: velero-57c7d7c6c4-tlzqn from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.453: INFO: 	Container velero ready: true, restart count 0
Sep  1 11:06:45.453: INFO: traefik-7cf5b5b95f-97lpm from ingress-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.454: INFO: 	Container traefik ready: true, restart count 0
Sep  1 11:06:45.454: INFO: kube-green-546cd595c4-mjs5l from kube-green-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.455: INFO: 	Container kube-green ready: true, restart count 0
Sep  1 11:06:45.455: INFO: cilium-64mb8 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.456: INFO: 	Container cilium-agent ready: true, restart count 0
Sep  1 11:06:45.456: INFO: cilium-operator-858666d4b6-mqxc6 from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.457: INFO: 	Container cilium-operator ready: true, restart count 0
Sep  1 11:06:45.457: INFO: coredns-787d4945fb-hw6lt from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.458: INFO: 	Container coredns ready: true, restart count 0
Sep  1 11:06:45.458: INFO: coredns-787d4945fb-zc797 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.459: INFO: 	Container coredns ready: true, restart count 0
Sep  1 11:06:45.459: INFO: hubble-relay-7956d48fc8-dcjfd from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.460: INFO: 	Container hubble-relay ready: true, restart count 0
Sep  1 11:06:45.460: INFO: hubble-ui-b86cb9bf7-7bzl2 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (2 container statuses recorded)
Sep  1 11:06:45.461: INFO: 	Container backend ready: true, restart count 0
Sep  1 11:06:45.461: INFO: 	Container frontend ready: true, restart count 0
Sep  1 11:06:45.462: INFO: kube-proxy-vfc2w from kube-system started at 2023-09-01 10:08:32 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.462: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  1 11:06:45.463: INFO: kyverno-5bbbd994c9-kdf4j from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.463: INFO: 	Container kyverno ready: true, restart count 0
Sep  1 11:06:45.464: INFO: kyverno-background-54b49bbb5d-wbc6r from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.464: INFO: 	Container kyverno-background ready: true, restart count 0
Sep  1 11:06:45.465: INFO: kyverno-cleanup-f5b6cdd5b-jgfw6 from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.465: INFO: 	Container kyverno-cleanup ready: true, restart count 0
Sep  1 11:06:45.466: INFO: kyverno-reports-b48cb544f-2sb6h from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.466: INFO: 	Container kyverno-reports ready: true, restart count 0
Sep  1 11:06:45.467: INFO: local-path-provisioner-5d8858f87d-gtwb4 from local-path-storage started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.467: INFO: 	Container local-path-provisioner ready: true, restart count 0
Sep  1 11:06:45.468: INFO: fluentbit-fluentbit-7986p from logging-system started at 2023-09-01 10:23:38 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.469: INFO: 	Container fluent-bit ready: true, restart count 0
Sep  1 11:06:45.469: INFO: logging-operator-5df74f78f5-zzgl8 from logging-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.470: INFO: 	Container logging-operator ready: true, restart count 0
Sep  1 11:06:45.470: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-09-01 10:49:23 +0000 UTC (2 container statuses recorded)
Sep  1 11:06:45.471: INFO: 	Container alertmanager ready: true, restart count 0
Sep  1 11:06:45.471: INFO: 	Container config-reloader ready: true, restart count 0
Sep  1 11:06:45.472: INFO: kube-state-metrics-8447695667-frzrq from monitoring-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.472: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  1 11:06:45.473: INFO: node-exporter-vwctl from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.473: INFO: 	Container node-exporter ready: true, restart count 0
Sep  1 11:06:45.474: INFO: prometheus-operator-75f79b8c5d-pjwn9 from monitoring-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.474: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  1 11:06:45.475: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-09-01 10:49:22 +0000 UTC (2 container statuses recorded)
Sep  1 11:06:45.475: INFO: 	Container config-reloader ready: true, restart count 0
Sep  1 11:06:45.476: INFO: 	Container prometheus ready: true, restart count 0
Sep  1 11:06:45.476: INFO: rbac-manager-58f6dc584b-xgdd9 from rbac-manager-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
Sep  1 11:06:45.477: INFO: 	Container rbac-manager ready: true, restart count 0
Sep  1 11:06:45.477: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-mqksc from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
Sep  1 11:06:45.478: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  1 11:06:45.478: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/01/23 11:06:45.48
Sep  1 11:06:45.489: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1237" to be "running"
Sep  1 11:06:45.494: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.103021ms
Sep  1 11:06:47.499: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009754761s
Sep  1 11:06:47.499: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/01/23 11:06:47.501
STEP: Trying to apply a random label on the found node. 09/01/23 11:06:47.512
STEP: verifying the node has the label kubernetes.io/e2e-a00b1fe9-7f08-4d2f-8dda-1874e6219ade 42 09/01/23 11:06:47.526
STEP: Trying to relaunch the pod, now with labels. 09/01/23 11:06:47.534
Sep  1 11:06:47.549: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1237" to be "not pending"
Sep  1 11:06:47.575: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 21.943596ms
Sep  1 11:06:49.607: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.053811265s
Sep  1 11:06:49.607: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-a00b1fe9-7f08-4d2f-8dda-1874e6219ade off the node k8s-worker-1.c.operations-lab.internal 09/01/23 11:06:49.61
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a00b1fe9-7f08-4d2f-8dda-1874e6219ade 09/01/23 11:06:49.629
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:06:49.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1237" for this suite. 09/01/23 11:06:49.647
------------------------------
• [4.331 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:06:45.327
    Sep  1 11:06:45.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename sched-pred 09/01/23 11:06:45.34
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:45.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:45.383
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  1 11:06:45.388: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  1 11:06:45.398: INFO: Waiting for terminating namespaces to be deleted...
    Sep  1 11:06:45.403: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
    Sep  1 11:06:45.416: INFO: cilium-operator-858666d4b6-t7brd from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.417: INFO: 	Container cilium-operator ready: true, restart count 0
    Sep  1 11:06:45.417: INFO: cilium-q8wf6 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.418: INFO: 	Container cilium-agent ready: true, restart count 0
    Sep  1 11:06:45.418: INFO: kube-proxy-wdplh from kube-system started at 2023-09-01 10:08:39 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.419: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  1 11:06:45.419: INFO: fluentbit-fluentbit-z77m2 from logging-system started at 2023-09-01 10:49:46 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.420: INFO: 	Container fluent-bit ready: true, restart count 0
    Sep  1 11:06:45.420: INFO: logging-fluentd-0 from logging-system started at 2023-09-01 10:49:45 +0000 UTC (2 container statuses recorded)
    Sep  1 11:06:45.421: INFO: 	Container config-reloader ready: true, restart count 0
    Sep  1 11:06:45.421: INFO: 	Container fluentd ready: true, restart count 1
    Sep  1 11:06:45.422: INFO: node-exporter-zgtk4 from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.422: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  1 11:06:45.423: INFO: sonobuoy from sonobuoy started at 2023-09-01 10:29:08 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.423: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  1 11:06:45.424: INFO: sonobuoy-e2e-job-2bf224d5c7cf4759 from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
    Sep  1 11:06:45.424: INFO: 	Container e2e ready: true, restart count 0
    Sep  1 11:06:45.425: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  1 11:06:45.425: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-v249x from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
    Sep  1 11:06:45.426: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  1 11:06:45.427: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  1 11:06:45.427: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
    Sep  1 11:06:45.448: INFO: cert-manager-66f9685c7f-jlr6s from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.449: INFO: 	Container cert-manager ready: true, restart count 0
    Sep  1 11:06:45.449: INFO: cert-manager-cainjector-6cfc589789-78hkg from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.450: INFO: 	Container cainjector ready: true, restart count 0
    Sep  1 11:06:45.450: INFO: cert-manager-webhook-59f6664d4d-cfggr from cert-manager-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.451: INFO: 	Container webhook ready: true, restart count 0
    Sep  1 11:06:45.451: INFO: minio-6c8d455566-dvvwz from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.452: INFO: 	Container minio ready: true, restart count 0
    Sep  1 11:06:45.452: INFO: velero-57c7d7c6c4-tlzqn from dr-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.453: INFO: 	Container velero ready: true, restart count 0
    Sep  1 11:06:45.453: INFO: traefik-7cf5b5b95f-97lpm from ingress-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.454: INFO: 	Container traefik ready: true, restart count 0
    Sep  1 11:06:45.454: INFO: kube-green-546cd595c4-mjs5l from kube-green-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.455: INFO: 	Container kube-green ready: true, restart count 0
    Sep  1 11:06:45.455: INFO: cilium-64mb8 from kube-system started at 2023-09-01 10:20:51 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.456: INFO: 	Container cilium-agent ready: true, restart count 0
    Sep  1 11:06:45.456: INFO: cilium-operator-858666d4b6-mqxc6 from kube-system started at 2023-09-01 10:20:53 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.457: INFO: 	Container cilium-operator ready: true, restart count 0
    Sep  1 11:06:45.457: INFO: coredns-787d4945fb-hw6lt from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.458: INFO: 	Container coredns ready: true, restart count 0
    Sep  1 11:06:45.458: INFO: coredns-787d4945fb-zc797 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.459: INFO: 	Container coredns ready: true, restart count 0
    Sep  1 11:06:45.459: INFO: hubble-relay-7956d48fc8-dcjfd from kube-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.460: INFO: 	Container hubble-relay ready: true, restart count 0
    Sep  1 11:06:45.460: INFO: hubble-ui-b86cb9bf7-7bzl2 from kube-system started at 2023-09-01 10:21:21 +0000 UTC (2 container statuses recorded)
    Sep  1 11:06:45.461: INFO: 	Container backend ready: true, restart count 0
    Sep  1 11:06:45.461: INFO: 	Container frontend ready: true, restart count 0
    Sep  1 11:06:45.462: INFO: kube-proxy-vfc2w from kube-system started at 2023-09-01 10:08:32 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.462: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  1 11:06:45.463: INFO: kyverno-5bbbd994c9-kdf4j from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.463: INFO: 	Container kyverno ready: true, restart count 0
    Sep  1 11:06:45.464: INFO: kyverno-background-54b49bbb5d-wbc6r from kyverno-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.464: INFO: 	Container kyverno-background ready: true, restart count 0
    Sep  1 11:06:45.465: INFO: kyverno-cleanup-f5b6cdd5b-jgfw6 from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.465: INFO: 	Container kyverno-cleanup ready: true, restart count 0
    Sep  1 11:06:45.466: INFO: kyverno-reports-b48cb544f-2sb6h from kyverno-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.466: INFO: 	Container kyverno-reports ready: true, restart count 0
    Sep  1 11:06:45.467: INFO: local-path-provisioner-5d8858f87d-gtwb4 from local-path-storage started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.467: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Sep  1 11:06:45.468: INFO: fluentbit-fluentbit-7986p from logging-system started at 2023-09-01 10:23:38 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.469: INFO: 	Container fluent-bit ready: true, restart count 0
    Sep  1 11:06:45.469: INFO: logging-operator-5df74f78f5-zzgl8 from logging-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.470: INFO: 	Container logging-operator ready: true, restart count 0
    Sep  1 11:06:45.470: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-09-01 10:49:23 +0000 UTC (2 container statuses recorded)
    Sep  1 11:06:45.471: INFO: 	Container alertmanager ready: true, restart count 0
    Sep  1 11:06:45.471: INFO: 	Container config-reloader ready: true, restart count 0
    Sep  1 11:06:45.472: INFO: kube-state-metrics-8447695667-frzrq from monitoring-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.472: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Sep  1 11:06:45.473: INFO: node-exporter-vwctl from monitoring-system started at 2023-09-01 10:20:52 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.473: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  1 11:06:45.474: INFO: prometheus-operator-75f79b8c5d-pjwn9 from monitoring-system started at 2023-09-01 10:21:21 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.474: INFO: 	Container prometheus-operator ready: true, restart count 0
    Sep  1 11:06:45.475: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-09-01 10:49:22 +0000 UTC (2 container statuses recorded)
    Sep  1 11:06:45.475: INFO: 	Container config-reloader ready: true, restart count 0
    Sep  1 11:06:45.476: INFO: 	Container prometheus ready: true, restart count 0
    Sep  1 11:06:45.476: INFO: rbac-manager-58f6dc584b-xgdd9 from rbac-manager-system started at 2023-09-01 10:21:22 +0000 UTC (1 container statuses recorded)
    Sep  1 11:06:45.477: INFO: 	Container rbac-manager ready: true, restart count 0
    Sep  1 11:06:45.477: INFO: sonobuoy-systemd-logs-daemon-set-ab917089f3cb4875-mqksc from sonobuoy started at 2023-09-01 10:29:13 +0000 UTC (2 container statuses recorded)
    Sep  1 11:06:45.478: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  1 11:06:45.478: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/01/23 11:06:45.48
    Sep  1 11:06:45.489: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1237" to be "running"
    Sep  1 11:06:45.494: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.103021ms
    Sep  1 11:06:47.499: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009754761s
    Sep  1 11:06:47.499: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/01/23 11:06:47.501
    STEP: Trying to apply a random label on the found node. 09/01/23 11:06:47.512
    STEP: verifying the node has the label kubernetes.io/e2e-a00b1fe9-7f08-4d2f-8dda-1874e6219ade 42 09/01/23 11:06:47.526
    STEP: Trying to relaunch the pod, now with labels. 09/01/23 11:06:47.534
    Sep  1 11:06:47.549: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1237" to be "not pending"
    Sep  1 11:06:47.575: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 21.943596ms
    Sep  1 11:06:49.607: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.053811265s
    Sep  1 11:06:49.607: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-a00b1fe9-7f08-4d2f-8dda-1874e6219ade off the node k8s-worker-1.c.operations-lab.internal 09/01/23 11:06:49.61
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-a00b1fe9-7f08-4d2f-8dda-1874e6219ade 09/01/23 11:06:49.629
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:06:49.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1237" for this suite. 09/01/23 11:06:49.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:06:49.667
Sep  1 11:06:49.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename crd-webhook 09/01/23 11:06:49.67
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:49.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:49.703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/01/23 11:06:49.709
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/01/23 11:06:50.177
STEP: Deploying the custom resource conversion webhook pod 09/01/23 11:06:50.185
STEP: Wait for the deployment to be ready 09/01/23 11:06:50.216
Sep  1 11:06:50.232: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/01/23 11:06:52.241
STEP: Verifying the service has paired with the endpoint 09/01/23 11:06:52.255
Sep  1 11:06:53.256: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Sep  1 11:06:53.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Creating a v1 custom resource 09/01/23 11:06:56.006
STEP: v2 custom resource should be converted 09/01/23 11:06:56.016
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:06:56.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-14" for this suite. 09/01/23 11:06:56.698
------------------------------
• [SLOW TEST] [7.057 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:06:49.667
    Sep  1 11:06:49.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename crd-webhook 09/01/23 11:06:49.67
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:49.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:49.703
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/01/23 11:06:49.709
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/01/23 11:06:50.177
    STEP: Deploying the custom resource conversion webhook pod 09/01/23 11:06:50.185
    STEP: Wait for the deployment to be ready 09/01/23 11:06:50.216
    Sep  1 11:06:50.232: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/01/23 11:06:52.241
    STEP: Verifying the service has paired with the endpoint 09/01/23 11:06:52.255
    Sep  1 11:06:53.256: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Sep  1 11:06:53.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Creating a v1 custom resource 09/01/23 11:06:56.006
    STEP: v2 custom resource should be converted 09/01/23 11:06:56.016
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:06:56.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-14" for this suite. 09/01/23 11:06:56.698
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:06:56.727
Sep  1 11:06:56.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 11:06:56.729
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:56.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:56.851
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 09/01/23 11:06:56.863
Sep  1 11:06:56.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 create -f -'
Sep  1 11:07:01.044: INFO: stderr: ""
Sep  1 11:07:01.044: INFO: stdout: "pod/pause created\n"
Sep  1 11:07:01.044: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  1 11:07:01.045: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6446" to be "running and ready"
Sep  1 11:07:01.063: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.53388ms
Sep  1 11:07:01.063: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
Sep  1 11:07:03.067: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.022279591s
Sep  1 11:07:03.067: INFO: Pod "pause" satisfied condition "running and ready"
Sep  1 11:07:03.067: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 09/01/23 11:07:03.067
Sep  1 11:07:03.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 label pods pause testing-label=testing-label-value'
Sep  1 11:07:03.187: INFO: stderr: ""
Sep  1 11:07:03.187: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 09/01/23 11:07:03.187
Sep  1 11:07:03.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 get pod pause -L testing-label'
Sep  1 11:07:03.332: INFO: stderr: ""
Sep  1 11:07:03.332: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 09/01/23 11:07:03.332
Sep  1 11:07:03.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 label pods pause testing-label-'
Sep  1 11:07:03.476: INFO: stderr: ""
Sep  1 11:07:03.476: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 09/01/23 11:07:03.476
Sep  1 11:07:03.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 get pod pause -L testing-label'
Sep  1 11:07:03.592: INFO: stderr: ""
Sep  1 11:07:03.592: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 09/01/23 11:07:03.592
Sep  1 11:07:03.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 delete --grace-period=0 --force -f -'
Sep  1 11:07:03.705: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  1 11:07:03.705: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  1 11:07:03.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 get rc,svc -l name=pause --no-headers'
Sep  1 11:07:03.850: INFO: stderr: "No resources found in kubectl-6446 namespace.\n"
Sep  1 11:07:03.850: INFO: stdout: ""
Sep  1 11:07:03.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  1 11:07:04.026: INFO: stderr: ""
Sep  1 11:07:04.026: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 11:07:04.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6446" for this suite. 09/01/23 11:07:04.03
------------------------------
• [SLOW TEST] [7.312 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:06:56.727
    Sep  1 11:06:56.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 11:06:56.729
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:06:56.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:06:56.851
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 09/01/23 11:06:56.863
    Sep  1 11:06:56.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 create -f -'
    Sep  1 11:07:01.044: INFO: stderr: ""
    Sep  1 11:07:01.044: INFO: stdout: "pod/pause created\n"
    Sep  1 11:07:01.044: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Sep  1 11:07:01.045: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6446" to be "running and ready"
    Sep  1 11:07:01.063: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.53388ms
    Sep  1 11:07:01.063: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
    Sep  1 11:07:03.067: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.022279591s
    Sep  1 11:07:03.067: INFO: Pod "pause" satisfied condition "running and ready"
    Sep  1 11:07:03.067: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 09/01/23 11:07:03.067
    Sep  1 11:07:03.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 label pods pause testing-label=testing-label-value'
    Sep  1 11:07:03.187: INFO: stderr: ""
    Sep  1 11:07:03.187: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 09/01/23 11:07:03.187
    Sep  1 11:07:03.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 get pod pause -L testing-label'
    Sep  1 11:07:03.332: INFO: stderr: ""
    Sep  1 11:07:03.332: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 09/01/23 11:07:03.332
    Sep  1 11:07:03.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 label pods pause testing-label-'
    Sep  1 11:07:03.476: INFO: stderr: ""
    Sep  1 11:07:03.476: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 09/01/23 11:07:03.476
    Sep  1 11:07:03.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 get pod pause -L testing-label'
    Sep  1 11:07:03.592: INFO: stderr: ""
    Sep  1 11:07:03.592: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 09/01/23 11:07:03.592
    Sep  1 11:07:03.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 delete --grace-period=0 --force -f -'
    Sep  1 11:07:03.705: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  1 11:07:03.705: INFO: stdout: "pod \"pause\" force deleted\n"
    Sep  1 11:07:03.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 get rc,svc -l name=pause --no-headers'
    Sep  1 11:07:03.850: INFO: stderr: "No resources found in kubectl-6446 namespace.\n"
    Sep  1 11:07:03.850: INFO: stdout: ""
    Sep  1 11:07:03.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6446 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  1 11:07:04.026: INFO: stderr: ""
    Sep  1 11:07:04.026: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:07:04.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6446" for this suite. 09/01/23 11:07:04.03
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:07:04.039
Sep  1 11:07:04.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:07:04.041
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:07:04.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:07:04.061
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-26bc8f95-bf6b-4cdc-8a18-20a5b65d2c13 09/01/23 11:07:04.065
STEP: Creating a pod to test consume secrets 09/01/23 11:07:04.073
Sep  1 11:07:04.084: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c" in namespace "projected-8949" to be "Succeeded or Failed"
Sep  1 11:07:04.089: INFO: Pod "pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.377186ms
Sep  1 11:07:06.094: INFO: Pod "pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010257478s
Sep  1 11:07:08.093: INFO: Pod "pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009213188s
STEP: Saw pod success 09/01/23 11:07:08.093
Sep  1 11:07:08.094: INFO: Pod "pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c" satisfied condition "Succeeded or Failed"
Sep  1 11:07:08.097: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c container secret-volume-test: <nil>
STEP: delete the pod 09/01/23 11:07:08.118
Sep  1 11:07:08.132: INFO: Waiting for pod pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c to disappear
Sep  1 11:07:08.135: INFO: Pod pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  1 11:07:08.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8949" for this suite. 09/01/23 11:07:08.14
------------------------------
• [4.109 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:07:04.039
    Sep  1 11:07:04.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:07:04.041
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:07:04.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:07:04.061
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-26bc8f95-bf6b-4cdc-8a18-20a5b65d2c13 09/01/23 11:07:04.065
    STEP: Creating a pod to test consume secrets 09/01/23 11:07:04.073
    Sep  1 11:07:04.084: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c" in namespace "projected-8949" to be "Succeeded or Failed"
    Sep  1 11:07:04.089: INFO: Pod "pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.377186ms
    Sep  1 11:07:06.094: INFO: Pod "pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010257478s
    Sep  1 11:07:08.093: INFO: Pod "pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009213188s
    STEP: Saw pod success 09/01/23 11:07:08.093
    Sep  1 11:07:08.094: INFO: Pod "pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c" satisfied condition "Succeeded or Failed"
    Sep  1 11:07:08.097: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c container secret-volume-test: <nil>
    STEP: delete the pod 09/01/23 11:07:08.118
    Sep  1 11:07:08.132: INFO: Waiting for pod pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c to disappear
    Sep  1 11:07:08.135: INFO: Pod pod-projected-secrets-fff069fd-b54e-4001-85c0-b92132d9a10c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:07:08.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8949" for this suite. 09/01/23 11:07:08.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:07:08.154
Sep  1 11:07:08.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 11:07:08.156
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:07:08.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:07:08.176
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 09/01/23 11:07:08.181
Sep  1 11:07:08.201: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38" in namespace "downward-api-115" to be "Succeeded or Failed"
Sep  1 11:07:08.204: INFO: Pod "downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38": Phase="Pending", Reason="", readiness=false. Elapsed: 3.738244ms
Sep  1 11:07:10.208: INFO: Pod "downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007795191s
Sep  1 11:07:12.209: INFO: Pod "downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008238482s
STEP: Saw pod success 09/01/23 11:07:12.209
Sep  1 11:07:12.209: INFO: Pod "downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38" satisfied condition "Succeeded or Failed"
Sep  1 11:07:12.214: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38 container client-container: <nil>
STEP: delete the pod 09/01/23 11:07:12.221
Sep  1 11:07:12.256: INFO: Waiting for pod downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38 to disappear
Sep  1 11:07:12.262: INFO: Pod downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  1 11:07:12.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-115" for this suite. 09/01/23 11:07:12.277
------------------------------
• [4.134 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:07:08.154
    Sep  1 11:07:08.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 11:07:08.156
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:07:08.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:07:08.176
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 09/01/23 11:07:08.181
    Sep  1 11:07:08.201: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38" in namespace "downward-api-115" to be "Succeeded or Failed"
    Sep  1 11:07:08.204: INFO: Pod "downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38": Phase="Pending", Reason="", readiness=false. Elapsed: 3.738244ms
    Sep  1 11:07:10.208: INFO: Pod "downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007795191s
    Sep  1 11:07:12.209: INFO: Pod "downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008238482s
    STEP: Saw pod success 09/01/23 11:07:12.209
    Sep  1 11:07:12.209: INFO: Pod "downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38" satisfied condition "Succeeded or Failed"
    Sep  1 11:07:12.214: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38 container client-container: <nil>
    STEP: delete the pod 09/01/23 11:07:12.221
    Sep  1 11:07:12.256: INFO: Waiting for pod downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38 to disappear
    Sep  1 11:07:12.262: INFO: Pod downwardapi-volume-72a07fb5-c270-48a2-b754-662caae3cb38 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:07:12.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-115" for this suite. 09/01/23 11:07:12.277
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:07:12.293
Sep  1 11:07:12.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename resourcequota 09/01/23 11:07:12.295
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:07:12.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:07:12.323
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 09/01/23 11:07:12.326
STEP: Counting existing ResourceQuota 09/01/23 11:07:17.335
STEP: Creating a ResourceQuota 09/01/23 11:07:22.339
STEP: Ensuring resource quota status is calculated 09/01/23 11:07:22.35
STEP: Creating a Secret 09/01/23 11:07:24.355
STEP: Ensuring resource quota status captures secret creation 09/01/23 11:07:24.369
STEP: Deleting a secret 09/01/23 11:07:26.375
STEP: Ensuring resource quota status released usage 09/01/23 11:07:26.381
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  1 11:07:28.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3352" for this suite. 09/01/23 11:07:28.391
------------------------------
• [SLOW TEST] [16.104 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:07:12.293
    Sep  1 11:07:12.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename resourcequota 09/01/23 11:07:12.295
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:07:12.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:07:12.323
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 09/01/23 11:07:12.326
    STEP: Counting existing ResourceQuota 09/01/23 11:07:17.335
    STEP: Creating a ResourceQuota 09/01/23 11:07:22.339
    STEP: Ensuring resource quota status is calculated 09/01/23 11:07:22.35
    STEP: Creating a Secret 09/01/23 11:07:24.355
    STEP: Ensuring resource quota status captures secret creation 09/01/23 11:07:24.369
    STEP: Deleting a secret 09/01/23 11:07:26.375
    STEP: Ensuring resource quota status released usage 09/01/23 11:07:26.381
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:07:28.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3352" for this suite. 09/01/23 11:07:28.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:07:28.4
Sep  1 11:07:28.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename var-expansion 09/01/23 11:07:28.401
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:07:28.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:07:28.425
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 09/01/23 11:07:28.43
STEP: waiting for pod running 09/01/23 11:07:28.443
Sep  1 11:07:28.443: INFO: Waiting up to 2m0s for pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f" in namespace "var-expansion-8193" to be "running"
Sep  1 11:07:28.451: INFO: Pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.947151ms
Sep  1 11:07:30.455: INFO: Pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011890728s
Sep  1 11:07:30.455: INFO: Pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f" satisfied condition "running"
STEP: creating a file in subpath 09/01/23 11:07:30.455
Sep  1 11:07:30.458: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8193 PodName:var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:07:30.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:07:30.459: INFO: ExecWithOptions: Clientset creation
Sep  1 11:07:30.459: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8193/pods/var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 09/01/23 11:07:30.566
Sep  1 11:07:30.570: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8193 PodName:var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:07:30.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:07:30.571: INFO: ExecWithOptions: Clientset creation
Sep  1 11:07:30.571: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8193/pods/var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 09/01/23 11:07:30.675
Sep  1 11:07:31.194: INFO: Successfully updated pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f"
STEP: waiting for annotated pod running 09/01/23 11:07:31.194
Sep  1 11:07:31.194: INFO: Waiting up to 2m0s for pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f" in namespace "var-expansion-8193" to be "running"
Sep  1 11:07:31.199: INFO: Pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f": Phase="Running", Reason="", readiness=true. Elapsed: 4.518065ms
Sep  1 11:07:31.199: INFO: Pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f" satisfied condition "running"
STEP: deleting the pod gracefully 09/01/23 11:07:31.199
Sep  1 11:07:31.199: INFO: Deleting pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f" in namespace "var-expansion-8193"
Sep  1 11:07:31.208: INFO: Wait up to 5m0s for pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  1 11:08:05.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8193" for this suite. 09/01/23 11:08:05.22
------------------------------
• [SLOW TEST] [36.827 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:07:28.4
    Sep  1 11:07:28.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename var-expansion 09/01/23 11:07:28.401
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:07:28.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:07:28.425
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 09/01/23 11:07:28.43
    STEP: waiting for pod running 09/01/23 11:07:28.443
    Sep  1 11:07:28.443: INFO: Waiting up to 2m0s for pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f" in namespace "var-expansion-8193" to be "running"
    Sep  1 11:07:28.451: INFO: Pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.947151ms
    Sep  1 11:07:30.455: INFO: Pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011890728s
    Sep  1 11:07:30.455: INFO: Pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f" satisfied condition "running"
    STEP: creating a file in subpath 09/01/23 11:07:30.455
    Sep  1 11:07:30.458: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8193 PodName:var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:07:30.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:07:30.459: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:07:30.459: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8193/pods/var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 09/01/23 11:07:30.566
    Sep  1 11:07:30.570: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8193 PodName:var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:07:30.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:07:30.571: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:07:30.571: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8193/pods/var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 09/01/23 11:07:30.675
    Sep  1 11:07:31.194: INFO: Successfully updated pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f"
    STEP: waiting for annotated pod running 09/01/23 11:07:31.194
    Sep  1 11:07:31.194: INFO: Waiting up to 2m0s for pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f" in namespace "var-expansion-8193" to be "running"
    Sep  1 11:07:31.199: INFO: Pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f": Phase="Running", Reason="", readiness=true. Elapsed: 4.518065ms
    Sep  1 11:07:31.199: INFO: Pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f" satisfied condition "running"
    STEP: deleting the pod gracefully 09/01/23 11:07:31.199
    Sep  1 11:07:31.199: INFO: Deleting pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f" in namespace "var-expansion-8193"
    Sep  1 11:07:31.208: INFO: Wait up to 5m0s for pod "var-expansion-b35b16d6-2f2f-4ecf-a017-3f5a993ceb1f" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:08:05.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8193" for this suite. 09/01/23 11:08:05.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:08:05.233
Sep  1 11:08:05.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 11:08:05.261
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:05.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:05.293
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-8224 09/01/23 11:08:05.298
STEP: creating service affinity-nodeport-transition in namespace services-8224 09/01/23 11:08:05.298
STEP: creating replication controller affinity-nodeport-transition in namespace services-8224 09/01/23 11:08:05.319
I0901 11:08:05.332989      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8224, replica count: 3
I0901 11:08:08.384692      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  1 11:08:08.394: INFO: Creating new exec pod
Sep  1 11:08:08.403: INFO: Waiting up to 5m0s for pod "execpod-affinitygkn65" in namespace "services-8224" to be "running"
Sep  1 11:08:08.407: INFO: Pod "execpod-affinitygkn65": Phase="Pending", Reason="", readiness=false. Elapsed: 3.599462ms
Sep  1 11:08:10.413: INFO: Pod "execpod-affinitygkn65": Phase="Running", Reason="", readiness=true. Elapsed: 2.009227433s
Sep  1 11:08:10.413: INFO: Pod "execpod-affinitygkn65" satisfied condition "running"
Sep  1 11:08:11.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-8224 exec execpod-affinitygkn65 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Sep  1 11:08:11.628: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep  1 11:08:11.628: INFO: stdout: ""
Sep  1 11:08:11.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-8224 exec execpod-affinitygkn65 -- /bin/sh -x -c nc -v -z -w 2 10.111.130.44 80'
Sep  1 11:08:11.835: INFO: stderr: "+ nc -v -z -w 2 10.111.130.44 80\nConnection to 10.111.130.44 80 port [tcp/http] succeeded!\n"
Sep  1 11:08:11.835: INFO: stdout: ""
Sep  1 11:08:11.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-8224 exec execpod-affinitygkn65 -- /bin/sh -x -c nc -v -z -w 2 172.16.0.3 31623'
Sep  1 11:08:12.026: INFO: stderr: "+ nc -v -z -w 2 172.16.0.3 31623\nConnection to 172.16.0.3 31623 port [tcp/*] succeeded!\n"
Sep  1 11:08:12.026: INFO: stdout: ""
Sep  1 11:08:12.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-8224 exec execpod-affinitygkn65 -- /bin/sh -x -c nc -v -z -w 2 172.16.0.4 31623'
Sep  1 11:08:12.226: INFO: stderr: "+ nc -v -z -w 2 172.16.0.4 31623\nConnection to 172.16.0.4 31623 port [tcp/*] succeeded!\n"
Sep  1 11:08:12.226: INFO: stdout: ""
Sep  1 11:08:12.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-8224 exec execpod-affinitygkn65 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.3:31623/ ; done'
Sep  1 11:08:12.674: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n"
Sep  1 11:08:12.674: INFO: stdout: "\naffinity-nodeport-transition-5nmmm\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-5nmmm\naffinity-nodeport-transition-5nmmm\naffinity-nodeport-transition-q9974\naffinity-nodeport-transition-q9974\naffinity-nodeport-transition-5nmmm\naffinity-nodeport-transition-q9974\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-q9974\naffinity-nodeport-transition-5nmmm\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-5nmmm\naffinity-nodeport-transition-q9974\naffinity-nodeport-transition-q9974"
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-5nmmm
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-5nmmm
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-5nmmm
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-q9974
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-q9974
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-5nmmm
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-q9974
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-q9974
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-5nmmm
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-5nmmm
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-q9974
Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-q9974
Sep  1 11:08:12.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-8224 exec execpod-affinitygkn65 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.3:31623/ ; done'
Sep  1 11:08:13.091: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n"
Sep  1 11:08:13.091: INFO: stdout: "\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb"
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
Sep  1 11:08:13.091: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8224, will wait for the garbage collector to delete the pods 09/01/23 11:08:13.104
Sep  1 11:08:13.169: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.235781ms
Sep  1 11:08:13.270: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.47848ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 11:08:15.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8224" for this suite. 09/01/23 11:08:15.413
------------------------------
• [SLOW TEST] [10.192 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:08:05.233
    Sep  1 11:08:05.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 11:08:05.261
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:05.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:05.293
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-8224 09/01/23 11:08:05.298
    STEP: creating service affinity-nodeport-transition in namespace services-8224 09/01/23 11:08:05.298
    STEP: creating replication controller affinity-nodeport-transition in namespace services-8224 09/01/23 11:08:05.319
    I0901 11:08:05.332989      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8224, replica count: 3
    I0901 11:08:08.384692      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  1 11:08:08.394: INFO: Creating new exec pod
    Sep  1 11:08:08.403: INFO: Waiting up to 5m0s for pod "execpod-affinitygkn65" in namespace "services-8224" to be "running"
    Sep  1 11:08:08.407: INFO: Pod "execpod-affinitygkn65": Phase="Pending", Reason="", readiness=false. Elapsed: 3.599462ms
    Sep  1 11:08:10.413: INFO: Pod "execpod-affinitygkn65": Phase="Running", Reason="", readiness=true. Elapsed: 2.009227433s
    Sep  1 11:08:10.413: INFO: Pod "execpod-affinitygkn65" satisfied condition "running"
    Sep  1 11:08:11.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-8224 exec execpod-affinitygkn65 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Sep  1 11:08:11.628: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Sep  1 11:08:11.628: INFO: stdout: ""
    Sep  1 11:08:11.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-8224 exec execpod-affinitygkn65 -- /bin/sh -x -c nc -v -z -w 2 10.111.130.44 80'
    Sep  1 11:08:11.835: INFO: stderr: "+ nc -v -z -w 2 10.111.130.44 80\nConnection to 10.111.130.44 80 port [tcp/http] succeeded!\n"
    Sep  1 11:08:11.835: INFO: stdout: ""
    Sep  1 11:08:11.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-8224 exec execpod-affinitygkn65 -- /bin/sh -x -c nc -v -z -w 2 172.16.0.3 31623'
    Sep  1 11:08:12.026: INFO: stderr: "+ nc -v -z -w 2 172.16.0.3 31623\nConnection to 172.16.0.3 31623 port [tcp/*] succeeded!\n"
    Sep  1 11:08:12.026: INFO: stdout: ""
    Sep  1 11:08:12.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-8224 exec execpod-affinitygkn65 -- /bin/sh -x -c nc -v -z -w 2 172.16.0.4 31623'
    Sep  1 11:08:12.226: INFO: stderr: "+ nc -v -z -w 2 172.16.0.4 31623\nConnection to 172.16.0.4 31623 port [tcp/*] succeeded!\n"
    Sep  1 11:08:12.226: INFO: stdout: ""
    Sep  1 11:08:12.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-8224 exec execpod-affinitygkn65 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.3:31623/ ; done'
    Sep  1 11:08:12.674: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n"
    Sep  1 11:08:12.674: INFO: stdout: "\naffinity-nodeport-transition-5nmmm\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-5nmmm\naffinity-nodeport-transition-5nmmm\naffinity-nodeport-transition-q9974\naffinity-nodeport-transition-q9974\naffinity-nodeport-transition-5nmmm\naffinity-nodeport-transition-q9974\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-q9974\naffinity-nodeport-transition-5nmmm\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-5nmmm\naffinity-nodeport-transition-q9974\naffinity-nodeport-transition-q9974"
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-5nmmm
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-5nmmm
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-5nmmm
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-q9974
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-q9974
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-5nmmm
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-q9974
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-q9974
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-5nmmm
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-5nmmm
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-q9974
    Sep  1 11:08:12.674: INFO: Received response from host: affinity-nodeport-transition-q9974
    Sep  1 11:08:12.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-8224 exec execpod-affinitygkn65 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.3:31623/ ; done'
    Sep  1 11:08:13.091: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:31623/\n"
    Sep  1 11:08:13.091: INFO: stdout: "\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb\naffinity-nodeport-transition-57pzb"
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Received response from host: affinity-nodeport-transition-57pzb
    Sep  1 11:08:13.091: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8224, will wait for the garbage collector to delete the pods 09/01/23 11:08:13.104
    Sep  1 11:08:13.169: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.235781ms
    Sep  1 11:08:13.270: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.47848ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:08:15.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8224" for this suite. 09/01/23 11:08:15.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:08:15.428
Sep  1 11:08:15.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:08:15.432
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:15.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:15.454
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-454cd83d-100d-4cf0-a170-9cc5c99c1af7 09/01/23 11:08:15.465
STEP: Creating secret with name s-test-opt-upd-e503ca34-27c2-4866-972c-5ba0c47e7d8b 09/01/23 11:08:15.471
STEP: Creating the pod 09/01/23 11:08:15.48
Sep  1 11:08:15.492: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-747e41c9-54e9-4bc7-995c-a0b086866014" in namespace "projected-5308" to be "running and ready"
Sep  1 11:08:15.497: INFO: Pod "pod-projected-secrets-747e41c9-54e9-4bc7-995c-a0b086866014": Phase="Pending", Reason="", readiness=false. Elapsed: 4.794984ms
Sep  1 11:08:15.497: INFO: The phase of Pod pod-projected-secrets-747e41c9-54e9-4bc7-995c-a0b086866014 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:08:17.501: INFO: Pod "pod-projected-secrets-747e41c9-54e9-4bc7-995c-a0b086866014": Phase="Running", Reason="", readiness=true. Elapsed: 2.008878834s
Sep  1 11:08:17.501: INFO: The phase of Pod pod-projected-secrets-747e41c9-54e9-4bc7-995c-a0b086866014 is Running (Ready = true)
Sep  1 11:08:17.501: INFO: Pod "pod-projected-secrets-747e41c9-54e9-4bc7-995c-a0b086866014" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-454cd83d-100d-4cf0-a170-9cc5c99c1af7 09/01/23 11:08:17.521
STEP: Updating secret s-test-opt-upd-e503ca34-27c2-4866-972c-5ba0c47e7d8b 09/01/23 11:08:17.526
STEP: Creating secret with name s-test-opt-create-6d9d61b2-07b6-49f9-b130-886254c84769 09/01/23 11:08:17.532
STEP: waiting to observe update in volume 09/01/23 11:08:17.537
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  1 11:08:19.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5308" for this suite. 09/01/23 11:08:19.569
------------------------------
• [4.149 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:08:15.428
    Sep  1 11:08:15.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:08:15.432
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:15.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:15.454
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-454cd83d-100d-4cf0-a170-9cc5c99c1af7 09/01/23 11:08:15.465
    STEP: Creating secret with name s-test-opt-upd-e503ca34-27c2-4866-972c-5ba0c47e7d8b 09/01/23 11:08:15.471
    STEP: Creating the pod 09/01/23 11:08:15.48
    Sep  1 11:08:15.492: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-747e41c9-54e9-4bc7-995c-a0b086866014" in namespace "projected-5308" to be "running and ready"
    Sep  1 11:08:15.497: INFO: Pod "pod-projected-secrets-747e41c9-54e9-4bc7-995c-a0b086866014": Phase="Pending", Reason="", readiness=false. Elapsed: 4.794984ms
    Sep  1 11:08:15.497: INFO: The phase of Pod pod-projected-secrets-747e41c9-54e9-4bc7-995c-a0b086866014 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:08:17.501: INFO: Pod "pod-projected-secrets-747e41c9-54e9-4bc7-995c-a0b086866014": Phase="Running", Reason="", readiness=true. Elapsed: 2.008878834s
    Sep  1 11:08:17.501: INFO: The phase of Pod pod-projected-secrets-747e41c9-54e9-4bc7-995c-a0b086866014 is Running (Ready = true)
    Sep  1 11:08:17.501: INFO: Pod "pod-projected-secrets-747e41c9-54e9-4bc7-995c-a0b086866014" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-454cd83d-100d-4cf0-a170-9cc5c99c1af7 09/01/23 11:08:17.521
    STEP: Updating secret s-test-opt-upd-e503ca34-27c2-4866-972c-5ba0c47e7d8b 09/01/23 11:08:17.526
    STEP: Creating secret with name s-test-opt-create-6d9d61b2-07b6-49f9-b130-886254c84769 09/01/23 11:08:17.532
    STEP: waiting to observe update in volume 09/01/23 11:08:17.537
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:08:19.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5308" for this suite. 09/01/23 11:08:19.569
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:08:19.577
Sep  1 11:08:19.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:08:19.581
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:19.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:19.621
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 09/01/23 11:08:19.627
Sep  1 11:08:19.645: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8" in namespace "projected-7827" to be "Succeeded or Failed"
Sep  1 11:08:19.653: INFO: Pod "downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.537267ms
Sep  1 11:08:21.658: INFO: Pod "downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012984968s
Sep  1 11:08:23.657: INFO: Pod "downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01135841s
STEP: Saw pod success 09/01/23 11:08:23.657
Sep  1 11:08:23.657: INFO: Pod "downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8" satisfied condition "Succeeded or Failed"
Sep  1 11:08:23.661: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8 container client-container: <nil>
STEP: delete the pod 09/01/23 11:08:23.669
Sep  1 11:08:23.686: INFO: Waiting for pod downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8 to disappear
Sep  1 11:08:23.690: INFO: Pod downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  1 11:08:23.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7827" for this suite. 09/01/23 11:08:23.695
------------------------------
• [4.131 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:08:19.577
    Sep  1 11:08:19.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:08:19.581
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:19.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:19.621
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 09/01/23 11:08:19.627
    Sep  1 11:08:19.645: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8" in namespace "projected-7827" to be "Succeeded or Failed"
    Sep  1 11:08:19.653: INFO: Pod "downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.537267ms
    Sep  1 11:08:21.658: INFO: Pod "downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012984968s
    Sep  1 11:08:23.657: INFO: Pod "downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01135841s
    STEP: Saw pod success 09/01/23 11:08:23.657
    Sep  1 11:08:23.657: INFO: Pod "downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8" satisfied condition "Succeeded or Failed"
    Sep  1 11:08:23.661: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8 container client-container: <nil>
    STEP: delete the pod 09/01/23 11:08:23.669
    Sep  1 11:08:23.686: INFO: Waiting for pod downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8 to disappear
    Sep  1 11:08:23.690: INFO: Pod downwardapi-volume-e69833d7-7c11-4720-959b-506597e7ebf8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:08:23.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7827" for this suite. 09/01/23 11:08:23.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:08:23.717
Sep  1 11:08:23.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 11:08:23.719
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:23.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:23.746
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 09/01/23 11:08:23.75
Sep  1 11:08:23.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: rename a version 09/01/23 11:08:34.225
STEP: check the new version name is served 09/01/23 11:08:34.247
STEP: check the old version name is removed 09/01/23 11:08:38.534
STEP: check the other version is not changed 09/01/23 11:08:39.736
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:08:47.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3179" for this suite. 09/01/23 11:08:47.529
------------------------------
• [SLOW TEST] [23.819 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:08:23.717
    Sep  1 11:08:23.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 11:08:23.719
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:23.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:23.746
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 09/01/23 11:08:23.75
    Sep  1 11:08:23.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: rename a version 09/01/23 11:08:34.225
    STEP: check the new version name is served 09/01/23 11:08:34.247
    STEP: check the old version name is removed 09/01/23 11:08:38.534
    STEP: check the other version is not changed 09/01/23 11:08:39.736
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:08:47.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3179" for this suite. 09/01/23 11:08:47.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:08:47.537
Sep  1 11:08:47.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:08:47.538
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:47.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:47.564
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 09/01/23 11:08:47.569
Sep  1 11:08:47.580: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1" in namespace "projected-3673" to be "Succeeded or Failed"
Sep  1 11:08:47.585: INFO: Pod "downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.863671ms
Sep  1 11:08:49.598: INFO: Pod "downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018337207s
Sep  1 11:08:51.589: INFO: Pod "downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008888561s
STEP: Saw pod success 09/01/23 11:08:51.589
Sep  1 11:08:51.589: INFO: Pod "downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1" satisfied condition "Succeeded or Failed"
Sep  1 11:08:51.592: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1 container client-container: <nil>
STEP: delete the pod 09/01/23 11:08:51.598
Sep  1 11:08:51.608: INFO: Waiting for pod downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1 to disappear
Sep  1 11:08:51.617: INFO: Pod downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  1 11:08:51.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3673" for this suite. 09/01/23 11:08:51.621
------------------------------
• [4.091 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:08:47.537
    Sep  1 11:08:47.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:08:47.538
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:47.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:47.564
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 09/01/23 11:08:47.569
    Sep  1 11:08:47.580: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1" in namespace "projected-3673" to be "Succeeded or Failed"
    Sep  1 11:08:47.585: INFO: Pod "downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.863671ms
    Sep  1 11:08:49.598: INFO: Pod "downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018337207s
    Sep  1 11:08:51.589: INFO: Pod "downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008888561s
    STEP: Saw pod success 09/01/23 11:08:51.589
    Sep  1 11:08:51.589: INFO: Pod "downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1" satisfied condition "Succeeded or Failed"
    Sep  1 11:08:51.592: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1 container client-container: <nil>
    STEP: delete the pod 09/01/23 11:08:51.598
    Sep  1 11:08:51.608: INFO: Waiting for pod downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1 to disappear
    Sep  1 11:08:51.617: INFO: Pod downwardapi-volume-f18895f1-2839-4ee9-b88e-32a29cb1aaf1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:08:51.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3673" for this suite. 09/01/23 11:08:51.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:08:51.633
Sep  1 11:08:51.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename deployment 09/01/23 11:08:51.635
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:51.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:51.655
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Sep  1 11:08:51.659: INFO: Creating simple deployment test-new-deployment
Sep  1 11:08:51.684: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 09/01/23 11:08:53.696
STEP: updating a scale subresource 09/01/23 11:08:53.699
STEP: verifying the deployment Spec.Replicas was modified 09/01/23 11:08:53.706
STEP: Patch a scale subresource 09/01/23 11:08:53.712
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  1 11:08:53.768: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-672  892c4606-b182-4be0-a183-41d24e4c82d2 28400 3 2023-09-01 11:08:51 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-09-01 11:08:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:08:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00ded3668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-01 11:08:53 +0000 UTC,LastTransitionTime:2023-09-01 11:08:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-09-01 11:08:53 +0000 UTC,LastTransitionTime:2023-09-01 11:08:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  1 11:08:53.774: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-672  6fbc839e-9cdf-49cc-b351-896e49cd72f5 28405 2 2023-09-01 11:08:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 892c4606-b182-4be0-a183-41d24e4c82d2 0xc00554df37 0xc00554df38}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:08:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"892c4606-b182-4be0-a183-41d24e4c82d2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:08:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00554dfc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  1 11:08:53.780: INFO: Pod "test-new-deployment-7f5969cbc7-2prx2" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-2prx2 test-new-deployment-7f5969cbc7- deployment-672  0516a9de-1a42-4cc6-8387-528f77542c47 28403 0 2023-09-01 11:08:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 6fbc839e-9cdf-49cc-b351-896e49cd72f5 0xc00ca1c3e7 0xc00ca1c3e8}] [] [{kube-controller-manager Update v1 2023-09-01 11:08:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fbc839e-9cdf-49cc-b351-896e49cd72f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6b62,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6b62,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:08:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:08:53.780: INFO: Pod "test-new-deployment-7f5969cbc7-ddhfv" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-ddhfv test-new-deployment-7f5969cbc7- deployment-672  5a785cf1-88f7-4fc8-85e5-9fc0fb186ddd 28393 0 2023-09-01 11:08:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 6fbc839e-9cdf-49cc-b351-896e49cd72f5 0xc00ca1c6c0 0xc00ca1c6c1}] [] [{kube-controller-manager Update v1 2023-09-01 11:08:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fbc839e-9cdf-49cc-b351-896e49cd72f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:08:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6k7qj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6k7qj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:08:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:08:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:08:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:08:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.63,StartTime:2023-09-01 11:08:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:08:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://31118cbbab700fd9a2d11c4abd6133df7934f00b59625cc92109b9f78ad5812e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  1 11:08:53.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-672" for this suite. 09/01/23 11:08:53.787
------------------------------
• [2.167 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:08:51.633
    Sep  1 11:08:51.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename deployment 09/01/23 11:08:51.635
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:51.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:51.655
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Sep  1 11:08:51.659: INFO: Creating simple deployment test-new-deployment
    Sep  1 11:08:51.684: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 09/01/23 11:08:53.696
    STEP: updating a scale subresource 09/01/23 11:08:53.699
    STEP: verifying the deployment Spec.Replicas was modified 09/01/23 11:08:53.706
    STEP: Patch a scale subresource 09/01/23 11:08:53.712
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  1 11:08:53.768: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-672  892c4606-b182-4be0-a183-41d24e4c82d2 28400 3 2023-09-01 11:08:51 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-09-01 11:08:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:08:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00ded3668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-01 11:08:53 +0000 UTC,LastTransitionTime:2023-09-01 11:08:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-09-01 11:08:53 +0000 UTC,LastTransitionTime:2023-09-01 11:08:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  1 11:08:53.774: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-672  6fbc839e-9cdf-49cc-b351-896e49cd72f5 28405 2 2023-09-01 11:08:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 892c4606-b182-4be0-a183-41d24e4c82d2 0xc00554df37 0xc00554df38}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:08:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"892c4606-b182-4be0-a183-41d24e4c82d2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:08:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00554dfc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  1 11:08:53.780: INFO: Pod "test-new-deployment-7f5969cbc7-2prx2" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-2prx2 test-new-deployment-7f5969cbc7- deployment-672  0516a9de-1a42-4cc6-8387-528f77542c47 28403 0 2023-09-01 11:08:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 6fbc839e-9cdf-49cc-b351-896e49cd72f5 0xc00ca1c3e7 0xc00ca1c3e8}] [] [{kube-controller-manager Update v1 2023-09-01 11:08:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fbc839e-9cdf-49cc-b351-896e49cd72f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6b62,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6b62,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:08:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:08:53.780: INFO: Pod "test-new-deployment-7f5969cbc7-ddhfv" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-ddhfv test-new-deployment-7f5969cbc7- deployment-672  5a785cf1-88f7-4fc8-85e5-9fc0fb186ddd 28393 0 2023-09-01 11:08:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 6fbc839e-9cdf-49cc-b351-896e49cd72f5 0xc00ca1c6c0 0xc00ca1c6c1}] [] [{kube-controller-manager Update v1 2023-09-01 11:08:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fbc839e-9cdf-49cc-b351-896e49cd72f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:08:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6k7qj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6k7qj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:08:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:08:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:08:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:08:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.63,StartTime:2023-09-01 11:08:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:08:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://31118cbbab700fd9a2d11c4abd6133df7934f00b59625cc92109b9f78ad5812e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:08:53.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-672" for this suite. 09/01/23 11:08:53.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:08:53.804
Sep  1 11:08:53.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename security-context 09/01/23 11:08:53.806
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:53.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:53.835
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/01/23 11:08:53.839
Sep  1 11:08:53.847: INFO: Waiting up to 5m0s for pod "security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c" in namespace "security-context-6363" to be "Succeeded or Failed"
Sep  1 11:08:53.851: INFO: Pod "security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.62312ms
Sep  1 11:08:55.855: INFO: Pod "security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008246134s
Sep  1 11:08:57.855: INFO: Pod "security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007608499s
STEP: Saw pod success 09/01/23 11:08:57.855
Sep  1 11:08:57.855: INFO: Pod "security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c" satisfied condition "Succeeded or Failed"
Sep  1 11:08:57.858: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c container test-container: <nil>
STEP: delete the pod 09/01/23 11:08:57.864
Sep  1 11:08:57.879: INFO: Waiting for pod security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c to disappear
Sep  1 11:08:57.883: INFO: Pod security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  1 11:08:57.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-6363" for this suite. 09/01/23 11:08:57.889
------------------------------
• [4.099 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:08:53.804
    Sep  1 11:08:53.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename security-context 09/01/23 11:08:53.806
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:53.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:53.835
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/01/23 11:08:53.839
    Sep  1 11:08:53.847: INFO: Waiting up to 5m0s for pod "security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c" in namespace "security-context-6363" to be "Succeeded or Failed"
    Sep  1 11:08:53.851: INFO: Pod "security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.62312ms
    Sep  1 11:08:55.855: INFO: Pod "security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008246134s
    Sep  1 11:08:57.855: INFO: Pod "security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007608499s
    STEP: Saw pod success 09/01/23 11:08:57.855
    Sep  1 11:08:57.855: INFO: Pod "security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c" satisfied condition "Succeeded or Failed"
    Sep  1 11:08:57.858: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c container test-container: <nil>
    STEP: delete the pod 09/01/23 11:08:57.864
    Sep  1 11:08:57.879: INFO: Waiting for pod security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c to disappear
    Sep  1 11:08:57.883: INFO: Pod security-context-c0558bb2-58a6-4d3a-a4a4-3c0a3934b16c no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:08:57.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-6363" for this suite. 09/01/23 11:08:57.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:08:57.91
Sep  1 11:08:57.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename statefulset 09/01/23 11:08:57.911
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:57.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:57.946
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-728 09/01/23 11:08:57.95
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-728 09/01/23 11:08:57.982
Sep  1 11:08:57.994: INFO: Found 0 stateful pods, waiting for 1
Sep  1 11:09:07.999: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 09/01/23 11:09:08.006
STEP: Getting /status 09/01/23 11:09:08.015
Sep  1 11:09:08.020: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 09/01/23 11:09:08.02
Sep  1 11:09:08.035: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 09/01/23 11:09:08.035
Sep  1 11:09:08.038: INFO: Observed &StatefulSet event: ADDED
Sep  1 11:09:08.038: INFO: Found Statefulset ss in namespace statefulset-728 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  1 11:09:08.038: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 09/01/23 11:09:08.038
Sep  1 11:09:08.039: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  1 11:09:08.057: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 09/01/23 11:09:08.057
Sep  1 11:09:08.060: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  1 11:09:08.061: INFO: Deleting all statefulset in ns statefulset-728
Sep  1 11:09:08.064: INFO: Scaling statefulset ss to 0
Sep  1 11:09:18.087: INFO: Waiting for statefulset status.replicas updated to 0
Sep  1 11:09:18.090: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  1 11:09:18.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-728" for this suite. 09/01/23 11:09:18.177
------------------------------
• [SLOW TEST] [20.293 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:08:57.91
    Sep  1 11:08:57.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename statefulset 09/01/23 11:08:57.911
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:08:57.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:08:57.946
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-728 09/01/23 11:08:57.95
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-728 09/01/23 11:08:57.982
    Sep  1 11:08:57.994: INFO: Found 0 stateful pods, waiting for 1
    Sep  1 11:09:07.999: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 09/01/23 11:09:08.006
    STEP: Getting /status 09/01/23 11:09:08.015
    Sep  1 11:09:08.020: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 09/01/23 11:09:08.02
    Sep  1 11:09:08.035: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 09/01/23 11:09:08.035
    Sep  1 11:09:08.038: INFO: Observed &StatefulSet event: ADDED
    Sep  1 11:09:08.038: INFO: Found Statefulset ss in namespace statefulset-728 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  1 11:09:08.038: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 09/01/23 11:09:08.038
    Sep  1 11:09:08.039: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  1 11:09:08.057: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 09/01/23 11:09:08.057
    Sep  1 11:09:08.060: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  1 11:09:08.061: INFO: Deleting all statefulset in ns statefulset-728
    Sep  1 11:09:08.064: INFO: Scaling statefulset ss to 0
    Sep  1 11:09:18.087: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  1 11:09:18.090: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:09:18.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-728" for this suite. 09/01/23 11:09:18.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:09:18.204
Sep  1 11:09:18.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename events 09/01/23 11:09:18.206
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:09:18.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:09:18.259
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 09/01/23 11:09:18.264
Sep  1 11:09:18.281: INFO: created test-event-1
Sep  1 11:09:18.287: INFO: created test-event-2
Sep  1 11:09:18.293: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 09/01/23 11:09:18.293
STEP: delete collection of events 09/01/23 11:09:18.297
Sep  1 11:09:18.297: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/01/23 11:09:18.322
Sep  1 11:09:18.322: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Sep  1 11:09:18.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9491" for this suite. 09/01/23 11:09:18.332
------------------------------
• [0.137 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:09:18.204
    Sep  1 11:09:18.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename events 09/01/23 11:09:18.206
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:09:18.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:09:18.259
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 09/01/23 11:09:18.264
    Sep  1 11:09:18.281: INFO: created test-event-1
    Sep  1 11:09:18.287: INFO: created test-event-2
    Sep  1 11:09:18.293: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 09/01/23 11:09:18.293
    STEP: delete collection of events 09/01/23 11:09:18.297
    Sep  1 11:09:18.297: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/01/23 11:09:18.322
    Sep  1 11:09:18.322: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:09:18.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9491" for this suite. 09/01/23 11:09:18.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:09:18.351
Sep  1 11:09:18.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir-wrapper 09/01/23 11:09:18.353
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:09:18.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:09:18.381
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Sep  1 11:09:18.429: INFO: Waiting up to 5m0s for pod "pod-secrets-8262aab9-43f5-44d5-9c88-9b138bc0fe7e" in namespace "emptydir-wrapper-2371" to be "running and ready"
Sep  1 11:09:18.452: INFO: Pod "pod-secrets-8262aab9-43f5-44d5-9c88-9b138bc0fe7e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.094342ms
Sep  1 11:09:18.452: INFO: The phase of Pod pod-secrets-8262aab9-43f5-44d5-9c88-9b138bc0fe7e is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:09:20.457: INFO: Pod "pod-secrets-8262aab9-43f5-44d5-9c88-9b138bc0fe7e": Phase="Running", Reason="", readiness=true. Elapsed: 2.027375869s
Sep  1 11:09:20.457: INFO: The phase of Pod pod-secrets-8262aab9-43f5-44d5-9c88-9b138bc0fe7e is Running (Ready = true)
Sep  1 11:09:20.457: INFO: Pod "pod-secrets-8262aab9-43f5-44d5-9c88-9b138bc0fe7e" satisfied condition "running and ready"
STEP: Cleaning up the secret 09/01/23 11:09:20.461
STEP: Cleaning up the configmap 09/01/23 11:09:20.468
STEP: Cleaning up the pod 09/01/23 11:09:20.475
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 11:09:20.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-2371" for this suite. 09/01/23 11:09:20.498
------------------------------
• [2.159 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:09:18.351
    Sep  1 11:09:18.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir-wrapper 09/01/23 11:09:18.353
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:09:18.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:09:18.381
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Sep  1 11:09:18.429: INFO: Waiting up to 5m0s for pod "pod-secrets-8262aab9-43f5-44d5-9c88-9b138bc0fe7e" in namespace "emptydir-wrapper-2371" to be "running and ready"
    Sep  1 11:09:18.452: INFO: Pod "pod-secrets-8262aab9-43f5-44d5-9c88-9b138bc0fe7e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.094342ms
    Sep  1 11:09:18.452: INFO: The phase of Pod pod-secrets-8262aab9-43f5-44d5-9c88-9b138bc0fe7e is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:09:20.457: INFO: Pod "pod-secrets-8262aab9-43f5-44d5-9c88-9b138bc0fe7e": Phase="Running", Reason="", readiness=true. Elapsed: 2.027375869s
    Sep  1 11:09:20.457: INFO: The phase of Pod pod-secrets-8262aab9-43f5-44d5-9c88-9b138bc0fe7e is Running (Ready = true)
    Sep  1 11:09:20.457: INFO: Pod "pod-secrets-8262aab9-43f5-44d5-9c88-9b138bc0fe7e" satisfied condition "running and ready"
    STEP: Cleaning up the secret 09/01/23 11:09:20.461
    STEP: Cleaning up the configmap 09/01/23 11:09:20.468
    STEP: Cleaning up the pod 09/01/23 11:09:20.475
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:09:20.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-2371" for this suite. 09/01/23 11:09:20.498
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:09:20.512
Sep  1 11:09:20.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename resourcequota 09/01/23 11:09:20.514
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:09:20.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:09:20.539
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-wsl7p" 09/01/23 11:09:20.546
Sep  1 11:09:20.563: INFO: Resource quota "e2e-rq-status-wsl7p" reports spec: hard cpu limit of 500m
Sep  1 11:09:20.563: INFO: Resource quota "e2e-rq-status-wsl7p" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-wsl7p" /status 09/01/23 11:09:20.563
STEP: Confirm /status for "e2e-rq-status-wsl7p" resourceQuota via watch 09/01/23 11:09:20.571
Sep  1 11:09:20.573: INFO: observed resourceQuota "e2e-rq-status-wsl7p" in namespace "resourcequota-8867" with hard status: v1.ResourceList(nil)
Sep  1 11:09:20.573: INFO: Found resourceQuota "e2e-rq-status-wsl7p" in namespace "resourcequota-8867" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Sep  1 11:09:20.573: INFO: ResourceQuota "e2e-rq-status-wsl7p" /status was updated
STEP: Patching hard spec values for cpu & memory 09/01/23 11:09:20.576
Sep  1 11:09:20.583: INFO: Resource quota "e2e-rq-status-wsl7p" reports spec: hard cpu limit of 1
Sep  1 11:09:20.583: INFO: Resource quota "e2e-rq-status-wsl7p" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-wsl7p" /status 09/01/23 11:09:20.583
STEP: Confirm /status for "e2e-rq-status-wsl7p" resourceQuota via watch 09/01/23 11:09:20.589
Sep  1 11:09:20.592: INFO: observed resourceQuota "e2e-rq-status-wsl7p" in namespace "resourcequota-8867" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Sep  1 11:09:20.592: INFO: Found resourceQuota "e2e-rq-status-wsl7p" in namespace "resourcequota-8867" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Sep  1 11:09:20.592: INFO: ResourceQuota "e2e-rq-status-wsl7p" /status was patched
STEP: Get "e2e-rq-status-wsl7p" /status 09/01/23 11:09:20.593
Sep  1 11:09:20.597: INFO: Resourcequota "e2e-rq-status-wsl7p" reports status: hard cpu of 1
Sep  1 11:09:20.597: INFO: Resourcequota "e2e-rq-status-wsl7p" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-wsl7p" /status before checking Spec is unchanged 09/01/23 11:09:20.603
Sep  1 11:09:20.614: INFO: Resourcequota "e2e-rq-status-wsl7p" reports status: hard cpu of 2
Sep  1 11:09:20.614: INFO: Resourcequota "e2e-rq-status-wsl7p" reports status: hard memory of 2Gi
Sep  1 11:09:20.616: INFO: observed resourceQuota "e2e-rq-status-wsl7p" in namespace "resourcequota-8867" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Sep  1 11:09:20.616: INFO: Found resourceQuota "e2e-rq-status-wsl7p" in namespace "resourcequota-8867" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Sep  1 11:13:15.624: INFO: ResourceQuota "e2e-rq-status-wsl7p" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  1 11:13:15.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8867" for this suite. 09/01/23 11:13:15.629
------------------------------
• [SLOW TEST] [235.125 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:09:20.512
    Sep  1 11:09:20.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename resourcequota 09/01/23 11:09:20.514
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:09:20.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:09:20.539
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-wsl7p" 09/01/23 11:09:20.546
    Sep  1 11:09:20.563: INFO: Resource quota "e2e-rq-status-wsl7p" reports spec: hard cpu limit of 500m
    Sep  1 11:09:20.563: INFO: Resource quota "e2e-rq-status-wsl7p" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-wsl7p" /status 09/01/23 11:09:20.563
    STEP: Confirm /status for "e2e-rq-status-wsl7p" resourceQuota via watch 09/01/23 11:09:20.571
    Sep  1 11:09:20.573: INFO: observed resourceQuota "e2e-rq-status-wsl7p" in namespace "resourcequota-8867" with hard status: v1.ResourceList(nil)
    Sep  1 11:09:20.573: INFO: Found resourceQuota "e2e-rq-status-wsl7p" in namespace "resourcequota-8867" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Sep  1 11:09:20.573: INFO: ResourceQuota "e2e-rq-status-wsl7p" /status was updated
    STEP: Patching hard spec values for cpu & memory 09/01/23 11:09:20.576
    Sep  1 11:09:20.583: INFO: Resource quota "e2e-rq-status-wsl7p" reports spec: hard cpu limit of 1
    Sep  1 11:09:20.583: INFO: Resource quota "e2e-rq-status-wsl7p" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-wsl7p" /status 09/01/23 11:09:20.583
    STEP: Confirm /status for "e2e-rq-status-wsl7p" resourceQuota via watch 09/01/23 11:09:20.589
    Sep  1 11:09:20.592: INFO: observed resourceQuota "e2e-rq-status-wsl7p" in namespace "resourcequota-8867" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Sep  1 11:09:20.592: INFO: Found resourceQuota "e2e-rq-status-wsl7p" in namespace "resourcequota-8867" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Sep  1 11:09:20.592: INFO: ResourceQuota "e2e-rq-status-wsl7p" /status was patched
    STEP: Get "e2e-rq-status-wsl7p" /status 09/01/23 11:09:20.593
    Sep  1 11:09:20.597: INFO: Resourcequota "e2e-rq-status-wsl7p" reports status: hard cpu of 1
    Sep  1 11:09:20.597: INFO: Resourcequota "e2e-rq-status-wsl7p" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-wsl7p" /status before checking Spec is unchanged 09/01/23 11:09:20.603
    Sep  1 11:09:20.614: INFO: Resourcequota "e2e-rq-status-wsl7p" reports status: hard cpu of 2
    Sep  1 11:09:20.614: INFO: Resourcequota "e2e-rq-status-wsl7p" reports status: hard memory of 2Gi
    Sep  1 11:09:20.616: INFO: observed resourceQuota "e2e-rq-status-wsl7p" in namespace "resourcequota-8867" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Sep  1 11:09:20.616: INFO: Found resourceQuota "e2e-rq-status-wsl7p" in namespace "resourcequota-8867" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Sep  1 11:13:15.624: INFO: ResourceQuota "e2e-rq-status-wsl7p" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:13:15.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8867" for this suite. 09/01/23 11:13:15.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:13:15.639
Sep  1 11:13:15.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename gc 09/01/23 11:13:15.641
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:13:15.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:13:15.666
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 09/01/23 11:13:15.675
STEP: delete the rc 09/01/23 11:13:20.685
STEP: wait for the rc to be deleted 09/01/23 11:13:20.693
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/01/23 11:13:25.699
STEP: Gathering metrics 09/01/23 11:13:55.721
Sep  1 11:13:55.761: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
Sep  1 11:13:55.764: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.101137ms
Sep  1 11:13:55.764: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
Sep  1 11:13:55.764: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
Sep  1 11:13:56.044: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  1 11:13:56.044: INFO: Deleting pod "simpletest.rc-24h2g" in namespace "gc-7469"
Sep  1 11:13:56.058: INFO: Deleting pod "simpletest.rc-2dgrs" in namespace "gc-7469"
Sep  1 11:13:56.093: INFO: Deleting pod "simpletest.rc-2rjvl" in namespace "gc-7469"
Sep  1 11:13:56.114: INFO: Deleting pod "simpletest.rc-2tr5f" in namespace "gc-7469"
Sep  1 11:13:56.150: INFO: Deleting pod "simpletest.rc-2zrt7" in namespace "gc-7469"
Sep  1 11:13:56.164: INFO: Deleting pod "simpletest.rc-42cqv" in namespace "gc-7469"
Sep  1 11:13:56.186: INFO: Deleting pod "simpletest.rc-46b64" in namespace "gc-7469"
Sep  1 11:13:56.199: INFO: Deleting pod "simpletest.rc-49cfq" in namespace "gc-7469"
Sep  1 11:13:56.226: INFO: Deleting pod "simpletest.rc-4m2jx" in namespace "gc-7469"
Sep  1 11:13:56.242: INFO: Deleting pod "simpletest.rc-4mrbt" in namespace "gc-7469"
Sep  1 11:13:56.273: INFO: Deleting pod "simpletest.rc-4q8js" in namespace "gc-7469"
Sep  1 11:13:56.302: INFO: Deleting pod "simpletest.rc-4r5bk" in namespace "gc-7469"
Sep  1 11:13:56.333: INFO: Deleting pod "simpletest.rc-4ttkf" in namespace "gc-7469"
Sep  1 11:13:56.355: INFO: Deleting pod "simpletest.rc-5fpfp" in namespace "gc-7469"
Sep  1 11:13:56.376: INFO: Deleting pod "simpletest.rc-5mkww" in namespace "gc-7469"
Sep  1 11:13:56.398: INFO: Deleting pod "simpletest.rc-5rscr" in namespace "gc-7469"
Sep  1 11:13:56.419: INFO: Deleting pod "simpletest.rc-5xhmj" in namespace "gc-7469"
Sep  1 11:13:56.452: INFO: Deleting pod "simpletest.rc-6hqm9" in namespace "gc-7469"
Sep  1 11:13:56.472: INFO: Deleting pod "simpletest.rc-6swvt" in namespace "gc-7469"
Sep  1 11:13:56.512: INFO: Deleting pod "simpletest.rc-74rk9" in namespace "gc-7469"
Sep  1 11:13:56.535: INFO: Deleting pod "simpletest.rc-7g6bl" in namespace "gc-7469"
Sep  1 11:13:56.554: INFO: Deleting pod "simpletest.rc-8ghpb" in namespace "gc-7469"
Sep  1 11:13:56.569: INFO: Deleting pod "simpletest.rc-8kvfk" in namespace "gc-7469"
Sep  1 11:13:56.591: INFO: Deleting pod "simpletest.rc-9d5q8" in namespace "gc-7469"
Sep  1 11:13:56.613: INFO: Deleting pod "simpletest.rc-9fwvv" in namespace "gc-7469"
Sep  1 11:13:56.631: INFO: Deleting pod "simpletest.rc-9mhs5" in namespace "gc-7469"
Sep  1 11:13:56.650: INFO: Deleting pod "simpletest.rc-9snhz" in namespace "gc-7469"
Sep  1 11:13:56.677: INFO: Deleting pod "simpletest.rc-blzmj" in namespace "gc-7469"
Sep  1 11:13:56.695: INFO: Deleting pod "simpletest.rc-bnwld" in namespace "gc-7469"
Sep  1 11:13:56.712: INFO: Deleting pod "simpletest.rc-bt7bk" in namespace "gc-7469"
Sep  1 11:13:56.729: INFO: Deleting pod "simpletest.rc-bw65h" in namespace "gc-7469"
Sep  1 11:13:56.742: INFO: Deleting pod "simpletest.rc-c7n49" in namespace "gc-7469"
Sep  1 11:13:56.760: INFO: Deleting pod "simpletest.rc-cdwj7" in namespace "gc-7469"
Sep  1 11:13:56.771: INFO: Deleting pod "simpletest.rc-cf55b" in namespace "gc-7469"
Sep  1 11:13:56.782: INFO: Deleting pod "simpletest.rc-d4chz" in namespace "gc-7469"
Sep  1 11:13:56.796: INFO: Deleting pod "simpletest.rc-d8brn" in namespace "gc-7469"
Sep  1 11:13:56.816: INFO: Deleting pod "simpletest.rc-fmgxf" in namespace "gc-7469"
Sep  1 11:13:56.834: INFO: Deleting pod "simpletest.rc-fq2s4" in namespace "gc-7469"
Sep  1 11:13:56.845: INFO: Deleting pod "simpletest.rc-fqd28" in namespace "gc-7469"
Sep  1 11:13:56.867: INFO: Deleting pod "simpletest.rc-frkx8" in namespace "gc-7469"
Sep  1 11:13:56.886: INFO: Deleting pod "simpletest.rc-fzbhv" in namespace "gc-7469"
Sep  1 11:13:56.905: INFO: Deleting pod "simpletest.rc-gcpxw" in namespace "gc-7469"
Sep  1 11:13:56.928: INFO: Deleting pod "simpletest.rc-gfffd" in namespace "gc-7469"
Sep  1 11:13:56.949: INFO: Deleting pod "simpletest.rc-gsb2d" in namespace "gc-7469"
Sep  1 11:13:56.969: INFO: Deleting pod "simpletest.rc-gv5cb" in namespace "gc-7469"
Sep  1 11:13:56.989: INFO: Deleting pod "simpletest.rc-h7mqv" in namespace "gc-7469"
Sep  1 11:13:57.007: INFO: Deleting pod "simpletest.rc-hc8v5" in namespace "gc-7469"
Sep  1 11:13:57.020: INFO: Deleting pod "simpletest.rc-hmhzb" in namespace "gc-7469"
Sep  1 11:13:57.047: INFO: Deleting pod "simpletest.rc-hrlvz" in namespace "gc-7469"
Sep  1 11:13:57.065: INFO: Deleting pod "simpletest.rc-hxsxp" in namespace "gc-7469"
Sep  1 11:13:57.081: INFO: Deleting pod "simpletest.rc-hzcf7" in namespace "gc-7469"
Sep  1 11:13:57.094: INFO: Deleting pod "simpletest.rc-j646l" in namespace "gc-7469"
Sep  1 11:13:57.109: INFO: Deleting pod "simpletest.rc-jcjgr" in namespace "gc-7469"
Sep  1 11:13:57.129: INFO: Deleting pod "simpletest.rc-jkmf9" in namespace "gc-7469"
Sep  1 11:13:57.154: INFO: Deleting pod "simpletest.rc-kcz55" in namespace "gc-7469"
Sep  1 11:13:57.167: INFO: Deleting pod "simpletest.rc-kqnk6" in namespace "gc-7469"
Sep  1 11:13:57.180: INFO: Deleting pod "simpletest.rc-kt5ks" in namespace "gc-7469"
Sep  1 11:13:57.199: INFO: Deleting pod "simpletest.rc-kwzt2" in namespace "gc-7469"
Sep  1 11:13:57.225: INFO: Deleting pod "simpletest.rc-l95b5" in namespace "gc-7469"
Sep  1 11:13:57.244: INFO: Deleting pod "simpletest.rc-m75cf" in namespace "gc-7469"
Sep  1 11:13:57.264: INFO: Deleting pod "simpletest.rc-mdhml" in namespace "gc-7469"
Sep  1 11:13:57.282: INFO: Deleting pod "simpletest.rc-mm88r" in namespace "gc-7469"
Sep  1 11:13:57.295: INFO: Deleting pod "simpletest.rc-n8r96" in namespace "gc-7469"
Sep  1 11:13:57.313: INFO: Deleting pod "simpletest.rc-nftwl" in namespace "gc-7469"
Sep  1 11:13:57.330: INFO: Deleting pod "simpletest.rc-p9fs2" in namespace "gc-7469"
Sep  1 11:13:57.344: INFO: Deleting pod "simpletest.rc-pc9qw" in namespace "gc-7469"
Sep  1 11:13:57.358: INFO: Deleting pod "simpletest.rc-pdkmw" in namespace "gc-7469"
Sep  1 11:13:57.371: INFO: Deleting pod "simpletest.rc-pkwjl" in namespace "gc-7469"
Sep  1 11:13:57.384: INFO: Deleting pod "simpletest.rc-pqskn" in namespace "gc-7469"
Sep  1 11:13:57.398: INFO: Deleting pod "simpletest.rc-qqkc2" in namespace "gc-7469"
Sep  1 11:13:57.410: INFO: Deleting pod "simpletest.rc-r9cmq" in namespace "gc-7469"
Sep  1 11:13:57.423: INFO: Deleting pod "simpletest.rc-rb8r8" in namespace "gc-7469"
Sep  1 11:13:57.434: INFO: Deleting pod "simpletest.rc-rfgf8" in namespace "gc-7469"
Sep  1 11:13:57.447: INFO: Deleting pod "simpletest.rc-rlxt2" in namespace "gc-7469"
Sep  1 11:13:57.461: INFO: Deleting pod "simpletest.rc-rtb84" in namespace "gc-7469"
Sep  1 11:13:57.478: INFO: Deleting pod "simpletest.rc-s56nc" in namespace "gc-7469"
Sep  1 11:13:57.498: INFO: Deleting pod "simpletest.rc-s68vq" in namespace "gc-7469"
Sep  1 11:13:57.510: INFO: Deleting pod "simpletest.rc-sbqzw" in namespace "gc-7469"
Sep  1 11:13:57.528: INFO: Deleting pod "simpletest.rc-sdvfd" in namespace "gc-7469"
Sep  1 11:13:57.580: INFO: Deleting pod "simpletest.rc-sf66h" in namespace "gc-7469"
Sep  1 11:13:57.593: INFO: Deleting pod "simpletest.rc-shrxn" in namespace "gc-7469"
Sep  1 11:13:57.606: INFO: Deleting pod "simpletest.rc-sjhgw" in namespace "gc-7469"
Sep  1 11:13:57.724: INFO: Deleting pod "simpletest.rc-smbnf" in namespace "gc-7469"
Sep  1 11:13:57.740: INFO: Deleting pod "simpletest.rc-ss4vf" in namespace "gc-7469"
Sep  1 11:13:57.757: INFO: Deleting pod "simpletest.rc-t8ghs" in namespace "gc-7469"
Sep  1 11:13:57.816: INFO: Deleting pod "simpletest.rc-ttm2h" in namespace "gc-7469"
Sep  1 11:13:57.860: INFO: Deleting pod "simpletest.rc-vgxqp" in namespace "gc-7469"
Sep  1 11:13:57.913: INFO: Deleting pod "simpletest.rc-vm9l5" in namespace "gc-7469"
Sep  1 11:13:57.961: INFO: Deleting pod "simpletest.rc-vxxlr" in namespace "gc-7469"
Sep  1 11:13:58.014: INFO: Deleting pod "simpletest.rc-wb9lb" in namespace "gc-7469"
Sep  1 11:13:58.057: INFO: Deleting pod "simpletest.rc-x4k65" in namespace "gc-7469"
Sep  1 11:13:58.111: INFO: Deleting pod "simpletest.rc-x4ndh" in namespace "gc-7469"
Sep  1 11:13:58.161: INFO: Deleting pod "simpletest.rc-x4s5b" in namespace "gc-7469"
Sep  1 11:13:58.206: INFO: Deleting pod "simpletest.rc-xhdcp" in namespace "gc-7469"
Sep  1 11:13:58.260: INFO: Deleting pod "simpletest.rc-xmqch" in namespace "gc-7469"
Sep  1 11:13:58.312: INFO: Deleting pod "simpletest.rc-xshgl" in namespace "gc-7469"
Sep  1 11:13:58.356: INFO: Deleting pod "simpletest.rc-z7rxr" in namespace "gc-7469"
Sep  1 11:13:58.408: INFO: Deleting pod "simpletest.rc-zlvzd" in namespace "gc-7469"
Sep  1 11:13:58.467: INFO: Deleting pod "simpletest.rc-zlx9v" in namespace "gc-7469"
Sep  1 11:13:58.513: INFO: Deleting pod "simpletest.rc-zz5qn" in namespace "gc-7469"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  1 11:13:58.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7469" for this suite. 09/01/23 11:13:58.599
------------------------------
• [SLOW TEST] [43.026 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:13:15.639
    Sep  1 11:13:15.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename gc 09/01/23 11:13:15.641
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:13:15.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:13:15.666
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 09/01/23 11:13:15.675
    STEP: delete the rc 09/01/23 11:13:20.685
    STEP: wait for the rc to be deleted 09/01/23 11:13:20.693
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/01/23 11:13:25.699
    STEP: Gathering metrics 09/01/23 11:13:55.721
    Sep  1 11:13:55.761: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
    Sep  1 11:13:55.764: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.101137ms
    Sep  1 11:13:55.764: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
    Sep  1 11:13:55.764: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
    Sep  1 11:13:56.044: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep  1 11:13:56.044: INFO: Deleting pod "simpletest.rc-24h2g" in namespace "gc-7469"
    Sep  1 11:13:56.058: INFO: Deleting pod "simpletest.rc-2dgrs" in namespace "gc-7469"
    Sep  1 11:13:56.093: INFO: Deleting pod "simpletest.rc-2rjvl" in namespace "gc-7469"
    Sep  1 11:13:56.114: INFO: Deleting pod "simpletest.rc-2tr5f" in namespace "gc-7469"
    Sep  1 11:13:56.150: INFO: Deleting pod "simpletest.rc-2zrt7" in namespace "gc-7469"
    Sep  1 11:13:56.164: INFO: Deleting pod "simpletest.rc-42cqv" in namespace "gc-7469"
    Sep  1 11:13:56.186: INFO: Deleting pod "simpletest.rc-46b64" in namespace "gc-7469"
    Sep  1 11:13:56.199: INFO: Deleting pod "simpletest.rc-49cfq" in namespace "gc-7469"
    Sep  1 11:13:56.226: INFO: Deleting pod "simpletest.rc-4m2jx" in namespace "gc-7469"
    Sep  1 11:13:56.242: INFO: Deleting pod "simpletest.rc-4mrbt" in namespace "gc-7469"
    Sep  1 11:13:56.273: INFO: Deleting pod "simpletest.rc-4q8js" in namespace "gc-7469"
    Sep  1 11:13:56.302: INFO: Deleting pod "simpletest.rc-4r5bk" in namespace "gc-7469"
    Sep  1 11:13:56.333: INFO: Deleting pod "simpletest.rc-4ttkf" in namespace "gc-7469"
    Sep  1 11:13:56.355: INFO: Deleting pod "simpletest.rc-5fpfp" in namespace "gc-7469"
    Sep  1 11:13:56.376: INFO: Deleting pod "simpletest.rc-5mkww" in namespace "gc-7469"
    Sep  1 11:13:56.398: INFO: Deleting pod "simpletest.rc-5rscr" in namespace "gc-7469"
    Sep  1 11:13:56.419: INFO: Deleting pod "simpletest.rc-5xhmj" in namespace "gc-7469"
    Sep  1 11:13:56.452: INFO: Deleting pod "simpletest.rc-6hqm9" in namespace "gc-7469"
    Sep  1 11:13:56.472: INFO: Deleting pod "simpletest.rc-6swvt" in namespace "gc-7469"
    Sep  1 11:13:56.512: INFO: Deleting pod "simpletest.rc-74rk9" in namespace "gc-7469"
    Sep  1 11:13:56.535: INFO: Deleting pod "simpletest.rc-7g6bl" in namespace "gc-7469"
    Sep  1 11:13:56.554: INFO: Deleting pod "simpletest.rc-8ghpb" in namespace "gc-7469"
    Sep  1 11:13:56.569: INFO: Deleting pod "simpletest.rc-8kvfk" in namespace "gc-7469"
    Sep  1 11:13:56.591: INFO: Deleting pod "simpletest.rc-9d5q8" in namespace "gc-7469"
    Sep  1 11:13:56.613: INFO: Deleting pod "simpletest.rc-9fwvv" in namespace "gc-7469"
    Sep  1 11:13:56.631: INFO: Deleting pod "simpletest.rc-9mhs5" in namespace "gc-7469"
    Sep  1 11:13:56.650: INFO: Deleting pod "simpletest.rc-9snhz" in namespace "gc-7469"
    Sep  1 11:13:56.677: INFO: Deleting pod "simpletest.rc-blzmj" in namespace "gc-7469"
    Sep  1 11:13:56.695: INFO: Deleting pod "simpletest.rc-bnwld" in namespace "gc-7469"
    Sep  1 11:13:56.712: INFO: Deleting pod "simpletest.rc-bt7bk" in namespace "gc-7469"
    Sep  1 11:13:56.729: INFO: Deleting pod "simpletest.rc-bw65h" in namespace "gc-7469"
    Sep  1 11:13:56.742: INFO: Deleting pod "simpletest.rc-c7n49" in namespace "gc-7469"
    Sep  1 11:13:56.760: INFO: Deleting pod "simpletest.rc-cdwj7" in namespace "gc-7469"
    Sep  1 11:13:56.771: INFO: Deleting pod "simpletest.rc-cf55b" in namespace "gc-7469"
    Sep  1 11:13:56.782: INFO: Deleting pod "simpletest.rc-d4chz" in namespace "gc-7469"
    Sep  1 11:13:56.796: INFO: Deleting pod "simpletest.rc-d8brn" in namespace "gc-7469"
    Sep  1 11:13:56.816: INFO: Deleting pod "simpletest.rc-fmgxf" in namespace "gc-7469"
    Sep  1 11:13:56.834: INFO: Deleting pod "simpletest.rc-fq2s4" in namespace "gc-7469"
    Sep  1 11:13:56.845: INFO: Deleting pod "simpletest.rc-fqd28" in namespace "gc-7469"
    Sep  1 11:13:56.867: INFO: Deleting pod "simpletest.rc-frkx8" in namespace "gc-7469"
    Sep  1 11:13:56.886: INFO: Deleting pod "simpletest.rc-fzbhv" in namespace "gc-7469"
    Sep  1 11:13:56.905: INFO: Deleting pod "simpletest.rc-gcpxw" in namespace "gc-7469"
    Sep  1 11:13:56.928: INFO: Deleting pod "simpletest.rc-gfffd" in namespace "gc-7469"
    Sep  1 11:13:56.949: INFO: Deleting pod "simpletest.rc-gsb2d" in namespace "gc-7469"
    Sep  1 11:13:56.969: INFO: Deleting pod "simpletest.rc-gv5cb" in namespace "gc-7469"
    Sep  1 11:13:56.989: INFO: Deleting pod "simpletest.rc-h7mqv" in namespace "gc-7469"
    Sep  1 11:13:57.007: INFO: Deleting pod "simpletest.rc-hc8v5" in namespace "gc-7469"
    Sep  1 11:13:57.020: INFO: Deleting pod "simpletest.rc-hmhzb" in namespace "gc-7469"
    Sep  1 11:13:57.047: INFO: Deleting pod "simpletest.rc-hrlvz" in namespace "gc-7469"
    Sep  1 11:13:57.065: INFO: Deleting pod "simpletest.rc-hxsxp" in namespace "gc-7469"
    Sep  1 11:13:57.081: INFO: Deleting pod "simpletest.rc-hzcf7" in namespace "gc-7469"
    Sep  1 11:13:57.094: INFO: Deleting pod "simpletest.rc-j646l" in namespace "gc-7469"
    Sep  1 11:13:57.109: INFO: Deleting pod "simpletest.rc-jcjgr" in namespace "gc-7469"
    Sep  1 11:13:57.129: INFO: Deleting pod "simpletest.rc-jkmf9" in namespace "gc-7469"
    Sep  1 11:13:57.154: INFO: Deleting pod "simpletest.rc-kcz55" in namespace "gc-7469"
    Sep  1 11:13:57.167: INFO: Deleting pod "simpletest.rc-kqnk6" in namespace "gc-7469"
    Sep  1 11:13:57.180: INFO: Deleting pod "simpletest.rc-kt5ks" in namespace "gc-7469"
    Sep  1 11:13:57.199: INFO: Deleting pod "simpletest.rc-kwzt2" in namespace "gc-7469"
    Sep  1 11:13:57.225: INFO: Deleting pod "simpletest.rc-l95b5" in namespace "gc-7469"
    Sep  1 11:13:57.244: INFO: Deleting pod "simpletest.rc-m75cf" in namespace "gc-7469"
    Sep  1 11:13:57.264: INFO: Deleting pod "simpletest.rc-mdhml" in namespace "gc-7469"
    Sep  1 11:13:57.282: INFO: Deleting pod "simpletest.rc-mm88r" in namespace "gc-7469"
    Sep  1 11:13:57.295: INFO: Deleting pod "simpletest.rc-n8r96" in namespace "gc-7469"
    Sep  1 11:13:57.313: INFO: Deleting pod "simpletest.rc-nftwl" in namespace "gc-7469"
    Sep  1 11:13:57.330: INFO: Deleting pod "simpletest.rc-p9fs2" in namespace "gc-7469"
    Sep  1 11:13:57.344: INFO: Deleting pod "simpletest.rc-pc9qw" in namespace "gc-7469"
    Sep  1 11:13:57.358: INFO: Deleting pod "simpletest.rc-pdkmw" in namespace "gc-7469"
    Sep  1 11:13:57.371: INFO: Deleting pod "simpletest.rc-pkwjl" in namespace "gc-7469"
    Sep  1 11:13:57.384: INFO: Deleting pod "simpletest.rc-pqskn" in namespace "gc-7469"
    Sep  1 11:13:57.398: INFO: Deleting pod "simpletest.rc-qqkc2" in namespace "gc-7469"
    Sep  1 11:13:57.410: INFO: Deleting pod "simpletest.rc-r9cmq" in namespace "gc-7469"
    Sep  1 11:13:57.423: INFO: Deleting pod "simpletest.rc-rb8r8" in namespace "gc-7469"
    Sep  1 11:13:57.434: INFO: Deleting pod "simpletest.rc-rfgf8" in namespace "gc-7469"
    Sep  1 11:13:57.447: INFO: Deleting pod "simpletest.rc-rlxt2" in namespace "gc-7469"
    Sep  1 11:13:57.461: INFO: Deleting pod "simpletest.rc-rtb84" in namespace "gc-7469"
    Sep  1 11:13:57.478: INFO: Deleting pod "simpletest.rc-s56nc" in namespace "gc-7469"
    Sep  1 11:13:57.498: INFO: Deleting pod "simpletest.rc-s68vq" in namespace "gc-7469"
    Sep  1 11:13:57.510: INFO: Deleting pod "simpletest.rc-sbqzw" in namespace "gc-7469"
    Sep  1 11:13:57.528: INFO: Deleting pod "simpletest.rc-sdvfd" in namespace "gc-7469"
    Sep  1 11:13:57.580: INFO: Deleting pod "simpletest.rc-sf66h" in namespace "gc-7469"
    Sep  1 11:13:57.593: INFO: Deleting pod "simpletest.rc-shrxn" in namespace "gc-7469"
    Sep  1 11:13:57.606: INFO: Deleting pod "simpletest.rc-sjhgw" in namespace "gc-7469"
    Sep  1 11:13:57.724: INFO: Deleting pod "simpletest.rc-smbnf" in namespace "gc-7469"
    Sep  1 11:13:57.740: INFO: Deleting pod "simpletest.rc-ss4vf" in namespace "gc-7469"
    Sep  1 11:13:57.757: INFO: Deleting pod "simpletest.rc-t8ghs" in namespace "gc-7469"
    Sep  1 11:13:57.816: INFO: Deleting pod "simpletest.rc-ttm2h" in namespace "gc-7469"
    Sep  1 11:13:57.860: INFO: Deleting pod "simpletest.rc-vgxqp" in namespace "gc-7469"
    Sep  1 11:13:57.913: INFO: Deleting pod "simpletest.rc-vm9l5" in namespace "gc-7469"
    Sep  1 11:13:57.961: INFO: Deleting pod "simpletest.rc-vxxlr" in namespace "gc-7469"
    Sep  1 11:13:58.014: INFO: Deleting pod "simpletest.rc-wb9lb" in namespace "gc-7469"
    Sep  1 11:13:58.057: INFO: Deleting pod "simpletest.rc-x4k65" in namespace "gc-7469"
    Sep  1 11:13:58.111: INFO: Deleting pod "simpletest.rc-x4ndh" in namespace "gc-7469"
    Sep  1 11:13:58.161: INFO: Deleting pod "simpletest.rc-x4s5b" in namespace "gc-7469"
    Sep  1 11:13:58.206: INFO: Deleting pod "simpletest.rc-xhdcp" in namespace "gc-7469"
    Sep  1 11:13:58.260: INFO: Deleting pod "simpletest.rc-xmqch" in namespace "gc-7469"
    Sep  1 11:13:58.312: INFO: Deleting pod "simpletest.rc-xshgl" in namespace "gc-7469"
    Sep  1 11:13:58.356: INFO: Deleting pod "simpletest.rc-z7rxr" in namespace "gc-7469"
    Sep  1 11:13:58.408: INFO: Deleting pod "simpletest.rc-zlvzd" in namespace "gc-7469"
    Sep  1 11:13:58.467: INFO: Deleting pod "simpletest.rc-zlx9v" in namespace "gc-7469"
    Sep  1 11:13:58.513: INFO: Deleting pod "simpletest.rc-zz5qn" in namespace "gc-7469"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:13:58.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7469" for this suite. 09/01/23 11:13:58.599
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:13:58.666
Sep  1 11:13:58.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename secrets 09/01/23 11:13:58.673
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:13:58.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:13:58.753
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-8f5e07fc-04ce-4d5e-804c-2aef42421cce 09/01/23 11:13:58.763
STEP: Creating a pod to test consume secrets 09/01/23 11:13:58.772
Sep  1 11:13:58.786: INFO: Waiting up to 5m0s for pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175" in namespace "secrets-9439" to be "Succeeded or Failed"
Sep  1 11:13:58.791: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 4.977628ms
Sep  1 11:14:00.796: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009650653s
Sep  1 11:14:02.808: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02183615s
Sep  1 11:14:04.797: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010271428s
Sep  1 11:14:06.798: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011168647s
Sep  1 11:14:08.797: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010645078s
Sep  1 11:14:10.795: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009044357s
Sep  1 11:14:12.797: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010280337s
Sep  1 11:14:14.795: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008965709s
Sep  1 11:14:16.796: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.009903867s
STEP: Saw pod success 09/01/23 11:14:16.796
Sep  1 11:14:16.797: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175" satisfied condition "Succeeded or Failed"
Sep  1 11:14:16.799: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175 container secret-env-test: <nil>
STEP: delete the pod 09/01/23 11:14:17.173
Sep  1 11:14:17.185: INFO: Waiting for pod pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175 to disappear
Sep  1 11:14:17.189: INFO: Pod pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  1 11:14:17.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9439" for this suite. 09/01/23 11:14:17.196
------------------------------
• [SLOW TEST] [18.538 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:13:58.666
    Sep  1 11:13:58.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename secrets 09/01/23 11:13:58.673
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:13:58.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:13:58.753
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-8f5e07fc-04ce-4d5e-804c-2aef42421cce 09/01/23 11:13:58.763
    STEP: Creating a pod to test consume secrets 09/01/23 11:13:58.772
    Sep  1 11:13:58.786: INFO: Waiting up to 5m0s for pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175" in namespace "secrets-9439" to be "Succeeded or Failed"
    Sep  1 11:13:58.791: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 4.977628ms
    Sep  1 11:14:00.796: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009650653s
    Sep  1 11:14:02.808: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02183615s
    Sep  1 11:14:04.797: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010271428s
    Sep  1 11:14:06.798: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011168647s
    Sep  1 11:14:08.797: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010645078s
    Sep  1 11:14:10.795: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009044357s
    Sep  1 11:14:12.797: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010280337s
    Sep  1 11:14:14.795: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008965709s
    Sep  1 11:14:16.796: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.009903867s
    STEP: Saw pod success 09/01/23 11:14:16.796
    Sep  1 11:14:16.797: INFO: Pod "pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175" satisfied condition "Succeeded or Failed"
    Sep  1 11:14:16.799: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175 container secret-env-test: <nil>
    STEP: delete the pod 09/01/23 11:14:17.173
    Sep  1 11:14:17.185: INFO: Waiting for pod pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175 to disappear
    Sep  1 11:14:17.189: INFO: Pod pod-secrets-168ab7b8-d8b0-4670-be22-78dfb2102175 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:14:17.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9439" for this suite. 09/01/23 11:14:17.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:14:17.217
Sep  1 11:14:17.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename replication-controller 09/01/23 11:14:17.219
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:14:17.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:14:17.244
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0 09/01/23 11:14:17.247
Sep  1 11:14:17.259: INFO: Pod name my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0: Found 0 pods out of 1
Sep  1 11:14:22.264: INFO: Pod name my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0: Found 1 pods out of 1
Sep  1 11:14:22.264: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0" are running
Sep  1 11:14:22.264: INFO: Waiting up to 5m0s for pod "my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0-nm2hm" in namespace "replication-controller-3027" to be "running"
Sep  1 11:14:22.267: INFO: Pod "my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0-nm2hm": Phase="Running", Reason="", readiness=true. Elapsed: 3.172332ms
Sep  1 11:14:22.267: INFO: Pod "my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0-nm2hm" satisfied condition "running"
Sep  1 11:14:22.267: INFO: Pod "my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0-nm2hm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 11:14:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 11:14:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 11:14:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 11:14:17 +0000 UTC Reason: Message:}])
Sep  1 11:14:22.267: INFO: Trying to dial the pod
Sep  1 11:14:27.286: INFO: Controller my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0: Got expected result from replica 1 [my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0-nm2hm]: "my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0-nm2hm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  1 11:14:27.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3027" for this suite. 09/01/23 11:14:27.291
------------------------------
• [SLOW TEST] [10.082 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:14:17.217
    Sep  1 11:14:17.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename replication-controller 09/01/23 11:14:17.219
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:14:17.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:14:17.244
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0 09/01/23 11:14:17.247
    Sep  1 11:14:17.259: INFO: Pod name my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0: Found 0 pods out of 1
    Sep  1 11:14:22.264: INFO: Pod name my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0: Found 1 pods out of 1
    Sep  1 11:14:22.264: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0" are running
    Sep  1 11:14:22.264: INFO: Waiting up to 5m0s for pod "my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0-nm2hm" in namespace "replication-controller-3027" to be "running"
    Sep  1 11:14:22.267: INFO: Pod "my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0-nm2hm": Phase="Running", Reason="", readiness=true. Elapsed: 3.172332ms
    Sep  1 11:14:22.267: INFO: Pod "my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0-nm2hm" satisfied condition "running"
    Sep  1 11:14:22.267: INFO: Pod "my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0-nm2hm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 11:14:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 11:14:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 11:14:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-01 11:14:17 +0000 UTC Reason: Message:}])
    Sep  1 11:14:22.267: INFO: Trying to dial the pod
    Sep  1 11:14:27.286: INFO: Controller my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0: Got expected result from replica 1 [my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0-nm2hm]: "my-hostname-basic-fe17169d-d802-4c0e-9f2d-3b0a85a9b5e0-nm2hm", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:14:27.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3027" for this suite. 09/01/23 11:14:27.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:14:27.307
Sep  1 11:14:27.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename svcaccounts 09/01/23 11:14:27.308
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:14:27.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:14:27.338
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Sep  1 11:14:27.359: INFO: created pod
Sep  1 11:14:27.359: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4174" to be "Succeeded or Failed"
Sep  1 11:14:27.365: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.155109ms
Sep  1 11:14:29.369: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009642125s
Sep  1 11:14:31.371: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 4.011521649s
Sep  1 11:14:33.369: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009150276s
STEP: Saw pod success 09/01/23 11:14:33.369
Sep  1 11:14:33.369: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Sep  1 11:15:03.370: INFO: polling logs
Sep  1 11:15:03.379: INFO: Pod logs: 
I0901 11:14:28.320089       1 log.go:198] OK: Got token
I0901 11:14:28.320310       1 log.go:198] validating with in-cluster discovery
I0901 11:14:28.320935       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0901 11:14:28.321060       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4174:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693567467, NotBefore:1693566867, IssuedAt:1693566867, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4174", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"bce1b7e8-678b-4248-a465-48c84dde3cee"}}}
I0901 11:14:28.379662       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0901 11:14:28.436285       1 log.go:198] OK: Validated signature on JWT
I0901 11:14:28.436461       1 log.go:198] OK: Got valid claims from token!
I0901 11:14:28.436556       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4174:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693567467, NotBefore:1693566867, IssuedAt:1693566867, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4174", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"bce1b7e8-678b-4248-a465-48c84dde3cee"}}}

Sep  1 11:15:03.379: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  1 11:15:03.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4174" for this suite. 09/01/23 11:15:03.428
------------------------------
• [SLOW TEST] [36.129 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:14:27.307
    Sep  1 11:14:27.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename svcaccounts 09/01/23 11:14:27.308
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:14:27.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:14:27.338
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Sep  1 11:14:27.359: INFO: created pod
    Sep  1 11:14:27.359: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4174" to be "Succeeded or Failed"
    Sep  1 11:14:27.365: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.155109ms
    Sep  1 11:14:29.369: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009642125s
    Sep  1 11:14:31.371: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 4.011521649s
    Sep  1 11:14:33.369: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009150276s
    STEP: Saw pod success 09/01/23 11:14:33.369
    Sep  1 11:14:33.369: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Sep  1 11:15:03.370: INFO: polling logs
    Sep  1 11:15:03.379: INFO: Pod logs: 
    I0901 11:14:28.320089       1 log.go:198] OK: Got token
    I0901 11:14:28.320310       1 log.go:198] validating with in-cluster discovery
    I0901 11:14:28.320935       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0901 11:14:28.321060       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4174:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693567467, NotBefore:1693566867, IssuedAt:1693566867, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4174", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"bce1b7e8-678b-4248-a465-48c84dde3cee"}}}
    I0901 11:14:28.379662       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0901 11:14:28.436285       1 log.go:198] OK: Validated signature on JWT
    I0901 11:14:28.436461       1 log.go:198] OK: Got valid claims from token!
    I0901 11:14:28.436556       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4174:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693567467, NotBefore:1693566867, IssuedAt:1693566867, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4174", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"bce1b7e8-678b-4248-a465-48c84dde3cee"}}}

    Sep  1 11:15:03.379: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:15:03.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4174" for this suite. 09/01/23 11:15:03.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:15:03.437
Sep  1 11:15:03.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename watch 09/01/23 11:15:03.439
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:03.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:03.458
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 09/01/23 11:15:03.462
STEP: creating a watch on configmaps with label B 09/01/23 11:15:03.463
STEP: creating a watch on configmaps with label A or B 09/01/23 11:15:03.464
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/01/23 11:15:03.466
Sep  1 11:15:03.471: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32483 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  1 11:15:03.472: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32483 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/01/23 11:15:03.473
Sep  1 11:15:03.482: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32484 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  1 11:15:03.483: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32484 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/01/23 11:15:03.483
Sep  1 11:15:03.491: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32485 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  1 11:15:03.492: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32485 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/01/23 11:15:03.492
Sep  1 11:15:03.501: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32486 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  1 11:15:03.501: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32486 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/01/23 11:15:03.501
Sep  1 11:15:03.507: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5207  02e906a9-e71f-4820-9315-c65c473ed32d 32487 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  1 11:15:03.508: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5207  02e906a9-e71f-4820-9315-c65c473ed32d 32487 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/01/23 11:15:13.509
Sep  1 11:15:13.516: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5207  02e906a9-e71f-4820-9315-c65c473ed32d 32552 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  1 11:15:13.516: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5207  02e906a9-e71f-4820-9315-c65c473ed32d 32552 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  1 11:15:23.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5207" for this suite. 09/01/23 11:15:23.524
------------------------------
• [SLOW TEST] [20.097 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:15:03.437
    Sep  1 11:15:03.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename watch 09/01/23 11:15:03.439
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:03.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:03.458
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 09/01/23 11:15:03.462
    STEP: creating a watch on configmaps with label B 09/01/23 11:15:03.463
    STEP: creating a watch on configmaps with label A or B 09/01/23 11:15:03.464
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/01/23 11:15:03.466
    Sep  1 11:15:03.471: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32483 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  1 11:15:03.472: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32483 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/01/23 11:15:03.473
    Sep  1 11:15:03.482: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32484 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  1 11:15:03.483: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32484 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/01/23 11:15:03.483
    Sep  1 11:15:03.491: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32485 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  1 11:15:03.492: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32485 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/01/23 11:15:03.492
    Sep  1 11:15:03.501: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32486 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  1 11:15:03.501: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5207  75d36e29-7ba9-4a22-850f-13ea4010a0cf 32486 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/01/23 11:15:03.501
    Sep  1 11:15:03.507: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5207  02e906a9-e71f-4820-9315-c65c473ed32d 32487 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  1 11:15:03.508: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5207  02e906a9-e71f-4820-9315-c65c473ed32d 32487 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/01/23 11:15:13.509
    Sep  1 11:15:13.516: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5207  02e906a9-e71f-4820-9315-c65c473ed32d 32552 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  1 11:15:13.516: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5207  02e906a9-e71f-4820-9315-c65c473ed32d 32552 0 2023-09-01 11:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:15:23.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5207" for this suite. 09/01/23 11:15:23.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:15:23.539
Sep  1 11:15:23.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename endpointslicemirroring 09/01/23 11:15:23.541
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:23.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:23.565
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 09/01/23 11:15:23.585
Sep  1 11:15:23.599: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 09/01/23 11:15:25.605
Sep  1 11:15:25.615: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 09/01/23 11:15:27.621
Sep  1 11:15:27.634: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Sep  1 11:15:29.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-2131" for this suite. 09/01/23 11:15:29.646
------------------------------
• [SLOW TEST] [6.115 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:15:23.539
    Sep  1 11:15:23.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename endpointslicemirroring 09/01/23 11:15:23.541
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:23.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:23.565
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 09/01/23 11:15:23.585
    Sep  1 11:15:23.599: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 09/01/23 11:15:25.605
    Sep  1 11:15:25.615: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 09/01/23 11:15:27.621
    Sep  1 11:15:27.634: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:15:29.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-2131" for this suite. 09/01/23 11:15:29.646
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:15:29.655
Sep  1 11:15:29.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:15:29.658
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:29.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:29.696
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-2a2da274-69ad-4515-bc34-6e88e7572a93 09/01/23 11:15:29.703
STEP: Creating a pod to test consume configMaps 09/01/23 11:15:29.711
Sep  1 11:15:29.736: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc" in namespace "projected-7310" to be "Succeeded or Failed"
Sep  1 11:15:29.742: INFO: Pod "pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.990698ms
Sep  1 11:15:31.748: INFO: Pod "pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012369176s
Sep  1 11:15:33.749: INFO: Pod "pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01272847s
STEP: Saw pod success 09/01/23 11:15:33.749
Sep  1 11:15:33.749: INFO: Pod "pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc" satisfied condition "Succeeded or Failed"
Sep  1 11:15:33.752: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc container agnhost-container: <nil>
STEP: delete the pod 09/01/23 11:15:33.76
Sep  1 11:15:33.777: INFO: Waiting for pod pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc to disappear
Sep  1 11:15:33.780: INFO: Pod pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:15:33.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7310" for this suite. 09/01/23 11:15:33.786
------------------------------
• [4.143 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:15:29.655
    Sep  1 11:15:29.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:15:29.658
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:29.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:29.696
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-2a2da274-69ad-4515-bc34-6e88e7572a93 09/01/23 11:15:29.703
    STEP: Creating a pod to test consume configMaps 09/01/23 11:15:29.711
    Sep  1 11:15:29.736: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc" in namespace "projected-7310" to be "Succeeded or Failed"
    Sep  1 11:15:29.742: INFO: Pod "pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.990698ms
    Sep  1 11:15:31.748: INFO: Pod "pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012369176s
    Sep  1 11:15:33.749: INFO: Pod "pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01272847s
    STEP: Saw pod success 09/01/23 11:15:33.749
    Sep  1 11:15:33.749: INFO: Pod "pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc" satisfied condition "Succeeded or Failed"
    Sep  1 11:15:33.752: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 11:15:33.76
    Sep  1 11:15:33.777: INFO: Waiting for pod pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc to disappear
    Sep  1 11:15:33.780: INFO: Pod pod-projected-configmaps-8b91eb4e-a044-4414-b07d-c5525cce96bc no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:15:33.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7310" for this suite. 09/01/23 11:15:33.786
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:15:33.799
Sep  1 11:15:33.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename var-expansion 09/01/23 11:15:33.8
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:33.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:33.838
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 09/01/23 11:15:33.845
Sep  1 11:15:33.855: INFO: Waiting up to 5m0s for pod "var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31" in namespace "var-expansion-9448" to be "Succeeded or Failed"
Sep  1 11:15:33.864: INFO: Pod "var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31": Phase="Pending", Reason="", readiness=false. Elapsed: 9.130358ms
Sep  1 11:15:35.870: INFO: Pod "var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014873224s
Sep  1 11:15:37.870: INFO: Pod "var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014665925s
STEP: Saw pod success 09/01/23 11:15:37.87
Sep  1 11:15:37.870: INFO: Pod "var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31" satisfied condition "Succeeded or Failed"
Sep  1 11:15:37.873: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31 container dapi-container: <nil>
STEP: delete the pod 09/01/23 11:15:37.882
Sep  1 11:15:37.911: INFO: Waiting for pod var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31 to disappear
Sep  1 11:15:37.914: INFO: Pod var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  1 11:15:37.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9448" for this suite. 09/01/23 11:15:37.922
------------------------------
• [4.136 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:15:33.799
    Sep  1 11:15:33.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename var-expansion 09/01/23 11:15:33.8
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:33.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:33.838
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 09/01/23 11:15:33.845
    Sep  1 11:15:33.855: INFO: Waiting up to 5m0s for pod "var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31" in namespace "var-expansion-9448" to be "Succeeded or Failed"
    Sep  1 11:15:33.864: INFO: Pod "var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31": Phase="Pending", Reason="", readiness=false. Elapsed: 9.130358ms
    Sep  1 11:15:35.870: INFO: Pod "var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014873224s
    Sep  1 11:15:37.870: INFO: Pod "var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014665925s
    STEP: Saw pod success 09/01/23 11:15:37.87
    Sep  1 11:15:37.870: INFO: Pod "var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31" satisfied condition "Succeeded or Failed"
    Sep  1 11:15:37.873: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31 container dapi-container: <nil>
    STEP: delete the pod 09/01/23 11:15:37.882
    Sep  1 11:15:37.911: INFO: Waiting for pod var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31 to disappear
    Sep  1 11:15:37.914: INFO: Pod var-expansion-2ff350a7-11cc-4757-9cda-187efdbc6e31 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:15:37.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9448" for this suite. 09/01/23 11:15:37.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:15:37.945
Sep  1 11:15:37.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename dns 09/01/23 11:15:37.947
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:37.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:37.974
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/01/23 11:15:37.98
Sep  1 11:15:37.994: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8584  6e561fe4-8f49-44dc-ac7e-16f1c77ede77 32746 0 2023-09-01 11:15:37 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:37 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bc7ct,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bc7ct,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:15:37.994: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8584" to be "running and ready"
Sep  1 11:15:38.002: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 7.930003ms
Sep  1 11:15:38.003: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:15:40.007: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.012077185s
Sep  1 11:15:40.007: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Sep  1 11:15:40.007: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 09/01/23 11:15:40.007
Sep  1 11:15:40.007: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8584 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:15:40.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:15:40.009: INFO: ExecWithOptions: Clientset creation
Sep  1 11:15:40.009: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8584/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 09/01/23 11:15:40.13
Sep  1 11:15:40.131: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8584 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:15:40.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:15:40.131: INFO: ExecWithOptions: Clientset creation
Sep  1 11:15:40.132: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8584/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  1 11:15:40.284: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  1 11:15:40.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8584" for this suite. 09/01/23 11:15:40.321
------------------------------
• [2.405 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:15:37.945
    Sep  1 11:15:37.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename dns 09/01/23 11:15:37.947
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:37.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:37.974
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/01/23 11:15:37.98
    Sep  1 11:15:37.994: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8584  6e561fe4-8f49-44dc-ac7e-16f1c77ede77 32746 0 2023-09-01 11:15:37 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-09-01 11:15:37 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bc7ct,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bc7ct,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:15:37.994: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8584" to be "running and ready"
    Sep  1 11:15:38.002: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 7.930003ms
    Sep  1 11:15:38.003: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:15:40.007: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.012077185s
    Sep  1 11:15:40.007: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Sep  1 11:15:40.007: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 09/01/23 11:15:40.007
    Sep  1 11:15:40.007: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8584 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:15:40.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:15:40.009: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:15:40.009: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8584/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 09/01/23 11:15:40.13
    Sep  1 11:15:40.131: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8584 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:15:40.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:15:40.131: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:15:40.132: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8584/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  1 11:15:40.284: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:15:40.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8584" for this suite. 09/01/23 11:15:40.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:15:40.355
Sep  1 11:15:40.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename replicaset 09/01/23 11:15:40.357
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:40.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:40.384
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/01/23 11:15:40.433
Sep  1 11:15:40.448: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  1 11:15:45.455: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/01/23 11:15:45.455
STEP: getting scale subresource 09/01/23 11:15:45.455
STEP: updating a scale subresource 09/01/23 11:15:45.458
STEP: verifying the replicaset Spec.Replicas was modified 09/01/23 11:15:45.473
STEP: Patch a scale subresource 09/01/23 11:15:45.48
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  1 11:15:45.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3282" for this suite. 09/01/23 11:15:45.538
------------------------------
• [SLOW TEST] [5.197 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:15:40.355
    Sep  1 11:15:40.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename replicaset 09/01/23 11:15:40.357
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:40.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:40.384
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/01/23 11:15:40.433
    Sep  1 11:15:40.448: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  1 11:15:45.455: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/01/23 11:15:45.455
    STEP: getting scale subresource 09/01/23 11:15:45.455
    STEP: updating a scale subresource 09/01/23 11:15:45.458
    STEP: verifying the replicaset Spec.Replicas was modified 09/01/23 11:15:45.473
    STEP: Patch a scale subresource 09/01/23 11:15:45.48
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:15:45.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3282" for this suite. 09/01/23 11:15:45.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:15:45.555
Sep  1 11:15:45.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename server-version 09/01/23 11:15:45.558
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:45.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:45.614
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 09/01/23 11:15:45.619
STEP: Confirm major version 09/01/23 11:15:45.62
Sep  1 11:15:45.620: INFO: Major version: 1
STEP: Confirm minor version 09/01/23 11:15:45.62
Sep  1 11:15:45.621: INFO: cleanMinorVersion: 26
Sep  1 11:15:45.621: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Sep  1 11:15:45.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-8547" for this suite. 09/01/23 11:15:45.646
------------------------------
• [0.103 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:15:45.555
    Sep  1 11:15:45.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename server-version 09/01/23 11:15:45.558
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:45.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:45.614
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 09/01/23 11:15:45.619
    STEP: Confirm major version 09/01/23 11:15:45.62
    Sep  1 11:15:45.620: INFO: Major version: 1
    STEP: Confirm minor version 09/01/23 11:15:45.62
    Sep  1 11:15:45.621: INFO: cleanMinorVersion: 26
    Sep  1 11:15:45.621: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:15:45.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-8547" for this suite. 09/01/23 11:15:45.646
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:15:45.663
Sep  1 11:15:45.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename cronjob 09/01/23 11:15:45.665
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:45.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:45.697
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 09/01/23 11:15:45.701
STEP: Ensuring a job is scheduled 09/01/23 11:15:45.713
STEP: Ensuring exactly one is scheduled 09/01/23 11:16:01.717
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/01/23 11:16:01.72
STEP: Ensuring the job is replaced with a new one 09/01/23 11:16:01.725
STEP: Removing cronjob 09/01/23 11:17:01.733
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  1 11:17:01.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2683" for this suite. 09/01/23 11:17:01.747
------------------------------
• [SLOW TEST] [76.096 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:15:45.663
    Sep  1 11:15:45.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename cronjob 09/01/23 11:15:45.665
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:15:45.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:15:45.697
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 09/01/23 11:15:45.701
    STEP: Ensuring a job is scheduled 09/01/23 11:15:45.713
    STEP: Ensuring exactly one is scheduled 09/01/23 11:16:01.717
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/01/23 11:16:01.72
    STEP: Ensuring the job is replaced with a new one 09/01/23 11:16:01.725
    STEP: Removing cronjob 09/01/23 11:17:01.733
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:17:01.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2683" for this suite. 09/01/23 11:17:01.747
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:17:01.761
Sep  1 11:17:01.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-probe 09/01/23 11:17:01.762
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:17:01.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:17:01.792
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-12f49de0-007d-4e47-8ba0-146da52cdc93 in namespace container-probe-9815 09/01/23 11:17:01.81
Sep  1 11:17:01.821: INFO: Waiting up to 5m0s for pod "test-webserver-12f49de0-007d-4e47-8ba0-146da52cdc93" in namespace "container-probe-9815" to be "not pending"
Sep  1 11:17:01.826: INFO: Pod "test-webserver-12f49de0-007d-4e47-8ba0-146da52cdc93": Phase="Pending", Reason="", readiness=false. Elapsed: 4.838171ms
Sep  1 11:17:03.831: INFO: Pod "test-webserver-12f49de0-007d-4e47-8ba0-146da52cdc93": Phase="Running", Reason="", readiness=true. Elapsed: 2.009239946s
Sep  1 11:17:03.831: INFO: Pod "test-webserver-12f49de0-007d-4e47-8ba0-146da52cdc93" satisfied condition "not pending"
Sep  1 11:17:03.831: INFO: Started pod test-webserver-12f49de0-007d-4e47-8ba0-146da52cdc93 in namespace container-probe-9815
STEP: checking the pod's current state and verifying that restartCount is present 09/01/23 11:17:03.831
Sep  1 11:17:03.834: INFO: Initial restart count of pod test-webserver-12f49de0-007d-4e47-8ba0-146da52cdc93 is 0
STEP: deleting the pod 09/01/23 11:21:04.528
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  1 11:21:04.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9815" for this suite. 09/01/23 11:21:04.553
------------------------------
• [SLOW TEST] [242.827 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:17:01.761
    Sep  1 11:17:01.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-probe 09/01/23 11:17:01.762
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:17:01.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:17:01.792
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-12f49de0-007d-4e47-8ba0-146da52cdc93 in namespace container-probe-9815 09/01/23 11:17:01.81
    Sep  1 11:17:01.821: INFO: Waiting up to 5m0s for pod "test-webserver-12f49de0-007d-4e47-8ba0-146da52cdc93" in namespace "container-probe-9815" to be "not pending"
    Sep  1 11:17:01.826: INFO: Pod "test-webserver-12f49de0-007d-4e47-8ba0-146da52cdc93": Phase="Pending", Reason="", readiness=false. Elapsed: 4.838171ms
    Sep  1 11:17:03.831: INFO: Pod "test-webserver-12f49de0-007d-4e47-8ba0-146da52cdc93": Phase="Running", Reason="", readiness=true. Elapsed: 2.009239946s
    Sep  1 11:17:03.831: INFO: Pod "test-webserver-12f49de0-007d-4e47-8ba0-146da52cdc93" satisfied condition "not pending"
    Sep  1 11:17:03.831: INFO: Started pod test-webserver-12f49de0-007d-4e47-8ba0-146da52cdc93 in namespace container-probe-9815
    STEP: checking the pod's current state and verifying that restartCount is present 09/01/23 11:17:03.831
    Sep  1 11:17:03.834: INFO: Initial restart count of pod test-webserver-12f49de0-007d-4e47-8ba0-146da52cdc93 is 0
    STEP: deleting the pod 09/01/23 11:21:04.528
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:21:04.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9815" for this suite. 09/01/23 11:21:04.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:21:04.591
Sep  1 11:21:04.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-runtime 09/01/23 11:21:04.594
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:21:04.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:21:04.618
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/01/23 11:21:04.632
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/01/23 11:21:23.745
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/01/23 11:21:23.749
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/01/23 11:21:23.758
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/01/23 11:21:23.758
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/01/23 11:21:23.807
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/01/23 11:21:26.831
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/01/23 11:21:28.847
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/01/23 11:21:28.86
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/01/23 11:21:28.86
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/01/23 11:21:28.892
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/01/23 11:21:29.905
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/01/23 11:21:32.924
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/01/23 11:21:32.93
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/01/23 11:21:32.93
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  1 11:21:32.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6479" for this suite. 09/01/23 11:21:32.973
------------------------------
• [SLOW TEST] [28.392 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:21:04.591
    Sep  1 11:21:04.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-runtime 09/01/23 11:21:04.594
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:21:04.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:21:04.618
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/01/23 11:21:04.632
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/01/23 11:21:23.745
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/01/23 11:21:23.749
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/01/23 11:21:23.758
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/01/23 11:21:23.758
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/01/23 11:21:23.807
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/01/23 11:21:26.831
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/01/23 11:21:28.847
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/01/23 11:21:28.86
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/01/23 11:21:28.86
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/01/23 11:21:28.892
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/01/23 11:21:29.905
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/01/23 11:21:32.924
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/01/23 11:21:32.93
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/01/23 11:21:32.93
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:21:32.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6479" for this suite. 09/01/23 11:21:32.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:21:32.992
Sep  1 11:21:32.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 11:21:32.994
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:21:33.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:21:33.03
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 11:21:33.047
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:21:33.394
STEP: Deploying the webhook pod 09/01/23 11:21:33.402
STEP: Wait for the deployment to be ready 09/01/23 11:21:33.425
Sep  1 11:21:33.444: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/01/23 11:21:35.458
STEP: Verifying the service has paired with the endpoint 09/01/23 11:21:35.473
Sep  1 11:21:36.473: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 09/01/23 11:21:36.477
STEP: Creating a custom resource definition that should be denied by the webhook 09/01/23 11:21:36.5
Sep  1 11:21:36.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:21:36.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9947" for this suite. 09/01/23 11:21:36.616
STEP: Destroying namespace "webhook-9947-markers" for this suite. 09/01/23 11:21:36.63
------------------------------
• [3.658 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:21:32.992
    Sep  1 11:21:32.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 11:21:32.994
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:21:33.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:21:33.03
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 11:21:33.047
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:21:33.394
    STEP: Deploying the webhook pod 09/01/23 11:21:33.402
    STEP: Wait for the deployment to be ready 09/01/23 11:21:33.425
    Sep  1 11:21:33.444: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/01/23 11:21:35.458
    STEP: Verifying the service has paired with the endpoint 09/01/23 11:21:35.473
    Sep  1 11:21:36.473: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 09/01/23 11:21:36.477
    STEP: Creating a custom resource definition that should be denied by the webhook 09/01/23 11:21:36.5
    Sep  1 11:21:36.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:21:36.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9947" for this suite. 09/01/23 11:21:36.616
    STEP: Destroying namespace "webhook-9947-markers" for this suite. 09/01/23 11:21:36.63
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:21:36.657
Sep  1 11:21:36.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 11:21:36.659
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:21:36.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:21:36.691
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/01/23 11:21:36.695
Sep  1 11:21:36.706: INFO: Waiting up to 5m0s for pod "pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4" in namespace "emptydir-5496" to be "Succeeded or Failed"
Sep  1 11:21:36.713: INFO: Pod "pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.123796ms
Sep  1 11:21:38.718: INFO: Pod "pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011846493s
Sep  1 11:21:40.717: INFO: Pod "pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011225903s
STEP: Saw pod success 09/01/23 11:21:40.718
Sep  1 11:21:40.718: INFO: Pod "pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4" satisfied condition "Succeeded or Failed"
Sep  1 11:21:40.720: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4 container test-container: <nil>
STEP: delete the pod 09/01/23 11:21:40.741
Sep  1 11:21:40.757: INFO: Waiting for pod pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4 to disappear
Sep  1 11:21:40.761: INFO: Pod pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 11:21:40.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5496" for this suite. 09/01/23 11:21:40.766
------------------------------
• [4.118 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:21:36.657
    Sep  1 11:21:36.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 11:21:36.659
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:21:36.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:21:36.691
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/01/23 11:21:36.695
    Sep  1 11:21:36.706: INFO: Waiting up to 5m0s for pod "pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4" in namespace "emptydir-5496" to be "Succeeded or Failed"
    Sep  1 11:21:36.713: INFO: Pod "pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.123796ms
    Sep  1 11:21:38.718: INFO: Pod "pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011846493s
    Sep  1 11:21:40.717: INFO: Pod "pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011225903s
    STEP: Saw pod success 09/01/23 11:21:40.718
    Sep  1 11:21:40.718: INFO: Pod "pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4" satisfied condition "Succeeded or Failed"
    Sep  1 11:21:40.720: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4 container test-container: <nil>
    STEP: delete the pod 09/01/23 11:21:40.741
    Sep  1 11:21:40.757: INFO: Waiting for pod pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4 to disappear
    Sep  1 11:21:40.761: INFO: Pod pod-58979bd2-a7aa-4974-9c57-8d747e0f4ab4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:21:40.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5496" for this suite. 09/01/23 11:21:40.766
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:21:40.78
Sep  1 11:21:40.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:21:40.783
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:21:40.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:21:40.804
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-16d432bf-1972-4c12-ae0f-782878ade031 09/01/23 11:21:40.807
STEP: Creating a pod to test consume secrets 09/01/23 11:21:40.815
Sep  1 11:21:40.827: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546" in namespace "projected-118" to be "Succeeded or Failed"
Sep  1 11:21:40.833: INFO: Pod "pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546": Phase="Pending", Reason="", readiness=false. Elapsed: 6.44923ms
Sep  1 11:21:42.850: INFO: Pod "pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546": Phase="Running", Reason="", readiness=false. Elapsed: 2.022743265s
Sep  1 11:21:44.838: INFO: Pod "pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010723166s
STEP: Saw pod success 09/01/23 11:21:44.838
Sep  1 11:21:44.838: INFO: Pod "pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546" satisfied condition "Succeeded or Failed"
Sep  1 11:21:44.843: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/01/23 11:21:44.85
Sep  1 11:21:44.871: INFO: Waiting for pod pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546 to disappear
Sep  1 11:21:44.875: INFO: Pod pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  1 11:21:44.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-118" for this suite. 09/01/23 11:21:44.881
------------------------------
• [4.108 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:21:40.78
    Sep  1 11:21:40.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:21:40.783
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:21:40.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:21:40.804
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-16d432bf-1972-4c12-ae0f-782878ade031 09/01/23 11:21:40.807
    STEP: Creating a pod to test consume secrets 09/01/23 11:21:40.815
    Sep  1 11:21:40.827: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546" in namespace "projected-118" to be "Succeeded or Failed"
    Sep  1 11:21:40.833: INFO: Pod "pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546": Phase="Pending", Reason="", readiness=false. Elapsed: 6.44923ms
    Sep  1 11:21:42.850: INFO: Pod "pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546": Phase="Running", Reason="", readiness=false. Elapsed: 2.022743265s
    Sep  1 11:21:44.838: INFO: Pod "pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010723166s
    STEP: Saw pod success 09/01/23 11:21:44.838
    Sep  1 11:21:44.838: INFO: Pod "pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546" satisfied condition "Succeeded or Failed"
    Sep  1 11:21:44.843: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/01/23 11:21:44.85
    Sep  1 11:21:44.871: INFO: Waiting for pod pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546 to disappear
    Sep  1 11:21:44.875: INFO: Pod pod-projected-secrets-3aef18cd-f71b-463b-8f17-49bf03155546 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:21:44.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-118" for this suite. 09/01/23 11:21:44.881
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:21:44.891
Sep  1 11:21:44.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 11:21:44.893
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:21:44.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:21:44.924
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-2a54206e-3768-4e0c-8b48-d132a5ed6658 09/01/23 11:21:44.933
STEP: Creating configMap with name cm-test-opt-upd-caaf6d58-35b6-404e-a8fc-7c1eb8c6c641 09/01/23 11:21:44.938
STEP: Creating the pod 09/01/23 11:21:44.944
Sep  1 11:21:44.956: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3c2f77e-b081-44bb-bc91-00b6e38ad17c" in namespace "configmap-974" to be "running and ready"
Sep  1 11:21:44.961: INFO: Pod "pod-configmaps-f3c2f77e-b081-44bb-bc91-00b6e38ad17c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.345424ms
Sep  1 11:21:44.961: INFO: The phase of Pod pod-configmaps-f3c2f77e-b081-44bb-bc91-00b6e38ad17c is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:21:46.966: INFO: Pod "pod-configmaps-f3c2f77e-b081-44bb-bc91-00b6e38ad17c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010183117s
Sep  1 11:21:46.966: INFO: The phase of Pod pod-configmaps-f3c2f77e-b081-44bb-bc91-00b6e38ad17c is Running (Ready = true)
Sep  1 11:21:46.966: INFO: Pod "pod-configmaps-f3c2f77e-b081-44bb-bc91-00b6e38ad17c" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-2a54206e-3768-4e0c-8b48-d132a5ed6658 09/01/23 11:21:46.992
STEP: Updating configmap cm-test-opt-upd-caaf6d58-35b6-404e-a8fc-7c1eb8c6c641 09/01/23 11:21:46.999
STEP: Creating configMap with name cm-test-opt-create-6a1164b9-efdf-4317-82ab-ad088968e1b6 09/01/23 11:21:47.008
STEP: waiting to observe update in volume 09/01/23 11:21:47.015
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:21:51.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-974" for this suite. 09/01/23 11:21:51.056
------------------------------
• [SLOW TEST] [6.174 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:21:44.891
    Sep  1 11:21:44.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 11:21:44.893
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:21:44.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:21:44.924
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-2a54206e-3768-4e0c-8b48-d132a5ed6658 09/01/23 11:21:44.933
    STEP: Creating configMap with name cm-test-opt-upd-caaf6d58-35b6-404e-a8fc-7c1eb8c6c641 09/01/23 11:21:44.938
    STEP: Creating the pod 09/01/23 11:21:44.944
    Sep  1 11:21:44.956: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3c2f77e-b081-44bb-bc91-00b6e38ad17c" in namespace "configmap-974" to be "running and ready"
    Sep  1 11:21:44.961: INFO: Pod "pod-configmaps-f3c2f77e-b081-44bb-bc91-00b6e38ad17c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.345424ms
    Sep  1 11:21:44.961: INFO: The phase of Pod pod-configmaps-f3c2f77e-b081-44bb-bc91-00b6e38ad17c is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:21:46.966: INFO: Pod "pod-configmaps-f3c2f77e-b081-44bb-bc91-00b6e38ad17c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010183117s
    Sep  1 11:21:46.966: INFO: The phase of Pod pod-configmaps-f3c2f77e-b081-44bb-bc91-00b6e38ad17c is Running (Ready = true)
    Sep  1 11:21:46.966: INFO: Pod "pod-configmaps-f3c2f77e-b081-44bb-bc91-00b6e38ad17c" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-2a54206e-3768-4e0c-8b48-d132a5ed6658 09/01/23 11:21:46.992
    STEP: Updating configmap cm-test-opt-upd-caaf6d58-35b6-404e-a8fc-7c1eb8c6c641 09/01/23 11:21:46.999
    STEP: Creating configMap with name cm-test-opt-create-6a1164b9-efdf-4317-82ab-ad088968e1b6 09/01/23 11:21:47.008
    STEP: waiting to observe update in volume 09/01/23 11:21:47.015
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:21:51.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-974" for this suite. 09/01/23 11:21:51.056
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:21:51.069
Sep  1 11:21:51.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pods 09/01/23 11:21:51.071
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:21:51.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:21:51.088
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 09/01/23 11:21:51.091
STEP: submitting the pod to kubernetes 09/01/23 11:21:51.092
Sep  1 11:21:51.099: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d" in namespace "pods-1270" to be "running and ready"
Sep  1 11:21:51.111: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.980914ms
Sep  1 11:21:51.111: INFO: The phase of Pod pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:21:53.115: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d": Phase="Running", Reason="", readiness=true. Elapsed: 2.01586221s
Sep  1 11:21:53.115: INFO: The phase of Pod pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d is Running (Ready = true)
Sep  1 11:21:53.115: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/01/23 11:21:53.118
STEP: updating the pod 09/01/23 11:21:53.122
Sep  1 11:21:53.636: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d"
Sep  1 11:21:53.636: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d" in namespace "pods-1270" to be "terminated with reason DeadlineExceeded"
Sep  1 11:21:53.639: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d": Phase="Running", Reason="", readiness=true. Elapsed: 2.908356ms
Sep  1 11:21:55.644: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007326532s
Sep  1 11:21:57.646: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d": Phase="Running", Reason="", readiness=false. Elapsed: 4.009880852s
Sep  1 11:21:59.643: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.007150065s
Sep  1 11:21:59.643: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  1 11:21:59.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1270" for this suite. 09/01/23 11:21:59.648
------------------------------
• [SLOW TEST] [8.586 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:21:51.069
    Sep  1 11:21:51.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pods 09/01/23 11:21:51.071
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:21:51.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:21:51.088
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 09/01/23 11:21:51.091
    STEP: submitting the pod to kubernetes 09/01/23 11:21:51.092
    Sep  1 11:21:51.099: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d" in namespace "pods-1270" to be "running and ready"
    Sep  1 11:21:51.111: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.980914ms
    Sep  1 11:21:51.111: INFO: The phase of Pod pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:21:53.115: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d": Phase="Running", Reason="", readiness=true. Elapsed: 2.01586221s
    Sep  1 11:21:53.115: INFO: The phase of Pod pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d is Running (Ready = true)
    Sep  1 11:21:53.115: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/01/23 11:21:53.118
    STEP: updating the pod 09/01/23 11:21:53.122
    Sep  1 11:21:53.636: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d"
    Sep  1 11:21:53.636: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d" in namespace "pods-1270" to be "terminated with reason DeadlineExceeded"
    Sep  1 11:21:53.639: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d": Phase="Running", Reason="", readiness=true. Elapsed: 2.908356ms
    Sep  1 11:21:55.644: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007326532s
    Sep  1 11:21:57.646: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d": Phase="Running", Reason="", readiness=false. Elapsed: 4.009880852s
    Sep  1 11:21:59.643: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.007150065s
    Sep  1 11:21:59.643: INFO: Pod "pod-update-activedeadlineseconds-6b10f649-636c-4732-88be-ec393bc5253d" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:21:59.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1270" for this suite. 09/01/23 11:21:59.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:21:59.666
Sep  1 11:21:59.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pod-network-test 09/01/23 11:21:59.668
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:21:59.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:21:59.703
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-4893 09/01/23 11:21:59.706
STEP: creating a selector 09/01/23 11:21:59.707
STEP: Creating the service pods in kubernetes 09/01/23 11:21:59.707
Sep  1 11:21:59.707: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  1 11:21:59.731: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4893" to be "running and ready"
Sep  1 11:21:59.735: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.711977ms
Sep  1 11:21:59.735: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:22:01.740: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008468405s
Sep  1 11:22:01.740: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:22:03.740: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008807455s
Sep  1 11:22:03.740: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:22:05.740: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009070547s
Sep  1 11:22:05.740: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:22:07.741: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009509182s
Sep  1 11:22:07.741: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:22:09.740: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008398217s
Sep  1 11:22:09.740: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:22:11.740: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.008596099s
Sep  1 11:22:11.740: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  1 11:22:11.740: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  1 11:22:11.744: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4893" to be "running and ready"
Sep  1 11:22:11.747: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.797964ms
Sep  1 11:22:11.747: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  1 11:22:11.747: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/01/23 11:22:11.75
Sep  1 11:22:11.766: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4893" to be "running"
Sep  1 11:22:11.772: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.726562ms
Sep  1 11:22:13.777: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010813829s
Sep  1 11:22:13.777: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  1 11:22:13.779: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4893" to be "running"
Sep  1 11:22:13.782: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.740656ms
Sep  1 11:22:13.782: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep  1 11:22:13.785: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  1 11:22:13.785: INFO: Going to poll 10.10.0.173 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Sep  1 11:22:13.788: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.0.173:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4893 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:22:13.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:22:13.789: INFO: ExecWithOptions: Clientset creation
Sep  1 11:22:13.789: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4893/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.0.173%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  1 11:22:13.912: INFO: Found all 1 expected endpoints: [netserver-0]
Sep  1 11:22:13.912: INFO: Going to poll 10.10.1.33 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Sep  1 11:22:13.915: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.1.33:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4893 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:22:13.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:22:13.916: INFO: ExecWithOptions: Clientset creation
Sep  1 11:22:13.916: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4893/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.1.33%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  1 11:22:14.028: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  1 11:22:14.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4893" for this suite. 09/01/23 11:22:14.034
------------------------------
• [SLOW TEST] [14.375 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:21:59.666
    Sep  1 11:21:59.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pod-network-test 09/01/23 11:21:59.668
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:21:59.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:21:59.703
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-4893 09/01/23 11:21:59.706
    STEP: creating a selector 09/01/23 11:21:59.707
    STEP: Creating the service pods in kubernetes 09/01/23 11:21:59.707
    Sep  1 11:21:59.707: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  1 11:21:59.731: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4893" to be "running and ready"
    Sep  1 11:21:59.735: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.711977ms
    Sep  1 11:21:59.735: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:22:01.740: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008468405s
    Sep  1 11:22:01.740: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:22:03.740: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008807455s
    Sep  1 11:22:03.740: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:22:05.740: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009070547s
    Sep  1 11:22:05.740: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:22:07.741: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009509182s
    Sep  1 11:22:07.741: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:22:09.740: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008398217s
    Sep  1 11:22:09.740: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:22:11.740: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.008596099s
    Sep  1 11:22:11.740: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  1 11:22:11.740: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  1 11:22:11.744: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4893" to be "running and ready"
    Sep  1 11:22:11.747: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.797964ms
    Sep  1 11:22:11.747: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  1 11:22:11.747: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/01/23 11:22:11.75
    Sep  1 11:22:11.766: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4893" to be "running"
    Sep  1 11:22:11.772: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.726562ms
    Sep  1 11:22:13.777: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010813829s
    Sep  1 11:22:13.777: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  1 11:22:13.779: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4893" to be "running"
    Sep  1 11:22:13.782: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.740656ms
    Sep  1 11:22:13.782: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep  1 11:22:13.785: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  1 11:22:13.785: INFO: Going to poll 10.10.0.173 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Sep  1 11:22:13.788: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.0.173:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4893 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:22:13.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:22:13.789: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:22:13.789: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4893/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.0.173%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  1 11:22:13.912: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep  1 11:22:13.912: INFO: Going to poll 10.10.1.33 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Sep  1 11:22:13.915: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.1.33:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4893 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:22:13.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:22:13.916: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:22:13.916: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4893/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.1.33%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  1 11:22:14.028: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:22:14.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4893" for this suite. 09/01/23 11:22:14.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:22:14.043
Sep  1 11:22:14.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename statefulset 09/01/23 11:22:14.046
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:22:14.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:22:14.07
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7872 09/01/23 11:22:14.074
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 09/01/23 11:22:14.092
STEP: Creating stateful set ss in namespace statefulset-7872 09/01/23 11:22:14.101
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7872 09/01/23 11:22:14.108
Sep  1 11:22:14.111: INFO: Found 0 stateful pods, waiting for 1
Sep  1 11:22:24.117: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/01/23 11:22:24.117
Sep  1 11:22:24.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  1 11:22:24.355: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  1 11:22:24.355: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  1 11:22:24.355: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  1 11:22:24.359: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  1 11:22:34.363: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  1 11:22:34.363: INFO: Waiting for statefulset status.replicas updated to 0
Sep  1 11:22:34.383: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999787s
Sep  1 11:22:35.387: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993904925s
Sep  1 11:22:36.391: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990158705s
Sep  1 11:22:37.394: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985691028s
Sep  1 11:22:38.398: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982856736s
Sep  1 11:22:39.401: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979196963s
Sep  1 11:22:40.405: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976216567s
Sep  1 11:22:41.409: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972403043s
Sep  1 11:22:42.412: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.968600416s
Sep  1 11:22:43.418: INFO: Verifying statefulset ss doesn't scale past 1 for another 964.101184ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7872 09/01/23 11:22:44.419
Sep  1 11:22:44.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  1 11:22:44.624: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  1 11:22:44.624: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  1 11:22:44.624: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  1 11:22:44.627: INFO: Found 1 stateful pods, waiting for 3
Sep  1 11:22:54.676: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  1 11:22:54.676: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  1 11:22:54.676: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 09/01/23 11:22:54.676
STEP: Scale down will halt with unhealthy stateful pod 09/01/23 11:22:54.676
Sep  1 11:22:54.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  1 11:22:55.306: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  1 11:22:55.306: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  1 11:22:55.306: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  1 11:22:55.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  1 11:22:55.537: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  1 11:22:55.537: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  1 11:22:55.537: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  1 11:22:55.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  1 11:22:55.817: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  1 11:22:55.817: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  1 11:22:55.817: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  1 11:22:55.817: INFO: Waiting for statefulset status.replicas updated to 0
Sep  1 11:22:55.821: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep  1 11:23:05.829: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  1 11:23:05.829: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  1 11:23:05.829: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  1 11:23:05.843: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999765s
Sep  1 11:23:06.852: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995442464s
Sep  1 11:23:07.857: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986205198s
Sep  1 11:23:08.861: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981727493s
Sep  1 11:23:09.866: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976610813s
Sep  1 11:23:10.870: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97270294s
Sep  1 11:23:11.875: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967929777s
Sep  1 11:23:12.880: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.963687838s
Sep  1 11:23:13.885: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.958431084s
Sep  1 11:23:14.891: INFO: Verifying statefulset ss doesn't scale past 3 for another 952.758237ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7872 09/01/23 11:23:15.891
Sep  1 11:23:15.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  1 11:23:16.098: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  1 11:23:16.098: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  1 11:23:16.098: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  1 11:23:16.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  1 11:23:16.297: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  1 11:23:16.297: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  1 11:23:16.297: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  1 11:23:16.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  1 11:23:16.478: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  1 11:23:16.478: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  1 11:23:16.478: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  1 11:23:16.478: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 09/01/23 11:23:26.518
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  1 11:23:26.519: INFO: Deleting all statefulset in ns statefulset-7872
Sep  1 11:23:26.521: INFO: Scaling statefulset ss to 0
Sep  1 11:23:26.533: INFO: Waiting for statefulset status.replicas updated to 0
Sep  1 11:23:26.536: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  1 11:23:26.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7872" for this suite. 09/01/23 11:23:26.566
------------------------------
• [SLOW TEST] [72.539 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:22:14.043
    Sep  1 11:22:14.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename statefulset 09/01/23 11:22:14.046
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:22:14.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:22:14.07
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7872 09/01/23 11:22:14.074
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 09/01/23 11:22:14.092
    STEP: Creating stateful set ss in namespace statefulset-7872 09/01/23 11:22:14.101
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7872 09/01/23 11:22:14.108
    Sep  1 11:22:14.111: INFO: Found 0 stateful pods, waiting for 1
    Sep  1 11:22:24.117: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/01/23 11:22:24.117
    Sep  1 11:22:24.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  1 11:22:24.355: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  1 11:22:24.355: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  1 11:22:24.355: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  1 11:22:24.359: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep  1 11:22:34.363: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  1 11:22:34.363: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  1 11:22:34.383: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999787s
    Sep  1 11:22:35.387: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993904925s
    Sep  1 11:22:36.391: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990158705s
    Sep  1 11:22:37.394: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985691028s
    Sep  1 11:22:38.398: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982856736s
    Sep  1 11:22:39.401: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979196963s
    Sep  1 11:22:40.405: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976216567s
    Sep  1 11:22:41.409: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972403043s
    Sep  1 11:22:42.412: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.968600416s
    Sep  1 11:22:43.418: INFO: Verifying statefulset ss doesn't scale past 1 for another 964.101184ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7872 09/01/23 11:22:44.419
    Sep  1 11:22:44.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  1 11:22:44.624: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  1 11:22:44.624: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  1 11:22:44.624: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  1 11:22:44.627: INFO: Found 1 stateful pods, waiting for 3
    Sep  1 11:22:54.676: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  1 11:22:54.676: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  1 11:22:54.676: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 09/01/23 11:22:54.676
    STEP: Scale down will halt with unhealthy stateful pod 09/01/23 11:22:54.676
    Sep  1 11:22:54.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  1 11:22:55.306: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  1 11:22:55.306: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  1 11:22:55.306: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  1 11:22:55.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  1 11:22:55.537: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  1 11:22:55.537: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  1 11:22:55.537: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  1 11:22:55.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  1 11:22:55.817: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  1 11:22:55.817: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  1 11:22:55.817: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  1 11:22:55.817: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  1 11:22:55.821: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Sep  1 11:23:05.829: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  1 11:23:05.829: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep  1 11:23:05.829: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep  1 11:23:05.843: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999765s
    Sep  1 11:23:06.852: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995442464s
    Sep  1 11:23:07.857: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986205198s
    Sep  1 11:23:08.861: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981727493s
    Sep  1 11:23:09.866: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976610813s
    Sep  1 11:23:10.870: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97270294s
    Sep  1 11:23:11.875: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967929777s
    Sep  1 11:23:12.880: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.963687838s
    Sep  1 11:23:13.885: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.958431084s
    Sep  1 11:23:14.891: INFO: Verifying statefulset ss doesn't scale past 3 for another 952.758237ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7872 09/01/23 11:23:15.891
    Sep  1 11:23:15.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  1 11:23:16.098: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  1 11:23:16.098: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  1 11:23:16.098: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  1 11:23:16.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  1 11:23:16.297: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  1 11:23:16.297: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  1 11:23:16.297: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  1 11:23:16.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-7872 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  1 11:23:16.478: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  1 11:23:16.478: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  1 11:23:16.478: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  1 11:23:16.478: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 09/01/23 11:23:26.518
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  1 11:23:26.519: INFO: Deleting all statefulset in ns statefulset-7872
    Sep  1 11:23:26.521: INFO: Scaling statefulset ss to 0
    Sep  1 11:23:26.533: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  1 11:23:26.536: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:23:26.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7872" for this suite. 09/01/23 11:23:26.566
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:23:26.586
Sep  1 11:23:26.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename secrets 09/01/23 11:23:26.589
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:23:26.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:23:26.62
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-d56b6005-6d4a-4219-8177-42994b5afd4a 09/01/23 11:23:26.628
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  1 11:23:26.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2662" for this suite. 09/01/23 11:23:26.639
------------------------------
• [0.059 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:23:26.586
    Sep  1 11:23:26.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename secrets 09/01/23 11:23:26.589
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:23:26.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:23:26.62
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-d56b6005-6d4a-4219-8177-42994b5afd4a 09/01/23 11:23:26.628
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:23:26.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2662" for this suite. 09/01/23 11:23:26.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:23:26.652
Sep  1 11:23:26.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename replicaset 09/01/23 11:23:26.654
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:23:26.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:23:26.674
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 09/01/23 11:23:26.683
STEP: Verify that the required pods have come up. 09/01/23 11:23:26.689
Sep  1 11:23:26.693: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  1 11:23:31.698: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/01/23 11:23:31.699
STEP: Getting /status 09/01/23 11:23:31.699
Sep  1 11:23:31.704: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 09/01/23 11:23:31.704
Sep  1 11:23:31.720: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 09/01/23 11:23:31.72
Sep  1 11:23:31.724: INFO: Observed &ReplicaSet event: ADDED
Sep  1 11:23:31.724: INFO: Observed &ReplicaSet event: MODIFIED
Sep  1 11:23:31.724: INFO: Observed &ReplicaSet event: MODIFIED
Sep  1 11:23:31.724: INFO: Observed &ReplicaSet event: MODIFIED
Sep  1 11:23:31.724: INFO: Found replicaset test-rs in namespace replicaset-4667 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  1 11:23:31.724: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 09/01/23 11:23:31.724
Sep  1 11:23:31.724: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  1 11:23:31.737: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 09/01/23 11:23:31.737
Sep  1 11:23:31.740: INFO: Observed &ReplicaSet event: ADDED
Sep  1 11:23:31.740: INFO: Observed &ReplicaSet event: MODIFIED
Sep  1 11:23:31.740: INFO: Observed &ReplicaSet event: MODIFIED
Sep  1 11:23:31.741: INFO: Observed &ReplicaSet event: MODIFIED
Sep  1 11:23:31.741: INFO: Observed replicaset test-rs in namespace replicaset-4667 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  1 11:23:31.741: INFO: Observed &ReplicaSet event: MODIFIED
Sep  1 11:23:31.741: INFO: Found replicaset test-rs in namespace replicaset-4667 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Sep  1 11:23:31.741: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  1 11:23:31.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4667" for this suite. 09/01/23 11:23:31.747
------------------------------
• [SLOW TEST] [5.105 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:23:26.652
    Sep  1 11:23:26.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename replicaset 09/01/23 11:23:26.654
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:23:26.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:23:26.674
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 09/01/23 11:23:26.683
    STEP: Verify that the required pods have come up. 09/01/23 11:23:26.689
    Sep  1 11:23:26.693: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  1 11:23:31.698: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/01/23 11:23:31.699
    STEP: Getting /status 09/01/23 11:23:31.699
    Sep  1 11:23:31.704: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 09/01/23 11:23:31.704
    Sep  1 11:23:31.720: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 09/01/23 11:23:31.72
    Sep  1 11:23:31.724: INFO: Observed &ReplicaSet event: ADDED
    Sep  1 11:23:31.724: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  1 11:23:31.724: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  1 11:23:31.724: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  1 11:23:31.724: INFO: Found replicaset test-rs in namespace replicaset-4667 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  1 11:23:31.724: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 09/01/23 11:23:31.724
    Sep  1 11:23:31.724: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  1 11:23:31.737: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 09/01/23 11:23:31.737
    Sep  1 11:23:31.740: INFO: Observed &ReplicaSet event: ADDED
    Sep  1 11:23:31.740: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  1 11:23:31.740: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  1 11:23:31.741: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  1 11:23:31.741: INFO: Observed replicaset test-rs in namespace replicaset-4667 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  1 11:23:31.741: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  1 11:23:31.741: INFO: Found replicaset test-rs in namespace replicaset-4667 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Sep  1 11:23:31.741: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:23:31.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4667" for this suite. 09/01/23 11:23:31.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:23:31.763
Sep  1 11:23:31.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubelet-test 09/01/23 11:23:31.765
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:23:31.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:23:31.787
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Sep  1 11:23:31.808: INFO: Waiting up to 5m0s for pod "busybox-scheduling-9d300d53-26cc-43b5-8e03-a207849c2274" in namespace "kubelet-test-9255" to be "running and ready"
Sep  1 11:23:31.814: INFO: Pod "busybox-scheduling-9d300d53-26cc-43b5-8e03-a207849c2274": Phase="Pending", Reason="", readiness=false. Elapsed: 5.752417ms
Sep  1 11:23:31.814: INFO: The phase of Pod busybox-scheduling-9d300d53-26cc-43b5-8e03-a207849c2274 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:23:33.819: INFO: Pod "busybox-scheduling-9d300d53-26cc-43b5-8e03-a207849c2274": Phase="Running", Reason="", readiness=true. Elapsed: 2.01001633s
Sep  1 11:23:33.819: INFO: The phase of Pod busybox-scheduling-9d300d53-26cc-43b5-8e03-a207849c2274 is Running (Ready = true)
Sep  1 11:23:33.819: INFO: Pod "busybox-scheduling-9d300d53-26cc-43b5-8e03-a207849c2274" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  1 11:23:33.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9255" for this suite. 09/01/23 11:23:33.845
------------------------------
• [2.088 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:23:31.763
    Sep  1 11:23:31.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubelet-test 09/01/23 11:23:31.765
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:23:31.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:23:31.787
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Sep  1 11:23:31.808: INFO: Waiting up to 5m0s for pod "busybox-scheduling-9d300d53-26cc-43b5-8e03-a207849c2274" in namespace "kubelet-test-9255" to be "running and ready"
    Sep  1 11:23:31.814: INFO: Pod "busybox-scheduling-9d300d53-26cc-43b5-8e03-a207849c2274": Phase="Pending", Reason="", readiness=false. Elapsed: 5.752417ms
    Sep  1 11:23:31.814: INFO: The phase of Pod busybox-scheduling-9d300d53-26cc-43b5-8e03-a207849c2274 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:23:33.819: INFO: Pod "busybox-scheduling-9d300d53-26cc-43b5-8e03-a207849c2274": Phase="Running", Reason="", readiness=true. Elapsed: 2.01001633s
    Sep  1 11:23:33.819: INFO: The phase of Pod busybox-scheduling-9d300d53-26cc-43b5-8e03-a207849c2274 is Running (Ready = true)
    Sep  1 11:23:33.819: INFO: Pod "busybox-scheduling-9d300d53-26cc-43b5-8e03-a207849c2274" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:23:33.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9255" for this suite. 09/01/23 11:23:33.845
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:23:33.857
Sep  1 11:23:33.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename disruption 09/01/23 11:23:33.858
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:23:33.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:23:33.875
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 09/01/23 11:23:33.964
STEP: Waiting for all pods to be running 09/01/23 11:23:36.037
Sep  1 11:23:36.053: INFO: running pods: 0 < 3
Sep  1 11:23:38.059: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  1 11:23:40.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8870" for this suite. 09/01/23 11:23:40.066
------------------------------
• [SLOW TEST] [6.216 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:23:33.857
    Sep  1 11:23:33.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename disruption 09/01/23 11:23:33.858
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:23:33.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:23:33.875
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 09/01/23 11:23:33.964
    STEP: Waiting for all pods to be running 09/01/23 11:23:36.037
    Sep  1 11:23:36.053: INFO: running pods: 0 < 3
    Sep  1 11:23:38.059: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:23:40.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8870" for this suite. 09/01/23 11:23:40.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:23:40.076
Sep  1 11:23:40.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename replication-controller 09/01/23 11:23:40.079
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:23:40.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:23:40.099
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 09/01/23 11:23:40.106
STEP: waiting for RC to be added 09/01/23 11:23:40.112
STEP: waiting for available Replicas 09/01/23 11:23:40.113
STEP: patching ReplicationController 09/01/23 11:23:41.264
STEP: waiting for RC to be modified 09/01/23 11:23:41.279
STEP: patching ReplicationController status 09/01/23 11:23:41.28
STEP: waiting for RC to be modified 09/01/23 11:23:41.287
STEP: waiting for available Replicas 09/01/23 11:23:41.287
STEP: fetching ReplicationController status 09/01/23 11:23:41.292
STEP: patching ReplicationController scale 09/01/23 11:23:41.296
STEP: waiting for RC to be modified 09/01/23 11:23:41.302
STEP: waiting for ReplicationController's scale to be the max amount 09/01/23 11:23:41.303
STEP: fetching ReplicationController; ensuring that it's patched 09/01/23 11:23:43.273
STEP: updating ReplicationController status 09/01/23 11:23:43.277
STEP: waiting for RC to be modified 09/01/23 11:23:43.283
STEP: listing all ReplicationControllers 09/01/23 11:23:43.283
STEP: checking that ReplicationController has expected values 09/01/23 11:23:43.286
STEP: deleting ReplicationControllers by collection 09/01/23 11:23:43.287
STEP: waiting for ReplicationController to have a DELETED watchEvent 09/01/23 11:23:43.296
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  1 11:23:43.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6935" for this suite. 09/01/23 11:23:43.332
------------------------------
• [3.265 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:23:40.076
    Sep  1 11:23:40.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename replication-controller 09/01/23 11:23:40.079
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:23:40.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:23:40.099
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 09/01/23 11:23:40.106
    STEP: waiting for RC to be added 09/01/23 11:23:40.112
    STEP: waiting for available Replicas 09/01/23 11:23:40.113
    STEP: patching ReplicationController 09/01/23 11:23:41.264
    STEP: waiting for RC to be modified 09/01/23 11:23:41.279
    STEP: patching ReplicationController status 09/01/23 11:23:41.28
    STEP: waiting for RC to be modified 09/01/23 11:23:41.287
    STEP: waiting for available Replicas 09/01/23 11:23:41.287
    STEP: fetching ReplicationController status 09/01/23 11:23:41.292
    STEP: patching ReplicationController scale 09/01/23 11:23:41.296
    STEP: waiting for RC to be modified 09/01/23 11:23:41.302
    STEP: waiting for ReplicationController's scale to be the max amount 09/01/23 11:23:41.303
    STEP: fetching ReplicationController; ensuring that it's patched 09/01/23 11:23:43.273
    STEP: updating ReplicationController status 09/01/23 11:23:43.277
    STEP: waiting for RC to be modified 09/01/23 11:23:43.283
    STEP: listing all ReplicationControllers 09/01/23 11:23:43.283
    STEP: checking that ReplicationController has expected values 09/01/23 11:23:43.286
    STEP: deleting ReplicationControllers by collection 09/01/23 11:23:43.287
    STEP: waiting for ReplicationController to have a DELETED watchEvent 09/01/23 11:23:43.296
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:23:43.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6935" for this suite. 09/01/23 11:23:43.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:23:43.343
Sep  1 11:23:43.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename dns 09/01/23 11:23:43.345
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:23:43.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:23:43.368
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 09/01/23 11:23:43.371
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2931.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2931.svc.cluster.local;sleep 1; done
 09/01/23 11:23:43.379
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2931.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2931.svc.cluster.local;sleep 1; done
 09/01/23 11:23:43.379
STEP: creating a pod to probe DNS 09/01/23 11:23:43.379
STEP: submitting the pod to kubernetes 09/01/23 11:23:43.379
Sep  1 11:23:43.393: INFO: Waiting up to 15m0s for pod "dns-test-67403069-2741-4af5-9b01-91a48bbf0d02" in namespace "dns-2931" to be "running"
Sep  1 11:23:43.398: INFO: Pod "dns-test-67403069-2741-4af5-9b01-91a48bbf0d02": Phase="Pending", Reason="", readiness=false. Elapsed: 5.090496ms
Sep  1 11:23:45.406: INFO: Pod "dns-test-67403069-2741-4af5-9b01-91a48bbf0d02": Phase="Running", Reason="", readiness=true. Elapsed: 2.012713509s
Sep  1 11:23:45.406: INFO: Pod "dns-test-67403069-2741-4af5-9b01-91a48bbf0d02" satisfied condition "running"
STEP: retrieving the pod 09/01/23 11:23:45.406
STEP: looking for the results for each expected name from probers 09/01/23 11:23:45.41
Sep  1 11:23:45.424: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local from pod dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02: the server could not find the requested resource (get pods dns-test-67403069-2741-4af5-9b01-91a48bbf0d02)
Sep  1 11:23:45.438: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local from pod dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02: the server could not find the requested resource (get pods dns-test-67403069-2741-4af5-9b01-91a48bbf0d02)
Sep  1 11:23:45.442: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2931.svc.cluster.local from pod dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02: the server could not find the requested resource (get pods dns-test-67403069-2741-4af5-9b01-91a48bbf0d02)
Sep  1 11:23:45.449: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2931.svc.cluster.local from pod dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02: the server could not find the requested resource (get pods dns-test-67403069-2741-4af5-9b01-91a48bbf0d02)
Sep  1 11:23:45.471: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2931.svc.cluster.local from pod dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02: the server could not find the requested resource (get pods dns-test-67403069-2741-4af5-9b01-91a48bbf0d02)
Sep  1 11:23:45.488: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2931.svc.cluster.local from pod dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02: the server could not find the requested resource (get pods dns-test-67403069-2741-4af5-9b01-91a48bbf0d02)
Sep  1 11:23:45.489: INFO: Lookups using dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2931.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2931.svc.cluster.local jessie_udp@dns-test-service-2.dns-2931.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2931.svc.cluster.local]

Sep  1 11:23:50.522: INFO: DNS probes using dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02 succeeded

STEP: deleting the pod 09/01/23 11:23:50.522
STEP: deleting the test headless service 09/01/23 11:23:50.554
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  1 11:23:50.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2931" for this suite. 09/01/23 11:23:50.594
------------------------------
• [SLOW TEST] [7.257 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:23:43.343
    Sep  1 11:23:43.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename dns 09/01/23 11:23:43.345
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:23:43.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:23:43.368
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 09/01/23 11:23:43.371
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2931.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2931.svc.cluster.local;sleep 1; done
     09/01/23 11:23:43.379
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2931.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2931.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2931.svc.cluster.local;sleep 1; done
     09/01/23 11:23:43.379
    STEP: creating a pod to probe DNS 09/01/23 11:23:43.379
    STEP: submitting the pod to kubernetes 09/01/23 11:23:43.379
    Sep  1 11:23:43.393: INFO: Waiting up to 15m0s for pod "dns-test-67403069-2741-4af5-9b01-91a48bbf0d02" in namespace "dns-2931" to be "running"
    Sep  1 11:23:43.398: INFO: Pod "dns-test-67403069-2741-4af5-9b01-91a48bbf0d02": Phase="Pending", Reason="", readiness=false. Elapsed: 5.090496ms
    Sep  1 11:23:45.406: INFO: Pod "dns-test-67403069-2741-4af5-9b01-91a48bbf0d02": Phase="Running", Reason="", readiness=true. Elapsed: 2.012713509s
    Sep  1 11:23:45.406: INFO: Pod "dns-test-67403069-2741-4af5-9b01-91a48bbf0d02" satisfied condition "running"
    STEP: retrieving the pod 09/01/23 11:23:45.406
    STEP: looking for the results for each expected name from probers 09/01/23 11:23:45.41
    Sep  1 11:23:45.424: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local from pod dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02: the server could not find the requested resource (get pods dns-test-67403069-2741-4af5-9b01-91a48bbf0d02)
    Sep  1 11:23:45.438: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local from pod dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02: the server could not find the requested resource (get pods dns-test-67403069-2741-4af5-9b01-91a48bbf0d02)
    Sep  1 11:23:45.442: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2931.svc.cluster.local from pod dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02: the server could not find the requested resource (get pods dns-test-67403069-2741-4af5-9b01-91a48bbf0d02)
    Sep  1 11:23:45.449: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2931.svc.cluster.local from pod dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02: the server could not find the requested resource (get pods dns-test-67403069-2741-4af5-9b01-91a48bbf0d02)
    Sep  1 11:23:45.471: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2931.svc.cluster.local from pod dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02: the server could not find the requested resource (get pods dns-test-67403069-2741-4af5-9b01-91a48bbf0d02)
    Sep  1 11:23:45.488: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2931.svc.cluster.local from pod dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02: the server could not find the requested resource (get pods dns-test-67403069-2741-4af5-9b01-91a48bbf0d02)
    Sep  1 11:23:45.489: INFO: Lookups using dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2931.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2931.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2931.svc.cluster.local jessie_udp@dns-test-service-2.dns-2931.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2931.svc.cluster.local]

    Sep  1 11:23:50.522: INFO: DNS probes using dns-2931/dns-test-67403069-2741-4af5-9b01-91a48bbf0d02 succeeded

    STEP: deleting the pod 09/01/23 11:23:50.522
    STEP: deleting the test headless service 09/01/23 11:23:50.554
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:23:50.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2931" for this suite. 09/01/23 11:23:50.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:23:50.619
Sep  1 11:23:50.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename subpath 09/01/23 11:23:50.623
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:23:50.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:23:50.666
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/01/23 11:23:50.67
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-jx8r 09/01/23 11:23:50.684
STEP: Creating a pod to test atomic-volume-subpath 09/01/23 11:23:50.684
Sep  1 11:23:50.703: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jx8r" in namespace "subpath-4015" to be "Succeeded or Failed"
Sep  1 11:23:50.709: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057215ms
Sep  1 11:23:52.713: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 2.010235009s
Sep  1 11:23:54.716: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 4.013082341s
Sep  1 11:23:56.714: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 6.011184587s
Sep  1 11:23:58.714: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 8.010731242s
Sep  1 11:24:00.715: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 10.011716747s
Sep  1 11:24:02.713: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 12.010006922s
Sep  1 11:24:04.714: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 14.010702186s
Sep  1 11:24:06.713: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 16.009689871s
Sep  1 11:24:08.714: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 18.011327141s
Sep  1 11:24:10.716: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 20.012680894s
Sep  1 11:24:12.713: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=false. Elapsed: 22.01013656s
Sep  1 11:24:14.716: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01316115s
STEP: Saw pod success 09/01/23 11:24:14.716
Sep  1 11:24:14.716: INFO: Pod "pod-subpath-test-configmap-jx8r" satisfied condition "Succeeded or Failed"
Sep  1 11:24:14.720: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-subpath-test-configmap-jx8r container test-container-subpath-configmap-jx8r: <nil>
STEP: delete the pod 09/01/23 11:24:14.728
Sep  1 11:24:14.743: INFO: Waiting for pod pod-subpath-test-configmap-jx8r to disappear
Sep  1 11:24:14.747: INFO: Pod pod-subpath-test-configmap-jx8r no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jx8r 09/01/23 11:24:14.747
Sep  1 11:24:14.747: INFO: Deleting pod "pod-subpath-test-configmap-jx8r" in namespace "subpath-4015"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  1 11:24:14.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4015" for this suite. 09/01/23 11:24:14.755
------------------------------
• [SLOW TEST] [24.143 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:23:50.619
    Sep  1 11:23:50.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename subpath 09/01/23 11:23:50.623
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:23:50.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:23:50.666
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/01/23 11:23:50.67
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-jx8r 09/01/23 11:23:50.684
    STEP: Creating a pod to test atomic-volume-subpath 09/01/23 11:23:50.684
    Sep  1 11:23:50.703: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jx8r" in namespace "subpath-4015" to be "Succeeded or Failed"
    Sep  1 11:23:50.709: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057215ms
    Sep  1 11:23:52.713: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 2.010235009s
    Sep  1 11:23:54.716: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 4.013082341s
    Sep  1 11:23:56.714: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 6.011184587s
    Sep  1 11:23:58.714: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 8.010731242s
    Sep  1 11:24:00.715: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 10.011716747s
    Sep  1 11:24:02.713: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 12.010006922s
    Sep  1 11:24:04.714: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 14.010702186s
    Sep  1 11:24:06.713: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 16.009689871s
    Sep  1 11:24:08.714: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 18.011327141s
    Sep  1 11:24:10.716: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=true. Elapsed: 20.012680894s
    Sep  1 11:24:12.713: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Running", Reason="", readiness=false. Elapsed: 22.01013656s
    Sep  1 11:24:14.716: INFO: Pod "pod-subpath-test-configmap-jx8r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01316115s
    STEP: Saw pod success 09/01/23 11:24:14.716
    Sep  1 11:24:14.716: INFO: Pod "pod-subpath-test-configmap-jx8r" satisfied condition "Succeeded or Failed"
    Sep  1 11:24:14.720: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-subpath-test-configmap-jx8r container test-container-subpath-configmap-jx8r: <nil>
    STEP: delete the pod 09/01/23 11:24:14.728
    Sep  1 11:24:14.743: INFO: Waiting for pod pod-subpath-test-configmap-jx8r to disappear
    Sep  1 11:24:14.747: INFO: Pod pod-subpath-test-configmap-jx8r no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-jx8r 09/01/23 11:24:14.747
    Sep  1 11:24:14.747: INFO: Deleting pod "pod-subpath-test-configmap-jx8r" in namespace "subpath-4015"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:24:14.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4015" for this suite. 09/01/23 11:24:14.755
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:24:14.765
Sep  1 11:24:14.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename deployment 09/01/23 11:24:14.767
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:24:14.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:24:14.79
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Sep  1 11:24:14.795: INFO: Creating deployment "webserver-deployment"
Sep  1 11:24:14.801: INFO: Waiting for observed generation 1
Sep  1 11:24:16.821: INFO: Waiting for all required pods to come up
Sep  1 11:24:16.832: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 09/01/23 11:24:16.833
Sep  1 11:24:16.833: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hzg8k" in namespace "deployment-6418" to be "running"
Sep  1 11:24:16.833: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-87c9n" in namespace "deployment-6418" to be "running"
Sep  1 11:24:16.833: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9dvkx" in namespace "deployment-6418" to be "running"
Sep  1 11:24:16.833: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-d9llm" in namespace "deployment-6418" to be "running"
Sep  1 11:24:16.834: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-gfhxl" in namespace "deployment-6418" to be "running"
Sep  1 11:24:16.834: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-mkhzx" in namespace "deployment-6418" to be "running"
Sep  1 11:24:16.834: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-kjdcp" in namespace "deployment-6418" to be "running"
Sep  1 11:24:16.834: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qt2vs" in namespace "deployment-6418" to be "running"
Sep  1 11:24:16.834: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-trtzv" in namespace "deployment-6418" to be "running"
Sep  1 11:24:16.833: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-tsk5m" in namespace "deployment-6418" to be "running"
Sep  1 11:24:16.841: INFO: Pod "webserver-deployment-7f5969cbc7-87c9n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.05457ms
Sep  1 11:24:16.842: INFO: Pod "webserver-deployment-7f5969cbc7-hzg8k": Phase="Pending", Reason="", readiness=false. Elapsed: 8.739038ms
Sep  1 11:24:16.844: INFO: Pod "webserver-deployment-7f5969cbc7-tsk5m": Phase="Pending", Reason="", readiness=false. Elapsed: 9.415828ms
Sep  1 11:24:16.844: INFO: Pod "webserver-deployment-7f5969cbc7-trtzv": Phase="Pending", Reason="", readiness=false. Elapsed: 9.759665ms
Sep  1 11:24:16.845: INFO: Pod "webserver-deployment-7f5969cbc7-gfhxl": Phase="Pending", Reason="", readiness=false. Elapsed: 11.092026ms
Sep  1 11:24:16.845: INFO: Pod "webserver-deployment-7f5969cbc7-mkhzx": Phase="Pending", Reason="", readiness=false. Elapsed: 11.530975ms
Sep  1 11:24:16.846: INFO: Pod "webserver-deployment-7f5969cbc7-kjdcp": Phase="Pending", Reason="", readiness=false. Elapsed: 11.55851ms
Sep  1 11:24:16.847: INFO: Pod "webserver-deployment-7f5969cbc7-9dvkx": Phase="Pending", Reason="", readiness=false. Elapsed: 13.341775ms
Sep  1 11:24:16.847: INFO: Pod "webserver-deployment-7f5969cbc7-qt2vs": Phase="Pending", Reason="", readiness=false. Elapsed: 12.589726ms
Sep  1 11:24:16.850: INFO: Pod "webserver-deployment-7f5969cbc7-d9llm": Phase="Pending", Reason="", readiness=false. Elapsed: 16.048058ms
Sep  1 11:24:18.846: INFO: Pod "webserver-deployment-7f5969cbc7-hzg8k": Phase="Running", Reason="", readiness=true. Elapsed: 2.012702018s
Sep  1 11:24:18.846: INFO: Pod "webserver-deployment-7f5969cbc7-hzg8k" satisfied condition "running"
Sep  1 11:24:18.846: INFO: Pod "webserver-deployment-7f5969cbc7-87c9n": Phase="Running", Reason="", readiness=true. Elapsed: 2.01308475s
Sep  1 11:24:18.846: INFO: Pod "webserver-deployment-7f5969cbc7-87c9n" satisfied condition "running"
Sep  1 11:24:18.847: INFO: Pod "webserver-deployment-7f5969cbc7-tsk5m": Phase="Running", Reason="", readiness=true. Elapsed: 2.012566084s
Sep  1 11:24:18.847: INFO: Pod "webserver-deployment-7f5969cbc7-tsk5m" satisfied condition "running"
Sep  1 11:24:18.851: INFO: Pod "webserver-deployment-7f5969cbc7-9dvkx": Phase="Running", Reason="", readiness=true. Elapsed: 2.017792212s
Sep  1 11:24:18.851: INFO: Pod "webserver-deployment-7f5969cbc7-9dvkx" satisfied condition "running"
Sep  1 11:24:18.852: INFO: Pod "webserver-deployment-7f5969cbc7-kjdcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.017579596s
Sep  1 11:24:18.852: INFO: Pod "webserver-deployment-7f5969cbc7-kjdcp" satisfied condition "running"
Sep  1 11:24:18.852: INFO: Pod "webserver-deployment-7f5969cbc7-mkhzx": Phase="Running", Reason="", readiness=true. Elapsed: 2.018178331s
Sep  1 11:24:18.854: INFO: Pod "webserver-deployment-7f5969cbc7-mkhzx" satisfied condition "running"
Sep  1 11:24:18.853: INFO: Pod "webserver-deployment-7f5969cbc7-gfhxl": Phase="Running", Reason="", readiness=true. Elapsed: 2.018982413s
Sep  1 11:24:18.855: INFO: Pod "webserver-deployment-7f5969cbc7-gfhxl" satisfied condition "running"
Sep  1 11:24:18.855: INFO: Pod "webserver-deployment-7f5969cbc7-trtzv": Phase="Running", Reason="", readiness=true. Elapsed: 2.020569889s
Sep  1 11:24:18.855: INFO: Pod "webserver-deployment-7f5969cbc7-trtzv" satisfied condition "running"
Sep  1 11:24:18.854: INFO: Pod "webserver-deployment-7f5969cbc7-d9llm": Phase="Running", Reason="", readiness=true. Elapsed: 2.020167451s
Sep  1 11:24:18.855: INFO: Pod "webserver-deployment-7f5969cbc7-d9llm" satisfied condition "running"
Sep  1 11:24:18.853: INFO: Pod "webserver-deployment-7f5969cbc7-qt2vs": Phase="Running", Reason="", readiness=true. Elapsed: 2.019242768s
Sep  1 11:24:18.855: INFO: Pod "webserver-deployment-7f5969cbc7-qt2vs" satisfied condition "running"
Sep  1 11:24:18.855: INFO: Waiting for deployment "webserver-deployment" to complete
Sep  1 11:24:18.861: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep  1 11:24:18.871: INFO: Updating deployment webserver-deployment
Sep  1 11:24:18.871: INFO: Waiting for observed generation 2
Sep  1 11:24:20.878: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  1 11:24:20.881: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  1 11:24:20.883: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  1 11:24:20.892: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  1 11:24:20.892: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  1 11:24:20.894: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  1 11:24:20.900: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep  1 11:24:20.900: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep  1 11:24:20.913: INFO: Updating deployment webserver-deployment
Sep  1 11:24:20.913: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep  1 11:24:20.921: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  1 11:24:20.927: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  1 11:24:20.944: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6418  e488d62a-f059-40a6-b833-8ed2e71524c6 36894 3 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:24:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-09-01 11:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c09b808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-01 11:24:18 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-09-01 11:24:19 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep  1 11:24:20.956: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-6418  6e3ea7e8-adfd-484f-b980-79c7d84fc79c 36898 3 2023-09-01 11:24:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e488d62a-f059-40a6-b833-8ed2e71524c6 0xc00c09bd07 0xc00c09bd08}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:24:19 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-01 11:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e488d62a-f059-40a6-b833-8ed2e71524c6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c09bda8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  1 11:24:20.956: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep  1 11:24:20.957: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-6418  b1323d28-7001-42df-84ba-596b8d511047 36895 3 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e488d62a-f059-40a6-b833-8ed2e71524c6 0xc00c09bc17 0xc00c09bc18}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:24:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-01 11:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e488d62a-f059-40a6-b833-8ed2e71524c6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c09bca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep  1 11:24:20.986: INFO: Pod "webserver-deployment-7f5969cbc7-87c9n" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-87c9n webserver-deployment-7f5969cbc7- deployment-6418  337153d2-265a-42f5-ad36-84d85a1de4c2 36790 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d2277 0xc0033d2278}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v2ltj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v2ltj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.1.179,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d6d1214b724434d9f1f56daf7367c9ae811c12c23c7bcd421f9f06d32f00a0ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:20.987: INFO: Pod "webserver-deployment-7f5969cbc7-9dvkx" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9dvkx webserver-deployment-7f5969cbc7- deployment-6418  b137301f-6304-4514-a11b-a160fb41afcf 36792 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d2450 0xc0033d2451}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dvrfr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dvrfr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.1.213,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9ef0e8ea3fb3c9b7ee5f2def388a8c0875c2e8ec8a075ab4487132b57bc7eb14,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:20.987: INFO: Pod "webserver-deployment-7f5969cbc7-d9llm" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d9llm webserver-deployment-7f5969cbc7- deployment-6418  db3619a0-748f-4b49-9c12-478813b48b1b 36760 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d2620 0xc0033d2621}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lsrc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lsrc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.170,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3ea566dd5e43f904414abc7ca97b2a014210dbc8c3a6106a9d4d51a41313ba83,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:20.988: INFO: Pod "webserver-deployment-7f5969cbc7-gfhxl" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gfhxl webserver-deployment-7f5969cbc7- deployment-6418  9f4d03ec-8375-4e26-839d-27a8f8f94024 36756 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d27f0 0xc0033d27f1}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6p4p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6p4p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.1.35,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9d75a8295563cf65cd574ff40c4d79454b2144626cfeafa3bdb346abf013009c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:20.988: INFO: Pod "webserver-deployment-7f5969cbc7-gpn9n" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gpn9n webserver-deployment-7f5969cbc7- deployment-6418  54418fb9-f8ad-4d05-bf85-e9144ade6cb5 36902 0 2023-09-01 11:24:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d29c0 0xc0033d29c1}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kpqbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kpqbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:20.989: INFO: Pod "webserver-deployment-7f5969cbc7-hzg8k" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hzg8k webserver-deployment-7f5969cbc7- deployment-6418  9d084328-3154-4701-87fe-dcdd01b92498 36762 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d2af7 0xc0033d2af8}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sq545,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sq545,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.162,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://336515918a414e04d9a77c634110cb58df74e67f7d259c66ca7d6321fe4419da,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:20.992: INFO: Pod "webserver-deployment-7f5969cbc7-jc4rr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jc4rr webserver-deployment-7f5969cbc7- deployment-6418  460c0f8d-3e03-40da-b5c1-c61e89a8f68e 36907 0 2023-09-01 11:24:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d2cd0 0xc0033d2cd1}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9p966,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9p966,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:20.993: INFO: Pod "webserver-deployment-7f5969cbc7-mkhzx" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mkhzx webserver-deployment-7f5969cbc7- deployment-6418  0bdc04f6-a809-4ea4-bfdc-73873cae0702 36801 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d2e30 0xc0033d2e31}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zwxw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zwxw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.1.84,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://41e9e02433c5039b746ee71aa36fdb62ed92f8052ee445e904633355c1ac8ece,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:20.993: INFO: Pod "webserver-deployment-7f5969cbc7-phggj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-phggj webserver-deployment-7f5969cbc7- deployment-6418  27017e83-4d43-442c-af59-a514d26ccb26 36899 0 2023-09-01 11:24:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d3000 0xc0033d3001}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grn7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grn7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:20.994: INFO: Pod "webserver-deployment-7f5969cbc7-qt2vs" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qt2vs webserver-deployment-7f5969cbc7- deployment-6418  1f41c83e-4053-4f87-b51d-046d1cf493dd 36751 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d3150 0xc0033d3151}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xz7lp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xz7lp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.1.28,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://62181a76f4efb2d93dcd43b869f476ebe18c223780c169d513ec73d1690794c0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:20.994: INFO: Pod "webserver-deployment-7f5969cbc7-trtzv" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-trtzv webserver-deployment-7f5969cbc7- deployment-6418  d0cf69b9-4db2-4214-b886-92f9b7f4b5e0 36765 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d3330 0xc0033d3331}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wnmg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wnmg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.45,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://35e8be334b4459f96defdff3ef7797a6782c2f129e13e6f5f5bd49f4be027ec1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:20.994: INFO: Pod "webserver-deployment-d9f79cb5-5t4cz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5t4cz webserver-deployment-d9f79cb5- deployment-6418  813f358b-83d0-4b05-9911-b92d3a65860e 36903 0 2023-09-01 11:24:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 6e3ea7e8-adfd-484f-b980-79c7d84fc79c 0xc0033d350f 0xc0033d3520}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e3ea7e8-adfd-484f-b980-79c7d84fc79c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wsh7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wsh7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:20.995: INFO: Pod "webserver-deployment-d9f79cb5-9f78k" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9f78k webserver-deployment-d9f79cb5- deployment-6418  a74ab8c1-bc26-4b1f-89ff-8512261f1532 36825 0 2023-09-01 11:24:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 6e3ea7e8-adfd-484f-b980-79c7d84fc79c 0xc0033d3667 0xc0033d3668}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e3ea7e8-adfd-484f-b980-79c7d84fc79c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6fdz4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6fdz4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:,StartTime:2023-09-01 11:24:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:20.996: INFO: Pod "webserver-deployment-d9f79cb5-bspm6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bspm6 webserver-deployment-d9f79cb5- deployment-6418  19ce530c-c650-4047-9cc1-dd3e5bb7d0b3 36851 0 2023-09-01 11:24:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 6e3ea7e8-adfd-484f-b980-79c7d84fc79c 0xc0033d382f 0xc0033d3840}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e3ea7e8-adfd-484f-b980-79c7d84fc79c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7clpl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7clpl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:,StartTime:2023-09-01 11:24:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:20.999: INFO: Pod "webserver-deployment-d9f79cb5-bwn7k" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bwn7k webserver-deployment-d9f79cb5- deployment-6418  1555e4bb-dfc6-41e8-9c2a-e0a5ba6b454c 36857 0 2023-09-01 11:24:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 6e3ea7e8-adfd-484f-b980-79c7d84fc79c 0xc0033d39ff 0xc0033d3a10}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e3ea7e8-adfd-484f-b980-79c7d84fc79c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h59bz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h59bz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:,StartTime:2023-09-01 11:24:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:21.000: INFO: Pod "webserver-deployment-d9f79cb5-gqrkj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gqrkj webserver-deployment-d9f79cb5- deployment-6418  20161567-bd02-4b47-8db3-52c5b96183dc 36834 0 2023-09-01 11:24:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 6e3ea7e8-adfd-484f-b980-79c7d84fc79c 0xc0033d3bcf 0xc0033d3be0}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e3ea7e8-adfd-484f-b980-79c7d84fc79c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-27z6m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-27z6m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:,StartTime:2023-09-01 11:24:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  1 11:24:21.000: INFO: Pod "webserver-deployment-d9f79cb5-t2j9w" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t2j9w webserver-deployment-d9f79cb5- deployment-6418  70203924-a31a-4373-9c2b-74f066b125c6 36829 0 2023-09-01 11:24:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 6e3ea7e8-adfd-484f-b980-79c7d84fc79c 0xc0033d3d9f 0xc0033d3db0}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e3ea7e8-adfd-484f-b980-79c7d84fc79c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p7n9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p7n9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:,StartTime:2023-09-01 11:24:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  1 11:24:21.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6418" for this suite. 09/01/23 11:24:21.015
------------------------------
• [SLOW TEST] [6.297 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:24:14.765
    Sep  1 11:24:14.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename deployment 09/01/23 11:24:14.767
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:24:14.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:24:14.79
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Sep  1 11:24:14.795: INFO: Creating deployment "webserver-deployment"
    Sep  1 11:24:14.801: INFO: Waiting for observed generation 1
    Sep  1 11:24:16.821: INFO: Waiting for all required pods to come up
    Sep  1 11:24:16.832: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 09/01/23 11:24:16.833
    Sep  1 11:24:16.833: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hzg8k" in namespace "deployment-6418" to be "running"
    Sep  1 11:24:16.833: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-87c9n" in namespace "deployment-6418" to be "running"
    Sep  1 11:24:16.833: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9dvkx" in namespace "deployment-6418" to be "running"
    Sep  1 11:24:16.833: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-d9llm" in namespace "deployment-6418" to be "running"
    Sep  1 11:24:16.834: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-gfhxl" in namespace "deployment-6418" to be "running"
    Sep  1 11:24:16.834: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-mkhzx" in namespace "deployment-6418" to be "running"
    Sep  1 11:24:16.834: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-kjdcp" in namespace "deployment-6418" to be "running"
    Sep  1 11:24:16.834: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qt2vs" in namespace "deployment-6418" to be "running"
    Sep  1 11:24:16.834: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-trtzv" in namespace "deployment-6418" to be "running"
    Sep  1 11:24:16.833: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-tsk5m" in namespace "deployment-6418" to be "running"
    Sep  1 11:24:16.841: INFO: Pod "webserver-deployment-7f5969cbc7-87c9n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.05457ms
    Sep  1 11:24:16.842: INFO: Pod "webserver-deployment-7f5969cbc7-hzg8k": Phase="Pending", Reason="", readiness=false. Elapsed: 8.739038ms
    Sep  1 11:24:16.844: INFO: Pod "webserver-deployment-7f5969cbc7-tsk5m": Phase="Pending", Reason="", readiness=false. Elapsed: 9.415828ms
    Sep  1 11:24:16.844: INFO: Pod "webserver-deployment-7f5969cbc7-trtzv": Phase="Pending", Reason="", readiness=false. Elapsed: 9.759665ms
    Sep  1 11:24:16.845: INFO: Pod "webserver-deployment-7f5969cbc7-gfhxl": Phase="Pending", Reason="", readiness=false. Elapsed: 11.092026ms
    Sep  1 11:24:16.845: INFO: Pod "webserver-deployment-7f5969cbc7-mkhzx": Phase="Pending", Reason="", readiness=false. Elapsed: 11.530975ms
    Sep  1 11:24:16.846: INFO: Pod "webserver-deployment-7f5969cbc7-kjdcp": Phase="Pending", Reason="", readiness=false. Elapsed: 11.55851ms
    Sep  1 11:24:16.847: INFO: Pod "webserver-deployment-7f5969cbc7-9dvkx": Phase="Pending", Reason="", readiness=false. Elapsed: 13.341775ms
    Sep  1 11:24:16.847: INFO: Pod "webserver-deployment-7f5969cbc7-qt2vs": Phase="Pending", Reason="", readiness=false. Elapsed: 12.589726ms
    Sep  1 11:24:16.850: INFO: Pod "webserver-deployment-7f5969cbc7-d9llm": Phase="Pending", Reason="", readiness=false. Elapsed: 16.048058ms
    Sep  1 11:24:18.846: INFO: Pod "webserver-deployment-7f5969cbc7-hzg8k": Phase="Running", Reason="", readiness=true. Elapsed: 2.012702018s
    Sep  1 11:24:18.846: INFO: Pod "webserver-deployment-7f5969cbc7-hzg8k" satisfied condition "running"
    Sep  1 11:24:18.846: INFO: Pod "webserver-deployment-7f5969cbc7-87c9n": Phase="Running", Reason="", readiness=true. Elapsed: 2.01308475s
    Sep  1 11:24:18.846: INFO: Pod "webserver-deployment-7f5969cbc7-87c9n" satisfied condition "running"
    Sep  1 11:24:18.847: INFO: Pod "webserver-deployment-7f5969cbc7-tsk5m": Phase="Running", Reason="", readiness=true. Elapsed: 2.012566084s
    Sep  1 11:24:18.847: INFO: Pod "webserver-deployment-7f5969cbc7-tsk5m" satisfied condition "running"
    Sep  1 11:24:18.851: INFO: Pod "webserver-deployment-7f5969cbc7-9dvkx": Phase="Running", Reason="", readiness=true. Elapsed: 2.017792212s
    Sep  1 11:24:18.851: INFO: Pod "webserver-deployment-7f5969cbc7-9dvkx" satisfied condition "running"
    Sep  1 11:24:18.852: INFO: Pod "webserver-deployment-7f5969cbc7-kjdcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.017579596s
    Sep  1 11:24:18.852: INFO: Pod "webserver-deployment-7f5969cbc7-kjdcp" satisfied condition "running"
    Sep  1 11:24:18.852: INFO: Pod "webserver-deployment-7f5969cbc7-mkhzx": Phase="Running", Reason="", readiness=true. Elapsed: 2.018178331s
    Sep  1 11:24:18.854: INFO: Pod "webserver-deployment-7f5969cbc7-mkhzx" satisfied condition "running"
    Sep  1 11:24:18.853: INFO: Pod "webserver-deployment-7f5969cbc7-gfhxl": Phase="Running", Reason="", readiness=true. Elapsed: 2.018982413s
    Sep  1 11:24:18.855: INFO: Pod "webserver-deployment-7f5969cbc7-gfhxl" satisfied condition "running"
    Sep  1 11:24:18.855: INFO: Pod "webserver-deployment-7f5969cbc7-trtzv": Phase="Running", Reason="", readiness=true. Elapsed: 2.020569889s
    Sep  1 11:24:18.855: INFO: Pod "webserver-deployment-7f5969cbc7-trtzv" satisfied condition "running"
    Sep  1 11:24:18.854: INFO: Pod "webserver-deployment-7f5969cbc7-d9llm": Phase="Running", Reason="", readiness=true. Elapsed: 2.020167451s
    Sep  1 11:24:18.855: INFO: Pod "webserver-deployment-7f5969cbc7-d9llm" satisfied condition "running"
    Sep  1 11:24:18.853: INFO: Pod "webserver-deployment-7f5969cbc7-qt2vs": Phase="Running", Reason="", readiness=true. Elapsed: 2.019242768s
    Sep  1 11:24:18.855: INFO: Pod "webserver-deployment-7f5969cbc7-qt2vs" satisfied condition "running"
    Sep  1 11:24:18.855: INFO: Waiting for deployment "webserver-deployment" to complete
    Sep  1 11:24:18.861: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Sep  1 11:24:18.871: INFO: Updating deployment webserver-deployment
    Sep  1 11:24:18.871: INFO: Waiting for observed generation 2
    Sep  1 11:24:20.878: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Sep  1 11:24:20.881: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Sep  1 11:24:20.883: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep  1 11:24:20.892: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Sep  1 11:24:20.892: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Sep  1 11:24:20.894: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep  1 11:24:20.900: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Sep  1 11:24:20.900: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Sep  1 11:24:20.913: INFO: Updating deployment webserver-deployment
    Sep  1 11:24:20.913: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Sep  1 11:24:20.921: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Sep  1 11:24:20.927: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  1 11:24:20.944: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-6418  e488d62a-f059-40a6-b833-8ed2e71524c6 36894 3 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:24:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-09-01 11:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c09b808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-01 11:24:18 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-09-01 11:24:19 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Sep  1 11:24:20.956: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-6418  6e3ea7e8-adfd-484f-b980-79c7d84fc79c 36898 3 2023-09-01 11:24:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e488d62a-f059-40a6-b833-8ed2e71524c6 0xc00c09bd07 0xc00c09bd08}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:24:19 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-01 11:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e488d62a-f059-40a6-b833-8ed2e71524c6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c09bda8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  1 11:24:20.956: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Sep  1 11:24:20.957: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-6418  b1323d28-7001-42df-84ba-596b8d511047 36895 3 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e488d62a-f059-40a6-b833-8ed2e71524c6 0xc00c09bc17 0xc00c09bc18}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:24:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-01 11:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e488d62a-f059-40a6-b833-8ed2e71524c6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c09bca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Sep  1 11:24:20.986: INFO: Pod "webserver-deployment-7f5969cbc7-87c9n" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-87c9n webserver-deployment-7f5969cbc7- deployment-6418  337153d2-265a-42f5-ad36-84d85a1de4c2 36790 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d2277 0xc0033d2278}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v2ltj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v2ltj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.1.179,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d6d1214b724434d9f1f56daf7367c9ae811c12c23c7bcd421f9f06d32f00a0ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:20.987: INFO: Pod "webserver-deployment-7f5969cbc7-9dvkx" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9dvkx webserver-deployment-7f5969cbc7- deployment-6418  b137301f-6304-4514-a11b-a160fb41afcf 36792 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d2450 0xc0033d2451}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dvrfr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dvrfr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.1.213,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9ef0e8ea3fb3c9b7ee5f2def388a8c0875c2e8ec8a075ab4487132b57bc7eb14,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:20.987: INFO: Pod "webserver-deployment-7f5969cbc7-d9llm" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d9llm webserver-deployment-7f5969cbc7- deployment-6418  db3619a0-748f-4b49-9c12-478813b48b1b 36760 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d2620 0xc0033d2621}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lsrc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lsrc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.170,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3ea566dd5e43f904414abc7ca97b2a014210dbc8c3a6106a9d4d51a41313ba83,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:20.988: INFO: Pod "webserver-deployment-7f5969cbc7-gfhxl" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gfhxl webserver-deployment-7f5969cbc7- deployment-6418  9f4d03ec-8375-4e26-839d-27a8f8f94024 36756 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d27f0 0xc0033d27f1}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6p4p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6p4p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.1.35,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9d75a8295563cf65cd574ff40c4d79454b2144626cfeafa3bdb346abf013009c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:20.988: INFO: Pod "webserver-deployment-7f5969cbc7-gpn9n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gpn9n webserver-deployment-7f5969cbc7- deployment-6418  54418fb9-f8ad-4d05-bf85-e9144ade6cb5 36902 0 2023-09-01 11:24:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d29c0 0xc0033d29c1}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kpqbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kpqbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:20.989: INFO: Pod "webserver-deployment-7f5969cbc7-hzg8k" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hzg8k webserver-deployment-7f5969cbc7- deployment-6418  9d084328-3154-4701-87fe-dcdd01b92498 36762 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d2af7 0xc0033d2af8}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sq545,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sq545,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.162,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://336515918a414e04d9a77c634110cb58df74e67f7d259c66ca7d6321fe4419da,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:20.992: INFO: Pod "webserver-deployment-7f5969cbc7-jc4rr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jc4rr webserver-deployment-7f5969cbc7- deployment-6418  460c0f8d-3e03-40da-b5c1-c61e89a8f68e 36907 0 2023-09-01 11:24:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d2cd0 0xc0033d2cd1}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9p966,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9p966,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:20.993: INFO: Pod "webserver-deployment-7f5969cbc7-mkhzx" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mkhzx webserver-deployment-7f5969cbc7- deployment-6418  0bdc04f6-a809-4ea4-bfdc-73873cae0702 36801 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d2e30 0xc0033d2e31}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zwxw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zwxw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.1.84,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://41e9e02433c5039b746ee71aa36fdb62ed92f8052ee445e904633355c1ac8ece,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:20.993: INFO: Pod "webserver-deployment-7f5969cbc7-phggj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-phggj webserver-deployment-7f5969cbc7- deployment-6418  27017e83-4d43-442c-af59-a514d26ccb26 36899 0 2023-09-01 11:24:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d3000 0xc0033d3001}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grn7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grn7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:20.994: INFO: Pod "webserver-deployment-7f5969cbc7-qt2vs" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qt2vs webserver-deployment-7f5969cbc7- deployment-6418  1f41c83e-4053-4f87-b51d-046d1cf493dd 36751 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d3150 0xc0033d3151}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xz7lp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xz7lp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.1.28,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://62181a76f4efb2d93dcd43b869f476ebe18c223780c169d513ec73d1690794c0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:20.994: INFO: Pod "webserver-deployment-7f5969cbc7-trtzv" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-trtzv webserver-deployment-7f5969cbc7- deployment-6418  d0cf69b9-4db2-4214-b886-92f9b7f4b5e0 36765 0 2023-09-01 11:24:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1323d28-7001-42df-84ba-596b8d511047 0xc0033d3330 0xc0033d3331}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1323d28-7001-42df-84ba-596b8d511047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wnmg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wnmg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.45,StartTime:2023-09-01 11:24:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:24:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://35e8be334b4459f96defdff3ef7797a6782c2f129e13e6f5f5bd49f4be027ec1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:20.994: INFO: Pod "webserver-deployment-d9f79cb5-5t4cz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5t4cz webserver-deployment-d9f79cb5- deployment-6418  813f358b-83d0-4b05-9911-b92d3a65860e 36903 0 2023-09-01 11:24:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 6e3ea7e8-adfd-484f-b980-79c7d84fc79c 0xc0033d350f 0xc0033d3520}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e3ea7e8-adfd-484f-b980-79c7d84fc79c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wsh7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wsh7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:20.995: INFO: Pod "webserver-deployment-d9f79cb5-9f78k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9f78k webserver-deployment-d9f79cb5- deployment-6418  a74ab8c1-bc26-4b1f-89ff-8512261f1532 36825 0 2023-09-01 11:24:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 6e3ea7e8-adfd-484f-b980-79c7d84fc79c 0xc0033d3667 0xc0033d3668}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e3ea7e8-adfd-484f-b980-79c7d84fc79c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6fdz4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6fdz4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:,StartTime:2023-09-01 11:24:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:20.996: INFO: Pod "webserver-deployment-d9f79cb5-bspm6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bspm6 webserver-deployment-d9f79cb5- deployment-6418  19ce530c-c650-4047-9cc1-dd3e5bb7d0b3 36851 0 2023-09-01 11:24:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 6e3ea7e8-adfd-484f-b980-79c7d84fc79c 0xc0033d382f 0xc0033d3840}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e3ea7e8-adfd-484f-b980-79c7d84fc79c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7clpl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7clpl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:,StartTime:2023-09-01 11:24:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:20.999: INFO: Pod "webserver-deployment-d9f79cb5-bwn7k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bwn7k webserver-deployment-d9f79cb5- deployment-6418  1555e4bb-dfc6-41e8-9c2a-e0a5ba6b454c 36857 0 2023-09-01 11:24:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 6e3ea7e8-adfd-484f-b980-79c7d84fc79c 0xc0033d39ff 0xc0033d3a10}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e3ea7e8-adfd-484f-b980-79c7d84fc79c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h59bz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h59bz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:,StartTime:2023-09-01 11:24:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:21.000: INFO: Pod "webserver-deployment-d9f79cb5-gqrkj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gqrkj webserver-deployment-d9f79cb5- deployment-6418  20161567-bd02-4b47-8db3-52c5b96183dc 36834 0 2023-09-01 11:24:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 6e3ea7e8-adfd-484f-b980-79c7d84fc79c 0xc0033d3bcf 0xc0033d3be0}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e3ea7e8-adfd-484f-b980-79c7d84fc79c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-27z6m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-27z6m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:,StartTime:2023-09-01 11:24:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  1 11:24:21.000: INFO: Pod "webserver-deployment-d9f79cb5-t2j9w" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t2j9w webserver-deployment-d9f79cb5- deployment-6418  70203924-a31a-4373-9c2b-74f066b125c6 36829 0 2023-09-01 11:24:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 6e3ea7e8-adfd-484f-b980-79c7d84fc79c 0xc0033d3d9f 0xc0033d3db0}] [] [{kube-controller-manager Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e3ea7e8-adfd-484f-b980-79c7d84fc79c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:24:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p7n9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p7n9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:24:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:,StartTime:2023-09-01 11:24:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:24:21.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6418" for this suite. 09/01/23 11:24:21.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:24:21.09
Sep  1 11:24:21.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 11:24:21.093
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:24:21.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:24:21.164
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-d4672aa4-8c75-4a26-9aaf-cccd1c8eca4c 09/01/23 11:24:21.198
STEP: Creating the pod 09/01/23 11:24:21.211
Sep  1 11:24:21.224: INFO: Waiting up to 5m0s for pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b" in namespace "configmap-1511" to be "running"
Sep  1 11:24:21.230: INFO: Pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.642802ms
Sep  1 11:24:23.236: INFO: Pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011383034s
Sep  1 11:24:25.234: INFO: Pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009792356s
Sep  1 11:24:27.234: INFO: Pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009425832s
Sep  1 11:24:29.234: INFO: Pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009458151s
Sep  1 11:24:31.234: INFO: Pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b": Phase="Running", Reason="", readiness=false. Elapsed: 10.009862093s
Sep  1 11:24:31.235: INFO: Pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b" satisfied condition "running"
STEP: Waiting for pod with text data 09/01/23 11:24:31.235
STEP: Waiting for pod with binary data 09/01/23 11:24:31.241
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:24:31.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1511" for this suite. 09/01/23 11:24:31.251
------------------------------
• [SLOW TEST] [10.166 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:24:21.09
    Sep  1 11:24:21.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 11:24:21.093
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:24:21.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:24:21.164
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-d4672aa4-8c75-4a26-9aaf-cccd1c8eca4c 09/01/23 11:24:21.198
    STEP: Creating the pod 09/01/23 11:24:21.211
    Sep  1 11:24:21.224: INFO: Waiting up to 5m0s for pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b" in namespace "configmap-1511" to be "running"
    Sep  1 11:24:21.230: INFO: Pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.642802ms
    Sep  1 11:24:23.236: INFO: Pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011383034s
    Sep  1 11:24:25.234: INFO: Pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009792356s
    Sep  1 11:24:27.234: INFO: Pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009425832s
    Sep  1 11:24:29.234: INFO: Pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009458151s
    Sep  1 11:24:31.234: INFO: Pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b": Phase="Running", Reason="", readiness=false. Elapsed: 10.009862093s
    Sep  1 11:24:31.235: INFO: Pod "pod-configmaps-95277e91-8fc2-497d-965e-5ef07344447b" satisfied condition "running"
    STEP: Waiting for pod with text data 09/01/23 11:24:31.235
    STEP: Waiting for pod with binary data 09/01/23 11:24:31.241
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:24:31.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1511" for this suite. 09/01/23 11:24:31.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:24:31.256
Sep  1 11:24:31.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename runtimeclass 09/01/23 11:24:31.258
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:24:31.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:24:31.283
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Sep  1 11:24:31.318: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9211 to be scheduled
Sep  1 11:24:31.322: INFO: 1 pods are not scheduled: [runtimeclass-9211/test-runtimeclass-runtimeclass-9211-preconfigured-handler-9p7ld(4b830ff0-c166-463e-9006-1d654324bcf1)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  1 11:24:33.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9211" for this suite. 09/01/23 11:24:33.338
------------------------------
• [2.090 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:24:31.256
    Sep  1 11:24:31.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename runtimeclass 09/01/23 11:24:31.258
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:24:31.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:24:31.283
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Sep  1 11:24:31.318: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9211 to be scheduled
    Sep  1 11:24:31.322: INFO: 1 pods are not scheduled: [runtimeclass-9211/test-runtimeclass-runtimeclass-9211-preconfigured-handler-9p7ld(4b830ff0-c166-463e-9006-1d654324bcf1)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:24:33.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9211" for this suite. 09/01/23 11:24:33.338
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:24:33.351
Sep  1 11:24:33.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename custom-resource-definition 09/01/23 11:24:33.353
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:24:33.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:24:33.37
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 09/01/23 11:24:33.374
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/01/23 11:24:33.375
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/01/23 11:24:33.375
STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/01/23 11:24:33.375
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/01/23 11:24:33.377
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/01/23 11:24:33.377
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/01/23 11:24:33.379
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:24:33.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2180" for this suite. 09/01/23 11:24:33.383
------------------------------
• [0.037 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:24:33.351
    Sep  1 11:24:33.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename custom-resource-definition 09/01/23 11:24:33.353
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:24:33.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:24:33.37
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 09/01/23 11:24:33.374
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/01/23 11:24:33.375
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/01/23 11:24:33.375
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/01/23 11:24:33.375
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/01/23 11:24:33.377
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/01/23 11:24:33.377
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/01/23 11:24:33.379
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:24:33.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2180" for this suite. 09/01/23 11:24:33.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:24:33.39
Sep  1 11:24:33.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename resourcequota 09/01/23 11:24:33.392
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:24:33.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:24:33.41
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 09/01/23 11:24:33.414
STEP: Creating a ResourceQuota 09/01/23 11:24:38.417
STEP: Ensuring resource quota status is calculated 09/01/23 11:24:38.424
STEP: Creating a ReplicaSet 09/01/23 11:24:40.428
STEP: Ensuring resource quota status captures replicaset creation 09/01/23 11:24:40.443
STEP: Deleting a ReplicaSet 09/01/23 11:24:42.446
STEP: Ensuring resource quota status released usage 09/01/23 11:24:42.452
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  1 11:24:44.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7081" for this suite. 09/01/23 11:24:44.461
------------------------------
• [SLOW TEST] [11.077 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:24:33.39
    Sep  1 11:24:33.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename resourcequota 09/01/23 11:24:33.392
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:24:33.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:24:33.41
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 09/01/23 11:24:33.414
    STEP: Creating a ResourceQuota 09/01/23 11:24:38.417
    STEP: Ensuring resource quota status is calculated 09/01/23 11:24:38.424
    STEP: Creating a ReplicaSet 09/01/23 11:24:40.428
    STEP: Ensuring resource quota status captures replicaset creation 09/01/23 11:24:40.443
    STEP: Deleting a ReplicaSet 09/01/23 11:24:42.446
    STEP: Ensuring resource quota status released usage 09/01/23 11:24:42.452
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:24:44.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7081" for this suite. 09/01/23 11:24:44.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:24:44.474
Sep  1 11:24:44.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename job 09/01/23 11:24:44.476
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:24:44.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:24:44.502
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 09/01/23 11:24:44.506
STEP: Ensuring job reaches completions 09/01/23 11:24:44.514
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  1 11:24:58.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7860" for this suite. 09/01/23 11:24:58.523
------------------------------
• [SLOW TEST] [14.055 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:24:44.474
    Sep  1 11:24:44.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename job 09/01/23 11:24:44.476
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:24:44.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:24:44.502
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 09/01/23 11:24:44.506
    STEP: Ensuring job reaches completions 09/01/23 11:24:44.514
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:24:58.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7860" for this suite. 09/01/23 11:24:58.523
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:24:58.533
Sep  1 11:24:58.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 11:24:58.535
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:24:58.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:24:58.559
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 09/01/23 11:24:58.562
Sep  1 11:24:58.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep  1 11:24:58.659: INFO: stderr: ""
Sep  1 11:24:58.660: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 09/01/23 11:24:58.66
Sep  1 11:24:58.660: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep  1 11:24:58.660: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8080" to be "running and ready, or succeeded"
Sep  1 11:24:58.664: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.247568ms
Sep  1 11:24:58.664: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'k8s-worker-1.c.operations-lab.internal' to be 'Running' but was 'Pending'
Sep  1 11:25:00.670: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009544552s
Sep  1 11:25:00.670: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep  1 11:25:00.670: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 09/01/23 11:25:00.67
Sep  1 11:25:00.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 logs logs-generator logs-generator'
Sep  1 11:25:00.777: INFO: stderr: ""
Sep  1 11:25:00.777: INFO: stdout: "I0901 11:24:59.573677       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/czh 271\nI0901 11:24:59.773758       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/fmz5 488\nI0901 11:24:59.974294       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/752d 252\nI0901 11:25:00.173986       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/x8w 576\nI0901 11:25:00.374715       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/pmp 411\nI0901 11:25:00.573957       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/v6v 286\n"
STEP: limiting log lines 09/01/23 11:25:00.777
Sep  1 11:25:00.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 logs logs-generator logs-generator --tail=1'
Sep  1 11:25:00.882: INFO: stderr: ""
Sep  1 11:25:00.882: INFO: stdout: "I0901 11:25:00.774592       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/dm7 377\n"
Sep  1 11:25:00.882: INFO: got output "I0901 11:25:00.774592       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/dm7 377\n"
STEP: limiting log bytes 09/01/23 11:25:00.882
Sep  1 11:25:00.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 logs logs-generator logs-generator --limit-bytes=1'
Sep  1 11:25:00.978: INFO: stderr: ""
Sep  1 11:25:00.978: INFO: stdout: "I"
Sep  1 11:25:00.979: INFO: got output "I"
STEP: exposing timestamps 09/01/23 11:25:00.979
Sep  1 11:25:00.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 logs logs-generator logs-generator --tail=1 --timestamps'
Sep  1 11:25:01.124: INFO: stderr: ""
Sep  1 11:25:01.124: INFO: stdout: "2023-09-01T11:25:00.974355912Z I0901 11:25:00.974136       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/7x5 453\n"
Sep  1 11:25:01.125: INFO: got output "2023-09-01T11:25:00.974355912Z I0901 11:25:00.974136       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/7x5 453\n"
STEP: restricting to a time range 09/01/23 11:25:01.125
Sep  1 11:25:03.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 logs logs-generator logs-generator --since=1s'
Sep  1 11:25:03.735: INFO: stderr: ""
Sep  1 11:25:03.735: INFO: stdout: "I0901 11:25:02.773762       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/xt8g 441\nI0901 11:25:02.974075       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/kz2 467\nI0901 11:25:03.174517       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/7k2f 566\nI0901 11:25:03.373763       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/7qv 226\nI0901 11:25:03.574201       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/dvc 549\n"
Sep  1 11:25:03.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 logs logs-generator logs-generator --since=24h'
Sep  1 11:25:03.843: INFO: stderr: ""
Sep  1 11:25:03.843: INFO: stdout: "I0901 11:24:59.573677       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/czh 271\nI0901 11:24:59.773758       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/fmz5 488\nI0901 11:24:59.974294       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/752d 252\nI0901 11:25:00.173986       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/x8w 576\nI0901 11:25:00.374715       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/pmp 411\nI0901 11:25:00.573957       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/v6v 286\nI0901 11:25:00.774592       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/dm7 377\nI0901 11:25:00.974136       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/7x5 453\nI0901 11:25:01.174508       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/gptx 312\nI0901 11:25:01.373965       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/vb49 375\nI0901 11:25:01.574511       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/mdfp 599\nI0901 11:25:01.773720       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/ns8 533\nI0901 11:25:01.974233       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/vtz 310\nI0901 11:25:02.173729       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/24s 458\nI0901 11:25:02.373920       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/rb9 371\nI0901 11:25:02.574396       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/t9g 220\nI0901 11:25:02.773762       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/xt8g 441\nI0901 11:25:02.974075       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/kz2 467\nI0901 11:25:03.174517       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/7k2f 566\nI0901 11:25:03.373763       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/7qv 226\nI0901 11:25:03.574201       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/dvc 549\nI0901 11:25:03.774616       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/448 281\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Sep  1 11:25:03.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 delete pod logs-generator'
Sep  1 11:25:04.833: INFO: stderr: ""
Sep  1 11:25:04.833: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:04.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8080" for this suite. 09/01/23 11:25:04.838
------------------------------
• [SLOW TEST] [6.312 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:24:58.533
    Sep  1 11:24:58.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 11:24:58.535
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:24:58.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:24:58.559
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 09/01/23 11:24:58.562
    Sep  1 11:24:58.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Sep  1 11:24:58.659: INFO: stderr: ""
    Sep  1 11:24:58.660: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 09/01/23 11:24:58.66
    Sep  1 11:24:58.660: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Sep  1 11:24:58.660: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8080" to be "running and ready, or succeeded"
    Sep  1 11:24:58.664: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.247568ms
    Sep  1 11:24:58.664: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'k8s-worker-1.c.operations-lab.internal' to be 'Running' but was 'Pending'
    Sep  1 11:25:00.670: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009544552s
    Sep  1 11:25:00.670: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Sep  1 11:25:00.670: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 09/01/23 11:25:00.67
    Sep  1 11:25:00.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 logs logs-generator logs-generator'
    Sep  1 11:25:00.777: INFO: stderr: ""
    Sep  1 11:25:00.777: INFO: stdout: "I0901 11:24:59.573677       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/czh 271\nI0901 11:24:59.773758       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/fmz5 488\nI0901 11:24:59.974294       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/752d 252\nI0901 11:25:00.173986       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/x8w 576\nI0901 11:25:00.374715       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/pmp 411\nI0901 11:25:00.573957       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/v6v 286\n"
    STEP: limiting log lines 09/01/23 11:25:00.777
    Sep  1 11:25:00.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 logs logs-generator logs-generator --tail=1'
    Sep  1 11:25:00.882: INFO: stderr: ""
    Sep  1 11:25:00.882: INFO: stdout: "I0901 11:25:00.774592       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/dm7 377\n"
    Sep  1 11:25:00.882: INFO: got output "I0901 11:25:00.774592       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/dm7 377\n"
    STEP: limiting log bytes 09/01/23 11:25:00.882
    Sep  1 11:25:00.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 logs logs-generator logs-generator --limit-bytes=1'
    Sep  1 11:25:00.978: INFO: stderr: ""
    Sep  1 11:25:00.978: INFO: stdout: "I"
    Sep  1 11:25:00.979: INFO: got output "I"
    STEP: exposing timestamps 09/01/23 11:25:00.979
    Sep  1 11:25:00.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 logs logs-generator logs-generator --tail=1 --timestamps'
    Sep  1 11:25:01.124: INFO: stderr: ""
    Sep  1 11:25:01.124: INFO: stdout: "2023-09-01T11:25:00.974355912Z I0901 11:25:00.974136       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/7x5 453\n"
    Sep  1 11:25:01.125: INFO: got output "2023-09-01T11:25:00.974355912Z I0901 11:25:00.974136       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/7x5 453\n"
    STEP: restricting to a time range 09/01/23 11:25:01.125
    Sep  1 11:25:03.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 logs logs-generator logs-generator --since=1s'
    Sep  1 11:25:03.735: INFO: stderr: ""
    Sep  1 11:25:03.735: INFO: stdout: "I0901 11:25:02.773762       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/xt8g 441\nI0901 11:25:02.974075       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/kz2 467\nI0901 11:25:03.174517       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/7k2f 566\nI0901 11:25:03.373763       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/7qv 226\nI0901 11:25:03.574201       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/dvc 549\n"
    Sep  1 11:25:03.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 logs logs-generator logs-generator --since=24h'
    Sep  1 11:25:03.843: INFO: stderr: ""
    Sep  1 11:25:03.843: INFO: stdout: "I0901 11:24:59.573677       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/czh 271\nI0901 11:24:59.773758       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/fmz5 488\nI0901 11:24:59.974294       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/752d 252\nI0901 11:25:00.173986       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/x8w 576\nI0901 11:25:00.374715       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/pmp 411\nI0901 11:25:00.573957       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/v6v 286\nI0901 11:25:00.774592       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/dm7 377\nI0901 11:25:00.974136       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/7x5 453\nI0901 11:25:01.174508       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/gptx 312\nI0901 11:25:01.373965       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/vb49 375\nI0901 11:25:01.574511       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/mdfp 599\nI0901 11:25:01.773720       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/ns8 533\nI0901 11:25:01.974233       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/vtz 310\nI0901 11:25:02.173729       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/24s 458\nI0901 11:25:02.373920       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/rb9 371\nI0901 11:25:02.574396       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/t9g 220\nI0901 11:25:02.773762       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/xt8g 441\nI0901 11:25:02.974075       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/kz2 467\nI0901 11:25:03.174517       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/7k2f 566\nI0901 11:25:03.373763       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/7qv 226\nI0901 11:25:03.574201       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/dvc 549\nI0901 11:25:03.774616       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/448 281\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Sep  1 11:25:03.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-8080 delete pod logs-generator'
    Sep  1 11:25:04.833: INFO: stderr: ""
    Sep  1 11:25:04.833: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:04.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8080" for this suite. 09/01/23 11:25:04.838
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:04.846
Sep  1 11:25:04.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 11:25:04.849
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:04.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:04.867
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 09/01/23 11:25:04.871
Sep  1 11:25:04.879: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db" in namespace "downward-api-8109" to be "Succeeded or Failed"
Sep  1 11:25:04.884: INFO: Pod "downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.853697ms
Sep  1 11:25:06.888: INFO: Pod "downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db": Phase="Running", Reason="", readiness=false. Elapsed: 2.009270818s
Sep  1 11:25:08.888: INFO: Pod "downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008815592s
STEP: Saw pod success 09/01/23 11:25:08.888
Sep  1 11:25:08.888: INFO: Pod "downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db" satisfied condition "Succeeded or Failed"
Sep  1 11:25:08.892: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db container client-container: <nil>
STEP: delete the pod 09/01/23 11:25:08.899
Sep  1 11:25:08.913: INFO: Waiting for pod downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db to disappear
Sep  1 11:25:08.917: INFO: Pod downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:08.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8109" for this suite. 09/01/23 11:25:08.923
------------------------------
• [4.089 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:04.846
    Sep  1 11:25:04.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 11:25:04.849
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:04.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:04.867
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 09/01/23 11:25:04.871
    Sep  1 11:25:04.879: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db" in namespace "downward-api-8109" to be "Succeeded or Failed"
    Sep  1 11:25:04.884: INFO: Pod "downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.853697ms
    Sep  1 11:25:06.888: INFO: Pod "downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db": Phase="Running", Reason="", readiness=false. Elapsed: 2.009270818s
    Sep  1 11:25:08.888: INFO: Pod "downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008815592s
    STEP: Saw pod success 09/01/23 11:25:08.888
    Sep  1 11:25:08.888: INFO: Pod "downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db" satisfied condition "Succeeded or Failed"
    Sep  1 11:25:08.892: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db container client-container: <nil>
    STEP: delete the pod 09/01/23 11:25:08.899
    Sep  1 11:25:08.913: INFO: Waiting for pod downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db to disappear
    Sep  1 11:25:08.917: INFO: Pod downwardapi-volume-ab8164f2-bf62-4872-b3e4-d20f7ef518db no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:08.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8109" for this suite. 09/01/23 11:25:08.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:08.941
Sep  1 11:25:08.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename containers 09/01/23 11:25:08.943
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:08.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:08.963
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 09/01/23 11:25:08.967
Sep  1 11:25:08.974: INFO: Waiting up to 5m0s for pod "client-containers-aa75a67c-decb-4a67-983f-1288425280ef" in namespace "containers-6519" to be "Succeeded or Failed"
Sep  1 11:25:08.979: INFO: Pod "client-containers-aa75a67c-decb-4a67-983f-1288425280ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.967513ms
Sep  1 11:25:10.984: INFO: Pod "client-containers-aa75a67c-decb-4a67-983f-1288425280ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010034834s
Sep  1 11:25:12.985: INFO: Pod "client-containers-aa75a67c-decb-4a67-983f-1288425280ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010569396s
STEP: Saw pod success 09/01/23 11:25:12.985
Sep  1 11:25:12.985: INFO: Pod "client-containers-aa75a67c-decb-4a67-983f-1288425280ef" satisfied condition "Succeeded or Failed"
Sep  1 11:25:12.988: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod client-containers-aa75a67c-decb-4a67-983f-1288425280ef container agnhost-container: <nil>
STEP: delete the pod 09/01/23 11:25:12.995
Sep  1 11:25:13.005: INFO: Waiting for pod client-containers-aa75a67c-decb-4a67-983f-1288425280ef to disappear
Sep  1 11:25:13.009: INFO: Pod client-containers-aa75a67c-decb-4a67-983f-1288425280ef no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:13.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6519" for this suite. 09/01/23 11:25:13.013
------------------------------
• [4.080 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:08.941
    Sep  1 11:25:08.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename containers 09/01/23 11:25:08.943
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:08.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:08.963
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 09/01/23 11:25:08.967
    Sep  1 11:25:08.974: INFO: Waiting up to 5m0s for pod "client-containers-aa75a67c-decb-4a67-983f-1288425280ef" in namespace "containers-6519" to be "Succeeded or Failed"
    Sep  1 11:25:08.979: INFO: Pod "client-containers-aa75a67c-decb-4a67-983f-1288425280ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.967513ms
    Sep  1 11:25:10.984: INFO: Pod "client-containers-aa75a67c-decb-4a67-983f-1288425280ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010034834s
    Sep  1 11:25:12.985: INFO: Pod "client-containers-aa75a67c-decb-4a67-983f-1288425280ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010569396s
    STEP: Saw pod success 09/01/23 11:25:12.985
    Sep  1 11:25:12.985: INFO: Pod "client-containers-aa75a67c-decb-4a67-983f-1288425280ef" satisfied condition "Succeeded or Failed"
    Sep  1 11:25:12.988: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod client-containers-aa75a67c-decb-4a67-983f-1288425280ef container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 11:25:12.995
    Sep  1 11:25:13.005: INFO: Waiting for pod client-containers-aa75a67c-decb-4a67-983f-1288425280ef to disappear
    Sep  1 11:25:13.009: INFO: Pod client-containers-aa75a67c-decb-4a67-983f-1288425280ef no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:13.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6519" for this suite. 09/01/23 11:25:13.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:13.029
Sep  1 11:25:13.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename namespaces 09/01/23 11:25:13.031
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:13.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:13.052
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 09/01/23 11:25:13.056
STEP: patching the Namespace 09/01/23 11:25:13.075
STEP: get the Namespace and ensuring it has the label 09/01/23 11:25:13.08
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:13.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6441" for this suite. 09/01/23 11:25:13.09
STEP: Destroying namespace "nspatchtest-a028dcd9-23c9-4a1a-9887-4d7d8f2ce890-6027" for this suite. 09/01/23 11:25:13.1
------------------------------
• [0.077 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:13.029
    Sep  1 11:25:13.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename namespaces 09/01/23 11:25:13.031
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:13.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:13.052
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 09/01/23 11:25:13.056
    STEP: patching the Namespace 09/01/23 11:25:13.075
    STEP: get the Namespace and ensuring it has the label 09/01/23 11:25:13.08
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:13.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6441" for this suite. 09/01/23 11:25:13.09
    STEP: Destroying namespace "nspatchtest-a028dcd9-23c9-4a1a-9887-4d7d8f2ce890-6027" for this suite. 09/01/23 11:25:13.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:13.111
Sep  1 11:25:13.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 11:25:13.113
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:13.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:13.134
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 11:25:13.181
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:25:13.557
STEP: Deploying the webhook pod 09/01/23 11:25:13.577
STEP: Wait for the deployment to be ready 09/01/23 11:25:13.605
Sep  1 11:25:13.623: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/01/23 11:25:15.632
STEP: Verifying the service has paired with the endpoint 09/01/23 11:25:15.648
Sep  1 11:25:16.649: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/01/23 11:25:16.653
STEP: create a namespace for the webhook 09/01/23 11:25:16.675
STEP: create a configmap should be unconditionally rejected by the webhook 09/01/23 11:25:16.684
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:16.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9578" for this suite. 09/01/23 11:25:16.794
STEP: Destroying namespace "webhook-9578-markers" for this suite. 09/01/23 11:25:16.813
------------------------------
• [3.716 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:13.111
    Sep  1 11:25:13.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 11:25:13.113
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:13.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:13.134
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 11:25:13.181
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:25:13.557
    STEP: Deploying the webhook pod 09/01/23 11:25:13.577
    STEP: Wait for the deployment to be ready 09/01/23 11:25:13.605
    Sep  1 11:25:13.623: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/01/23 11:25:15.632
    STEP: Verifying the service has paired with the endpoint 09/01/23 11:25:15.648
    Sep  1 11:25:16.649: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/01/23 11:25:16.653
    STEP: create a namespace for the webhook 09/01/23 11:25:16.675
    STEP: create a configmap should be unconditionally rejected by the webhook 09/01/23 11:25:16.684
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:16.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9578" for this suite. 09/01/23 11:25:16.794
    STEP: Destroying namespace "webhook-9578-markers" for this suite. 09/01/23 11:25:16.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:16.833
Sep  1 11:25:16.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename proxy 09/01/23 11:25:16.836
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:16.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:16.869
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 09/01/23 11:25:16.905
STEP: creating replication controller proxy-service-9nj99 in namespace proxy-7711 09/01/23 11:25:16.905
I0901 11:25:16.922630      19 runners.go:193] Created replication controller with name: proxy-service-9nj99, namespace: proxy-7711, replica count: 1
I0901 11:25:17.973243      19 runners.go:193] proxy-service-9nj99 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0901 11:25:18.973653      19 runners.go:193] proxy-service-9nj99 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  1 11:25:18.979: INFO: setup took 2.105642886s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/01/23 11:25:18.98
Sep  1 11:25:19.019: INFO: (0) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 37.415819ms)
Sep  1 11:25:19.020: INFO: (0) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 38.027578ms)
Sep  1 11:25:19.020: INFO: (0) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 38.795103ms)
Sep  1 11:25:19.022: INFO: (0) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 41.071016ms)
Sep  1 11:25:19.022: INFO: (0) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 40.657265ms)
Sep  1 11:25:19.022: INFO: (0) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 40.789368ms)
Sep  1 11:25:19.023: INFO: (0) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 40.887121ms)
Sep  1 11:25:19.027: INFO: (0) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 45.269718ms)
Sep  1 11:25:19.028: INFO: (0) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 46.888299ms)
Sep  1 11:25:19.029: INFO: (0) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 48.26751ms)
Sep  1 11:25:19.029: INFO: (0) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 47.418221ms)
Sep  1 11:25:19.029: INFO: (0) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 48.688551ms)
Sep  1 11:25:19.030: INFO: (0) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 48.354197ms)
Sep  1 11:25:19.030: INFO: (0) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 48.497218ms)
Sep  1 11:25:19.031: INFO: (0) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 51.007445ms)
Sep  1 11:25:19.032: INFO: (0) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 51.185038ms)
Sep  1 11:25:19.055: INFO: (1) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 22.863342ms)
Sep  1 11:25:19.056: INFO: (1) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 23.900356ms)
Sep  1 11:25:19.057: INFO: (1) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 24.85792ms)
Sep  1 11:25:19.057: INFO: (1) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 25.023763ms)
Sep  1 11:25:19.057: INFO: (1) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 25.366751ms)
Sep  1 11:25:19.058: INFO: (1) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 25.524774ms)
Sep  1 11:25:19.058: INFO: (1) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 25.582932ms)
Sep  1 11:25:19.058: INFO: (1) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 25.429438ms)
Sep  1 11:25:19.059: INFO: (1) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 26.672994ms)
Sep  1 11:25:19.062: INFO: (1) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 28.896296ms)
Sep  1 11:25:19.063: INFO: (1) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 30.668036ms)
Sep  1 11:25:19.063: INFO: (1) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 31.005804ms)
Sep  1 11:25:19.063: INFO: (1) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 30.589343ms)
Sep  1 11:25:19.063: INFO: (1) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 30.577848ms)
Sep  1 11:25:19.072: INFO: (1) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 39.735905ms)
Sep  1 11:25:19.072: INFO: (1) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 39.792979ms)
Sep  1 11:25:19.084: INFO: (2) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 10.436397ms)
Sep  1 11:25:19.084: INFO: (2) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 10.972687ms)
Sep  1 11:25:19.085: INFO: (2) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 11.514739ms)
Sep  1 11:25:19.085: INFO: (2) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 11.472587ms)
Sep  1 11:25:19.087: INFO: (2) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 14.011174ms)
Sep  1 11:25:19.088: INFO: (2) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 14.570935ms)
Sep  1 11:25:19.089: INFO: (2) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 16.050483ms)
Sep  1 11:25:19.089: INFO: (2) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 16.014136ms)
Sep  1 11:25:19.090: INFO: (2) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 16.855889ms)
Sep  1 11:25:19.091: INFO: (2) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 17.927182ms)
Sep  1 11:25:19.099: INFO: (2) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 26.265442ms)
Sep  1 11:25:19.104: INFO: (2) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 30.843205ms)
Sep  1 11:25:19.104: INFO: (2) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 30.864135ms)
Sep  1 11:25:19.104: INFO: (2) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 30.831077ms)
Sep  1 11:25:19.104: INFO: (2) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 31.081704ms)
Sep  1 11:25:19.104: INFO: (2) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 31.076587ms)
Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 11.416822ms)
Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 12.005333ms)
Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 12.163998ms)
Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 12.164825ms)
Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 12.950469ms)
Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 12.256193ms)
Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 12.5043ms)
Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 12.048237ms)
Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 13.185823ms)
Sep  1 11:25:19.118: INFO: (3) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 12.82913ms)
Sep  1 11:25:19.121: INFO: (3) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 15.281393ms)
Sep  1 11:25:19.121: INFO: (3) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 16.210772ms)
Sep  1 11:25:19.121: INFO: (3) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 15.97561ms)
Sep  1 11:25:19.122: INFO: (3) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 16.750478ms)
Sep  1 11:25:19.123: INFO: (3) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 17.980576ms)
Sep  1 11:25:19.123: INFO: (3) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 17.752327ms)
Sep  1 11:25:19.135: INFO: (4) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 11.697559ms)
Sep  1 11:25:19.136: INFO: (4) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 12.410247ms)
Sep  1 11:25:19.137: INFO: (4) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 13.623578ms)
Sep  1 11:25:19.143: INFO: (4) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 18.913831ms)
Sep  1 11:25:19.143: INFO: (4) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 18.932488ms)
Sep  1 11:25:19.143: INFO: (4) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 19.087571ms)
Sep  1 11:25:19.144: INFO: (4) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 20.641033ms)
Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 20.786846ms)
Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 20.76087ms)
Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 20.663914ms)
Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 20.37901ms)
Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 20.557138ms)
Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 20.493572ms)
Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 20.473248ms)
Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 20.782488ms)
Sep  1 11:25:19.146: INFO: (4) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 21.834352ms)
Sep  1 11:25:19.160: INFO: (5) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 14.291515ms)
Sep  1 11:25:19.160: INFO: (5) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 14.022251ms)
Sep  1 11:25:19.161: INFO: (5) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 14.645004ms)
Sep  1 11:25:19.161: INFO: (5) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 14.772527ms)
Sep  1 11:25:19.161: INFO: (5) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 15.076565ms)
Sep  1 11:25:19.161: INFO: (5) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 15.008276ms)
Sep  1 11:25:19.161: INFO: (5) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 14.684921ms)
Sep  1 11:25:19.161: INFO: (5) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 14.799755ms)
Sep  1 11:25:19.161: INFO: (5) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 14.75318ms)
Sep  1 11:25:19.168: INFO: (5) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 21.128805ms)
Sep  1 11:25:19.168: INFO: (5) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 21.553321ms)
Sep  1 11:25:19.168: INFO: (5) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 21.727132ms)
Sep  1 11:25:19.168: INFO: (5) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 21.674729ms)
Sep  1 11:25:19.168: INFO: (5) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 21.739754ms)
Sep  1 11:25:19.168: INFO: (5) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 21.559111ms)
Sep  1 11:25:19.168: INFO: (5) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 21.445958ms)
Sep  1 11:25:19.181: INFO: (6) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 11.001942ms)
Sep  1 11:25:19.181: INFO: (6) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 11.139366ms)
Sep  1 11:25:19.181: INFO: (6) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 10.843939ms)
Sep  1 11:25:19.181: INFO: (6) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 11.755553ms)
Sep  1 11:25:19.181: INFO: (6) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 11.269182ms)
Sep  1 11:25:19.181: INFO: (6) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 11.59958ms)
Sep  1 11:25:19.187: INFO: (6) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 17.275004ms)
Sep  1 11:25:19.187: INFO: (6) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 17.224977ms)
Sep  1 11:25:19.187: INFO: (6) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 17.595362ms)
Sep  1 11:25:19.187: INFO: (6) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 17.518344ms)
Sep  1 11:25:19.187: INFO: (6) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 17.811165ms)
Sep  1 11:25:19.188: INFO: (6) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 17.81172ms)
Sep  1 11:25:19.188: INFO: (6) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 18.761727ms)
Sep  1 11:25:19.192: INFO: (6) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 22.212618ms)
Sep  1 11:25:19.192: INFO: (6) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 22.07991ms)
Sep  1 11:25:19.192: INFO: (6) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 22.49777ms)
Sep  1 11:25:19.206: INFO: (7) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 12.818238ms)
Sep  1 11:25:19.208: INFO: (7) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 14.753262ms)
Sep  1 11:25:19.208: INFO: (7) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 15.085352ms)
Sep  1 11:25:19.209: INFO: (7) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 16.098083ms)
Sep  1 11:25:19.209: INFO: (7) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 15.956529ms)
Sep  1 11:25:19.210: INFO: (7) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 16.935056ms)
Sep  1 11:25:19.211: INFO: (7) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 17.368918ms)
Sep  1 11:25:19.211: INFO: (7) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 18.2283ms)
Sep  1 11:25:19.211: INFO: (7) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 18.187248ms)
Sep  1 11:25:19.211: INFO: (7) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 18.206827ms)
Sep  1 11:25:19.211: INFO: (7) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 18.568414ms)
Sep  1 11:25:19.212: INFO: (7) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 18.351356ms)
Sep  1 11:25:19.212: INFO: (7) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 18.847851ms)
Sep  1 11:25:19.212: INFO: (7) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 18.752624ms)
Sep  1 11:25:19.212: INFO: (7) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 18.518562ms)
Sep  1 11:25:19.213: INFO: (7) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 20.668991ms)
Sep  1 11:25:19.231: INFO: (8) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 17.388917ms)
Sep  1 11:25:19.231: INFO: (8) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 17.922939ms)
Sep  1 11:25:19.232: INFO: (8) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 17.46461ms)
Sep  1 11:25:19.233: INFO: (8) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 19.318844ms)
Sep  1 11:25:19.234: INFO: (8) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 19.983923ms)
Sep  1 11:25:19.234: INFO: (8) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 20.013052ms)
Sep  1 11:25:19.234: INFO: (8) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 20.273783ms)
Sep  1 11:25:19.234: INFO: (8) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 20.024438ms)
Sep  1 11:25:19.234: INFO: (8) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 20.013437ms)
Sep  1 11:25:19.236: INFO: (8) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 22.035906ms)
Sep  1 11:25:19.236: INFO: (8) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 22.169017ms)
Sep  1 11:25:19.237: INFO: (8) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 23.211952ms)
Sep  1 11:25:19.237: INFO: (8) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 23.116754ms)
Sep  1 11:25:19.238: INFO: (8) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 23.322199ms)
Sep  1 11:25:19.238: INFO: (8) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 23.53683ms)
Sep  1 11:25:19.239: INFO: (8) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 24.617251ms)
Sep  1 11:25:19.246: INFO: (9) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 6.484712ms)
Sep  1 11:25:19.246: INFO: (9) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 6.980164ms)
Sep  1 11:25:19.253: INFO: (9) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 13.981754ms)
Sep  1 11:25:19.257: INFO: (9) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 16.966513ms)
Sep  1 11:25:19.260: INFO: (9) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 19.754761ms)
Sep  1 11:25:19.260: INFO: (9) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 20.440414ms)
Sep  1 11:25:19.260: INFO: (9) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 20.580805ms)
Sep  1 11:25:19.260: INFO: (9) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 20.708849ms)
Sep  1 11:25:19.260: INFO: (9) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 20.354881ms)
Sep  1 11:25:19.262: INFO: (9) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 22.251033ms)
Sep  1 11:25:19.262: INFO: (9) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 22.433617ms)
Sep  1 11:25:19.263: INFO: (9) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 22.940449ms)
Sep  1 11:25:19.263: INFO: (9) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 23.145748ms)
Sep  1 11:25:19.263: INFO: (9) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 23.520129ms)
Sep  1 11:25:19.263: INFO: (9) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 23.667752ms)
Sep  1 11:25:19.265: INFO: (9) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 24.59127ms)
Sep  1 11:25:19.279: INFO: (10) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 13.944368ms)
Sep  1 11:25:19.279: INFO: (10) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 13.582125ms)
Sep  1 11:25:19.280: INFO: (10) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 14.249847ms)
Sep  1 11:25:19.280: INFO: (10) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 14.289788ms)
Sep  1 11:25:19.281: INFO: (10) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 15.912611ms)
Sep  1 11:25:19.281: INFO: (10) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 15.662476ms)
Sep  1 11:25:19.286: INFO: (10) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 20.321943ms)
Sep  1 11:25:19.288: INFO: (10) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 22.863112ms)
Sep  1 11:25:19.288: INFO: (10) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 23.738524ms)
Sep  1 11:25:19.289: INFO: (10) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 23.204709ms)
Sep  1 11:25:19.289: INFO: (10) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 23.479488ms)
Sep  1 11:25:19.289: INFO: (10) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 23.739911ms)
Sep  1 11:25:19.289: INFO: (10) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 24.011093ms)
Sep  1 11:25:19.289: INFO: (10) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 23.388285ms)
Sep  1 11:25:19.289: INFO: (10) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 23.628281ms)
Sep  1 11:25:19.290: INFO: (10) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 25.286614ms)
Sep  1 11:25:19.301: INFO: (11) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 10.301938ms)
Sep  1 11:25:19.301: INFO: (11) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 10.275236ms)
Sep  1 11:25:19.301: INFO: (11) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 10.946708ms)
Sep  1 11:25:19.301: INFO: (11) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 10.408752ms)
Sep  1 11:25:19.301: INFO: (11) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 10.129365ms)
Sep  1 11:25:19.302: INFO: (11) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 10.066092ms)
Sep  1 11:25:19.302: INFO: (11) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 10.383117ms)
Sep  1 11:25:19.302: INFO: (11) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 10.191619ms)
Sep  1 11:25:19.303: INFO: (11) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 11.948443ms)
Sep  1 11:25:19.303: INFO: (11) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 11.717305ms)
Sep  1 11:25:19.306: INFO: (11) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 14.948626ms)
Sep  1 11:25:19.306: INFO: (11) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 15.785875ms)
Sep  1 11:25:19.306: INFO: (11) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 15.097989ms)
Sep  1 11:25:19.307: INFO: (11) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 15.251227ms)
Sep  1 11:25:19.307: INFO: (11) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 15.59752ms)
Sep  1 11:25:19.307: INFO: (11) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 15.9207ms)
Sep  1 11:25:19.321: INFO: (12) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 13.711456ms)
Sep  1 11:25:19.322: INFO: (12) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 13.750966ms)
Sep  1 11:25:19.322: INFO: (12) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 14.372583ms)
Sep  1 11:25:19.322: INFO: (12) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 14.960707ms)
Sep  1 11:25:19.323: INFO: (12) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 15.137009ms)
Sep  1 11:25:19.323: INFO: (12) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 14.813832ms)
Sep  1 11:25:19.323: INFO: (12) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 15.138865ms)
Sep  1 11:25:19.323: INFO: (12) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 15.250034ms)
Sep  1 11:25:19.322: INFO: (12) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 14.755337ms)
Sep  1 11:25:19.323: INFO: (12) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 15.133526ms)
Sep  1 11:25:19.326: INFO: (12) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 18.527669ms)
Sep  1 11:25:19.326: INFO: (12) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 18.865455ms)
Sep  1 11:25:19.327: INFO: (12) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 19.627251ms)
Sep  1 11:25:19.327: INFO: (12) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 19.99781ms)
Sep  1 11:25:19.328: INFO: (12) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 20.046956ms)
Sep  1 11:25:19.328: INFO: (12) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 20.456597ms)
Sep  1 11:25:19.335: INFO: (13) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 6.93597ms)
Sep  1 11:25:19.344: INFO: (13) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 14.945647ms)
Sep  1 11:25:19.344: INFO: (13) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 15.918625ms)
Sep  1 11:25:19.345: INFO: (13) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 16.201608ms)
Sep  1 11:25:19.345: INFO: (13) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 16.344834ms)
Sep  1 11:25:19.345: INFO: (13) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 16.262882ms)
Sep  1 11:25:19.345: INFO: (13) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 17.112132ms)
Sep  1 11:25:19.345: INFO: (13) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 16.752931ms)
Sep  1 11:25:19.345: INFO: (13) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 16.379756ms)
Sep  1 11:25:19.345: INFO: (13) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 16.77316ms)
Sep  1 11:25:19.346: INFO: (13) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 17.027846ms)
Sep  1 11:25:19.346: INFO: (13) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 17.277391ms)
Sep  1 11:25:19.346: INFO: (13) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 17.611277ms)
Sep  1 11:25:19.346: INFO: (13) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 17.459098ms)
Sep  1 11:25:19.346: INFO: (13) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 17.290608ms)
Sep  1 11:25:19.346: INFO: (13) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 17.436931ms)
Sep  1 11:25:19.355: INFO: (14) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 7.552055ms)
Sep  1 11:25:19.356: INFO: (14) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 8.02159ms)
Sep  1 11:25:19.356: INFO: (14) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 8.389283ms)
Sep  1 11:25:19.357: INFO: (14) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 8.684559ms)
Sep  1 11:25:19.360: INFO: (14) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 11.394519ms)
Sep  1 11:25:19.360: INFO: (14) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 11.475296ms)
Sep  1 11:25:19.360: INFO: (14) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 11.92316ms)
Sep  1 11:25:19.361: INFO: (14) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 12.683686ms)
Sep  1 11:25:19.361: INFO: (14) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 12.414106ms)
Sep  1 11:25:19.361: INFO: (14) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 12.483795ms)
Sep  1 11:25:19.363: INFO: (14) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 14.151137ms)
Sep  1 11:25:19.363: INFO: (14) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 14.658125ms)
Sep  1 11:25:19.364: INFO: (14) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 15.338697ms)
Sep  1 11:25:19.364: INFO: (14) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 15.10373ms)
Sep  1 11:25:19.364: INFO: (14) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 15.371612ms)
Sep  1 11:25:19.364: INFO: (14) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 15.418392ms)
Sep  1 11:25:19.377: INFO: (15) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 12.432082ms)
Sep  1 11:25:19.378: INFO: (15) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 13.336363ms)
Sep  1 11:25:19.377: INFO: (15) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 12.653113ms)
Sep  1 11:25:19.379: INFO: (15) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 13.915426ms)
Sep  1 11:25:19.379: INFO: (15) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 13.885098ms)
Sep  1 11:25:19.380: INFO: (15) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 15.975558ms)
Sep  1 11:25:19.381: INFO: (15) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 16.083766ms)
Sep  1 11:25:19.381: INFO: (15) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 16.725329ms)
Sep  1 11:25:19.381: INFO: (15) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 16.57282ms)
Sep  1 11:25:19.381: INFO: (15) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 16.525132ms)
Sep  1 11:25:19.381: INFO: (15) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 16.818349ms)
Sep  1 11:25:19.382: INFO: (15) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 17.285813ms)
Sep  1 11:25:19.382: INFO: (15) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 16.777483ms)
Sep  1 11:25:19.383: INFO: (15) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 18.345804ms)
Sep  1 11:25:19.383: INFO: (15) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 18.778737ms)
Sep  1 11:25:19.384: INFO: (15) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 18.811617ms)
Sep  1 11:25:19.392: INFO: (16) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 7.58029ms)
Sep  1 11:25:19.392: INFO: (16) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 7.964911ms)
Sep  1 11:25:19.392: INFO: (16) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 7.306127ms)
Sep  1 11:25:19.392: INFO: (16) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 7.646503ms)
Sep  1 11:25:19.397: INFO: (16) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 12.328696ms)
Sep  1 11:25:19.397: INFO: (16) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 12.657316ms)
Sep  1 11:25:19.397: INFO: (16) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 12.597652ms)
Sep  1 11:25:19.397: INFO: (16) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 12.989399ms)
Sep  1 11:25:19.397: INFO: (16) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 12.692713ms)
Sep  1 11:25:19.400: INFO: (16) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 15.469382ms)
Sep  1 11:25:19.404: INFO: (16) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 19.840492ms)
Sep  1 11:25:19.404: INFO: (16) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 19.731148ms)
Sep  1 11:25:19.405: INFO: (16) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 20.24301ms)
Sep  1 11:25:19.405: INFO: (16) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 20.715915ms)
Sep  1 11:25:19.405: INFO: (16) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 20.784802ms)
Sep  1 11:25:19.406: INFO: (16) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 21.50159ms)
Sep  1 11:25:19.416: INFO: (17) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 9.618989ms)
Sep  1 11:25:19.416: INFO: (17) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 9.354188ms)
Sep  1 11:25:19.416: INFO: (17) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 9.55739ms)
Sep  1 11:25:19.417: INFO: (17) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 10.266727ms)
Sep  1 11:25:19.417: INFO: (17) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 10.119191ms)
Sep  1 11:25:19.417: INFO: (17) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 9.997799ms)
Sep  1 11:25:19.417: INFO: (17) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 10.074653ms)
Sep  1 11:25:19.417: INFO: (17) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 10.876969ms)
Sep  1 11:25:19.417: INFO: (17) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 10.270964ms)
Sep  1 11:25:19.417: INFO: (17) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 10.621693ms)
Sep  1 11:25:19.420: INFO: (17) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 13.210815ms)
Sep  1 11:25:19.420: INFO: (17) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 13.235656ms)
Sep  1 11:25:19.420: INFO: (17) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 13.846533ms)
Sep  1 11:25:19.420: INFO: (17) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 14.019962ms)
Sep  1 11:25:19.421: INFO: (17) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 14.061081ms)
Sep  1 11:25:19.421: INFO: (17) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 14.355816ms)
Sep  1 11:25:19.432: INFO: (18) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 10.646316ms)
Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 11.224838ms)
Sep  1 11:25:19.432: INFO: (18) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 10.711047ms)
Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 11.168256ms)
Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 10.949361ms)
Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 11.316295ms)
Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 11.042983ms)
Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 11.599536ms)
Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 11.978947ms)
Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 11.632069ms)
Sep  1 11:25:19.436: INFO: (18) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 13.830205ms)
Sep  1 11:25:19.436: INFO: (18) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 14.214763ms)
Sep  1 11:25:19.436: INFO: (18) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 14.354657ms)
Sep  1 11:25:19.436: INFO: (18) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 13.95749ms)
Sep  1 11:25:19.436: INFO: (18) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 14.553968ms)
Sep  1 11:25:19.436: INFO: (18) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 14.236076ms)
Sep  1 11:25:19.449: INFO: (19) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 11.24823ms)
Sep  1 11:25:19.449: INFO: (19) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 11.151015ms)
Sep  1 11:25:19.449: INFO: (19) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 10.899539ms)
Sep  1 11:25:19.449: INFO: (19) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 11.584845ms)
Sep  1 11:25:19.450: INFO: (19) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 12.82793ms)
Sep  1 11:25:19.450: INFO: (19) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 13.206432ms)
Sep  1 11:25:19.451: INFO: (19) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 13.653976ms)
Sep  1 11:25:19.451: INFO: (19) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 13.172739ms)
Sep  1 11:25:19.451: INFO: (19) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 13.762557ms)
Sep  1 11:25:19.452: INFO: (19) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 13.767967ms)
Sep  1 11:25:19.452: INFO: (19) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 14.044311ms)
Sep  1 11:25:19.450: INFO: (19) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 12.769696ms)
Sep  1 11:25:19.452: INFO: (19) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 14.61312ms)
Sep  1 11:25:19.453: INFO: (19) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 14.895104ms)
Sep  1 11:25:19.453: INFO: (19) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 15.660261ms)
Sep  1 11:25:19.453: INFO: (19) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 15.959515ms)
STEP: deleting ReplicationController proxy-service-9nj99 in namespace proxy-7711, will wait for the garbage collector to delete the pods 09/01/23 11:25:19.453
Sep  1 11:25:19.514: INFO: Deleting ReplicationController proxy-service-9nj99 took: 6.353799ms
Sep  1 11:25:19.615: INFO: Terminating ReplicationController proxy-service-9nj99 pods took: 101.114627ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:22.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7711" for this suite. 09/01/23 11:25:22.028
------------------------------
• [SLOW TEST] [5.214 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:16.833
    Sep  1 11:25:16.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename proxy 09/01/23 11:25:16.836
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:16.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:16.869
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 09/01/23 11:25:16.905
    STEP: creating replication controller proxy-service-9nj99 in namespace proxy-7711 09/01/23 11:25:16.905
    I0901 11:25:16.922630      19 runners.go:193] Created replication controller with name: proxy-service-9nj99, namespace: proxy-7711, replica count: 1
    I0901 11:25:17.973243      19 runners.go:193] proxy-service-9nj99 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0901 11:25:18.973653      19 runners.go:193] proxy-service-9nj99 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  1 11:25:18.979: INFO: setup took 2.105642886s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/01/23 11:25:18.98
    Sep  1 11:25:19.019: INFO: (0) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 37.415819ms)
    Sep  1 11:25:19.020: INFO: (0) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 38.027578ms)
    Sep  1 11:25:19.020: INFO: (0) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 38.795103ms)
    Sep  1 11:25:19.022: INFO: (0) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 41.071016ms)
    Sep  1 11:25:19.022: INFO: (0) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 40.657265ms)
    Sep  1 11:25:19.022: INFO: (0) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 40.789368ms)
    Sep  1 11:25:19.023: INFO: (0) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 40.887121ms)
    Sep  1 11:25:19.027: INFO: (0) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 45.269718ms)
    Sep  1 11:25:19.028: INFO: (0) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 46.888299ms)
    Sep  1 11:25:19.029: INFO: (0) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 48.26751ms)
    Sep  1 11:25:19.029: INFO: (0) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 47.418221ms)
    Sep  1 11:25:19.029: INFO: (0) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 48.688551ms)
    Sep  1 11:25:19.030: INFO: (0) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 48.354197ms)
    Sep  1 11:25:19.030: INFO: (0) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 48.497218ms)
    Sep  1 11:25:19.031: INFO: (0) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 51.007445ms)
    Sep  1 11:25:19.032: INFO: (0) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 51.185038ms)
    Sep  1 11:25:19.055: INFO: (1) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 22.863342ms)
    Sep  1 11:25:19.056: INFO: (1) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 23.900356ms)
    Sep  1 11:25:19.057: INFO: (1) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 24.85792ms)
    Sep  1 11:25:19.057: INFO: (1) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 25.023763ms)
    Sep  1 11:25:19.057: INFO: (1) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 25.366751ms)
    Sep  1 11:25:19.058: INFO: (1) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 25.524774ms)
    Sep  1 11:25:19.058: INFO: (1) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 25.582932ms)
    Sep  1 11:25:19.058: INFO: (1) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 25.429438ms)
    Sep  1 11:25:19.059: INFO: (1) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 26.672994ms)
    Sep  1 11:25:19.062: INFO: (1) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 28.896296ms)
    Sep  1 11:25:19.063: INFO: (1) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 30.668036ms)
    Sep  1 11:25:19.063: INFO: (1) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 31.005804ms)
    Sep  1 11:25:19.063: INFO: (1) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 30.589343ms)
    Sep  1 11:25:19.063: INFO: (1) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 30.577848ms)
    Sep  1 11:25:19.072: INFO: (1) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 39.735905ms)
    Sep  1 11:25:19.072: INFO: (1) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 39.792979ms)
    Sep  1 11:25:19.084: INFO: (2) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 10.436397ms)
    Sep  1 11:25:19.084: INFO: (2) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 10.972687ms)
    Sep  1 11:25:19.085: INFO: (2) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 11.514739ms)
    Sep  1 11:25:19.085: INFO: (2) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 11.472587ms)
    Sep  1 11:25:19.087: INFO: (2) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 14.011174ms)
    Sep  1 11:25:19.088: INFO: (2) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 14.570935ms)
    Sep  1 11:25:19.089: INFO: (2) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 16.050483ms)
    Sep  1 11:25:19.089: INFO: (2) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 16.014136ms)
    Sep  1 11:25:19.090: INFO: (2) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 16.855889ms)
    Sep  1 11:25:19.091: INFO: (2) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 17.927182ms)
    Sep  1 11:25:19.099: INFO: (2) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 26.265442ms)
    Sep  1 11:25:19.104: INFO: (2) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 30.843205ms)
    Sep  1 11:25:19.104: INFO: (2) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 30.864135ms)
    Sep  1 11:25:19.104: INFO: (2) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 30.831077ms)
    Sep  1 11:25:19.104: INFO: (2) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 31.081704ms)
    Sep  1 11:25:19.104: INFO: (2) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 31.076587ms)
    Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 11.416822ms)
    Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 12.005333ms)
    Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 12.163998ms)
    Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 12.164825ms)
    Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 12.950469ms)
    Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 12.256193ms)
    Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 12.5043ms)
    Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 12.048237ms)
    Sep  1 11:25:19.117: INFO: (3) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 13.185823ms)
    Sep  1 11:25:19.118: INFO: (3) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 12.82913ms)
    Sep  1 11:25:19.121: INFO: (3) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 15.281393ms)
    Sep  1 11:25:19.121: INFO: (3) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 16.210772ms)
    Sep  1 11:25:19.121: INFO: (3) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 15.97561ms)
    Sep  1 11:25:19.122: INFO: (3) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 16.750478ms)
    Sep  1 11:25:19.123: INFO: (3) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 17.980576ms)
    Sep  1 11:25:19.123: INFO: (3) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 17.752327ms)
    Sep  1 11:25:19.135: INFO: (4) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 11.697559ms)
    Sep  1 11:25:19.136: INFO: (4) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 12.410247ms)
    Sep  1 11:25:19.137: INFO: (4) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 13.623578ms)
    Sep  1 11:25:19.143: INFO: (4) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 18.913831ms)
    Sep  1 11:25:19.143: INFO: (4) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 18.932488ms)
    Sep  1 11:25:19.143: INFO: (4) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 19.087571ms)
    Sep  1 11:25:19.144: INFO: (4) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 20.641033ms)
    Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 20.786846ms)
    Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 20.76087ms)
    Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 20.663914ms)
    Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 20.37901ms)
    Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 20.557138ms)
    Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 20.493572ms)
    Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 20.473248ms)
    Sep  1 11:25:19.145: INFO: (4) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 20.782488ms)
    Sep  1 11:25:19.146: INFO: (4) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 21.834352ms)
    Sep  1 11:25:19.160: INFO: (5) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 14.291515ms)
    Sep  1 11:25:19.160: INFO: (5) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 14.022251ms)
    Sep  1 11:25:19.161: INFO: (5) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 14.645004ms)
    Sep  1 11:25:19.161: INFO: (5) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 14.772527ms)
    Sep  1 11:25:19.161: INFO: (5) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 15.076565ms)
    Sep  1 11:25:19.161: INFO: (5) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 15.008276ms)
    Sep  1 11:25:19.161: INFO: (5) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 14.684921ms)
    Sep  1 11:25:19.161: INFO: (5) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 14.799755ms)
    Sep  1 11:25:19.161: INFO: (5) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 14.75318ms)
    Sep  1 11:25:19.168: INFO: (5) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 21.128805ms)
    Sep  1 11:25:19.168: INFO: (5) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 21.553321ms)
    Sep  1 11:25:19.168: INFO: (5) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 21.727132ms)
    Sep  1 11:25:19.168: INFO: (5) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 21.674729ms)
    Sep  1 11:25:19.168: INFO: (5) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 21.739754ms)
    Sep  1 11:25:19.168: INFO: (5) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 21.559111ms)
    Sep  1 11:25:19.168: INFO: (5) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 21.445958ms)
    Sep  1 11:25:19.181: INFO: (6) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 11.001942ms)
    Sep  1 11:25:19.181: INFO: (6) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 11.139366ms)
    Sep  1 11:25:19.181: INFO: (6) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 10.843939ms)
    Sep  1 11:25:19.181: INFO: (6) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 11.755553ms)
    Sep  1 11:25:19.181: INFO: (6) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 11.269182ms)
    Sep  1 11:25:19.181: INFO: (6) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 11.59958ms)
    Sep  1 11:25:19.187: INFO: (6) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 17.275004ms)
    Sep  1 11:25:19.187: INFO: (6) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 17.224977ms)
    Sep  1 11:25:19.187: INFO: (6) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 17.595362ms)
    Sep  1 11:25:19.187: INFO: (6) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 17.518344ms)
    Sep  1 11:25:19.187: INFO: (6) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 17.811165ms)
    Sep  1 11:25:19.188: INFO: (6) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 17.81172ms)
    Sep  1 11:25:19.188: INFO: (6) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 18.761727ms)
    Sep  1 11:25:19.192: INFO: (6) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 22.212618ms)
    Sep  1 11:25:19.192: INFO: (6) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 22.07991ms)
    Sep  1 11:25:19.192: INFO: (6) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 22.49777ms)
    Sep  1 11:25:19.206: INFO: (7) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 12.818238ms)
    Sep  1 11:25:19.208: INFO: (7) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 14.753262ms)
    Sep  1 11:25:19.208: INFO: (7) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 15.085352ms)
    Sep  1 11:25:19.209: INFO: (7) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 16.098083ms)
    Sep  1 11:25:19.209: INFO: (7) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 15.956529ms)
    Sep  1 11:25:19.210: INFO: (7) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 16.935056ms)
    Sep  1 11:25:19.211: INFO: (7) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 17.368918ms)
    Sep  1 11:25:19.211: INFO: (7) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 18.2283ms)
    Sep  1 11:25:19.211: INFO: (7) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 18.187248ms)
    Sep  1 11:25:19.211: INFO: (7) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 18.206827ms)
    Sep  1 11:25:19.211: INFO: (7) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 18.568414ms)
    Sep  1 11:25:19.212: INFO: (7) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 18.351356ms)
    Sep  1 11:25:19.212: INFO: (7) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 18.847851ms)
    Sep  1 11:25:19.212: INFO: (7) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 18.752624ms)
    Sep  1 11:25:19.212: INFO: (7) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 18.518562ms)
    Sep  1 11:25:19.213: INFO: (7) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 20.668991ms)
    Sep  1 11:25:19.231: INFO: (8) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 17.388917ms)
    Sep  1 11:25:19.231: INFO: (8) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 17.922939ms)
    Sep  1 11:25:19.232: INFO: (8) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 17.46461ms)
    Sep  1 11:25:19.233: INFO: (8) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 19.318844ms)
    Sep  1 11:25:19.234: INFO: (8) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 19.983923ms)
    Sep  1 11:25:19.234: INFO: (8) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 20.013052ms)
    Sep  1 11:25:19.234: INFO: (8) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 20.273783ms)
    Sep  1 11:25:19.234: INFO: (8) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 20.024438ms)
    Sep  1 11:25:19.234: INFO: (8) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 20.013437ms)
    Sep  1 11:25:19.236: INFO: (8) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 22.035906ms)
    Sep  1 11:25:19.236: INFO: (8) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 22.169017ms)
    Sep  1 11:25:19.237: INFO: (8) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 23.211952ms)
    Sep  1 11:25:19.237: INFO: (8) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 23.116754ms)
    Sep  1 11:25:19.238: INFO: (8) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 23.322199ms)
    Sep  1 11:25:19.238: INFO: (8) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 23.53683ms)
    Sep  1 11:25:19.239: INFO: (8) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 24.617251ms)
    Sep  1 11:25:19.246: INFO: (9) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 6.484712ms)
    Sep  1 11:25:19.246: INFO: (9) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 6.980164ms)
    Sep  1 11:25:19.253: INFO: (9) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 13.981754ms)
    Sep  1 11:25:19.257: INFO: (9) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 16.966513ms)
    Sep  1 11:25:19.260: INFO: (9) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 19.754761ms)
    Sep  1 11:25:19.260: INFO: (9) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 20.440414ms)
    Sep  1 11:25:19.260: INFO: (9) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 20.580805ms)
    Sep  1 11:25:19.260: INFO: (9) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 20.708849ms)
    Sep  1 11:25:19.260: INFO: (9) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 20.354881ms)
    Sep  1 11:25:19.262: INFO: (9) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 22.251033ms)
    Sep  1 11:25:19.262: INFO: (9) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 22.433617ms)
    Sep  1 11:25:19.263: INFO: (9) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 22.940449ms)
    Sep  1 11:25:19.263: INFO: (9) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 23.145748ms)
    Sep  1 11:25:19.263: INFO: (9) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 23.520129ms)
    Sep  1 11:25:19.263: INFO: (9) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 23.667752ms)
    Sep  1 11:25:19.265: INFO: (9) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 24.59127ms)
    Sep  1 11:25:19.279: INFO: (10) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 13.944368ms)
    Sep  1 11:25:19.279: INFO: (10) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 13.582125ms)
    Sep  1 11:25:19.280: INFO: (10) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 14.249847ms)
    Sep  1 11:25:19.280: INFO: (10) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 14.289788ms)
    Sep  1 11:25:19.281: INFO: (10) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 15.912611ms)
    Sep  1 11:25:19.281: INFO: (10) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 15.662476ms)
    Sep  1 11:25:19.286: INFO: (10) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 20.321943ms)
    Sep  1 11:25:19.288: INFO: (10) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 22.863112ms)
    Sep  1 11:25:19.288: INFO: (10) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 23.738524ms)
    Sep  1 11:25:19.289: INFO: (10) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 23.204709ms)
    Sep  1 11:25:19.289: INFO: (10) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 23.479488ms)
    Sep  1 11:25:19.289: INFO: (10) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 23.739911ms)
    Sep  1 11:25:19.289: INFO: (10) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 24.011093ms)
    Sep  1 11:25:19.289: INFO: (10) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 23.388285ms)
    Sep  1 11:25:19.289: INFO: (10) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 23.628281ms)
    Sep  1 11:25:19.290: INFO: (10) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 25.286614ms)
    Sep  1 11:25:19.301: INFO: (11) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 10.301938ms)
    Sep  1 11:25:19.301: INFO: (11) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 10.275236ms)
    Sep  1 11:25:19.301: INFO: (11) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 10.946708ms)
    Sep  1 11:25:19.301: INFO: (11) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 10.408752ms)
    Sep  1 11:25:19.301: INFO: (11) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 10.129365ms)
    Sep  1 11:25:19.302: INFO: (11) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 10.066092ms)
    Sep  1 11:25:19.302: INFO: (11) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 10.383117ms)
    Sep  1 11:25:19.302: INFO: (11) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 10.191619ms)
    Sep  1 11:25:19.303: INFO: (11) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 11.948443ms)
    Sep  1 11:25:19.303: INFO: (11) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 11.717305ms)
    Sep  1 11:25:19.306: INFO: (11) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 14.948626ms)
    Sep  1 11:25:19.306: INFO: (11) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 15.785875ms)
    Sep  1 11:25:19.306: INFO: (11) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 15.097989ms)
    Sep  1 11:25:19.307: INFO: (11) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 15.251227ms)
    Sep  1 11:25:19.307: INFO: (11) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 15.59752ms)
    Sep  1 11:25:19.307: INFO: (11) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 15.9207ms)
    Sep  1 11:25:19.321: INFO: (12) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 13.711456ms)
    Sep  1 11:25:19.322: INFO: (12) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 13.750966ms)
    Sep  1 11:25:19.322: INFO: (12) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 14.372583ms)
    Sep  1 11:25:19.322: INFO: (12) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 14.960707ms)
    Sep  1 11:25:19.323: INFO: (12) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 15.137009ms)
    Sep  1 11:25:19.323: INFO: (12) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 14.813832ms)
    Sep  1 11:25:19.323: INFO: (12) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 15.138865ms)
    Sep  1 11:25:19.323: INFO: (12) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 15.250034ms)
    Sep  1 11:25:19.322: INFO: (12) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 14.755337ms)
    Sep  1 11:25:19.323: INFO: (12) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 15.133526ms)
    Sep  1 11:25:19.326: INFO: (12) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 18.527669ms)
    Sep  1 11:25:19.326: INFO: (12) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 18.865455ms)
    Sep  1 11:25:19.327: INFO: (12) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 19.627251ms)
    Sep  1 11:25:19.327: INFO: (12) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 19.99781ms)
    Sep  1 11:25:19.328: INFO: (12) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 20.046956ms)
    Sep  1 11:25:19.328: INFO: (12) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 20.456597ms)
    Sep  1 11:25:19.335: INFO: (13) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 6.93597ms)
    Sep  1 11:25:19.344: INFO: (13) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 14.945647ms)
    Sep  1 11:25:19.344: INFO: (13) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 15.918625ms)
    Sep  1 11:25:19.345: INFO: (13) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 16.201608ms)
    Sep  1 11:25:19.345: INFO: (13) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 16.344834ms)
    Sep  1 11:25:19.345: INFO: (13) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 16.262882ms)
    Sep  1 11:25:19.345: INFO: (13) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 17.112132ms)
    Sep  1 11:25:19.345: INFO: (13) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 16.752931ms)
    Sep  1 11:25:19.345: INFO: (13) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 16.379756ms)
    Sep  1 11:25:19.345: INFO: (13) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 16.77316ms)
    Sep  1 11:25:19.346: INFO: (13) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 17.027846ms)
    Sep  1 11:25:19.346: INFO: (13) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 17.277391ms)
    Sep  1 11:25:19.346: INFO: (13) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 17.611277ms)
    Sep  1 11:25:19.346: INFO: (13) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 17.459098ms)
    Sep  1 11:25:19.346: INFO: (13) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 17.290608ms)
    Sep  1 11:25:19.346: INFO: (13) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 17.436931ms)
    Sep  1 11:25:19.355: INFO: (14) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 7.552055ms)
    Sep  1 11:25:19.356: INFO: (14) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 8.02159ms)
    Sep  1 11:25:19.356: INFO: (14) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 8.389283ms)
    Sep  1 11:25:19.357: INFO: (14) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 8.684559ms)
    Sep  1 11:25:19.360: INFO: (14) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 11.394519ms)
    Sep  1 11:25:19.360: INFO: (14) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 11.475296ms)
    Sep  1 11:25:19.360: INFO: (14) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 11.92316ms)
    Sep  1 11:25:19.361: INFO: (14) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 12.683686ms)
    Sep  1 11:25:19.361: INFO: (14) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 12.414106ms)
    Sep  1 11:25:19.361: INFO: (14) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 12.483795ms)
    Sep  1 11:25:19.363: INFO: (14) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 14.151137ms)
    Sep  1 11:25:19.363: INFO: (14) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 14.658125ms)
    Sep  1 11:25:19.364: INFO: (14) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 15.338697ms)
    Sep  1 11:25:19.364: INFO: (14) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 15.10373ms)
    Sep  1 11:25:19.364: INFO: (14) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 15.371612ms)
    Sep  1 11:25:19.364: INFO: (14) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 15.418392ms)
    Sep  1 11:25:19.377: INFO: (15) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 12.432082ms)
    Sep  1 11:25:19.378: INFO: (15) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 13.336363ms)
    Sep  1 11:25:19.377: INFO: (15) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 12.653113ms)
    Sep  1 11:25:19.379: INFO: (15) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 13.915426ms)
    Sep  1 11:25:19.379: INFO: (15) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 13.885098ms)
    Sep  1 11:25:19.380: INFO: (15) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 15.975558ms)
    Sep  1 11:25:19.381: INFO: (15) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 16.083766ms)
    Sep  1 11:25:19.381: INFO: (15) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 16.725329ms)
    Sep  1 11:25:19.381: INFO: (15) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 16.57282ms)
    Sep  1 11:25:19.381: INFO: (15) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 16.525132ms)
    Sep  1 11:25:19.381: INFO: (15) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 16.818349ms)
    Sep  1 11:25:19.382: INFO: (15) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 17.285813ms)
    Sep  1 11:25:19.382: INFO: (15) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 16.777483ms)
    Sep  1 11:25:19.383: INFO: (15) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 18.345804ms)
    Sep  1 11:25:19.383: INFO: (15) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 18.778737ms)
    Sep  1 11:25:19.384: INFO: (15) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 18.811617ms)
    Sep  1 11:25:19.392: INFO: (16) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 7.58029ms)
    Sep  1 11:25:19.392: INFO: (16) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 7.964911ms)
    Sep  1 11:25:19.392: INFO: (16) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 7.306127ms)
    Sep  1 11:25:19.392: INFO: (16) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 7.646503ms)
    Sep  1 11:25:19.397: INFO: (16) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 12.328696ms)
    Sep  1 11:25:19.397: INFO: (16) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 12.657316ms)
    Sep  1 11:25:19.397: INFO: (16) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 12.597652ms)
    Sep  1 11:25:19.397: INFO: (16) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 12.989399ms)
    Sep  1 11:25:19.397: INFO: (16) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 12.692713ms)
    Sep  1 11:25:19.400: INFO: (16) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 15.469382ms)
    Sep  1 11:25:19.404: INFO: (16) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 19.840492ms)
    Sep  1 11:25:19.404: INFO: (16) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 19.731148ms)
    Sep  1 11:25:19.405: INFO: (16) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 20.24301ms)
    Sep  1 11:25:19.405: INFO: (16) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 20.715915ms)
    Sep  1 11:25:19.405: INFO: (16) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 20.784802ms)
    Sep  1 11:25:19.406: INFO: (16) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 21.50159ms)
    Sep  1 11:25:19.416: INFO: (17) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 9.618989ms)
    Sep  1 11:25:19.416: INFO: (17) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 9.354188ms)
    Sep  1 11:25:19.416: INFO: (17) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 9.55739ms)
    Sep  1 11:25:19.417: INFO: (17) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 10.266727ms)
    Sep  1 11:25:19.417: INFO: (17) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 10.119191ms)
    Sep  1 11:25:19.417: INFO: (17) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 9.997799ms)
    Sep  1 11:25:19.417: INFO: (17) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 10.074653ms)
    Sep  1 11:25:19.417: INFO: (17) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 10.876969ms)
    Sep  1 11:25:19.417: INFO: (17) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 10.270964ms)
    Sep  1 11:25:19.417: INFO: (17) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 10.621693ms)
    Sep  1 11:25:19.420: INFO: (17) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 13.210815ms)
    Sep  1 11:25:19.420: INFO: (17) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 13.235656ms)
    Sep  1 11:25:19.420: INFO: (17) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 13.846533ms)
    Sep  1 11:25:19.420: INFO: (17) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 14.019962ms)
    Sep  1 11:25:19.421: INFO: (17) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 14.061081ms)
    Sep  1 11:25:19.421: INFO: (17) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 14.355816ms)
    Sep  1 11:25:19.432: INFO: (18) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 10.646316ms)
    Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 11.224838ms)
    Sep  1 11:25:19.432: INFO: (18) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 10.711047ms)
    Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 11.168256ms)
    Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 10.949361ms)
    Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 11.316295ms)
    Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 11.042983ms)
    Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 11.599536ms)
    Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 11.978947ms)
    Sep  1 11:25:19.433: INFO: (18) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 11.632069ms)
    Sep  1 11:25:19.436: INFO: (18) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 13.830205ms)
    Sep  1 11:25:19.436: INFO: (18) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 14.214763ms)
    Sep  1 11:25:19.436: INFO: (18) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 14.354657ms)
    Sep  1 11:25:19.436: INFO: (18) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 13.95749ms)
    Sep  1 11:25:19.436: INFO: (18) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 14.553968ms)
    Sep  1 11:25:19.436: INFO: (18) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 14.236076ms)
    Sep  1 11:25:19.449: INFO: (19) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:443/proxy/tlsrewritem... (200; 11.24823ms)
    Sep  1 11:25:19.449: INFO: (19) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:162/proxy/: bar (200; 11.151015ms)
    Sep  1 11:25:19.449: INFO: (19) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:1080/proxy/rewriteme">test<... (200; 10.899539ms)
    Sep  1 11:25:19.449: INFO: (19) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26/proxy/rewriteme">test</a> (200; 11.584845ms)
    Sep  1 11:25:19.450: INFO: (19) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/: <a href="/api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:1080/proxy/rewriteme">... (200; 12.82793ms)
    Sep  1 11:25:19.450: INFO: (19) /api/v1/namespaces/proxy-7711/pods/http:proxy-service-9nj99-k2h26:160/proxy/: foo (200; 13.206432ms)
    Sep  1 11:25:19.451: INFO: (19) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:160/proxy/: foo (200; 13.653976ms)
    Sep  1 11:25:19.451: INFO: (19) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:460/proxy/: tls baz (200; 13.172739ms)
    Sep  1 11:25:19.451: INFO: (19) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname1/proxy/: foo (200; 13.762557ms)
    Sep  1 11:25:19.452: INFO: (19) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname2/proxy/: bar (200; 13.767967ms)
    Sep  1 11:25:19.452: INFO: (19) /api/v1/namespaces/proxy-7711/services/http:proxy-service-9nj99:portname2/proxy/: bar (200; 14.044311ms)
    Sep  1 11:25:19.450: INFO: (19) /api/v1/namespaces/proxy-7711/pods/https:proxy-service-9nj99-k2h26:462/proxy/: tls qux (200; 12.769696ms)
    Sep  1 11:25:19.452: INFO: (19) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname1/proxy/: tls baz (200; 14.61312ms)
    Sep  1 11:25:19.453: INFO: (19) /api/v1/namespaces/proxy-7711/services/proxy-service-9nj99:portname1/proxy/: foo (200; 14.895104ms)
    Sep  1 11:25:19.453: INFO: (19) /api/v1/namespaces/proxy-7711/services/https:proxy-service-9nj99:tlsportname2/proxy/: tls qux (200; 15.660261ms)
    Sep  1 11:25:19.453: INFO: (19) /api/v1/namespaces/proxy-7711/pods/proxy-service-9nj99-k2h26:162/proxy/: bar (200; 15.959515ms)
    STEP: deleting ReplicationController proxy-service-9nj99 in namespace proxy-7711, will wait for the garbage collector to delete the pods 09/01/23 11:25:19.453
    Sep  1 11:25:19.514: INFO: Deleting ReplicationController proxy-service-9nj99 took: 6.353799ms
    Sep  1 11:25:19.615: INFO: Terminating ReplicationController proxy-service-9nj99 pods took: 101.114627ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:22.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7711" for this suite. 09/01/23 11:25:22.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:22.09
Sep  1 11:25:22.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 11:25:22.093
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:22.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:22.131
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 09/01/23 11:25:22.137
STEP: fetching the ConfigMap 09/01/23 11:25:22.147
STEP: patching the ConfigMap 09/01/23 11:25:22.154
STEP: listing all ConfigMaps in all namespaces with a label selector 09/01/23 11:25:22.163
STEP: deleting the ConfigMap by collection with a label selector 09/01/23 11:25:22.175
STEP: listing all ConfigMaps in test namespace 09/01/23 11:25:22.189
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:22.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1565" for this suite. 09/01/23 11:25:22.202
------------------------------
• [0.124 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:22.09
    Sep  1 11:25:22.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 11:25:22.093
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:22.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:22.131
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 09/01/23 11:25:22.137
    STEP: fetching the ConfigMap 09/01/23 11:25:22.147
    STEP: patching the ConfigMap 09/01/23 11:25:22.154
    STEP: listing all ConfigMaps in all namespaces with a label selector 09/01/23 11:25:22.163
    STEP: deleting the ConfigMap by collection with a label selector 09/01/23 11:25:22.175
    STEP: listing all ConfigMaps in test namespace 09/01/23 11:25:22.189
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:22.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1565" for this suite. 09/01/23 11:25:22.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:22.214
Sep  1 11:25:22.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename sysctl 09/01/23 11:25:22.215
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:22.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:22.26
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 09/01/23 11:25:22.266
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:22.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-871" for this suite. 09/01/23 11:25:22.279
------------------------------
• [0.078 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:22.214
    Sep  1 11:25:22.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename sysctl 09/01/23 11:25:22.215
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:22.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:22.26
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 09/01/23 11:25:22.266
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:22.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-871" for this suite. 09/01/23 11:25:22.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:22.299
Sep  1 11:25:22.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 11:25:22.3
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:22.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:22.335
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-49c17a34-f3e8-4096-9f1a-23b749fd6080 09/01/23 11:25:22.339
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:22.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-982" for this suite. 09/01/23 11:25:22.367
------------------------------
• [0.086 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:22.299
    Sep  1 11:25:22.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 11:25:22.3
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:22.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:22.335
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-49c17a34-f3e8-4096-9f1a-23b749fd6080 09/01/23 11:25:22.339
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:22.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-982" for this suite. 09/01/23 11:25:22.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:22.394
Sep  1 11:25:22.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 11:25:22.395
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:22.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:22.436
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/01/23 11:25:22.44
Sep  1 11:25:22.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9456 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Sep  1 11:25:22.545: INFO: stderr: ""
Sep  1 11:25:22.545: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 09/01/23 11:25:22.545
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Sep  1 11:25:22.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9456 delete pods e2e-test-httpd-pod'
Sep  1 11:25:24.923: INFO: stderr: ""
Sep  1 11:25:24.923: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:24.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9456" for this suite. 09/01/23 11:25:24.928
------------------------------
• [2.552 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:22.394
    Sep  1 11:25:22.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 11:25:22.395
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:22.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:22.436
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/01/23 11:25:22.44
    Sep  1 11:25:22.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9456 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Sep  1 11:25:22.545: INFO: stderr: ""
    Sep  1 11:25:22.545: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 09/01/23 11:25:22.545
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Sep  1 11:25:22.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9456 delete pods e2e-test-httpd-pod'
    Sep  1 11:25:24.923: INFO: stderr: ""
    Sep  1 11:25:24.923: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:24.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9456" for this suite. 09/01/23 11:25:24.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:24.963
Sep  1 11:25:24.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-lifecycle-hook 09/01/23 11:25:24.966
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:24.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:24.988
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/01/23 11:25:24.996
Sep  1 11:25:25.007: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7931" to be "running and ready"
Sep  1 11:25:25.011: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990326ms
Sep  1 11:25:25.012: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:25:27.016: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008735526s
Sep  1 11:25:27.016: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  1 11:25:27.016: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 09/01/23 11:25:27.019
Sep  1 11:25:27.026: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7931" to be "running and ready"
Sep  1 11:25:27.031: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078973ms
Sep  1 11:25:27.031: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:25:29.034: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007779895s
Sep  1 11:25:29.034: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Sep  1 11:25:29.034: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/01/23 11:25:29.037
Sep  1 11:25:29.044: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  1 11:25:29.047: INFO: Pod pod-with-prestop-http-hook still exists
Sep  1 11:25:31.047: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  1 11:25:31.051: INFO: Pod pod-with-prestop-http-hook still exists
Sep  1 11:25:33.047: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  1 11:25:33.051: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 09/01/23 11:25:33.051
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:33.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7931" for this suite. 09/01/23 11:25:33.074
------------------------------
• [SLOW TEST] [8.118 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:24.963
    Sep  1 11:25:24.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/01/23 11:25:24.966
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:24.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:24.988
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/01/23 11:25:24.996
    Sep  1 11:25:25.007: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7931" to be "running and ready"
    Sep  1 11:25:25.011: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990326ms
    Sep  1 11:25:25.012: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:25:27.016: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008735526s
    Sep  1 11:25:27.016: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  1 11:25:27.016: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 09/01/23 11:25:27.019
    Sep  1 11:25:27.026: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7931" to be "running and ready"
    Sep  1 11:25:27.031: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078973ms
    Sep  1 11:25:27.031: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:25:29.034: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007779895s
    Sep  1 11:25:29.034: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Sep  1 11:25:29.034: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/01/23 11:25:29.037
    Sep  1 11:25:29.044: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  1 11:25:29.047: INFO: Pod pod-with-prestop-http-hook still exists
    Sep  1 11:25:31.047: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  1 11:25:31.051: INFO: Pod pod-with-prestop-http-hook still exists
    Sep  1 11:25:33.047: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  1 11:25:33.051: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 09/01/23 11:25:33.051
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:33.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7931" for this suite. 09/01/23 11:25:33.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:33.088
Sep  1 11:25:33.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename ingress 09/01/23 11:25:33.09
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:33.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:33.109
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 09/01/23 11:25:33.112
STEP: getting /apis/networking.k8s.io 09/01/23 11:25:33.115
STEP: getting /apis/networking.k8s.iov1 09/01/23 11:25:33.117
STEP: creating 09/01/23 11:25:33.118
STEP: getting 09/01/23 11:25:33.362
STEP: listing 09/01/23 11:25:33.365
STEP: watching 09/01/23 11:25:33.368
Sep  1 11:25:33.368: INFO: starting watch
STEP: cluster-wide listing 09/01/23 11:25:33.37
STEP: cluster-wide watching 09/01/23 11:25:33.373
Sep  1 11:25:33.373: INFO: starting watch
STEP: patching 09/01/23 11:25:33.374
STEP: updating 09/01/23 11:25:33.381
Sep  1 11:25:33.391: INFO: waiting for watch events with expected annotations
Sep  1 11:25:33.391: INFO: saw patched and updated annotations
STEP: patching /status 09/01/23 11:25:33.392
STEP: updating /status 09/01/23 11:25:33.398
STEP: get /status 09/01/23 11:25:33.408
STEP: deleting 09/01/23 11:25:33.411
STEP: deleting a collection 09/01/23 11:25:33.424
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:33.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-6896" for this suite. 09/01/23 11:25:33.441
------------------------------
• [0.360 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:33.088
    Sep  1 11:25:33.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename ingress 09/01/23 11:25:33.09
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:33.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:33.109
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 09/01/23 11:25:33.112
    STEP: getting /apis/networking.k8s.io 09/01/23 11:25:33.115
    STEP: getting /apis/networking.k8s.iov1 09/01/23 11:25:33.117
    STEP: creating 09/01/23 11:25:33.118
    STEP: getting 09/01/23 11:25:33.362
    STEP: listing 09/01/23 11:25:33.365
    STEP: watching 09/01/23 11:25:33.368
    Sep  1 11:25:33.368: INFO: starting watch
    STEP: cluster-wide listing 09/01/23 11:25:33.37
    STEP: cluster-wide watching 09/01/23 11:25:33.373
    Sep  1 11:25:33.373: INFO: starting watch
    STEP: patching 09/01/23 11:25:33.374
    STEP: updating 09/01/23 11:25:33.381
    Sep  1 11:25:33.391: INFO: waiting for watch events with expected annotations
    Sep  1 11:25:33.391: INFO: saw patched and updated annotations
    STEP: patching /status 09/01/23 11:25:33.392
    STEP: updating /status 09/01/23 11:25:33.398
    STEP: get /status 09/01/23 11:25:33.408
    STEP: deleting 09/01/23 11:25:33.411
    STEP: deleting a collection 09/01/23 11:25:33.424
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:33.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-6896" for this suite. 09/01/23 11:25:33.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:33.452
Sep  1 11:25:33.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:25:33.453
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:33.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:33.474
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-b7f7d588-91d4-4135-b3eb-2fa4ea4690be 09/01/23 11:25:33.476
STEP: Creating a pod to test consume configMaps 09/01/23 11:25:33.483
Sep  1 11:25:33.491: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa" in namespace "projected-994" to be "Succeeded or Failed"
Sep  1 11:25:33.494: INFO: Pod "pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.717572ms
Sep  1 11:25:35.499: INFO: Pod "pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00752358s
Sep  1 11:25:37.499: INFO: Pod "pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007879456s
STEP: Saw pod success 09/01/23 11:25:37.5
Sep  1 11:25:37.500: INFO: Pod "pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa" satisfied condition "Succeeded or Failed"
Sep  1 11:25:37.503: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa container agnhost-container: <nil>
STEP: delete the pod 09/01/23 11:25:37.509
Sep  1 11:25:37.522: INFO: Waiting for pod pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa to disappear
Sep  1 11:25:37.525: INFO: Pod pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:37.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-994" for this suite. 09/01/23 11:25:37.529
------------------------------
• [4.083 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:33.452
    Sep  1 11:25:33.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:25:33.453
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:33.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:33.474
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-b7f7d588-91d4-4135-b3eb-2fa4ea4690be 09/01/23 11:25:33.476
    STEP: Creating a pod to test consume configMaps 09/01/23 11:25:33.483
    Sep  1 11:25:33.491: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa" in namespace "projected-994" to be "Succeeded or Failed"
    Sep  1 11:25:33.494: INFO: Pod "pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.717572ms
    Sep  1 11:25:35.499: INFO: Pod "pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00752358s
    Sep  1 11:25:37.499: INFO: Pod "pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007879456s
    STEP: Saw pod success 09/01/23 11:25:37.5
    Sep  1 11:25:37.500: INFO: Pod "pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa" satisfied condition "Succeeded or Failed"
    Sep  1 11:25:37.503: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 11:25:37.509
    Sep  1 11:25:37.522: INFO: Waiting for pod pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa to disappear
    Sep  1 11:25:37.525: INFO: Pod pod-projected-configmaps-33406f84-caa9-4ca5-ad05-2750dabde1fa no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:37.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-994" for this suite. 09/01/23 11:25:37.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:37.541
Sep  1 11:25:37.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename svcaccounts 09/01/23 11:25:37.543
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:37.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:37.564
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-k44pg"  09/01/23 11:25:37.567
Sep  1 11:25:37.571: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-k44pg"  09/01/23 11:25:37.571
Sep  1 11:25:37.579: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:37.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2025" for this suite. 09/01/23 11:25:37.583
------------------------------
• [0.050 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:37.541
    Sep  1 11:25:37.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename svcaccounts 09/01/23 11:25:37.543
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:37.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:37.564
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-k44pg"  09/01/23 11:25:37.567
    Sep  1 11:25:37.571: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-k44pg"  09/01/23 11:25:37.571
    Sep  1 11:25:37.579: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:37.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2025" for this suite. 09/01/23 11:25:37.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:37.593
Sep  1 11:25:37.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 11:25:37.596
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:37.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:37.62
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/01/23 11:25:37.623
Sep  1 11:25:37.634: INFO: Waiting up to 5m0s for pod "pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3" in namespace "emptydir-4506" to be "Succeeded or Failed"
Sep  1 11:25:37.637: INFO: Pod "pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.386035ms
Sep  1 11:25:39.642: INFO: Pod "pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008198766s
Sep  1 11:25:41.642: INFO: Pod "pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007788874s
STEP: Saw pod success 09/01/23 11:25:41.642
Sep  1 11:25:41.643: INFO: Pod "pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3" satisfied condition "Succeeded or Failed"
Sep  1 11:25:41.646: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3 container test-container: <nil>
STEP: delete the pod 09/01/23 11:25:41.652
Sep  1 11:25:41.691: INFO: Waiting for pod pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3 to disappear
Sep  1 11:25:41.696: INFO: Pod pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:41.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4506" for this suite. 09/01/23 11:25:41.701
------------------------------
• [4.115 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:37.593
    Sep  1 11:25:37.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 11:25:37.596
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:37.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:37.62
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/01/23 11:25:37.623
    Sep  1 11:25:37.634: INFO: Waiting up to 5m0s for pod "pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3" in namespace "emptydir-4506" to be "Succeeded or Failed"
    Sep  1 11:25:37.637: INFO: Pod "pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.386035ms
    Sep  1 11:25:39.642: INFO: Pod "pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008198766s
    Sep  1 11:25:41.642: INFO: Pod "pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007788874s
    STEP: Saw pod success 09/01/23 11:25:41.642
    Sep  1 11:25:41.643: INFO: Pod "pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3" satisfied condition "Succeeded or Failed"
    Sep  1 11:25:41.646: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3 container test-container: <nil>
    STEP: delete the pod 09/01/23 11:25:41.652
    Sep  1 11:25:41.691: INFO: Waiting for pod pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3 to disappear
    Sep  1 11:25:41.696: INFO: Pod pod-a02807d1-8ad6-4706-b05a-2de9070b1bc3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:41.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4506" for this suite. 09/01/23 11:25:41.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:41.714
Sep  1 11:25:41.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 11:25:41.715
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:41.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:41.74
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 09/01/23 11:25:41.745
Sep  1 11:25:41.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8" in namespace "downward-api-3040" to be "Succeeded or Failed"
Sep  1 11:25:41.758: INFO: Pod "downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.291594ms
Sep  1 11:25:43.763: INFO: Pod "downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00917083s
Sep  1 11:25:45.763: INFO: Pod "downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008858834s
STEP: Saw pod success 09/01/23 11:25:45.763
Sep  1 11:25:45.763: INFO: Pod "downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8" satisfied condition "Succeeded or Failed"
Sep  1 11:25:45.766: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8 container client-container: <nil>
STEP: delete the pod 09/01/23 11:25:45.773
Sep  1 11:25:45.790: INFO: Waiting for pod downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8 to disappear
Sep  1 11:25:45.794: INFO: Pod downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  1 11:25:45.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3040" for this suite. 09/01/23 11:25:45.799
------------------------------
• [4.091 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:41.714
    Sep  1 11:25:41.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 11:25:41.715
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:41.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:41.74
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 09/01/23 11:25:41.745
    Sep  1 11:25:41.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8" in namespace "downward-api-3040" to be "Succeeded or Failed"
    Sep  1 11:25:41.758: INFO: Pod "downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.291594ms
    Sep  1 11:25:43.763: INFO: Pod "downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00917083s
    Sep  1 11:25:45.763: INFO: Pod "downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008858834s
    STEP: Saw pod success 09/01/23 11:25:45.763
    Sep  1 11:25:45.763: INFO: Pod "downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8" satisfied condition "Succeeded or Failed"
    Sep  1 11:25:45.766: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8 container client-container: <nil>
    STEP: delete the pod 09/01/23 11:25:45.773
    Sep  1 11:25:45.790: INFO: Waiting for pod downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8 to disappear
    Sep  1 11:25:45.794: INFO: Pod downwardapi-volume-473f02a9-09c8-4af4-ad71-0a6cf977e2e8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:25:45.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3040" for this suite. 09/01/23 11:25:45.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:25:45.823
Sep  1 11:25:45.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename dns 09/01/23 11:25:45.824
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:45.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:45.852
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 09/01/23 11:25:45.856
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8755 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8755;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8755 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8755;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8755.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8755.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8755.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8755.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8755.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8755.svc;check="$$(dig +notcp +noall +answer +search 45.9.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.9.45_udp@PTR;check="$$(dig +tcp +noall +answer +search 45.9.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.9.45_tcp@PTR;sleep 1; done
 09/01/23 11:25:45.887
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8755 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8755;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8755 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8755;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8755.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8755.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8755.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8755.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8755.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8755.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8755.svc;check="$$(dig +notcp +noall +answer +search 45.9.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.9.45_udp@PTR;check="$$(dig +tcp +noall +answer +search 45.9.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.9.45_tcp@PTR;sleep 1; done
 09/01/23 11:25:45.887
STEP: creating a pod to probe DNS 09/01/23 11:25:45.887
STEP: submitting the pod to kubernetes 09/01/23 11:25:45.888
Sep  1 11:25:45.906: INFO: Waiting up to 15m0s for pod "dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa" in namespace "dns-8755" to be "running"
Sep  1 11:25:45.912: INFO: Pod "dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.514546ms
Sep  1 11:25:47.918: INFO: Pod "dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011554893s
Sep  1 11:25:49.917: INFO: Pod "dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa": Phase="Running", Reason="", readiness=true. Elapsed: 4.010867809s
Sep  1 11:25:49.918: INFO: Pod "dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa" satisfied condition "running"
STEP: retrieving the pod 09/01/23 11:25:49.919
STEP: looking for the results for each expected name from probers 09/01/23 11:25:49.924
Sep  1 11:25:49.930: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:49.936: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:49.949: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:49.954: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:49.961: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:49.966: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:49.972: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:49.975: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:50.004: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:50.009: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:50.013: INFO: Unable to read jessie_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:50.018: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:50.023: INFO: Unable to read jessie_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:50.028: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:50.033: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:50.038: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:50.057: INFO: Lookups using dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8755 wheezy_tcp@dns-test-service.dns-8755 wheezy_udp@dns-test-service.dns-8755.svc wheezy_tcp@dns-test-service.dns-8755.svc wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8755 jessie_tcp@dns-test-service.dns-8755 jessie_udp@dns-test-service.dns-8755.svc jessie_tcp@dns-test-service.dns-8755.svc jessie_udp@_http._tcp.dns-test-service.dns-8755.svc jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc]

Sep  1 11:25:55.062: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.065: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.070: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.076: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.082: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.089: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.094: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.099: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.128: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.131: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.136: INFO: Unable to read jessie_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.141: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.149: INFO: Unable to read jessie_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.154: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.157: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.162: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:25:55.178: INFO: Lookups using dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8755 wheezy_tcp@dns-test-service.dns-8755 wheezy_udp@dns-test-service.dns-8755.svc wheezy_tcp@dns-test-service.dns-8755.svc wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8755 jessie_tcp@dns-test-service.dns-8755 jessie_udp@dns-test-service.dns-8755.svc jessie_tcp@dns-test-service.dns-8755.svc jessie_udp@_http._tcp.dns-test-service.dns-8755.svc jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc]

Sep  1 11:26:00.062: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.066: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.071: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.074: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.079: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.083: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.088: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.093: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.114: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.119: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.125: INFO: Unable to read jessie_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.130: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.135: INFO: Unable to read jessie_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.138: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.142: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.146: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:00.162: INFO: Lookups using dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8755 wheezy_tcp@dns-test-service.dns-8755 wheezy_udp@dns-test-service.dns-8755.svc wheezy_tcp@dns-test-service.dns-8755.svc wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8755 jessie_tcp@dns-test-service.dns-8755 jessie_udp@dns-test-service.dns-8755.svc jessie_tcp@dns-test-service.dns-8755.svc jessie_udp@_http._tcp.dns-test-service.dns-8755.svc jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc]

Sep  1 11:26:05.063: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.068: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.074: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.079: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.093: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.097: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.103: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.108: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.131: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.137: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.141: INFO: Unable to read jessie_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.146: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.150: INFO: Unable to read jessie_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.154: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.157: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.162: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:05.178: INFO: Lookups using dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8755 wheezy_tcp@dns-test-service.dns-8755 wheezy_udp@dns-test-service.dns-8755.svc wheezy_tcp@dns-test-service.dns-8755.svc wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8755 jessie_tcp@dns-test-service.dns-8755 jessie_udp@dns-test-service.dns-8755.svc jessie_tcp@dns-test-service.dns-8755.svc jessie_udp@_http._tcp.dns-test-service.dns-8755.svc jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc]

Sep  1 11:26:10.065: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.069: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.073: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.078: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.083: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.087: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.091: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.096: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.114: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.118: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.122: INFO: Unable to read jessie_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.126: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.132: INFO: Unable to read jessie_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.135: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.139: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.144: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:10.164: INFO: Lookups using dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8755 wheezy_tcp@dns-test-service.dns-8755 wheezy_udp@dns-test-service.dns-8755.svc wheezy_tcp@dns-test-service.dns-8755.svc wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8755 jessie_tcp@dns-test-service.dns-8755 jessie_udp@dns-test-service.dns-8755.svc jessie_tcp@dns-test-service.dns-8755.svc jessie_udp@_http._tcp.dns-test-service.dns-8755.svc jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc]

Sep  1 11:26:15.063: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.069: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.076: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.083: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.091: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.095: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.100: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.107: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.131: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.134: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.138: INFO: Unable to read jessie_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.142: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.147: INFO: Unable to read jessie_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.153: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.159: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.164: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
Sep  1 11:26:15.184: INFO: Lookups using dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8755 wheezy_tcp@dns-test-service.dns-8755 wheezy_udp@dns-test-service.dns-8755.svc wheezy_tcp@dns-test-service.dns-8755.svc wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8755 jessie_tcp@dns-test-service.dns-8755 jessie_udp@dns-test-service.dns-8755.svc jessie_tcp@dns-test-service.dns-8755.svc jessie_udp@_http._tcp.dns-test-service.dns-8755.svc jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc]

Sep  1 11:26:20.170: INFO: DNS probes using dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa succeeded

STEP: deleting the pod 09/01/23 11:26:20.17
STEP: deleting the test service 09/01/23 11:26:20.196
STEP: deleting the test headless service 09/01/23 11:26:20.292
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  1 11:26:20.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8755" for this suite. 09/01/23 11:26:20.339
------------------------------
• [SLOW TEST] [34.534 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:25:45.823
    Sep  1 11:25:45.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename dns 09/01/23 11:25:45.824
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:25:45.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:25:45.852
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 09/01/23 11:25:45.856
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8755 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8755;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8755 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8755;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8755.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8755.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8755.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8755.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8755.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8755.svc;check="$$(dig +notcp +noall +answer +search 45.9.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.9.45_udp@PTR;check="$$(dig +tcp +noall +answer +search 45.9.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.9.45_tcp@PTR;sleep 1; done
     09/01/23 11:25:45.887
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8755 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8755;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8755 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8755;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8755.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8755.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8755.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8755.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8755.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8755.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8755.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8755.svc;check="$$(dig +notcp +noall +answer +search 45.9.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.9.45_udp@PTR;check="$$(dig +tcp +noall +answer +search 45.9.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.9.45_tcp@PTR;sleep 1; done
     09/01/23 11:25:45.887
    STEP: creating a pod to probe DNS 09/01/23 11:25:45.887
    STEP: submitting the pod to kubernetes 09/01/23 11:25:45.888
    Sep  1 11:25:45.906: INFO: Waiting up to 15m0s for pod "dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa" in namespace "dns-8755" to be "running"
    Sep  1 11:25:45.912: INFO: Pod "dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.514546ms
    Sep  1 11:25:47.918: INFO: Pod "dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011554893s
    Sep  1 11:25:49.917: INFO: Pod "dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa": Phase="Running", Reason="", readiness=true. Elapsed: 4.010867809s
    Sep  1 11:25:49.918: INFO: Pod "dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa" satisfied condition "running"
    STEP: retrieving the pod 09/01/23 11:25:49.919
    STEP: looking for the results for each expected name from probers 09/01/23 11:25:49.924
    Sep  1 11:25:49.930: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:49.936: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:49.949: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:49.954: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:49.961: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:49.966: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:49.972: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:49.975: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:50.004: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:50.009: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:50.013: INFO: Unable to read jessie_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:50.018: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:50.023: INFO: Unable to read jessie_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:50.028: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:50.033: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:50.038: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:50.057: INFO: Lookups using dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8755 wheezy_tcp@dns-test-service.dns-8755 wheezy_udp@dns-test-service.dns-8755.svc wheezy_tcp@dns-test-service.dns-8755.svc wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8755 jessie_tcp@dns-test-service.dns-8755 jessie_udp@dns-test-service.dns-8755.svc jessie_tcp@dns-test-service.dns-8755.svc jessie_udp@_http._tcp.dns-test-service.dns-8755.svc jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc]

    Sep  1 11:25:55.062: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.065: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.070: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.076: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.082: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.089: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.094: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.099: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.128: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.131: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.136: INFO: Unable to read jessie_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.141: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.149: INFO: Unable to read jessie_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.154: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.157: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.162: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:25:55.178: INFO: Lookups using dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8755 wheezy_tcp@dns-test-service.dns-8755 wheezy_udp@dns-test-service.dns-8755.svc wheezy_tcp@dns-test-service.dns-8755.svc wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8755 jessie_tcp@dns-test-service.dns-8755 jessie_udp@dns-test-service.dns-8755.svc jessie_tcp@dns-test-service.dns-8755.svc jessie_udp@_http._tcp.dns-test-service.dns-8755.svc jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc]

    Sep  1 11:26:00.062: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.066: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.071: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.074: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.079: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.083: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.088: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.093: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.114: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.119: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.125: INFO: Unable to read jessie_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.130: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.135: INFO: Unable to read jessie_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.138: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.142: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.146: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:00.162: INFO: Lookups using dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8755 wheezy_tcp@dns-test-service.dns-8755 wheezy_udp@dns-test-service.dns-8755.svc wheezy_tcp@dns-test-service.dns-8755.svc wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8755 jessie_tcp@dns-test-service.dns-8755 jessie_udp@dns-test-service.dns-8755.svc jessie_tcp@dns-test-service.dns-8755.svc jessie_udp@_http._tcp.dns-test-service.dns-8755.svc jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc]

    Sep  1 11:26:05.063: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.068: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.074: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.079: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.093: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.097: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.103: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.108: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.131: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.137: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.141: INFO: Unable to read jessie_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.146: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.150: INFO: Unable to read jessie_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.154: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.157: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.162: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:05.178: INFO: Lookups using dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8755 wheezy_tcp@dns-test-service.dns-8755 wheezy_udp@dns-test-service.dns-8755.svc wheezy_tcp@dns-test-service.dns-8755.svc wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8755 jessie_tcp@dns-test-service.dns-8755 jessie_udp@dns-test-service.dns-8755.svc jessie_tcp@dns-test-service.dns-8755.svc jessie_udp@_http._tcp.dns-test-service.dns-8755.svc jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc]

    Sep  1 11:26:10.065: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.069: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.073: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.078: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.083: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.087: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.091: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.096: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.114: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.118: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.122: INFO: Unable to read jessie_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.126: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.132: INFO: Unable to read jessie_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.135: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.139: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.144: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:10.164: INFO: Lookups using dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8755 wheezy_tcp@dns-test-service.dns-8755 wheezy_udp@dns-test-service.dns-8755.svc wheezy_tcp@dns-test-service.dns-8755.svc wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8755 jessie_tcp@dns-test-service.dns-8755 jessie_udp@dns-test-service.dns-8755.svc jessie_tcp@dns-test-service.dns-8755.svc jessie_udp@_http._tcp.dns-test-service.dns-8755.svc jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc]

    Sep  1 11:26:15.063: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.069: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.076: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.083: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.091: INFO: Unable to read wheezy_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.095: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.100: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.107: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.131: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.134: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.138: INFO: Unable to read jessie_udp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.142: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755 from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.147: INFO: Unable to read jessie_udp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.153: INFO: Unable to read jessie_tcp@dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.159: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.164: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc from pod dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa: the server could not find the requested resource (get pods dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa)
    Sep  1 11:26:15.184: INFO: Lookups using dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8755 wheezy_tcp@dns-test-service.dns-8755 wheezy_udp@dns-test-service.dns-8755.svc wheezy_tcp@dns-test-service.dns-8755.svc wheezy_udp@_http._tcp.dns-test-service.dns-8755.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8755.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8755 jessie_tcp@dns-test-service.dns-8755 jessie_udp@dns-test-service.dns-8755.svc jessie_tcp@dns-test-service.dns-8755.svc jessie_udp@_http._tcp.dns-test-service.dns-8755.svc jessie_tcp@_http._tcp.dns-test-service.dns-8755.svc]

    Sep  1 11:26:20.170: INFO: DNS probes using dns-8755/dns-test-26f980fe-ea65-40d8-b335-1381a9f338fa succeeded

    STEP: deleting the pod 09/01/23 11:26:20.17
    STEP: deleting the test service 09/01/23 11:26:20.196
    STEP: deleting the test headless service 09/01/23 11:26:20.292
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:26:20.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8755" for this suite. 09/01/23 11:26:20.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:26:20.371
Sep  1 11:26:20.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename gc 09/01/23 11:26:20.375
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:26:20.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:26:20.429
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 09/01/23 11:26:20.453
STEP: delete the rc 09/01/23 11:26:25.481
STEP: wait for the rc to be deleted 09/01/23 11:26:25.493
Sep  1 11:26:26.528: INFO: 81 pods remaining
Sep  1 11:26:26.528: INFO: 80 pods has nil DeletionTimestamp
Sep  1 11:26:26.528: INFO: 
Sep  1 11:26:27.577: INFO: 73 pods remaining
Sep  1 11:26:27.577: INFO: 71 pods has nil DeletionTimestamp
Sep  1 11:26:27.577: INFO: 
Sep  1 11:26:28.514: INFO: 62 pods remaining
Sep  1 11:26:28.520: INFO: 59 pods has nil DeletionTimestamp
Sep  1 11:26:28.520: INFO: 
Sep  1 11:26:29.540: INFO: 47 pods remaining
Sep  1 11:26:29.540: INFO: 40 pods has nil DeletionTimestamp
Sep  1 11:26:29.540: INFO: 
Sep  1 11:26:30.503: INFO: 41 pods remaining
Sep  1 11:26:30.504: INFO: 31 pods has nil DeletionTimestamp
Sep  1 11:26:30.504: INFO: 
Sep  1 11:26:31.513: INFO: 36 pods remaining
Sep  1 11:26:31.518: INFO: 19 pods has nil DeletionTimestamp
Sep  1 11:26:31.518: INFO: 
Sep  1 11:26:32.514: INFO: 22 pods remaining
Sep  1 11:26:32.514: INFO: 0 pods has nil DeletionTimestamp
Sep  1 11:26:32.514: INFO: 
Sep  1 11:26:33.541: INFO: 19 pods remaining
Sep  1 11:26:33.541: INFO: 0 pods has nil DeletionTimestamp
Sep  1 11:26:33.541: INFO: 
Sep  1 11:26:34.513: INFO: 12 pods remaining
Sep  1 11:26:34.513: INFO: 0 pods has nil DeletionTimestamp
Sep  1 11:26:34.513: INFO: 
Sep  1 11:26:35.513: INFO: 3 pods remaining
Sep  1 11:26:35.513: INFO: 0 pods has nil DeletionTimestamp
Sep  1 11:26:35.513: INFO: 
STEP: Gathering metrics 09/01/23 11:26:36.526
Sep  1 11:26:36.578: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
Sep  1 11:26:36.590: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 11.815237ms
Sep  1 11:26:36.590: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
Sep  1 11:26:36.590: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
Sep  1 11:26:36.771: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  1 11:26:36.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8318" for this suite. 09/01/23 11:26:36.777
------------------------------
• [SLOW TEST] [16.414 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:26:20.371
    Sep  1 11:26:20.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename gc 09/01/23 11:26:20.375
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:26:20.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:26:20.429
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 09/01/23 11:26:20.453
    STEP: delete the rc 09/01/23 11:26:25.481
    STEP: wait for the rc to be deleted 09/01/23 11:26:25.493
    Sep  1 11:26:26.528: INFO: 81 pods remaining
    Sep  1 11:26:26.528: INFO: 80 pods has nil DeletionTimestamp
    Sep  1 11:26:26.528: INFO: 
    Sep  1 11:26:27.577: INFO: 73 pods remaining
    Sep  1 11:26:27.577: INFO: 71 pods has nil DeletionTimestamp
    Sep  1 11:26:27.577: INFO: 
    Sep  1 11:26:28.514: INFO: 62 pods remaining
    Sep  1 11:26:28.520: INFO: 59 pods has nil DeletionTimestamp
    Sep  1 11:26:28.520: INFO: 
    Sep  1 11:26:29.540: INFO: 47 pods remaining
    Sep  1 11:26:29.540: INFO: 40 pods has nil DeletionTimestamp
    Sep  1 11:26:29.540: INFO: 
    Sep  1 11:26:30.503: INFO: 41 pods remaining
    Sep  1 11:26:30.504: INFO: 31 pods has nil DeletionTimestamp
    Sep  1 11:26:30.504: INFO: 
    Sep  1 11:26:31.513: INFO: 36 pods remaining
    Sep  1 11:26:31.518: INFO: 19 pods has nil DeletionTimestamp
    Sep  1 11:26:31.518: INFO: 
    Sep  1 11:26:32.514: INFO: 22 pods remaining
    Sep  1 11:26:32.514: INFO: 0 pods has nil DeletionTimestamp
    Sep  1 11:26:32.514: INFO: 
    Sep  1 11:26:33.541: INFO: 19 pods remaining
    Sep  1 11:26:33.541: INFO: 0 pods has nil DeletionTimestamp
    Sep  1 11:26:33.541: INFO: 
    Sep  1 11:26:34.513: INFO: 12 pods remaining
    Sep  1 11:26:34.513: INFO: 0 pods has nil DeletionTimestamp
    Sep  1 11:26:34.513: INFO: 
    Sep  1 11:26:35.513: INFO: 3 pods remaining
    Sep  1 11:26:35.513: INFO: 0 pods has nil DeletionTimestamp
    Sep  1 11:26:35.513: INFO: 
    STEP: Gathering metrics 09/01/23 11:26:36.526
    Sep  1 11:26:36.578: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
    Sep  1 11:26:36.590: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 11.815237ms
    Sep  1 11:26:36.590: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
    Sep  1 11:26:36.590: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
    Sep  1 11:26:36.771: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:26:36.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8318" for this suite. 09/01/23 11:26:36.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:26:36.785
Sep  1 11:26:36.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-probe 09/01/23 11:26:36.787
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:26:36.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:26:36.831
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f in namespace container-probe-3337 09/01/23 11:26:36.845
Sep  1 11:26:36.858: INFO: Waiting up to 5m0s for pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f" in namespace "container-probe-3337" to be "not pending"
Sep  1 11:26:36.886: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.128927ms
Sep  1 11:26:38.910: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052300006s
Sep  1 11:26:40.893: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034758172s
Sep  1 11:26:42.900: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041974554s
Sep  1 11:26:44.890: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032007504s
Sep  1 11:26:46.904: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.046169179s
Sep  1 11:26:48.921: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.063049538s
Sep  1 11:26:50.902: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.04406449s
Sep  1 11:26:52.891: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.03305379s
Sep  1 11:26:54.891: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.032774174s
Sep  1 11:26:56.891: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.033129214s
Sep  1 11:26:58.890: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.032298374s
Sep  1 11:27:00.890: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Running", Reason="", readiness=true. Elapsed: 24.032063901s
Sep  1 11:27:00.890: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f" satisfied condition "not pending"
Sep  1 11:27:00.890: INFO: Started pod liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f in namespace container-probe-3337
STEP: checking the pod's current state and verifying that restartCount is present 09/01/23 11:27:00.89
Sep  1 11:27:00.893: INFO: Initial restart count of pod liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f is 0
Sep  1 11:27:20.946: INFO: Restart count of pod container-probe-3337/liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f is now 1 (20.052988514s elapsed)
STEP: deleting the pod 09/01/23 11:27:20.946
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  1 11:27:20.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3337" for this suite. 09/01/23 11:27:20.963
------------------------------
• [SLOW TEST] [44.215 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:26:36.785
    Sep  1 11:26:36.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-probe 09/01/23 11:26:36.787
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:26:36.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:26:36.831
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f in namespace container-probe-3337 09/01/23 11:26:36.845
    Sep  1 11:26:36.858: INFO: Waiting up to 5m0s for pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f" in namespace "container-probe-3337" to be "not pending"
    Sep  1 11:26:36.886: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.128927ms
    Sep  1 11:26:38.910: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052300006s
    Sep  1 11:26:40.893: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034758172s
    Sep  1 11:26:42.900: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041974554s
    Sep  1 11:26:44.890: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032007504s
    Sep  1 11:26:46.904: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.046169179s
    Sep  1 11:26:48.921: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.063049538s
    Sep  1 11:26:50.902: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.04406449s
    Sep  1 11:26:52.891: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.03305379s
    Sep  1 11:26:54.891: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.032774174s
    Sep  1 11:26:56.891: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.033129214s
    Sep  1 11:26:58.890: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.032298374s
    Sep  1 11:27:00.890: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f": Phase="Running", Reason="", readiness=true. Elapsed: 24.032063901s
    Sep  1 11:27:00.890: INFO: Pod "liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f" satisfied condition "not pending"
    Sep  1 11:27:00.890: INFO: Started pod liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f in namespace container-probe-3337
    STEP: checking the pod's current state and verifying that restartCount is present 09/01/23 11:27:00.89
    Sep  1 11:27:00.893: INFO: Initial restart count of pod liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f is 0
    Sep  1 11:27:20.946: INFO: Restart count of pod container-probe-3337/liveness-e8bcfe31-ce9e-4091-92f1-2e3b8ee9fc5f is now 1 (20.052988514s elapsed)
    STEP: deleting the pod 09/01/23 11:27:20.946
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:27:20.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3337" for this suite. 09/01/23 11:27:20.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:27:21.006
Sep  1 11:27:21.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename deployment 09/01/23 11:27:21.008
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:21.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:21.085
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Sep  1 11:27:21.089: INFO: Creating deployment "test-recreate-deployment"
Sep  1 11:27:21.094: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  1 11:27:21.102: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep  1 11:27:23.110: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  1 11:27:23.114: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  1 11:27:23.127: INFO: Updating deployment test-recreate-deployment
Sep  1 11:27:23.127: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  1 11:27:23.241: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3860  8a6c2435-d8cc-43d6-bd0c-badc3ea500a5 39917 2 2023-09-01 11:27:21 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c8853a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-01 11:27:23 +0000 UTC,LastTransitionTime:2023-09-01 11:27:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-09-01 11:27:23 +0000 UTC,LastTransitionTime:2023-09-01 11:27:21 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep  1 11:27:23.245: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-3860  46c45e60-b762-4891-9841-368f1b48917f 39915 1 2023-09-01 11:27:23 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8a6c2435-d8cc-43d6-bd0c-badc3ea500a5 0xc0070a7720 0xc0070a7721}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a6c2435-d8cc-43d6-bd0c-badc3ea500a5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0070a77b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  1 11:27:23.245: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  1 11:27:23.245: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-3860  44aaf113-3d9e-435f-8d0d-52e83c24407d 39904 2 2023-09-01 11:27:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8a6c2435-d8cc-43d6-bd0c-badc3ea500a5 0xc0070a7607 0xc0070a7608}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a6c2435-d8cc-43d6-bd0c-badc3ea500a5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0070a76b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  1 11:27:23.253: INFO: Pod "test-recreate-deployment-cff6dc657-s89dt" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-s89dt test-recreate-deployment-cff6dc657- deployment-3860  757d01aa-13cc-4924-9821-2688d12193a5 39913 0 2023-09-01 11:27:23 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 46c45e60-b762-4891-9841-368f1b48917f 0xc00c885730 0xc00c885731}] [] [{kube-controller-manager Update v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46c45e60-b762-4891-9841-368f1b48917f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bk4jn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bk4jn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:27:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:27:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:27:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:27:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:,StartTime:2023-09-01 11:27:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  1 11:27:23.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3860" for this suite. 09/01/23 11:27:23.258
------------------------------
• [2.261 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:27:21.006
    Sep  1 11:27:21.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename deployment 09/01/23 11:27:21.008
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:21.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:21.085
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Sep  1 11:27:21.089: INFO: Creating deployment "test-recreate-deployment"
    Sep  1 11:27:21.094: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Sep  1 11:27:21.102: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Sep  1 11:27:23.110: INFO: Waiting deployment "test-recreate-deployment" to complete
    Sep  1 11:27:23.114: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Sep  1 11:27:23.127: INFO: Updating deployment test-recreate-deployment
    Sep  1 11:27:23.127: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  1 11:27:23.241: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-3860  8a6c2435-d8cc-43d6-bd0c-badc3ea500a5 39917 2 2023-09-01 11:27:21 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c8853a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-01 11:27:23 +0000 UTC,LastTransitionTime:2023-09-01 11:27:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-09-01 11:27:23 +0000 UTC,LastTransitionTime:2023-09-01 11:27:21 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Sep  1 11:27:23.245: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-3860  46c45e60-b762-4891-9841-368f1b48917f 39915 1 2023-09-01 11:27:23 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8a6c2435-d8cc-43d6-bd0c-badc3ea500a5 0xc0070a7720 0xc0070a7721}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a6c2435-d8cc-43d6-bd0c-badc3ea500a5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0070a77b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  1 11:27:23.245: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Sep  1 11:27:23.245: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-3860  44aaf113-3d9e-435f-8d0d-52e83c24407d 39904 2 2023-09-01 11:27:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8a6c2435-d8cc-43d6-bd0c-badc3ea500a5 0xc0070a7607 0xc0070a7608}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a6c2435-d8cc-43d6-bd0c-badc3ea500a5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0070a76b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  1 11:27:23.253: INFO: Pod "test-recreate-deployment-cff6dc657-s89dt" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-s89dt test-recreate-deployment-cff6dc657- deployment-3860  757d01aa-13cc-4924-9821-2688d12193a5 39913 0 2023-09-01 11:27:23 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 46c45e60-b762-4891-9841-368f1b48917f 0xc00c885730 0xc00c885731}] [] [{kube-controller-manager Update v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46c45e60-b762-4891-9841-368f1b48917f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bk4jn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bk4jn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:27:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:27:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:27:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:27:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:,StartTime:2023-09-01 11:27:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:27:23.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3860" for this suite. 09/01/23 11:27:23.258
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:27:23.277
Sep  1 11:27:23.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename svcaccounts 09/01/23 11:27:23.28
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:23.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:23.3
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Sep  1 11:27:23.321: INFO: created pod pod-service-account-defaultsa
Sep  1 11:27:23.321: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  1 11:27:23.330: INFO: created pod pod-service-account-mountsa
Sep  1 11:27:23.330: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  1 11:27:23.344: INFO: created pod pod-service-account-nomountsa
Sep  1 11:27:23.344: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  1 11:27:23.360: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  1 11:27:23.360: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  1 11:27:23.367: INFO: created pod pod-service-account-mountsa-mountspec
Sep  1 11:27:23.367: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  1 11:27:23.381: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  1 11:27:23.381: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  1 11:27:23.392: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  1 11:27:23.392: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  1 11:27:23.403: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  1 11:27:23.403: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  1 11:27:23.413: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  1 11:27:23.413: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  1 11:27:23.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3017" for this suite. 09/01/23 11:27:23.42
------------------------------
• [0.156 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:27:23.277
    Sep  1 11:27:23.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename svcaccounts 09/01/23 11:27:23.28
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:23.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:23.3
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Sep  1 11:27:23.321: INFO: created pod pod-service-account-defaultsa
    Sep  1 11:27:23.321: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Sep  1 11:27:23.330: INFO: created pod pod-service-account-mountsa
    Sep  1 11:27:23.330: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Sep  1 11:27:23.344: INFO: created pod pod-service-account-nomountsa
    Sep  1 11:27:23.344: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Sep  1 11:27:23.360: INFO: created pod pod-service-account-defaultsa-mountspec
    Sep  1 11:27:23.360: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Sep  1 11:27:23.367: INFO: created pod pod-service-account-mountsa-mountspec
    Sep  1 11:27:23.367: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Sep  1 11:27:23.381: INFO: created pod pod-service-account-nomountsa-mountspec
    Sep  1 11:27:23.381: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Sep  1 11:27:23.392: INFO: created pod pod-service-account-defaultsa-nomountspec
    Sep  1 11:27:23.392: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Sep  1 11:27:23.403: INFO: created pod pod-service-account-mountsa-nomountspec
    Sep  1 11:27:23.403: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Sep  1 11:27:23.413: INFO: created pod pod-service-account-nomountsa-nomountspec
    Sep  1 11:27:23.413: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:27:23.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3017" for this suite. 09/01/23 11:27:23.42
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:27:23.433
Sep  1 11:27:23.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename watch 09/01/23 11:27:23.436
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:23.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:23.466
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 09/01/23 11:27:23.475
STEP: modifying the configmap once 09/01/23 11:27:23.487
STEP: modifying the configmap a second time 09/01/23 11:27:23.499
STEP: deleting the configmap 09/01/23 11:27:23.513
STEP: creating a watch on configmaps from the resource version returned by the first update 09/01/23 11:27:23.519
STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/01/23 11:27:23.522
Sep  1 11:27:23.522: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8482  5a28d487-f6b8-47c9-8bba-3df0b06c2a80 39962 0 2023-09-01 11:27:23 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  1 11:27:23.523: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8482  5a28d487-f6b8-47c9-8bba-3df0b06c2a80 39963 0 2023-09-01 11:27:23 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  1 11:27:23.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8482" for this suite. 09/01/23 11:27:23.53
------------------------------
• [0.104 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:27:23.433
    Sep  1 11:27:23.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename watch 09/01/23 11:27:23.436
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:23.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:23.466
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 09/01/23 11:27:23.475
    STEP: modifying the configmap once 09/01/23 11:27:23.487
    STEP: modifying the configmap a second time 09/01/23 11:27:23.499
    STEP: deleting the configmap 09/01/23 11:27:23.513
    STEP: creating a watch on configmaps from the resource version returned by the first update 09/01/23 11:27:23.519
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/01/23 11:27:23.522
    Sep  1 11:27:23.522: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8482  5a28d487-f6b8-47c9-8bba-3df0b06c2a80 39962 0 2023-09-01 11:27:23 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  1 11:27:23.523: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8482  5a28d487-f6b8-47c9-8bba-3df0b06c2a80 39963 0 2023-09-01 11:27:23 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-01 11:27:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:27:23.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8482" for this suite. 09/01/23 11:27:23.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:27:23.545
Sep  1 11:27:23.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename security-context-test 09/01/23 11:27:23.546
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:23.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:23.577
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Sep  1 11:27:23.593: INFO: Waiting up to 5m0s for pod "busybox-user-65534-d48e5274-2407-4b0e-9063-8f264a5da191" in namespace "security-context-test-8804" to be "Succeeded or Failed"
Sep  1 11:27:23.600: INFO: Pod "busybox-user-65534-d48e5274-2407-4b0e-9063-8f264a5da191": Phase="Pending", Reason="", readiness=false. Elapsed: 6.606911ms
Sep  1 11:27:25.604: INFO: Pod "busybox-user-65534-d48e5274-2407-4b0e-9063-8f264a5da191": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010535184s
Sep  1 11:27:27.604: INFO: Pod "busybox-user-65534-d48e5274-2407-4b0e-9063-8f264a5da191": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010970225s
Sep  1 11:27:27.604: INFO: Pod "busybox-user-65534-d48e5274-2407-4b0e-9063-8f264a5da191" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  1 11:27:27.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8804" for this suite. 09/01/23 11:27:27.611
------------------------------
• [4.078 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:27:23.545
    Sep  1 11:27:23.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename security-context-test 09/01/23 11:27:23.546
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:23.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:23.577
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Sep  1 11:27:23.593: INFO: Waiting up to 5m0s for pod "busybox-user-65534-d48e5274-2407-4b0e-9063-8f264a5da191" in namespace "security-context-test-8804" to be "Succeeded or Failed"
    Sep  1 11:27:23.600: INFO: Pod "busybox-user-65534-d48e5274-2407-4b0e-9063-8f264a5da191": Phase="Pending", Reason="", readiness=false. Elapsed: 6.606911ms
    Sep  1 11:27:25.604: INFO: Pod "busybox-user-65534-d48e5274-2407-4b0e-9063-8f264a5da191": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010535184s
    Sep  1 11:27:27.604: INFO: Pod "busybox-user-65534-d48e5274-2407-4b0e-9063-8f264a5da191": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010970225s
    Sep  1 11:27:27.604: INFO: Pod "busybox-user-65534-d48e5274-2407-4b0e-9063-8f264a5da191" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:27:27.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8804" for this suite. 09/01/23 11:27:27.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:27:27.629
Sep  1 11:27:27.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 11:27:27.632
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:27.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:27.658
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 09/01/23 11:27:27.662
Sep  1 11:27:27.676: INFO: Waiting up to 5m0s for pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab" in namespace "emptydir-1661" to be "Succeeded or Failed"
Sep  1 11:27:27.680: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.963928ms
Sep  1 11:27:29.684: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008339186s
Sep  1 11:27:31.684: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008641927s
Sep  1 11:27:33.684: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab": Phase="Running", Reason="", readiness=true. Elapsed: 6.00870678s
Sep  1 11:27:35.684: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab": Phase="Running", Reason="", readiness=true. Elapsed: 8.008168756s
Sep  1 11:27:37.685: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab": Phase="Running", Reason="", readiness=true. Elapsed: 10.009406988s
Sep  1 11:27:39.685: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.009249454s
STEP: Saw pod success 09/01/23 11:27:39.685
Sep  1 11:27:39.686: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab" satisfied condition "Succeeded or Failed"
Sep  1 11:27:39.689: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-27911911-7b5c-47d8-ac2b-20fc636c6dab container test-container: <nil>
STEP: delete the pod 09/01/23 11:27:39.71
Sep  1 11:27:39.726: INFO: Waiting for pod pod-27911911-7b5c-47d8-ac2b-20fc636c6dab to disappear
Sep  1 11:27:39.729: INFO: Pod pod-27911911-7b5c-47d8-ac2b-20fc636c6dab no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 11:27:39.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1661" for this suite. 09/01/23 11:27:39.734
------------------------------
• [SLOW TEST] [12.112 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:27:27.629
    Sep  1 11:27:27.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 11:27:27.632
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:27.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:27.658
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/01/23 11:27:27.662
    Sep  1 11:27:27.676: INFO: Waiting up to 5m0s for pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab" in namespace "emptydir-1661" to be "Succeeded or Failed"
    Sep  1 11:27:27.680: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.963928ms
    Sep  1 11:27:29.684: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008339186s
    Sep  1 11:27:31.684: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008641927s
    Sep  1 11:27:33.684: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab": Phase="Running", Reason="", readiness=true. Elapsed: 6.00870678s
    Sep  1 11:27:35.684: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab": Phase="Running", Reason="", readiness=true. Elapsed: 8.008168756s
    Sep  1 11:27:37.685: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab": Phase="Running", Reason="", readiness=true. Elapsed: 10.009406988s
    Sep  1 11:27:39.685: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.009249454s
    STEP: Saw pod success 09/01/23 11:27:39.685
    Sep  1 11:27:39.686: INFO: Pod "pod-27911911-7b5c-47d8-ac2b-20fc636c6dab" satisfied condition "Succeeded or Failed"
    Sep  1 11:27:39.689: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-27911911-7b5c-47d8-ac2b-20fc636c6dab container test-container: <nil>
    STEP: delete the pod 09/01/23 11:27:39.71
    Sep  1 11:27:39.726: INFO: Waiting for pod pod-27911911-7b5c-47d8-ac2b-20fc636c6dab to disappear
    Sep  1 11:27:39.729: INFO: Pod pod-27911911-7b5c-47d8-ac2b-20fc636c6dab no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:27:39.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1661" for this suite. 09/01/23 11:27:39.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:27:39.743
Sep  1 11:27:39.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:27:39.745
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:39.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:39.774
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 09/01/23 11:27:39.783
Sep  1 11:27:39.796: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a" in namespace "projected-2308" to be "Succeeded or Failed"
Sep  1 11:27:39.800: INFO: Pod "downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07358ms
Sep  1 11:27:41.805: INFO: Pod "downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008650847s
Sep  1 11:27:43.805: INFO: Pod "downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008718107s
STEP: Saw pod success 09/01/23 11:27:43.805
Sep  1 11:27:43.805: INFO: Pod "downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a" satisfied condition "Succeeded or Failed"
Sep  1 11:27:43.812: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a container client-container: <nil>
STEP: delete the pod 09/01/23 11:27:43.819
Sep  1 11:27:43.835: INFO: Waiting for pod downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a to disappear
Sep  1 11:27:43.838: INFO: Pod downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  1 11:27:43.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2308" for this suite. 09/01/23 11:27:43.842
------------------------------
• [4.107 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:27:39.743
    Sep  1 11:27:39.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:27:39.745
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:39.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:39.774
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 09/01/23 11:27:39.783
    Sep  1 11:27:39.796: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a" in namespace "projected-2308" to be "Succeeded or Failed"
    Sep  1 11:27:39.800: INFO: Pod "downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07358ms
    Sep  1 11:27:41.805: INFO: Pod "downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008650847s
    Sep  1 11:27:43.805: INFO: Pod "downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008718107s
    STEP: Saw pod success 09/01/23 11:27:43.805
    Sep  1 11:27:43.805: INFO: Pod "downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a" satisfied condition "Succeeded or Failed"
    Sep  1 11:27:43.812: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a container client-container: <nil>
    STEP: delete the pod 09/01/23 11:27:43.819
    Sep  1 11:27:43.835: INFO: Waiting for pod downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a to disappear
    Sep  1 11:27:43.838: INFO: Pod downwardapi-volume-6d3085db-557b-432e-91ee-2ef46511682a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:27:43.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2308" for this suite. 09/01/23 11:27:43.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:27:43.854
Sep  1 11:27:43.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pods 09/01/23 11:27:43.855
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:43.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:43.881
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 09/01/23 11:27:43.885
Sep  1 11:27:43.895: INFO: Waiting up to 5m0s for pod "pod-hostip-57457bdf-3044-4e74-9124-72b749cb9c3f" in namespace "pods-1700" to be "running and ready"
Sep  1 11:27:43.900: INFO: Pod "pod-hostip-57457bdf-3044-4e74-9124-72b749cb9c3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.953864ms
Sep  1 11:27:43.901: INFO: The phase of Pod pod-hostip-57457bdf-3044-4e74-9124-72b749cb9c3f is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:27:45.907: INFO: Pod "pod-hostip-57457bdf-3044-4e74-9124-72b749cb9c3f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011751937s
Sep  1 11:27:45.907: INFO: The phase of Pod pod-hostip-57457bdf-3044-4e74-9124-72b749cb9c3f is Running (Ready = true)
Sep  1 11:27:45.907: INFO: Pod "pod-hostip-57457bdf-3044-4e74-9124-72b749cb9c3f" satisfied condition "running and ready"
Sep  1 11:27:45.919: INFO: Pod pod-hostip-57457bdf-3044-4e74-9124-72b749cb9c3f has hostIP: 172.16.0.3
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  1 11:27:45.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1700" for this suite. 09/01/23 11:27:45.926
------------------------------
• [2.081 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:27:43.854
    Sep  1 11:27:43.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pods 09/01/23 11:27:43.855
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:43.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:43.881
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 09/01/23 11:27:43.885
    Sep  1 11:27:43.895: INFO: Waiting up to 5m0s for pod "pod-hostip-57457bdf-3044-4e74-9124-72b749cb9c3f" in namespace "pods-1700" to be "running and ready"
    Sep  1 11:27:43.900: INFO: Pod "pod-hostip-57457bdf-3044-4e74-9124-72b749cb9c3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.953864ms
    Sep  1 11:27:43.901: INFO: The phase of Pod pod-hostip-57457bdf-3044-4e74-9124-72b749cb9c3f is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:27:45.907: INFO: Pod "pod-hostip-57457bdf-3044-4e74-9124-72b749cb9c3f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011751937s
    Sep  1 11:27:45.907: INFO: The phase of Pod pod-hostip-57457bdf-3044-4e74-9124-72b749cb9c3f is Running (Ready = true)
    Sep  1 11:27:45.907: INFO: Pod "pod-hostip-57457bdf-3044-4e74-9124-72b749cb9c3f" satisfied condition "running and ready"
    Sep  1 11:27:45.919: INFO: Pod pod-hostip-57457bdf-3044-4e74-9124-72b749cb9c3f has hostIP: 172.16.0.3
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:27:45.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1700" for this suite. 09/01/23 11:27:45.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:27:45.936
Sep  1 11:27:45.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pods 09/01/23 11:27:45.939
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:45.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:45.959
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 09/01/23 11:27:45.962
Sep  1 11:27:45.971: INFO: Waiting up to 5m0s for pod "pod-z9hqn" in namespace "pods-3290" to be "running"
Sep  1 11:27:45.974: INFO: Pod "pod-z9hqn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.917406ms
Sep  1 11:27:47.979: INFO: Pod "pod-z9hqn": Phase="Running", Reason="", readiness=true. Elapsed: 2.007442194s
Sep  1 11:27:47.979: INFO: Pod "pod-z9hqn" satisfied condition "running"
STEP: patching /status 09/01/23 11:27:47.979
Sep  1 11:27:47.988: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  1 11:27:47.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3290" for this suite. 09/01/23 11:27:47.992
------------------------------
• [2.065 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:27:45.936
    Sep  1 11:27:45.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pods 09/01/23 11:27:45.939
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:45.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:45.959
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 09/01/23 11:27:45.962
    Sep  1 11:27:45.971: INFO: Waiting up to 5m0s for pod "pod-z9hqn" in namespace "pods-3290" to be "running"
    Sep  1 11:27:45.974: INFO: Pod "pod-z9hqn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.917406ms
    Sep  1 11:27:47.979: INFO: Pod "pod-z9hqn": Phase="Running", Reason="", readiness=true. Elapsed: 2.007442194s
    Sep  1 11:27:47.979: INFO: Pod "pod-z9hqn" satisfied condition "running"
    STEP: patching /status 09/01/23 11:27:47.979
    Sep  1 11:27:47.988: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:27:47.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3290" for this suite. 09/01/23 11:27:47.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:27:48.004
Sep  1 11:27:48.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename cronjob 09/01/23 11:27:48.005
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:48.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:48.024
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 09/01/23 11:27:48.027
STEP: Ensuring a job is scheduled 09/01/23 11:27:48.034
STEP: Ensuring exactly one is scheduled 09/01/23 11:28:02.038
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/01/23 11:28:02.042
STEP: Ensuring no more jobs are scheduled 09/01/23 11:28:02.046
STEP: Removing cronjob 09/01/23 11:33:02.094
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  1 11:33:02.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1724" for this suite. 09/01/23 11:33:02.105
------------------------------
• [SLOW TEST] [314.140 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:27:48.004
    Sep  1 11:27:48.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename cronjob 09/01/23 11:27:48.005
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:27:48.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:27:48.024
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 09/01/23 11:27:48.027
    STEP: Ensuring a job is scheduled 09/01/23 11:27:48.034
    STEP: Ensuring exactly one is scheduled 09/01/23 11:28:02.038
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/01/23 11:28:02.042
    STEP: Ensuring no more jobs are scheduled 09/01/23 11:28:02.046
    STEP: Removing cronjob 09/01/23 11:33:02.094
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:33:02.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1724" for this suite. 09/01/23 11:33:02.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:33:02.152
Sep  1 11:33:02.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename resourcequota 09/01/23 11:33:02.154
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:02.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:02.186
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 09/01/23 11:33:02.19
STEP: Ensuring ResourceQuota status is calculated 09/01/23 11:33:02.195
STEP: Creating a ResourceQuota with not terminating scope 09/01/23 11:33:04.2
STEP: Ensuring ResourceQuota status is calculated 09/01/23 11:33:04.205
STEP: Creating a long running pod 09/01/23 11:33:06.21
STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/01/23 11:33:06.229
STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/01/23 11:33:08.233
STEP: Deleting the pod 09/01/23 11:33:10.238
STEP: Ensuring resource quota status released the pod usage 09/01/23 11:33:10.263
STEP: Creating a terminating pod 09/01/23 11:33:12.267
STEP: Ensuring resource quota with terminating scope captures the pod usage 09/01/23 11:33:12.284
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/01/23 11:33:14.288
STEP: Deleting the pod 09/01/23 11:33:16.293
STEP: Ensuring resource quota status released the pod usage 09/01/23 11:33:16.312
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  1 11:33:18.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-799" for this suite. 09/01/23 11:33:18.323
------------------------------
• [SLOW TEST] [16.178 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:33:02.152
    Sep  1 11:33:02.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename resourcequota 09/01/23 11:33:02.154
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:02.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:02.186
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 09/01/23 11:33:02.19
    STEP: Ensuring ResourceQuota status is calculated 09/01/23 11:33:02.195
    STEP: Creating a ResourceQuota with not terminating scope 09/01/23 11:33:04.2
    STEP: Ensuring ResourceQuota status is calculated 09/01/23 11:33:04.205
    STEP: Creating a long running pod 09/01/23 11:33:06.21
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/01/23 11:33:06.229
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/01/23 11:33:08.233
    STEP: Deleting the pod 09/01/23 11:33:10.238
    STEP: Ensuring resource quota status released the pod usage 09/01/23 11:33:10.263
    STEP: Creating a terminating pod 09/01/23 11:33:12.267
    STEP: Ensuring resource quota with terminating scope captures the pod usage 09/01/23 11:33:12.284
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/01/23 11:33:14.288
    STEP: Deleting the pod 09/01/23 11:33:16.293
    STEP: Ensuring resource quota status released the pod usage 09/01/23 11:33:16.312
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:33:18.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-799" for this suite. 09/01/23 11:33:18.323
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:33:18.335
Sep  1 11:33:18.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pods 09/01/23 11:33:18.338
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:18.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:18.424
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 09/01/23 11:33:18.427
STEP: setting up watch 09/01/23 11:33:18.427
STEP: submitting the pod to kubernetes 09/01/23 11:33:18.531
STEP: verifying the pod is in kubernetes 09/01/23 11:33:18.54
STEP: verifying pod creation was observed 09/01/23 11:33:18.544
Sep  1 11:33:18.544: INFO: Waiting up to 5m0s for pod "pod-submit-remove-e5850d03-6a97-4908-aeeb-6959a5e2215c" in namespace "pods-767" to be "running"
Sep  1 11:33:18.550: INFO: Pod "pod-submit-remove-e5850d03-6a97-4908-aeeb-6959a5e2215c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.134113ms
Sep  1 11:33:20.556: INFO: Pod "pod-submit-remove-e5850d03-6a97-4908-aeeb-6959a5e2215c": Phase="Running", Reason="", readiness=true. Elapsed: 2.01171218s
Sep  1 11:33:20.556: INFO: Pod "pod-submit-remove-e5850d03-6a97-4908-aeeb-6959a5e2215c" satisfied condition "running"
STEP: deleting the pod gracefully 09/01/23 11:33:20.561
STEP: verifying pod deletion was observed 09/01/23 11:33:20.571
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  1 11:33:22.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-767" for this suite. 09/01/23 11:33:22.553
------------------------------
• [4.226 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:33:18.335
    Sep  1 11:33:18.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pods 09/01/23 11:33:18.338
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:18.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:18.424
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 09/01/23 11:33:18.427
    STEP: setting up watch 09/01/23 11:33:18.427
    STEP: submitting the pod to kubernetes 09/01/23 11:33:18.531
    STEP: verifying the pod is in kubernetes 09/01/23 11:33:18.54
    STEP: verifying pod creation was observed 09/01/23 11:33:18.544
    Sep  1 11:33:18.544: INFO: Waiting up to 5m0s for pod "pod-submit-remove-e5850d03-6a97-4908-aeeb-6959a5e2215c" in namespace "pods-767" to be "running"
    Sep  1 11:33:18.550: INFO: Pod "pod-submit-remove-e5850d03-6a97-4908-aeeb-6959a5e2215c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.134113ms
    Sep  1 11:33:20.556: INFO: Pod "pod-submit-remove-e5850d03-6a97-4908-aeeb-6959a5e2215c": Phase="Running", Reason="", readiness=true. Elapsed: 2.01171218s
    Sep  1 11:33:20.556: INFO: Pod "pod-submit-remove-e5850d03-6a97-4908-aeeb-6959a5e2215c" satisfied condition "running"
    STEP: deleting the pod gracefully 09/01/23 11:33:20.561
    STEP: verifying pod deletion was observed 09/01/23 11:33:20.571
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:33:22.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-767" for this suite. 09/01/23 11:33:22.553
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:33:22.564
Sep  1 11:33:22.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:33:22.566
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:22.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:22.589
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 09/01/23 11:33:22.592
Sep  1 11:33:22.602: INFO: Waiting up to 5m0s for pod "annotationupdate5d72e727-4967-4942-b5da-64a38367a447" in namespace "projected-2540" to be "running and ready"
Sep  1 11:33:22.608: INFO: Pod "annotationupdate5d72e727-4967-4942-b5da-64a38367a447": Phase="Pending", Reason="", readiness=false. Elapsed: 5.961871ms
Sep  1 11:33:22.608: INFO: The phase of Pod annotationupdate5d72e727-4967-4942-b5da-64a38367a447 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:33:24.613: INFO: Pod "annotationupdate5d72e727-4967-4942-b5da-64a38367a447": Phase="Running", Reason="", readiness=true. Elapsed: 2.011106037s
Sep  1 11:33:24.613: INFO: The phase of Pod annotationupdate5d72e727-4967-4942-b5da-64a38367a447 is Running (Ready = true)
Sep  1 11:33:24.613: INFO: Pod "annotationupdate5d72e727-4967-4942-b5da-64a38367a447" satisfied condition "running and ready"
Sep  1 11:33:25.167: INFO: Successfully updated pod "annotationupdate5d72e727-4967-4942-b5da-64a38367a447"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  1 11:33:29.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2540" for this suite. 09/01/23 11:33:29.195
------------------------------
• [SLOW TEST] [6.641 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:33:22.564
    Sep  1 11:33:22.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:33:22.566
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:22.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:22.589
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 09/01/23 11:33:22.592
    Sep  1 11:33:22.602: INFO: Waiting up to 5m0s for pod "annotationupdate5d72e727-4967-4942-b5da-64a38367a447" in namespace "projected-2540" to be "running and ready"
    Sep  1 11:33:22.608: INFO: Pod "annotationupdate5d72e727-4967-4942-b5da-64a38367a447": Phase="Pending", Reason="", readiness=false. Elapsed: 5.961871ms
    Sep  1 11:33:22.608: INFO: The phase of Pod annotationupdate5d72e727-4967-4942-b5da-64a38367a447 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:33:24.613: INFO: Pod "annotationupdate5d72e727-4967-4942-b5da-64a38367a447": Phase="Running", Reason="", readiness=true. Elapsed: 2.011106037s
    Sep  1 11:33:24.613: INFO: The phase of Pod annotationupdate5d72e727-4967-4942-b5da-64a38367a447 is Running (Ready = true)
    Sep  1 11:33:24.613: INFO: Pod "annotationupdate5d72e727-4967-4942-b5da-64a38367a447" satisfied condition "running and ready"
    Sep  1 11:33:25.167: INFO: Successfully updated pod "annotationupdate5d72e727-4967-4942-b5da-64a38367a447"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:33:29.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2540" for this suite. 09/01/23 11:33:29.195
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:33:29.211
Sep  1 11:33:29.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename gc 09/01/23 11:33:29.214
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:29.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:29.262
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Sep  1 11:33:29.318: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b0b8728e-5e01-46d9-9394-1e1fefbb617a", Controller:(*bool)(0xc00ded33b6), BlockOwnerDeletion:(*bool)(0xc00ded33b7)}}
Sep  1 11:33:29.334: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"18ddf2f7-1c26-4232-bb1f-fcfc204f6aa2", Controller:(*bool)(0xc007e2f39e), BlockOwnerDeletion:(*bool)(0xc007e2f39f)}}
Sep  1 11:33:29.351: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"eb45e4ba-3851-4694-a817-64d70dcc3d1a", Controller:(*bool)(0xc007e2f5ae), BlockOwnerDeletion:(*bool)(0xc007e2f5af)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  1 11:33:34.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-369" for this suite. 09/01/23 11:33:34.388
------------------------------
• [SLOW TEST] [5.186 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:33:29.211
    Sep  1 11:33:29.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename gc 09/01/23 11:33:29.214
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:29.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:29.262
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Sep  1 11:33:29.318: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b0b8728e-5e01-46d9-9394-1e1fefbb617a", Controller:(*bool)(0xc00ded33b6), BlockOwnerDeletion:(*bool)(0xc00ded33b7)}}
    Sep  1 11:33:29.334: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"18ddf2f7-1c26-4232-bb1f-fcfc204f6aa2", Controller:(*bool)(0xc007e2f39e), BlockOwnerDeletion:(*bool)(0xc007e2f39f)}}
    Sep  1 11:33:29.351: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"eb45e4ba-3851-4694-a817-64d70dcc3d1a", Controller:(*bool)(0xc007e2f5ae), BlockOwnerDeletion:(*bool)(0xc007e2f5af)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:33:34.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-369" for this suite. 09/01/23 11:33:34.388
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:33:34.398
Sep  1 11:33:34.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename runtimeclass 09/01/23 11:33:34.401
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:34.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:34.433
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Sep  1 11:33:34.461: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-425 to be scheduled
Sep  1 11:33:34.469: INFO: 1 pods are not scheduled: [runtimeclass-425/test-runtimeclass-runtimeclass-425-preconfigured-handler-kq2kx(6438e446-f230-4891-b704-30e77305504b)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  1 11:33:36.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-425" for this suite. 09/01/23 11:33:36.485
------------------------------
• [2.098 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:33:34.398
    Sep  1 11:33:34.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename runtimeclass 09/01/23 11:33:34.401
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:34.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:34.433
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Sep  1 11:33:34.461: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-425 to be scheduled
    Sep  1 11:33:34.469: INFO: 1 pods are not scheduled: [runtimeclass-425/test-runtimeclass-runtimeclass-425-preconfigured-handler-kq2kx(6438e446-f230-4891-b704-30e77305504b)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:33:36.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-425" for this suite. 09/01/23 11:33:36.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:33:36.504
Sep  1 11:33:36.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename namespaces 09/01/23 11:33:36.505
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:36.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:36.529
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 09/01/23 11:33:36.533
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:36.555
STEP: Creating a service in the namespace 09/01/23 11:33:36.559
STEP: Deleting the namespace 09/01/23 11:33:36.578
STEP: Waiting for the namespace to be removed. 09/01/23 11:33:36.599
STEP: Recreating the namespace 09/01/23 11:33:43.605
STEP: Verifying there is no service in the namespace 09/01/23 11:33:43.63
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:33:43.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8955" for this suite. 09/01/23 11:33:43.646
STEP: Destroying namespace "nsdeletetest-2487" for this suite. 09/01/23 11:33:43.652
Sep  1 11:33:43.656: INFO: Namespace nsdeletetest-2487 was already deleted
STEP: Destroying namespace "nsdeletetest-1622" for this suite. 09/01/23 11:33:43.656
------------------------------
• [SLOW TEST] [7.168 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:33:36.504
    Sep  1 11:33:36.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename namespaces 09/01/23 11:33:36.505
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:36.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:36.529
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 09/01/23 11:33:36.533
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:36.555
    STEP: Creating a service in the namespace 09/01/23 11:33:36.559
    STEP: Deleting the namespace 09/01/23 11:33:36.578
    STEP: Waiting for the namespace to be removed. 09/01/23 11:33:36.599
    STEP: Recreating the namespace 09/01/23 11:33:43.605
    STEP: Verifying there is no service in the namespace 09/01/23 11:33:43.63
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:33:43.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8955" for this suite. 09/01/23 11:33:43.646
    STEP: Destroying namespace "nsdeletetest-2487" for this suite. 09/01/23 11:33:43.652
    Sep  1 11:33:43.656: INFO: Namespace nsdeletetest-2487 was already deleted
    STEP: Destroying namespace "nsdeletetest-1622" for this suite. 09/01/23 11:33:43.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:33:43.678
Sep  1 11:33:43.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename var-expansion 09/01/23 11:33:43.679
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:43.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:43.71
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 09/01/23 11:33:43.714
Sep  1 11:33:43.727: INFO: Waiting up to 5m0s for pod "var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60" in namespace "var-expansion-2867" to be "Succeeded or Failed"
Sep  1 11:33:43.732: INFO: Pod "var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.998525ms
Sep  1 11:33:45.738: INFO: Pod "var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010745511s
Sep  1 11:33:47.759: INFO: Pod "var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031673421s
STEP: Saw pod success 09/01/23 11:33:47.759
Sep  1 11:33:47.759: INFO: Pod "var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60" satisfied condition "Succeeded or Failed"
Sep  1 11:33:47.768: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60 container dapi-container: <nil>
STEP: delete the pod 09/01/23 11:33:47.781
Sep  1 11:33:47.813: INFO: Waiting for pod var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60 to disappear
Sep  1 11:33:47.828: INFO: Pod var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  1 11:33:47.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2867" for this suite. 09/01/23 11:33:47.837
------------------------------
• [4.237 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:33:43.678
    Sep  1 11:33:43.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename var-expansion 09/01/23 11:33:43.679
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:43.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:43.71
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 09/01/23 11:33:43.714
    Sep  1 11:33:43.727: INFO: Waiting up to 5m0s for pod "var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60" in namespace "var-expansion-2867" to be "Succeeded or Failed"
    Sep  1 11:33:43.732: INFO: Pod "var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.998525ms
    Sep  1 11:33:45.738: INFO: Pod "var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010745511s
    Sep  1 11:33:47.759: INFO: Pod "var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031673421s
    STEP: Saw pod success 09/01/23 11:33:47.759
    Sep  1 11:33:47.759: INFO: Pod "var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60" satisfied condition "Succeeded or Failed"
    Sep  1 11:33:47.768: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60 container dapi-container: <nil>
    STEP: delete the pod 09/01/23 11:33:47.781
    Sep  1 11:33:47.813: INFO: Waiting for pod var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60 to disappear
    Sep  1 11:33:47.828: INFO: Pod var-expansion-c44a4038-a832-49c8-84a1-a23637b12f60 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:33:47.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2867" for this suite. 09/01/23 11:33:47.837
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:33:47.916
Sep  1 11:33:47.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 11:33:47.918
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:48.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:48.025
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 11:33:48.148
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:33:48.596
STEP: Deploying the webhook pod 09/01/23 11:33:48.646
STEP: Wait for the deployment to be ready 09/01/23 11:33:48.728
Sep  1 11:33:48.844: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/01/23 11:33:50.856
STEP: Verifying the service has paired with the endpoint 09/01/23 11:33:50.876
Sep  1 11:33:51.877: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 09/01/23 11:33:51.988
STEP: Creating a configMap that does not comply to the validation webhook rules 09/01/23 11:33:52.043
STEP: Deleting the collection of validation webhooks 09/01/23 11:33:52.093
STEP: Creating a configMap that does not comply to the validation webhook rules 09/01/23 11:33:52.168
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:33:52.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9685" for this suite. 09/01/23 11:33:52.286
STEP: Destroying namespace "webhook-9685-markers" for this suite. 09/01/23 11:33:52.308
------------------------------
• [4.416 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:33:47.916
    Sep  1 11:33:47.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 11:33:47.918
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:48.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:48.025
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 11:33:48.148
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:33:48.596
    STEP: Deploying the webhook pod 09/01/23 11:33:48.646
    STEP: Wait for the deployment to be ready 09/01/23 11:33:48.728
    Sep  1 11:33:48.844: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/01/23 11:33:50.856
    STEP: Verifying the service has paired with the endpoint 09/01/23 11:33:50.876
    Sep  1 11:33:51.877: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 09/01/23 11:33:51.988
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/01/23 11:33:52.043
    STEP: Deleting the collection of validation webhooks 09/01/23 11:33:52.093
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/01/23 11:33:52.168
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:33:52.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9685" for this suite. 09/01/23 11:33:52.286
    STEP: Destroying namespace "webhook-9685-markers" for this suite. 09/01/23 11:33:52.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:33:52.337
Sep  1 11:33:52.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename disruption 09/01/23 11:33:52.339
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:52.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:52.369
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 09/01/23 11:33:52.375
STEP: Waiting for the pdb to be processed 09/01/23 11:33:52.387
STEP: updating the pdb 09/01/23 11:33:52.395
STEP: Waiting for the pdb to be processed 09/01/23 11:33:52.409
STEP: patching the pdb 09/01/23 11:33:52.425
STEP: Waiting for the pdb to be processed 09/01/23 11:33:52.455
STEP: Waiting for the pdb to be deleted 09/01/23 11:33:54.481
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  1 11:33:54.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7285" for this suite. 09/01/23 11:33:54.492
------------------------------
• [2.171 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:33:52.337
    Sep  1 11:33:52.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename disruption 09/01/23 11:33:52.339
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:52.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:52.369
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 09/01/23 11:33:52.375
    STEP: Waiting for the pdb to be processed 09/01/23 11:33:52.387
    STEP: updating the pdb 09/01/23 11:33:52.395
    STEP: Waiting for the pdb to be processed 09/01/23 11:33:52.409
    STEP: patching the pdb 09/01/23 11:33:52.425
    STEP: Waiting for the pdb to be processed 09/01/23 11:33:52.455
    STEP: Waiting for the pdb to be deleted 09/01/23 11:33:54.481
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:33:54.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7285" for this suite. 09/01/23 11:33:54.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:33:54.508
Sep  1 11:33:54.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 11:33:54.509
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:54.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:54.537
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 11:33:54.554
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:33:54.925
STEP: Deploying the webhook pod 09/01/23 11:33:54.931
STEP: Wait for the deployment to be ready 09/01/23 11:33:54.947
Sep  1 11:33:54.957: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/01/23 11:33:56.968
STEP: Verifying the service has paired with the endpoint 09/01/23 11:33:56.985
Sep  1 11:33:57.986: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 09/01/23 11:33:58.094
STEP: Creating a configMap that should be mutated 09/01/23 11:33:58.114
STEP: Deleting the collection of validation webhooks 09/01/23 11:33:58.156
STEP: Creating a configMap that should not be mutated 09/01/23 11:33:58.204
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:33:58.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2729" for this suite. 09/01/23 11:33:58.398
STEP: Destroying namespace "webhook-2729-markers" for this suite. 09/01/23 11:33:58.426
------------------------------
• [3.944 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:33:54.508
    Sep  1 11:33:54.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 11:33:54.509
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:54.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:54.537
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 11:33:54.554
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:33:54.925
    STEP: Deploying the webhook pod 09/01/23 11:33:54.931
    STEP: Wait for the deployment to be ready 09/01/23 11:33:54.947
    Sep  1 11:33:54.957: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/01/23 11:33:56.968
    STEP: Verifying the service has paired with the endpoint 09/01/23 11:33:56.985
    Sep  1 11:33:57.986: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 09/01/23 11:33:58.094
    STEP: Creating a configMap that should be mutated 09/01/23 11:33:58.114
    STEP: Deleting the collection of validation webhooks 09/01/23 11:33:58.156
    STEP: Creating a configMap that should not be mutated 09/01/23 11:33:58.204
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:33:58.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2729" for this suite. 09/01/23 11:33:58.398
    STEP: Destroying namespace "webhook-2729-markers" for this suite. 09/01/23 11:33:58.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:33:58.466
Sep  1 11:33:58.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename custom-resource-definition 09/01/23 11:33:58.468
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:58.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:58.517
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Sep  1 11:33:58.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:34:08.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8622" for this suite. 09/01/23 11:34:08.633
------------------------------
• [SLOW TEST] [10.178 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:33:58.466
    Sep  1 11:33:58.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename custom-resource-definition 09/01/23 11:33:58.468
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:33:58.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:33:58.517
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Sep  1 11:33:58.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:34:08.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8622" for this suite. 09/01/23 11:34:08.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:34:08.646
Sep  1 11:34:08.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename security-context-test 09/01/23 11:34:08.649
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:34:08.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:34:08.671
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Sep  1 11:34:08.688: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-eedd0742-6786-4bcc-8940-85f3ea921608" in namespace "security-context-test-4162" to be "Succeeded or Failed"
Sep  1 11:34:08.693: INFO: Pod "busybox-readonly-false-eedd0742-6786-4bcc-8940-85f3ea921608": Phase="Pending", Reason="", readiness=false. Elapsed: 5.080557ms
Sep  1 11:34:10.714: INFO: Pod "busybox-readonly-false-eedd0742-6786-4bcc-8940-85f3ea921608": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02574861s
Sep  1 11:34:12.700: INFO: Pod "busybox-readonly-false-eedd0742-6786-4bcc-8940-85f3ea921608": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012365184s
Sep  1 11:34:12.701: INFO: Pod "busybox-readonly-false-eedd0742-6786-4bcc-8940-85f3ea921608" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  1 11:34:12.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4162" for this suite. 09/01/23 11:34:12.71
------------------------------
• [4.074 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:34:08.646
    Sep  1 11:34:08.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename security-context-test 09/01/23 11:34:08.649
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:34:08.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:34:08.671
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Sep  1 11:34:08.688: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-eedd0742-6786-4bcc-8940-85f3ea921608" in namespace "security-context-test-4162" to be "Succeeded or Failed"
    Sep  1 11:34:08.693: INFO: Pod "busybox-readonly-false-eedd0742-6786-4bcc-8940-85f3ea921608": Phase="Pending", Reason="", readiness=false. Elapsed: 5.080557ms
    Sep  1 11:34:10.714: INFO: Pod "busybox-readonly-false-eedd0742-6786-4bcc-8940-85f3ea921608": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02574861s
    Sep  1 11:34:12.700: INFO: Pod "busybox-readonly-false-eedd0742-6786-4bcc-8940-85f3ea921608": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012365184s
    Sep  1 11:34:12.701: INFO: Pod "busybox-readonly-false-eedd0742-6786-4bcc-8940-85f3ea921608" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:34:12.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4162" for this suite. 09/01/23 11:34:12.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:34:12.728
Sep  1 11:34:12.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename namespaces 09/01/23 11:34:12.73
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:34:12.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:34:12.759
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-2212" 09/01/23 11:34:12.764
Sep  1 11:34:12.778: INFO: Namespace "namespaces-2212" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"8adb3b46-bd81-48ef-adc4-5eb7453ac860", "kubernetes.io/metadata.name":"namespaces-2212", "namespaces-2212":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:34:12.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2212" for this suite. 09/01/23 11:34:12.787
------------------------------
• [0.066 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:34:12.728
    Sep  1 11:34:12.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename namespaces 09/01/23 11:34:12.73
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:34:12.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:34:12.759
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-2212" 09/01/23 11:34:12.764
    Sep  1 11:34:12.778: INFO: Namespace "namespaces-2212" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"8adb3b46-bd81-48ef-adc4-5eb7453ac860", "kubernetes.io/metadata.name":"namespaces-2212", "namespaces-2212":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:34:12.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2212" for this suite. 09/01/23 11:34:12.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:34:12.807
Sep  1 11:34:12.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 11:34:12.809
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:34:12.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:34:12.868
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-2ae5b8b8-430d-4947-978b-a50463225094 09/01/23 11:34:12.886
STEP: Creating the pod 09/01/23 11:34:12.895
Sep  1 11:34:12.907: INFO: Waiting up to 5m0s for pod "pod-configmaps-c56eaaa0-4924-48b8-9dca-df820a7bd803" in namespace "configmap-6533" to be "running and ready"
Sep  1 11:34:12.916: INFO: Pod "pod-configmaps-c56eaaa0-4924-48b8-9dca-df820a7bd803": Phase="Pending", Reason="", readiness=false. Elapsed: 8.576712ms
Sep  1 11:34:12.916: INFO: The phase of Pod pod-configmaps-c56eaaa0-4924-48b8-9dca-df820a7bd803 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:34:14.920: INFO: Pod "pod-configmaps-c56eaaa0-4924-48b8-9dca-df820a7bd803": Phase="Running", Reason="", readiness=true. Elapsed: 2.012519249s
Sep  1 11:34:14.920: INFO: The phase of Pod pod-configmaps-c56eaaa0-4924-48b8-9dca-df820a7bd803 is Running (Ready = true)
Sep  1 11:34:14.920: INFO: Pod "pod-configmaps-c56eaaa0-4924-48b8-9dca-df820a7bd803" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-2ae5b8b8-430d-4947-978b-a50463225094 09/01/23 11:34:14.928
STEP: waiting to observe update in volume 09/01/23 11:34:14.934
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:34:16.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6533" for this suite. 09/01/23 11:34:16.96
------------------------------
• [4.160 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:34:12.807
    Sep  1 11:34:12.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 11:34:12.809
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:34:12.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:34:12.868
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-2ae5b8b8-430d-4947-978b-a50463225094 09/01/23 11:34:12.886
    STEP: Creating the pod 09/01/23 11:34:12.895
    Sep  1 11:34:12.907: INFO: Waiting up to 5m0s for pod "pod-configmaps-c56eaaa0-4924-48b8-9dca-df820a7bd803" in namespace "configmap-6533" to be "running and ready"
    Sep  1 11:34:12.916: INFO: Pod "pod-configmaps-c56eaaa0-4924-48b8-9dca-df820a7bd803": Phase="Pending", Reason="", readiness=false. Elapsed: 8.576712ms
    Sep  1 11:34:12.916: INFO: The phase of Pod pod-configmaps-c56eaaa0-4924-48b8-9dca-df820a7bd803 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:34:14.920: INFO: Pod "pod-configmaps-c56eaaa0-4924-48b8-9dca-df820a7bd803": Phase="Running", Reason="", readiness=true. Elapsed: 2.012519249s
    Sep  1 11:34:14.920: INFO: The phase of Pod pod-configmaps-c56eaaa0-4924-48b8-9dca-df820a7bd803 is Running (Ready = true)
    Sep  1 11:34:14.920: INFO: Pod "pod-configmaps-c56eaaa0-4924-48b8-9dca-df820a7bd803" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-2ae5b8b8-430d-4947-978b-a50463225094 09/01/23 11:34:14.928
    STEP: waiting to observe update in volume 09/01/23 11:34:14.934
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:34:16.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6533" for this suite. 09/01/23 11:34:16.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:34:16.973
Sep  1 11:34:16.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename sched-preemption 09/01/23 11:34:16.975
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:34:16.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:34:16.997
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  1 11:34:17.014: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  1 11:35:17.062: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:35:17.065
Sep  1 11:35:17.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename sched-preemption-path 09/01/23 11:35:17.067
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:35:17.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:35:17.088
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 09/01/23 11:35:17.091
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/01/23 11:35:17.092
Sep  1 11:35:17.101: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6193" to be "running"
Sep  1 11:35:17.104: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.162938ms
Sep  1 11:35:19.109: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007226358s
Sep  1 11:35:19.109: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/01/23 11:35:19.115
Sep  1 11:35:19.130: INFO: found a healthy node: k8s-worker-1.c.operations-lab.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Sep  1 11:35:25.330: INFO: pods created so far: [1 1 1]
Sep  1 11:35:25.330: INFO: length of pods created so far: 3
Sep  1 11:35:27.341: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Sep  1 11:35:34.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:35:34.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-6193" for this suite. 09/01/23 11:35:34.443
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5" for this suite. 09/01/23 11:35:34.451
------------------------------
• [SLOW TEST] [77.488 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:34:16.973
    Sep  1 11:34:16.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename sched-preemption 09/01/23 11:34:16.975
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:34:16.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:34:16.997
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  1 11:34:17.014: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  1 11:35:17.062: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:35:17.065
    Sep  1 11:35:17.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename sched-preemption-path 09/01/23 11:35:17.067
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:35:17.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:35:17.088
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 09/01/23 11:35:17.091
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/01/23 11:35:17.092
    Sep  1 11:35:17.101: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6193" to be "running"
    Sep  1 11:35:17.104: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.162938ms
    Sep  1 11:35:19.109: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007226358s
    Sep  1 11:35:19.109: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/01/23 11:35:19.115
    Sep  1 11:35:19.130: INFO: found a healthy node: k8s-worker-1.c.operations-lab.internal
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Sep  1 11:35:25.330: INFO: pods created so far: [1 1 1]
    Sep  1 11:35:25.330: INFO: length of pods created so far: 3
    Sep  1 11:35:27.341: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:35:34.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:35:34.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-6193" for this suite. 09/01/23 11:35:34.443
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5" for this suite. 09/01/23 11:35:34.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:35:34.464
Sep  1 11:35:34.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 11:35:34.466
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:35:34.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:35:34.494
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 09/01/23 11:35:34.499
Sep  1 11:35:34.509: INFO: Waiting up to 5m0s for pod "downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb" in namespace "downward-api-6113" to be "Succeeded or Failed"
Sep  1 11:35:34.513: INFO: Pod "downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.790993ms
Sep  1 11:35:36.517: INFO: Pod "downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007711093s
Sep  1 11:35:38.517: INFO: Pod "downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007412498s
STEP: Saw pod success 09/01/23 11:35:38.517
Sep  1 11:35:38.517: INFO: Pod "downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb" satisfied condition "Succeeded or Failed"
Sep  1 11:35:38.521: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb container dapi-container: <nil>
STEP: delete the pod 09/01/23 11:35:38.527
Sep  1 11:35:38.541: INFO: Waiting for pod downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb to disappear
Sep  1 11:35:38.544: INFO: Pod downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  1 11:35:38.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6113" for this suite. 09/01/23 11:35:38.549
------------------------------
• [4.095 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:35:34.464
    Sep  1 11:35:34.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 11:35:34.466
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:35:34.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:35:34.494
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 09/01/23 11:35:34.499
    Sep  1 11:35:34.509: INFO: Waiting up to 5m0s for pod "downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb" in namespace "downward-api-6113" to be "Succeeded or Failed"
    Sep  1 11:35:34.513: INFO: Pod "downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.790993ms
    Sep  1 11:35:36.517: INFO: Pod "downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007711093s
    Sep  1 11:35:38.517: INFO: Pod "downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007412498s
    STEP: Saw pod success 09/01/23 11:35:38.517
    Sep  1 11:35:38.517: INFO: Pod "downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb" satisfied condition "Succeeded or Failed"
    Sep  1 11:35:38.521: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb container dapi-container: <nil>
    STEP: delete the pod 09/01/23 11:35:38.527
    Sep  1 11:35:38.541: INFO: Waiting for pod downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb to disappear
    Sep  1 11:35:38.544: INFO: Pod downward-api-437e66b6-025c-48c0-9ef9-8914c975b0bb no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:35:38.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6113" for this suite. 09/01/23 11:35:38.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:35:38.571
Sep  1 11:35:38.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename custom-resource-definition 09/01/23 11:35:38.573
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:35:38.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:35:38.593
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Sep  1 11:35:38.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:35:39.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9375" for this suite. 09/01/23 11:35:39.677
------------------------------
• [1.135 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:35:38.571
    Sep  1 11:35:38.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename custom-resource-definition 09/01/23 11:35:38.573
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:35:38.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:35:38.593
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Sep  1 11:35:38.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:35:39.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9375" for this suite. 09/01/23 11:35:39.677
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:35:39.711
Sep  1 11:35:39.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:35:39.713
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:35:39.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:35:39.86
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-a27ff97b-6e55-4bc4-9646-ebb1e7d5e7a8 09/01/23 11:35:39.871
STEP: Creating a pod to test consume configMaps 09/01/23 11:35:39.902
Sep  1 11:35:39.964: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921" in namespace "projected-5253" to be "Succeeded or Failed"
Sep  1 11:35:40.052: INFO: Pod "pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921": Phase="Pending", Reason="", readiness=false. Elapsed: 87.213101ms
Sep  1 11:35:42.069: INFO: Pod "pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921": Phase="Running", Reason="", readiness=false. Elapsed: 2.10511619s
Sep  1 11:35:44.058: INFO: Pod "pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.09340723s
STEP: Saw pod success 09/01/23 11:35:44.058
Sep  1 11:35:44.058: INFO: Pod "pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921" satisfied condition "Succeeded or Failed"
Sep  1 11:35:44.061: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921 container projected-configmap-volume-test: <nil>
STEP: delete the pod 09/01/23 11:35:44.069
Sep  1 11:35:44.111: INFO: Waiting for pod pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921 to disappear
Sep  1 11:35:44.118: INFO: Pod pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:35:44.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5253" for this suite. 09/01/23 11:35:44.124
------------------------------
• [4.421 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:35:39.711
    Sep  1 11:35:39.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:35:39.713
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:35:39.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:35:39.86
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-a27ff97b-6e55-4bc4-9646-ebb1e7d5e7a8 09/01/23 11:35:39.871
    STEP: Creating a pod to test consume configMaps 09/01/23 11:35:39.902
    Sep  1 11:35:39.964: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921" in namespace "projected-5253" to be "Succeeded or Failed"
    Sep  1 11:35:40.052: INFO: Pod "pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921": Phase="Pending", Reason="", readiness=false. Elapsed: 87.213101ms
    Sep  1 11:35:42.069: INFO: Pod "pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921": Phase="Running", Reason="", readiness=false. Elapsed: 2.10511619s
    Sep  1 11:35:44.058: INFO: Pod "pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.09340723s
    STEP: Saw pod success 09/01/23 11:35:44.058
    Sep  1 11:35:44.058: INFO: Pod "pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921" satisfied condition "Succeeded or Failed"
    Sep  1 11:35:44.061: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 09/01/23 11:35:44.069
    Sep  1 11:35:44.111: INFO: Waiting for pod pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921 to disappear
    Sep  1 11:35:44.118: INFO: Pod pod-projected-configmaps-2313d794-dd4d-459c-93a9-dfa636095921 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:35:44.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5253" for this suite. 09/01/23 11:35:44.124
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:35:44.137
Sep  1 11:35:44.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 11:35:44.14
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:35:44.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:35:44.165
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 09/01/23 11:35:44.169
Sep  1 11:35:44.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b" in namespace "downward-api-8548" to be "Succeeded or Failed"
Sep  1 11:35:44.185: INFO: Pod "downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.88248ms
Sep  1 11:35:46.189: INFO: Pod "downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008697083s
Sep  1 11:35:48.190: INFO: Pod "downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00941111s
STEP: Saw pod success 09/01/23 11:35:48.19
Sep  1 11:35:48.190: INFO: Pod "downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b" satisfied condition "Succeeded or Failed"
Sep  1 11:35:48.193: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b container client-container: <nil>
STEP: delete the pod 09/01/23 11:35:48.2
Sep  1 11:35:48.213: INFO: Waiting for pod downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b to disappear
Sep  1 11:35:48.216: INFO: Pod downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  1 11:35:48.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8548" for this suite. 09/01/23 11:35:48.222
------------------------------
• [4.092 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:35:44.137
    Sep  1 11:35:44.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 11:35:44.14
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:35:44.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:35:44.165
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 09/01/23 11:35:44.169
    Sep  1 11:35:44.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b" in namespace "downward-api-8548" to be "Succeeded or Failed"
    Sep  1 11:35:44.185: INFO: Pod "downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.88248ms
    Sep  1 11:35:46.189: INFO: Pod "downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008697083s
    Sep  1 11:35:48.190: INFO: Pod "downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00941111s
    STEP: Saw pod success 09/01/23 11:35:48.19
    Sep  1 11:35:48.190: INFO: Pod "downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b" satisfied condition "Succeeded or Failed"
    Sep  1 11:35:48.193: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b container client-container: <nil>
    STEP: delete the pod 09/01/23 11:35:48.2
    Sep  1 11:35:48.213: INFO: Waiting for pod downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b to disappear
    Sep  1 11:35:48.216: INFO: Pod downwardapi-volume-ae312940-4f1f-4e38-89ec-d22bdbcbf14b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:35:48.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8548" for this suite. 09/01/23 11:35:48.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:35:48.235
Sep  1 11:35:48.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:35:48.236
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:35:48.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:35:48.258
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 09/01/23 11:35:48.262
Sep  1 11:35:48.276: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9" in namespace "projected-7176" to be "Succeeded or Failed"
Sep  1 11:35:48.282: INFO: Pod "downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.893561ms
Sep  1 11:35:50.286: INFO: Pod "downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009361414s
Sep  1 11:35:52.289: INFO: Pod "downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012756381s
STEP: Saw pod success 09/01/23 11:35:52.289
Sep  1 11:35:52.290: INFO: Pod "downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9" satisfied condition "Succeeded or Failed"
Sep  1 11:35:52.293: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9 container client-container: <nil>
STEP: delete the pod 09/01/23 11:35:52.3
Sep  1 11:35:52.314: INFO: Waiting for pod downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9 to disappear
Sep  1 11:35:52.316: INFO: Pod downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  1 11:35:52.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7176" for this suite. 09/01/23 11:35:52.321
------------------------------
• [4.093 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:35:48.235
    Sep  1 11:35:48.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:35:48.236
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:35:48.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:35:48.258
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 09/01/23 11:35:48.262
    Sep  1 11:35:48.276: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9" in namespace "projected-7176" to be "Succeeded or Failed"
    Sep  1 11:35:48.282: INFO: Pod "downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.893561ms
    Sep  1 11:35:50.286: INFO: Pod "downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009361414s
    Sep  1 11:35:52.289: INFO: Pod "downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012756381s
    STEP: Saw pod success 09/01/23 11:35:52.289
    Sep  1 11:35:52.290: INFO: Pod "downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9" satisfied condition "Succeeded or Failed"
    Sep  1 11:35:52.293: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9 container client-container: <nil>
    STEP: delete the pod 09/01/23 11:35:52.3
    Sep  1 11:35:52.314: INFO: Waiting for pod downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9 to disappear
    Sep  1 11:35:52.316: INFO: Pod downwardapi-volume-ac7e2348-e01b-450a-9f69-05ca881786b9 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:35:52.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7176" for this suite. 09/01/23 11:35:52.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:35:52.329
Sep  1 11:35:52.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-probe 09/01/23 11:35:52.33
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:35:52.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:35:52.354
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-b29b5d8d-fa99-4344-a225-9afcd89e60a0 in namespace container-probe-3286 09/01/23 11:35:52.357
Sep  1 11:35:52.367: INFO: Waiting up to 5m0s for pod "busybox-b29b5d8d-fa99-4344-a225-9afcd89e60a0" in namespace "container-probe-3286" to be "not pending"
Sep  1 11:35:52.371: INFO: Pod "busybox-b29b5d8d-fa99-4344-a225-9afcd89e60a0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.526743ms
Sep  1 11:35:54.376: INFO: Pod "busybox-b29b5d8d-fa99-4344-a225-9afcd89e60a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.008231125s
Sep  1 11:35:54.376: INFO: Pod "busybox-b29b5d8d-fa99-4344-a225-9afcd89e60a0" satisfied condition "not pending"
Sep  1 11:35:54.376: INFO: Started pod busybox-b29b5d8d-fa99-4344-a225-9afcd89e60a0 in namespace container-probe-3286
STEP: checking the pod's current state and verifying that restartCount is present 09/01/23 11:35:54.376
Sep  1 11:35:54.380: INFO: Initial restart count of pod busybox-b29b5d8d-fa99-4344-a225-9afcd89e60a0 is 0
STEP: deleting the pod 09/01/23 11:39:54.922
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  1 11:39:54.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3286" for this suite. 09/01/23 11:39:54.95
------------------------------
• [SLOW TEST] [242.659 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:35:52.329
    Sep  1 11:35:52.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-probe 09/01/23 11:35:52.33
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:35:52.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:35:52.354
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-b29b5d8d-fa99-4344-a225-9afcd89e60a0 in namespace container-probe-3286 09/01/23 11:35:52.357
    Sep  1 11:35:52.367: INFO: Waiting up to 5m0s for pod "busybox-b29b5d8d-fa99-4344-a225-9afcd89e60a0" in namespace "container-probe-3286" to be "not pending"
    Sep  1 11:35:52.371: INFO: Pod "busybox-b29b5d8d-fa99-4344-a225-9afcd89e60a0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.526743ms
    Sep  1 11:35:54.376: INFO: Pod "busybox-b29b5d8d-fa99-4344-a225-9afcd89e60a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.008231125s
    Sep  1 11:35:54.376: INFO: Pod "busybox-b29b5d8d-fa99-4344-a225-9afcd89e60a0" satisfied condition "not pending"
    Sep  1 11:35:54.376: INFO: Started pod busybox-b29b5d8d-fa99-4344-a225-9afcd89e60a0 in namespace container-probe-3286
    STEP: checking the pod's current state and verifying that restartCount is present 09/01/23 11:35:54.376
    Sep  1 11:35:54.380: INFO: Initial restart count of pod busybox-b29b5d8d-fa99-4344-a225-9afcd89e60a0 is 0
    STEP: deleting the pod 09/01/23 11:39:54.922
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:39:54.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3286" for this suite. 09/01/23 11:39:54.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:39:54.988
Sep  1 11:39:54.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename endpointslice 09/01/23 11:39:54.99
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:39:55.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:39:55.022
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  1 11:39:57.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-299" for this suite. 09/01/23 11:39:57.122
------------------------------
• [2.148 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:39:54.988
    Sep  1 11:39:54.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename endpointslice 09/01/23 11:39:54.99
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:39:55.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:39:55.022
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:39:57.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-299" for this suite. 09/01/23 11:39:57.122
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:39:57.142
Sep  1 11:39:57.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 11:39:57.144
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:39:57.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:39:57.183
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-3707 09/01/23 11:39:57.188
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3707 to expose endpoints map[] 09/01/23 11:39:57.231
Sep  1 11:39:57.238: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Sep  1 11:39:58.247: INFO: successfully validated that service endpoint-test2 in namespace services-3707 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3707 09/01/23 11:39:58.248
Sep  1 11:39:58.256: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3707" to be "running and ready"
Sep  1 11:39:58.261: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.539172ms
Sep  1 11:39:58.261: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:40:00.266: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009062757s
Sep  1 11:40:00.266: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  1 11:40:00.266: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3707 to expose endpoints map[pod1:[80]] 09/01/23 11:40:00.279
Sep  1 11:40:00.292: INFO: successfully validated that service endpoint-test2 in namespace services-3707 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 09/01/23 11:40:00.292
Sep  1 11:40:00.292: INFO: Creating new exec pod
Sep  1 11:40:00.300: INFO: Waiting up to 5m0s for pod "execpod7qwrb" in namespace "services-3707" to be "running"
Sep  1 11:40:00.304: INFO: Pod "execpod7qwrb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.342172ms
Sep  1 11:40:02.308: INFO: Pod "execpod7qwrb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007384787s
Sep  1 11:40:02.308: INFO: Pod "execpod7qwrb" satisfied condition "running"
Sep  1 11:40:03.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-3707 exec execpod7qwrb -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  1 11:40:03.512: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  1 11:40:03.512: INFO: stdout: ""
Sep  1 11:40:03.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-3707 exec execpod7qwrb -- /bin/sh -x -c nc -v -z -w 2 10.99.82.173 80'
Sep  1 11:40:03.707: INFO: stderr: "+ nc -v -z -w 2 10.99.82.173 80\nConnection to 10.99.82.173 80 port [tcp/http] succeeded!\n"
Sep  1 11:40:03.707: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-3707 09/01/23 11:40:03.707
Sep  1 11:40:03.714: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3707" to be "running and ready"
Sep  1 11:40:03.725: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.768919ms
Sep  1 11:40:03.725: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:40:05.729: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015439405s
Sep  1 11:40:05.730: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:40:07.729: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.014813751s
Sep  1 11:40:07.729: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  1 11:40:07.729: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3707 to expose endpoints map[pod1:[80] pod2:[80]] 09/01/23 11:40:07.732
Sep  1 11:40:07.748: INFO: successfully validated that service endpoint-test2 in namespace services-3707 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 09/01/23 11:40:07.748
Sep  1 11:40:08.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-3707 exec execpod7qwrb -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  1 11:40:08.963: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  1 11:40:08.963: INFO: stdout: ""
Sep  1 11:40:08.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-3707 exec execpod7qwrb -- /bin/sh -x -c nc -v -z -w 2 10.99.82.173 80'
Sep  1 11:40:09.152: INFO: stderr: "+ nc -v -z -w 2 10.99.82.173 80\nConnection to 10.99.82.173 80 port [tcp/http] succeeded!\n"
Sep  1 11:40:09.153: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-3707 09/01/23 11:40:09.153
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3707 to expose endpoints map[pod2:[80]] 09/01/23 11:40:09.172
Sep  1 11:40:09.255: INFO: successfully validated that service endpoint-test2 in namespace services-3707 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 09/01/23 11:40:09.255
Sep  1 11:40:10.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-3707 exec execpod7qwrb -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  1 11:40:10.467: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  1 11:40:10.467: INFO: stdout: ""
Sep  1 11:40:10.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-3707 exec execpod7qwrb -- /bin/sh -x -c nc -v -z -w 2 10.99.82.173 80'
Sep  1 11:40:10.658: INFO: stderr: "+ nc -v -z -w 2 10.99.82.173 80\nConnection to 10.99.82.173 80 port [tcp/http] succeeded!\n"
Sep  1 11:40:10.658: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-3707 09/01/23 11:40:10.658
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3707 to expose endpoints map[] 09/01/23 11:40:10.675
Sep  1 11:40:11.714: INFO: successfully validated that service endpoint-test2 in namespace services-3707 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 11:40:11.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3707" for this suite. 09/01/23 11:40:11.746
------------------------------
• [SLOW TEST] [14.616 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:39:57.142
    Sep  1 11:39:57.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 11:39:57.144
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:39:57.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:39:57.183
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-3707 09/01/23 11:39:57.188
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3707 to expose endpoints map[] 09/01/23 11:39:57.231
    Sep  1 11:39:57.238: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Sep  1 11:39:58.247: INFO: successfully validated that service endpoint-test2 in namespace services-3707 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3707 09/01/23 11:39:58.248
    Sep  1 11:39:58.256: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3707" to be "running and ready"
    Sep  1 11:39:58.261: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.539172ms
    Sep  1 11:39:58.261: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:40:00.266: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009062757s
    Sep  1 11:40:00.266: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  1 11:40:00.266: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3707 to expose endpoints map[pod1:[80]] 09/01/23 11:40:00.279
    Sep  1 11:40:00.292: INFO: successfully validated that service endpoint-test2 in namespace services-3707 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 09/01/23 11:40:00.292
    Sep  1 11:40:00.292: INFO: Creating new exec pod
    Sep  1 11:40:00.300: INFO: Waiting up to 5m0s for pod "execpod7qwrb" in namespace "services-3707" to be "running"
    Sep  1 11:40:00.304: INFO: Pod "execpod7qwrb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.342172ms
    Sep  1 11:40:02.308: INFO: Pod "execpod7qwrb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007384787s
    Sep  1 11:40:02.308: INFO: Pod "execpod7qwrb" satisfied condition "running"
    Sep  1 11:40:03.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-3707 exec execpod7qwrb -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  1 11:40:03.512: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  1 11:40:03.512: INFO: stdout: ""
    Sep  1 11:40:03.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-3707 exec execpod7qwrb -- /bin/sh -x -c nc -v -z -w 2 10.99.82.173 80'
    Sep  1 11:40:03.707: INFO: stderr: "+ nc -v -z -w 2 10.99.82.173 80\nConnection to 10.99.82.173 80 port [tcp/http] succeeded!\n"
    Sep  1 11:40:03.707: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-3707 09/01/23 11:40:03.707
    Sep  1 11:40:03.714: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3707" to be "running and ready"
    Sep  1 11:40:03.725: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.768919ms
    Sep  1 11:40:03.725: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:40:05.729: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015439405s
    Sep  1 11:40:05.730: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:40:07.729: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.014813751s
    Sep  1 11:40:07.729: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  1 11:40:07.729: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3707 to expose endpoints map[pod1:[80] pod2:[80]] 09/01/23 11:40:07.732
    Sep  1 11:40:07.748: INFO: successfully validated that service endpoint-test2 in namespace services-3707 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 09/01/23 11:40:07.748
    Sep  1 11:40:08.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-3707 exec execpod7qwrb -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  1 11:40:08.963: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  1 11:40:08.963: INFO: stdout: ""
    Sep  1 11:40:08.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-3707 exec execpod7qwrb -- /bin/sh -x -c nc -v -z -w 2 10.99.82.173 80'
    Sep  1 11:40:09.152: INFO: stderr: "+ nc -v -z -w 2 10.99.82.173 80\nConnection to 10.99.82.173 80 port [tcp/http] succeeded!\n"
    Sep  1 11:40:09.153: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-3707 09/01/23 11:40:09.153
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3707 to expose endpoints map[pod2:[80]] 09/01/23 11:40:09.172
    Sep  1 11:40:09.255: INFO: successfully validated that service endpoint-test2 in namespace services-3707 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 09/01/23 11:40:09.255
    Sep  1 11:40:10.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-3707 exec execpod7qwrb -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  1 11:40:10.467: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  1 11:40:10.467: INFO: stdout: ""
    Sep  1 11:40:10.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-3707 exec execpod7qwrb -- /bin/sh -x -c nc -v -z -w 2 10.99.82.173 80'
    Sep  1 11:40:10.658: INFO: stderr: "+ nc -v -z -w 2 10.99.82.173 80\nConnection to 10.99.82.173 80 port [tcp/http] succeeded!\n"
    Sep  1 11:40:10.658: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-3707 09/01/23 11:40:10.658
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3707 to expose endpoints map[] 09/01/23 11:40:10.675
    Sep  1 11:40:11.714: INFO: successfully validated that service endpoint-test2 in namespace services-3707 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:40:11.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3707" for this suite. 09/01/23 11:40:11.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:40:11.765
Sep  1 11:40:11.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 11:40:11.766
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:40:11.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:40:11.789
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 09/01/23 11:40:11.793
Sep  1 11:40:11.807: INFO: Waiting up to 5m0s for pod "pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14" in namespace "emptydir-2741" to be "Succeeded or Failed"
Sep  1 11:40:11.811: INFO: Pod "pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14": Phase="Pending", Reason="", readiness=false. Elapsed: 3.683912ms
Sep  1 11:40:13.814: INFO: Pod "pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14": Phase="Running", Reason="", readiness=false. Elapsed: 2.006977688s
Sep  1 11:40:15.819: INFO: Pod "pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012183112s
STEP: Saw pod success 09/01/23 11:40:15.819
Sep  1 11:40:15.819: INFO: Pod "pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14" satisfied condition "Succeeded or Failed"
Sep  1 11:40:15.822: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14 container test-container: <nil>
STEP: delete the pod 09/01/23 11:40:15.846
Sep  1 11:40:15.862: INFO: Waiting for pod pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14 to disappear
Sep  1 11:40:15.865: INFO: Pod pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 11:40:15.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2741" for this suite. 09/01/23 11:40:15.87
------------------------------
• [4.112 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:40:11.765
    Sep  1 11:40:11.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 11:40:11.766
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:40:11.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:40:11.789
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/01/23 11:40:11.793
    Sep  1 11:40:11.807: INFO: Waiting up to 5m0s for pod "pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14" in namespace "emptydir-2741" to be "Succeeded or Failed"
    Sep  1 11:40:11.811: INFO: Pod "pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14": Phase="Pending", Reason="", readiness=false. Elapsed: 3.683912ms
    Sep  1 11:40:13.814: INFO: Pod "pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14": Phase="Running", Reason="", readiness=false. Elapsed: 2.006977688s
    Sep  1 11:40:15.819: INFO: Pod "pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012183112s
    STEP: Saw pod success 09/01/23 11:40:15.819
    Sep  1 11:40:15.819: INFO: Pod "pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14" satisfied condition "Succeeded or Failed"
    Sep  1 11:40:15.822: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14 container test-container: <nil>
    STEP: delete the pod 09/01/23 11:40:15.846
    Sep  1 11:40:15.862: INFO: Waiting for pod pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14 to disappear
    Sep  1 11:40:15.865: INFO: Pod pod-e0df3691-e71d-45a4-a163-55b2eeeb0a14 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:40:15.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2741" for this suite. 09/01/23 11:40:15.87
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:40:15.877
Sep  1 11:40:15.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 11:40:15.878
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:40:15.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:40:15.947
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 11:40:15.971
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:40:16.392
STEP: Deploying the webhook pod 09/01/23 11:40:16.402
STEP: Wait for the deployment to be ready 09/01/23 11:40:16.421
Sep  1 11:40:16.449: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/01/23 11:40:18.458
STEP: Verifying the service has paired with the endpoint 09/01/23 11:40:18.473
Sep  1 11:40:19.474: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 09/01/23 11:40:19.479
STEP: create a pod 09/01/23 11:40:19.499
Sep  1 11:40:19.507: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4929" to be "running"
Sep  1 11:40:19.511: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.806115ms
Sep  1 11:40:21.516: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008636323s
Sep  1 11:40:21.516: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 09/01/23 11:40:21.516
Sep  1 11:40:21.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=webhook-4929 attach --namespace=webhook-4929 to-be-attached-pod -i -c=container1'
Sep  1 11:40:21.633: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:40:21.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4929" for this suite. 09/01/23 11:40:21.714
STEP: Destroying namespace "webhook-4929-markers" for this suite. 09/01/23 11:40:21.739
------------------------------
• [SLOW TEST] [5.874 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:40:15.877
    Sep  1 11:40:15.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 11:40:15.878
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:40:15.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:40:15.947
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 11:40:15.971
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:40:16.392
    STEP: Deploying the webhook pod 09/01/23 11:40:16.402
    STEP: Wait for the deployment to be ready 09/01/23 11:40:16.421
    Sep  1 11:40:16.449: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/01/23 11:40:18.458
    STEP: Verifying the service has paired with the endpoint 09/01/23 11:40:18.473
    Sep  1 11:40:19.474: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 09/01/23 11:40:19.479
    STEP: create a pod 09/01/23 11:40:19.499
    Sep  1 11:40:19.507: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4929" to be "running"
    Sep  1 11:40:19.511: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.806115ms
    Sep  1 11:40:21.516: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008636323s
    Sep  1 11:40:21.516: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 09/01/23 11:40:21.516
    Sep  1 11:40:21.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=webhook-4929 attach --namespace=webhook-4929 to-be-attached-pod -i -c=container1'
    Sep  1 11:40:21.633: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:40:21.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4929" for this suite. 09/01/23 11:40:21.714
    STEP: Destroying namespace "webhook-4929-markers" for this suite. 09/01/23 11:40:21.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:40:21.752
Sep  1 11:40:21.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename containers 09/01/23 11:40:21.759
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:40:21.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:40:21.801
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Sep  1 11:40:21.821: INFO: Waiting up to 5m0s for pod "client-containers-a31000b4-26a2-49d4-a66c-a02429041433" in namespace "containers-2065" to be "running"
Sep  1 11:40:21.829: INFO: Pod "client-containers-a31000b4-26a2-49d4-a66c-a02429041433": Phase="Pending", Reason="", readiness=false. Elapsed: 7.281757ms
Sep  1 11:40:23.834: INFO: Pod "client-containers-a31000b4-26a2-49d4-a66c-a02429041433": Phase="Running", Reason="", readiness=true. Elapsed: 2.012626048s
Sep  1 11:40:23.834: INFO: Pod "client-containers-a31000b4-26a2-49d4-a66c-a02429041433" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  1 11:40:23.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2065" for this suite. 09/01/23 11:40:23.847
------------------------------
• [2.101 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:40:21.752
    Sep  1 11:40:21.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename containers 09/01/23 11:40:21.759
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:40:21.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:40:21.801
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Sep  1 11:40:21.821: INFO: Waiting up to 5m0s for pod "client-containers-a31000b4-26a2-49d4-a66c-a02429041433" in namespace "containers-2065" to be "running"
    Sep  1 11:40:21.829: INFO: Pod "client-containers-a31000b4-26a2-49d4-a66c-a02429041433": Phase="Pending", Reason="", readiness=false. Elapsed: 7.281757ms
    Sep  1 11:40:23.834: INFO: Pod "client-containers-a31000b4-26a2-49d4-a66c-a02429041433": Phase="Running", Reason="", readiness=true. Elapsed: 2.012626048s
    Sep  1 11:40:23.834: INFO: Pod "client-containers-a31000b4-26a2-49d4-a66c-a02429041433" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:40:23.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2065" for this suite. 09/01/23 11:40:23.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:40:23.859
Sep  1 11:40:23.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename var-expansion 09/01/23 11:40:23.861
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:40:23.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:40:23.888
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 09/01/23 11:40:23.892
Sep  1 11:40:23.902: INFO: Waiting up to 2m0s for pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08" in namespace "var-expansion-4653" to be "running"
Sep  1 11:40:23.910: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 7.516893ms
Sep  1 11:40:25.929: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02739009s
Sep  1 11:40:27.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011046107s
Sep  1 11:40:29.916: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013760073s
Sep  1 11:40:31.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01177749s
Sep  1 11:40:33.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011200017s
Sep  1 11:40:35.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 12.01295496s
Sep  1 11:40:37.916: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013853683s
Sep  1 11:40:39.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 16.012529146s
Sep  1 11:40:41.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 18.011763959s
Sep  1 11:40:43.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 20.011450393s
Sep  1 11:40:45.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01234507s
Sep  1 11:40:47.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01165528s
Sep  1 11:40:49.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012141953s
Sep  1 11:40:51.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 28.01298683s
Sep  1 11:40:53.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012401169s
Sep  1 11:40:55.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011178485s
Sep  1 11:40:57.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011972291s
Sep  1 11:40:59.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012603399s
Sep  1 11:41:01.916: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 38.013531497s
Sep  1 11:41:03.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 40.011635823s
Sep  1 11:41:05.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012092588s
Sep  1 11:41:07.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 44.011237165s
Sep  1 11:41:09.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 46.013245635s
Sep  1 11:41:11.917: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 48.01471031s
Sep  1 11:41:13.930: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 50.028167448s
Sep  1 11:41:15.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 52.012894737s
Sep  1 11:41:17.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011578344s
Sep  1 11:41:19.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 56.011606951s
Sep  1 11:41:21.916: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 58.013600421s
Sep  1 11:41:23.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012561806s
Sep  1 11:41:25.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.012399776s
Sep  1 11:41:27.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.0130259s
Sep  1 11:41:29.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011765251s
Sep  1 11:41:31.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010986614s
Sep  1 11:41:33.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012429555s
Sep  1 11:41:35.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.013049413s
Sep  1 11:41:37.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011249274s
Sep  1 11:41:39.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011520521s
Sep  1 11:41:41.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.011454728s
Sep  1 11:41:43.916: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.014106875s
Sep  1 11:41:45.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011782554s
Sep  1 11:41:47.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.013091948s
Sep  1 11:41:49.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012030635s
Sep  1 11:41:51.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.012475228s
Sep  1 11:41:53.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.011830696s
Sep  1 11:41:55.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011330875s
Sep  1 11:41:57.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.012234666s
Sep  1 11:41:59.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011483223s
Sep  1 11:42:01.921: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.019163982s
Sep  1 11:42:03.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.013048601s
Sep  1 11:42:05.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012735495s
Sep  1 11:42:07.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.011475254s
Sep  1 11:42:09.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013202964s
Sep  1 11:42:11.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012877047s
Sep  1 11:42:13.916: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.013643434s
Sep  1 11:42:15.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011612996s
Sep  1 11:42:17.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.012418071s
Sep  1 11:42:19.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011994952s
Sep  1 11:42:21.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.012742174s
Sep  1 11:42:23.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013032475s
Sep  1 11:42:23.918: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.015972968s
STEP: updating the pod 09/01/23 11:42:23.918
Sep  1 11:42:24.434: INFO: Successfully updated pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08"
STEP: waiting for pod running 09/01/23 11:42:24.434
Sep  1 11:42:24.434: INFO: Waiting up to 2m0s for pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08" in namespace "var-expansion-4653" to be "running"
Sep  1 11:42:24.441: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 6.08309ms
Sep  1 11:42:26.452: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Running", Reason="", readiness=true. Elapsed: 2.018027427s
Sep  1 11:42:26.453: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08" satisfied condition "running"
STEP: deleting the pod gracefully 09/01/23 11:42:26.453
Sep  1 11:42:26.453: INFO: Deleting pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08" in namespace "var-expansion-4653"
Sep  1 11:42:26.462: INFO: Wait up to 5m0s for pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  1 11:42:58.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4653" for this suite. 09/01/23 11:42:58.473
------------------------------
• [SLOW TEST] [154.624 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:40:23.859
    Sep  1 11:40:23.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename var-expansion 09/01/23 11:40:23.861
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:40:23.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:40:23.888
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 09/01/23 11:40:23.892
    Sep  1 11:40:23.902: INFO: Waiting up to 2m0s for pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08" in namespace "var-expansion-4653" to be "running"
    Sep  1 11:40:23.910: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 7.516893ms
    Sep  1 11:40:25.929: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02739009s
    Sep  1 11:40:27.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011046107s
    Sep  1 11:40:29.916: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013760073s
    Sep  1 11:40:31.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01177749s
    Sep  1 11:40:33.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011200017s
    Sep  1 11:40:35.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 12.01295496s
    Sep  1 11:40:37.916: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013853683s
    Sep  1 11:40:39.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 16.012529146s
    Sep  1 11:40:41.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 18.011763959s
    Sep  1 11:40:43.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 20.011450393s
    Sep  1 11:40:45.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01234507s
    Sep  1 11:40:47.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 24.01165528s
    Sep  1 11:40:49.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012141953s
    Sep  1 11:40:51.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 28.01298683s
    Sep  1 11:40:53.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012401169s
    Sep  1 11:40:55.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011178485s
    Sep  1 11:40:57.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011972291s
    Sep  1 11:40:59.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012603399s
    Sep  1 11:41:01.916: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 38.013531497s
    Sep  1 11:41:03.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 40.011635823s
    Sep  1 11:41:05.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012092588s
    Sep  1 11:41:07.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 44.011237165s
    Sep  1 11:41:09.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 46.013245635s
    Sep  1 11:41:11.917: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 48.01471031s
    Sep  1 11:41:13.930: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 50.028167448s
    Sep  1 11:41:15.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 52.012894737s
    Sep  1 11:41:17.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011578344s
    Sep  1 11:41:19.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 56.011606951s
    Sep  1 11:41:21.916: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 58.013600421s
    Sep  1 11:41:23.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012561806s
    Sep  1 11:41:25.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.012399776s
    Sep  1 11:41:27.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.0130259s
    Sep  1 11:41:29.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011765251s
    Sep  1 11:41:31.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010986614s
    Sep  1 11:41:33.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012429555s
    Sep  1 11:41:35.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.013049413s
    Sep  1 11:41:37.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011249274s
    Sep  1 11:41:39.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011520521s
    Sep  1 11:41:41.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.011454728s
    Sep  1 11:41:43.916: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.014106875s
    Sep  1 11:41:45.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011782554s
    Sep  1 11:41:47.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.013091948s
    Sep  1 11:41:49.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012030635s
    Sep  1 11:41:51.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.012475228s
    Sep  1 11:41:53.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.011830696s
    Sep  1 11:41:55.913: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011330875s
    Sep  1 11:41:57.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.012234666s
    Sep  1 11:41:59.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011483223s
    Sep  1 11:42:01.921: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.019163982s
    Sep  1 11:42:03.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.013048601s
    Sep  1 11:42:05.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012735495s
    Sep  1 11:42:07.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.011475254s
    Sep  1 11:42:09.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013202964s
    Sep  1 11:42:11.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012877047s
    Sep  1 11:42:13.916: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.013643434s
    Sep  1 11:42:15.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011612996s
    Sep  1 11:42:17.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.012418071s
    Sep  1 11:42:19.914: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011994952s
    Sep  1 11:42:21.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.012742174s
    Sep  1 11:42:23.915: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013032475s
    Sep  1 11:42:23.918: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.015972968s
    STEP: updating the pod 09/01/23 11:42:23.918
    Sep  1 11:42:24.434: INFO: Successfully updated pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08"
    STEP: waiting for pod running 09/01/23 11:42:24.434
    Sep  1 11:42:24.434: INFO: Waiting up to 2m0s for pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08" in namespace "var-expansion-4653" to be "running"
    Sep  1 11:42:24.441: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Pending", Reason="", readiness=false. Elapsed: 6.08309ms
    Sep  1 11:42:26.452: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08": Phase="Running", Reason="", readiness=true. Elapsed: 2.018027427s
    Sep  1 11:42:26.453: INFO: Pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08" satisfied condition "running"
    STEP: deleting the pod gracefully 09/01/23 11:42:26.453
    Sep  1 11:42:26.453: INFO: Deleting pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08" in namespace "var-expansion-4653"
    Sep  1 11:42:26.462: INFO: Wait up to 5m0s for pod "var-expansion-625ce04b-bab7-4e01-80e1-b4727a20ea08" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:42:58.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4653" for this suite. 09/01/23 11:42:58.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:42:58.49
Sep  1 11:42:58.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/01/23 11:42:58.492
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:42:58.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:42:58.515
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 09/01/23 11:42:58.522
STEP: Creating hostNetwork=false pod 09/01/23 11:42:58.523
Sep  1 11:42:58.535: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9738" to be "running and ready"
Sep  1 11:42:58.541: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.900099ms
Sep  1 11:42:58.541: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:43:00.546: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011737551s
Sep  1 11:43:00.547: INFO: The phase of Pod test-pod is Running (Ready = true)
Sep  1 11:43:00.547: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 09/01/23 11:43:00.55
Sep  1 11:43:00.558: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9738" to be "running and ready"
Sep  1 11:43:00.563: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.901418ms
Sep  1 11:43:00.563: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:43:02.569: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011243949s
Sep  1 11:43:02.570: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Sep  1 11:43:02.570: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 09/01/23 11:43:02.575
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/01/23 11:43:02.576
Sep  1 11:43:02.576: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:43:02.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:43:02.578: INFO: ExecWithOptions: Clientset creation
Sep  1 11:43:02.579: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  1 11:43:02.686: INFO: Exec stderr: ""
Sep  1 11:43:02.687: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:43:02.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:43:02.688: INFO: ExecWithOptions: Clientset creation
Sep  1 11:43:02.688: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  1 11:43:02.796: INFO: Exec stderr: ""
Sep  1 11:43:02.796: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:43:02.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:43:02.797: INFO: ExecWithOptions: Clientset creation
Sep  1 11:43:02.797: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  1 11:43:02.899: INFO: Exec stderr: ""
Sep  1 11:43:02.899: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:43:02.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:43:02.901: INFO: ExecWithOptions: Clientset creation
Sep  1 11:43:02.901: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  1 11:43:03.000: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/01/23 11:43:03
Sep  1 11:43:03.000: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:43:03.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:43:03.001: INFO: ExecWithOptions: Clientset creation
Sep  1 11:43:03.001: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep  1 11:43:03.104: INFO: Exec stderr: ""
Sep  1 11:43:03.104: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:43:03.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:43:03.105: INFO: ExecWithOptions: Clientset creation
Sep  1 11:43:03.105: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep  1 11:43:03.198: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/01/23 11:43:03.198
Sep  1 11:43:03.198: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:43:03.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:43:03.199: INFO: ExecWithOptions: Clientset creation
Sep  1 11:43:03.199: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  1 11:43:03.319: INFO: Exec stderr: ""
Sep  1 11:43:03.320: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:43:03.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:43:03.321: INFO: ExecWithOptions: Clientset creation
Sep  1 11:43:03.321: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  1 11:43:03.421: INFO: Exec stderr: ""
Sep  1 11:43:03.421: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:43:03.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:43:03.422: INFO: ExecWithOptions: Clientset creation
Sep  1 11:43:03.422: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  1 11:43:03.519: INFO: Exec stderr: ""
Sep  1 11:43:03.519: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:43:03.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:43:03.520: INFO: ExecWithOptions: Clientset creation
Sep  1 11:43:03.520: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  1 11:43:03.608: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:03.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9738" for this suite. 09/01/23 11:43:03.613
------------------------------
• [SLOW TEST] [5.135 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:42:58.49
    Sep  1 11:42:58.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/01/23 11:42:58.492
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:42:58.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:42:58.515
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 09/01/23 11:42:58.522
    STEP: Creating hostNetwork=false pod 09/01/23 11:42:58.523
    Sep  1 11:42:58.535: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9738" to be "running and ready"
    Sep  1 11:42:58.541: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.900099ms
    Sep  1 11:42:58.541: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:43:00.546: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011737551s
    Sep  1 11:43:00.547: INFO: The phase of Pod test-pod is Running (Ready = true)
    Sep  1 11:43:00.547: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 09/01/23 11:43:00.55
    Sep  1 11:43:00.558: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9738" to be "running and ready"
    Sep  1 11:43:00.563: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.901418ms
    Sep  1 11:43:00.563: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:43:02.569: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011243949s
    Sep  1 11:43:02.570: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Sep  1 11:43:02.570: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 09/01/23 11:43:02.575
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/01/23 11:43:02.576
    Sep  1 11:43:02.576: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:43:02.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:43:02.578: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:43:02.579: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  1 11:43:02.686: INFO: Exec stderr: ""
    Sep  1 11:43:02.687: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:43:02.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:43:02.688: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:43:02.688: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  1 11:43:02.796: INFO: Exec stderr: ""
    Sep  1 11:43:02.796: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:43:02.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:43:02.797: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:43:02.797: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  1 11:43:02.899: INFO: Exec stderr: ""
    Sep  1 11:43:02.899: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:43:02.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:43:02.901: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:43:02.901: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  1 11:43:03.000: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/01/23 11:43:03
    Sep  1 11:43:03.000: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:43:03.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:43:03.001: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:43:03.001: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep  1 11:43:03.104: INFO: Exec stderr: ""
    Sep  1 11:43:03.104: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:43:03.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:43:03.105: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:43:03.105: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep  1 11:43:03.198: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/01/23 11:43:03.198
    Sep  1 11:43:03.198: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:43:03.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:43:03.199: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:43:03.199: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  1 11:43:03.319: INFO: Exec stderr: ""
    Sep  1 11:43:03.320: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:43:03.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:43:03.321: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:43:03.321: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  1 11:43:03.421: INFO: Exec stderr: ""
    Sep  1 11:43:03.421: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:43:03.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:43:03.422: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:43:03.422: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  1 11:43:03.519: INFO: Exec stderr: ""
    Sep  1 11:43:03.519: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9738 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:43:03.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:43:03.520: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:43:03.520: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9738/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  1 11:43:03.608: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:03.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-9738" for this suite. 09/01/23 11:43:03.613
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:03.629
Sep  1 11:43:03.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 11:43:03.631
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:03.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:03.658
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 09/01/23 11:43:03.663
Sep  1 11:43:03.664: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7608 proxy --unix-socket=/tmp/kubectl-proxy-unix705329049/test'
STEP: retrieving proxy /api/ output 09/01/23 11:43:03.741
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:03.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7608" for this suite. 09/01/23 11:43:03.748
------------------------------
• [0.129 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:03.629
    Sep  1 11:43:03.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 11:43:03.631
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:03.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:03.658
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 09/01/23 11:43:03.663
    Sep  1 11:43:03.664: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-7608 proxy --unix-socket=/tmp/kubectl-proxy-unix705329049/test'
    STEP: retrieving proxy /api/ output 09/01/23 11:43:03.741
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:03.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7608" for this suite. 09/01/23 11:43:03.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:03.759
Sep  1 11:43:03.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:43:03.76
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:03.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:03.801
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-38b5af53-a7db-47ba-80b5-d3356f27f06a 09/01/23 11:43:03.809
STEP: Creating a pod to test consume secrets 09/01/23 11:43:03.84
Sep  1 11:43:03.859: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4" in namespace "projected-8166" to be "Succeeded or Failed"
Sep  1 11:43:03.867: INFO: Pod "pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.218148ms
Sep  1 11:43:05.872: INFO: Pod "pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013382622s
Sep  1 11:43:07.873: INFO: Pod "pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014316818s
STEP: Saw pod success 09/01/23 11:43:07.873
Sep  1 11:43:07.874: INFO: Pod "pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4" satisfied condition "Succeeded or Failed"
Sep  1 11:43:07.878: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/01/23 11:43:07.896
Sep  1 11:43:07.911: INFO: Waiting for pod pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4 to disappear
Sep  1 11:43:07.914: INFO: Pod pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:07.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8166" for this suite. 09/01/23 11:43:07.919
------------------------------
• [4.167 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:03.759
    Sep  1 11:43:03.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:43:03.76
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:03.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:03.801
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-38b5af53-a7db-47ba-80b5-d3356f27f06a 09/01/23 11:43:03.809
    STEP: Creating a pod to test consume secrets 09/01/23 11:43:03.84
    Sep  1 11:43:03.859: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4" in namespace "projected-8166" to be "Succeeded or Failed"
    Sep  1 11:43:03.867: INFO: Pod "pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.218148ms
    Sep  1 11:43:05.872: INFO: Pod "pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013382622s
    Sep  1 11:43:07.873: INFO: Pod "pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014316818s
    STEP: Saw pod success 09/01/23 11:43:07.873
    Sep  1 11:43:07.874: INFO: Pod "pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4" satisfied condition "Succeeded or Failed"
    Sep  1 11:43:07.878: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/01/23 11:43:07.896
    Sep  1 11:43:07.911: INFO: Waiting for pod pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4 to disappear
    Sep  1 11:43:07.914: INFO: Pod pod-projected-secrets-d640799e-9ae5-4964-b697-8700a323b4c4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:07.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8166" for this suite. 09/01/23 11:43:07.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:07.93
Sep  1 11:43:07.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename resourcequota 09/01/23 11:43:07.931
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:07.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:07.952
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 09/01/23 11:43:24.958
STEP: Creating a ResourceQuota 09/01/23 11:43:29.961
STEP: Ensuring resource quota status is calculated 09/01/23 11:43:29.968
STEP: Creating a ConfigMap 09/01/23 11:43:31.972
STEP: Ensuring resource quota status captures configMap creation 09/01/23 11:43:31.986
STEP: Deleting a ConfigMap 09/01/23 11:43:33.991
STEP: Ensuring resource quota status released usage 09/01/23 11:43:33.996
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:36.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8875" for this suite. 09/01/23 11:43:36.004
------------------------------
• [SLOW TEST] [28.087 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:07.93
    Sep  1 11:43:07.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename resourcequota 09/01/23 11:43:07.931
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:07.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:07.952
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 09/01/23 11:43:24.958
    STEP: Creating a ResourceQuota 09/01/23 11:43:29.961
    STEP: Ensuring resource quota status is calculated 09/01/23 11:43:29.968
    STEP: Creating a ConfigMap 09/01/23 11:43:31.972
    STEP: Ensuring resource quota status captures configMap creation 09/01/23 11:43:31.986
    STEP: Deleting a ConfigMap 09/01/23 11:43:33.991
    STEP: Ensuring resource quota status released usage 09/01/23 11:43:33.996
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:36.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8875" for this suite. 09/01/23 11:43:36.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:36.027
Sep  1 11:43:36.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename resourcequota 09/01/23 11:43:36.028
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:36.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:36.046
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 09/01/23 11:43:36.05
STEP: Getting a ResourceQuota 09/01/23 11:43:36.055
STEP: Listing all ResourceQuotas with LabelSelector 09/01/23 11:43:36.058
STEP: Patching the ResourceQuota 09/01/23 11:43:36.061
STEP: Deleting a Collection of ResourceQuotas 09/01/23 11:43:36.066
STEP: Verifying the deleted ResourceQuota 09/01/23 11:43:36.075
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:36.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9488" for this suite. 09/01/23 11:43:36.084
------------------------------
• [0.063 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:36.027
    Sep  1 11:43:36.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename resourcequota 09/01/23 11:43:36.028
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:36.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:36.046
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 09/01/23 11:43:36.05
    STEP: Getting a ResourceQuota 09/01/23 11:43:36.055
    STEP: Listing all ResourceQuotas with LabelSelector 09/01/23 11:43:36.058
    STEP: Patching the ResourceQuota 09/01/23 11:43:36.061
    STEP: Deleting a Collection of ResourceQuotas 09/01/23 11:43:36.066
    STEP: Verifying the deleted ResourceQuota 09/01/23 11:43:36.075
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:36.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9488" for this suite. 09/01/23 11:43:36.084
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:36.094
Sep  1 11:43:36.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename events 09/01/23 11:43:36.096
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:36.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:36.118
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 09/01/23 11:43:36.122
STEP: get a list of Events with a label in the current namespace 09/01/23 11:43:36.14
STEP: delete a list of events 09/01/23 11:43:36.144
Sep  1 11:43:36.144: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/01/23 11:43:36.16
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:36.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-792" for this suite. 09/01/23 11:43:36.168
------------------------------
• [0.080 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:36.094
    Sep  1 11:43:36.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename events 09/01/23 11:43:36.096
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:36.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:36.118
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 09/01/23 11:43:36.122
    STEP: get a list of Events with a label in the current namespace 09/01/23 11:43:36.14
    STEP: delete a list of events 09/01/23 11:43:36.144
    Sep  1 11:43:36.144: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/01/23 11:43:36.16
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:36.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-792" for this suite. 09/01/23 11:43:36.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:36.181
Sep  1 11:43:36.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename csiinlinevolumes 09/01/23 11:43:36.182
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:36.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:36.203
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 09/01/23 11:43:36.207
STEP: getting 09/01/23 11:43:36.226
STEP: listing 09/01/23 11:43:36.232
STEP: deleting 09/01/23 11:43:36.236
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:36.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2173" for this suite. 09/01/23 11:43:36.255
------------------------------
• [0.081 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:36.181
    Sep  1 11:43:36.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename csiinlinevolumes 09/01/23 11:43:36.182
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:36.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:36.203
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 09/01/23 11:43:36.207
    STEP: getting 09/01/23 11:43:36.226
    STEP: listing 09/01/23 11:43:36.232
    STEP: deleting 09/01/23 11:43:36.236
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:36.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2173" for this suite. 09/01/23 11:43:36.255
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:36.262
Sep  1 11:43:36.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 11:43:36.263
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:36.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:36.292
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-310e63f3-dcb8-4518-8143-830fbaa375ab 09/01/23 11:43:36.296
STEP: Creating a pod to test consume configMaps 09/01/23 11:43:36.301
Sep  1 11:43:36.311: INFO: Waiting up to 5m0s for pod "pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc" in namespace "configmap-3237" to be "Succeeded or Failed"
Sep  1 11:43:36.315: INFO: Pod "pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006168ms
Sep  1 11:43:38.319: INFO: Pod "pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008297979s
Sep  1 11:43:40.319: INFO: Pod "pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008004651s
STEP: Saw pod success 09/01/23 11:43:40.319
Sep  1 11:43:40.319: INFO: Pod "pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc" satisfied condition "Succeeded or Failed"
Sep  1 11:43:40.322: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc container agnhost-container: <nil>
STEP: delete the pod 09/01/23 11:43:40.328
Sep  1 11:43:40.339: INFO: Waiting for pod pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc to disappear
Sep  1 11:43:40.344: INFO: Pod pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:40.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3237" for this suite. 09/01/23 11:43:40.349
------------------------------
• [4.093 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:36.262
    Sep  1 11:43:36.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 11:43:36.263
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:36.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:36.292
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-310e63f3-dcb8-4518-8143-830fbaa375ab 09/01/23 11:43:36.296
    STEP: Creating a pod to test consume configMaps 09/01/23 11:43:36.301
    Sep  1 11:43:36.311: INFO: Waiting up to 5m0s for pod "pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc" in namespace "configmap-3237" to be "Succeeded or Failed"
    Sep  1 11:43:36.315: INFO: Pod "pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006168ms
    Sep  1 11:43:38.319: INFO: Pod "pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008297979s
    Sep  1 11:43:40.319: INFO: Pod "pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008004651s
    STEP: Saw pod success 09/01/23 11:43:40.319
    Sep  1 11:43:40.319: INFO: Pod "pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc" satisfied condition "Succeeded or Failed"
    Sep  1 11:43:40.322: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 11:43:40.328
    Sep  1 11:43:40.339: INFO: Waiting for pod pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc to disappear
    Sep  1 11:43:40.344: INFO: Pod pod-configmaps-a33b7383-f3d9-47e8-a73c-10fce8c12dfc no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:40.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3237" for this suite. 09/01/23 11:43:40.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:40.357
Sep  1 11:43:40.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename replication-controller 09/01/23 11:43:40.359
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:40.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:40.379
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Sep  1 11:43:40.383: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/01/23 11:43:41.398
STEP: Checking rc "condition-test" has the desired failure condition set 09/01/23 11:43:41.407
STEP: Scaling down rc "condition-test" to satisfy pod quota 09/01/23 11:43:42.419
Sep  1 11:43:42.428: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 09/01/23 11:43:42.428
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:42.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9923" for this suite. 09/01/23 11:43:42.442
------------------------------
• [2.094 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:40.357
    Sep  1 11:43:40.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename replication-controller 09/01/23 11:43:40.359
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:40.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:40.379
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Sep  1 11:43:40.383: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/01/23 11:43:41.398
    STEP: Checking rc "condition-test" has the desired failure condition set 09/01/23 11:43:41.407
    STEP: Scaling down rc "condition-test" to satisfy pod quota 09/01/23 11:43:42.419
    Sep  1 11:43:42.428: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 09/01/23 11:43:42.428
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:42.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9923" for this suite. 09/01/23 11:43:42.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:42.453
Sep  1 11:43:42.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename custom-resource-definition 09/01/23 11:43:42.455
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:42.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:42.475
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Sep  1 11:43:42.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:43.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1626" for this suite. 09/01/23 11:43:43.057
------------------------------
• [0.612 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:42.453
    Sep  1 11:43:42.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename custom-resource-definition 09/01/23 11:43:42.455
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:42.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:42.475
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Sep  1 11:43:42.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:43.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1626" for this suite. 09/01/23 11:43:43.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:43.066
Sep  1 11:43:43.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:43:43.068
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:43.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:43.09
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-02ec2a3a-ba87-45c4-b40f-fff3ddf7ee27 09/01/23 11:43:43.094
STEP: Creating a pod to test consume configMaps 09/01/23 11:43:43.099
Sep  1 11:43:43.106: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a" in namespace "projected-9835" to be "Succeeded or Failed"
Sep  1 11:43:43.110: INFO: Pod "pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.23284ms
Sep  1 11:43:45.114: INFO: Pod "pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008151763s
Sep  1 11:43:47.115: INFO: Pod "pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00858417s
STEP: Saw pod success 09/01/23 11:43:47.115
Sep  1 11:43:47.115: INFO: Pod "pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a" satisfied condition "Succeeded or Failed"
Sep  1 11:43:47.119: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a container agnhost-container: <nil>
STEP: delete the pod 09/01/23 11:43:47.126
Sep  1 11:43:47.143: INFO: Waiting for pod pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a to disappear
Sep  1 11:43:47.147: INFO: Pod pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:47.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9835" for this suite. 09/01/23 11:43:47.152
------------------------------
• [4.095 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:43.066
    Sep  1 11:43:43.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:43:43.068
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:43.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:43.09
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-02ec2a3a-ba87-45c4-b40f-fff3ddf7ee27 09/01/23 11:43:43.094
    STEP: Creating a pod to test consume configMaps 09/01/23 11:43:43.099
    Sep  1 11:43:43.106: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a" in namespace "projected-9835" to be "Succeeded or Failed"
    Sep  1 11:43:43.110: INFO: Pod "pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.23284ms
    Sep  1 11:43:45.114: INFO: Pod "pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008151763s
    Sep  1 11:43:47.115: INFO: Pod "pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00858417s
    STEP: Saw pod success 09/01/23 11:43:47.115
    Sep  1 11:43:47.115: INFO: Pod "pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a" satisfied condition "Succeeded or Failed"
    Sep  1 11:43:47.119: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 11:43:47.126
    Sep  1 11:43:47.143: INFO: Waiting for pod pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a to disappear
    Sep  1 11:43:47.147: INFO: Pod pod-projected-configmaps-c6351a8c-b154-432e-a561-d745a6c3e65a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:47.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9835" for this suite. 09/01/23 11:43:47.152
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:47.168
Sep  1 11:43:47.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename secrets 09/01/23 11:43:47.17
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:47.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:47.196
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-b84f7355-fa9e-4101-ba7e-aa21eb288140 09/01/23 11:43:47.2
STEP: Creating a pod to test consume secrets 09/01/23 11:43:47.206
Sep  1 11:43:47.218: INFO: Waiting up to 5m0s for pod "pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7" in namespace "secrets-7535" to be "Succeeded or Failed"
Sep  1 11:43:47.223: INFO: Pod "pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.28086ms
Sep  1 11:43:49.227: INFO: Pod "pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00911177s
Sep  1 11:43:51.226: INFO: Pod "pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008707082s
STEP: Saw pod success 09/01/23 11:43:51.227
Sep  1 11:43:51.227: INFO: Pod "pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7" satisfied condition "Succeeded or Failed"
Sep  1 11:43:51.229: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7 container secret-volume-test: <nil>
STEP: delete the pod 09/01/23 11:43:51.235
Sep  1 11:43:51.247: INFO: Waiting for pod pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7 to disappear
Sep  1 11:43:51.250: INFO: Pod pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:51.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7535" for this suite. 09/01/23 11:43:51.254
------------------------------
• [4.091 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:47.168
    Sep  1 11:43:47.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename secrets 09/01/23 11:43:47.17
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:47.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:47.196
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-b84f7355-fa9e-4101-ba7e-aa21eb288140 09/01/23 11:43:47.2
    STEP: Creating a pod to test consume secrets 09/01/23 11:43:47.206
    Sep  1 11:43:47.218: INFO: Waiting up to 5m0s for pod "pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7" in namespace "secrets-7535" to be "Succeeded or Failed"
    Sep  1 11:43:47.223: INFO: Pod "pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.28086ms
    Sep  1 11:43:49.227: INFO: Pod "pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00911177s
    Sep  1 11:43:51.226: INFO: Pod "pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008707082s
    STEP: Saw pod success 09/01/23 11:43:51.227
    Sep  1 11:43:51.227: INFO: Pod "pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7" satisfied condition "Succeeded or Failed"
    Sep  1 11:43:51.229: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7 container secret-volume-test: <nil>
    STEP: delete the pod 09/01/23 11:43:51.235
    Sep  1 11:43:51.247: INFO: Waiting for pod pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7 to disappear
    Sep  1 11:43:51.250: INFO: Pod pod-secrets-6d78ef2f-88ed-4c7c-88d4-015618f4a5b7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:51.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7535" for this suite. 09/01/23 11:43:51.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:51.259
Sep  1 11:43:51.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename secrets 09/01/23 11:43:51.263
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:51.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:51.29
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-a79b3e61-9cca-4628-aec8-5c2673ef1ff1 09/01/23 11:43:51.293
STEP: Creating a pod to test consume secrets 09/01/23 11:43:51.298
Sep  1 11:43:51.307: INFO: Waiting up to 5m0s for pod "pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9" in namespace "secrets-7953" to be "Succeeded or Failed"
Sep  1 11:43:51.311: INFO: Pod "pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.22431ms
Sep  1 11:43:53.315: INFO: Pod "pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007405081s
Sep  1 11:43:55.314: INFO: Pod "pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006845134s
STEP: Saw pod success 09/01/23 11:43:55.314
Sep  1 11:43:55.314: INFO: Pod "pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9" satisfied condition "Succeeded or Failed"
Sep  1 11:43:55.318: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9 container secret-volume-test: <nil>
STEP: delete the pod 09/01/23 11:43:55.324
Sep  1 11:43:55.338: INFO: Waiting for pod pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9 to disappear
Sep  1 11:43:55.342: INFO: Pod pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:55.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7953" for this suite. 09/01/23 11:43:55.346
------------------------------
• [4.092 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:51.259
    Sep  1 11:43:51.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename secrets 09/01/23 11:43:51.263
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:51.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:51.29
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-a79b3e61-9cca-4628-aec8-5c2673ef1ff1 09/01/23 11:43:51.293
    STEP: Creating a pod to test consume secrets 09/01/23 11:43:51.298
    Sep  1 11:43:51.307: INFO: Waiting up to 5m0s for pod "pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9" in namespace "secrets-7953" to be "Succeeded or Failed"
    Sep  1 11:43:51.311: INFO: Pod "pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.22431ms
    Sep  1 11:43:53.315: INFO: Pod "pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007405081s
    Sep  1 11:43:55.314: INFO: Pod "pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006845134s
    STEP: Saw pod success 09/01/23 11:43:55.314
    Sep  1 11:43:55.314: INFO: Pod "pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9" satisfied condition "Succeeded or Failed"
    Sep  1 11:43:55.318: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9 container secret-volume-test: <nil>
    STEP: delete the pod 09/01/23 11:43:55.324
    Sep  1 11:43:55.338: INFO: Waiting for pod pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9 to disappear
    Sep  1 11:43:55.342: INFO: Pod pod-secrets-8685a450-1d26-4f33-bef8-6f2e26ddb7e9 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:55.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7953" for this suite. 09/01/23 11:43:55.346
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:55.355
Sep  1 11:43:55.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubelet-test 09/01/23 11:43:55.356
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:55.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:55.378
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:59.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1186" for this suite. 09/01/23 11:43:59.401
------------------------------
• [4.055 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:55.355
    Sep  1 11:43:55.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubelet-test 09/01/23 11:43:55.356
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:55.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:55.378
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:59.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1186" for this suite. 09/01/23 11:43:59.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:59.414
Sep  1 11:43:59.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename endpointslice 09/01/23 11:43:59.415
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:59.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:59.441
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Sep  1 11:43:59.453: INFO: Endpoints addresses: [172.16.0.2] , ports: [6443]
Sep  1 11:43:59.453: INFO: EndpointSlices addresses: [172.16.0.2] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  1 11:43:59.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7437" for this suite. 09/01/23 11:43:59.461
------------------------------
• [0.062 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:59.414
    Sep  1 11:43:59.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename endpointslice 09/01/23 11:43:59.415
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:59.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:59.441
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Sep  1 11:43:59.453: INFO: Endpoints addresses: [172.16.0.2] , ports: [6443]
    Sep  1 11:43:59.453: INFO: EndpointSlices addresses: [172.16.0.2] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:43:59.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7437" for this suite. 09/01/23 11:43:59.461
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:43:59.477
Sep  1 11:43:59.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 11:43:59.479
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:59.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:59.502
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3692 09/01/23 11:43:59.506
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/01/23 11:43:59.524
STEP: creating service externalsvc in namespace services-3692 09/01/23 11:43:59.525
STEP: creating replication controller externalsvc in namespace services-3692 09/01/23 11:43:59.551
I0901 11:43:59.563368      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3692, replica count: 2
I0901 11:44:02.613919      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 09/01/23 11:44:02.618
Sep  1 11:44:02.636: INFO: Creating new exec pod
Sep  1 11:44:02.643: INFO: Waiting up to 5m0s for pod "execpodhm2b8" in namespace "services-3692" to be "running"
Sep  1 11:44:02.652: INFO: Pod "execpodhm2b8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.571296ms
Sep  1 11:44:04.660: INFO: Pod "execpodhm2b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.016377361s
Sep  1 11:44:04.660: INFO: Pod "execpodhm2b8" satisfied condition "running"
Sep  1 11:44:04.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-3692 exec execpodhm2b8 -- /bin/sh -x -c nslookup nodeport-service.services-3692.svc.cluster.local'
Sep  1 11:44:04.962: INFO: stderr: "+ nslookup nodeport-service.services-3692.svc.cluster.local\n"
Sep  1 11:44:04.963: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-3692.svc.cluster.local\tcanonical name = externalsvc.services-3692.svc.cluster.local.\nName:\texternalsvc.services-3692.svc.cluster.local\nAddress: 10.108.148.207\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3692, will wait for the garbage collector to delete the pods 09/01/23 11:44:04.963
Sep  1 11:44:05.029: INFO: Deleting ReplicationController externalsvc took: 10.669463ms
Sep  1 11:44:05.130: INFO: Terminating ReplicationController externalsvc pods took: 100.669864ms
Sep  1 11:44:06.958: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 11:44:06.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3692" for this suite. 09/01/23 11:44:06.978
------------------------------
• [SLOW TEST] [7.518 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:43:59.477
    Sep  1 11:43:59.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 11:43:59.479
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:43:59.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:43:59.502
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3692 09/01/23 11:43:59.506
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/01/23 11:43:59.524
    STEP: creating service externalsvc in namespace services-3692 09/01/23 11:43:59.525
    STEP: creating replication controller externalsvc in namespace services-3692 09/01/23 11:43:59.551
    I0901 11:43:59.563368      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3692, replica count: 2
    I0901 11:44:02.613919      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 09/01/23 11:44:02.618
    Sep  1 11:44:02.636: INFO: Creating new exec pod
    Sep  1 11:44:02.643: INFO: Waiting up to 5m0s for pod "execpodhm2b8" in namespace "services-3692" to be "running"
    Sep  1 11:44:02.652: INFO: Pod "execpodhm2b8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.571296ms
    Sep  1 11:44:04.660: INFO: Pod "execpodhm2b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.016377361s
    Sep  1 11:44:04.660: INFO: Pod "execpodhm2b8" satisfied condition "running"
    Sep  1 11:44:04.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-3692 exec execpodhm2b8 -- /bin/sh -x -c nslookup nodeport-service.services-3692.svc.cluster.local'
    Sep  1 11:44:04.962: INFO: stderr: "+ nslookup nodeport-service.services-3692.svc.cluster.local\n"
    Sep  1 11:44:04.963: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-3692.svc.cluster.local\tcanonical name = externalsvc.services-3692.svc.cluster.local.\nName:\texternalsvc.services-3692.svc.cluster.local\nAddress: 10.108.148.207\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3692, will wait for the garbage collector to delete the pods 09/01/23 11:44:04.963
    Sep  1 11:44:05.029: INFO: Deleting ReplicationController externalsvc took: 10.669463ms
    Sep  1 11:44:05.130: INFO: Terminating ReplicationController externalsvc pods took: 100.669864ms
    Sep  1 11:44:06.958: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:44:06.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3692" for this suite. 09/01/23 11:44:06.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:44:07.002
Sep  1 11:44:07.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename subpath 09/01/23 11:44:07.003
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:44:07.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:44:07.041
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/01/23 11:44:07.046
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-r8gj 09/01/23 11:44:07.062
STEP: Creating a pod to test atomic-volume-subpath 09/01/23 11:44:07.062
Sep  1 11:44:07.074: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-r8gj" in namespace "subpath-856" to be "Succeeded or Failed"
Sep  1 11:44:07.083: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.735854ms
Sep  1 11:44:09.086: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 2.012400391s
Sep  1 11:44:11.088: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 4.01427401s
Sep  1 11:44:13.089: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 6.014948847s
Sep  1 11:44:15.089: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 8.01553608s
Sep  1 11:44:17.087: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 10.012855929s
Sep  1 11:44:19.086: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 12.012471001s
Sep  1 11:44:21.086: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 14.012589996s
Sep  1 11:44:23.087: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 16.013230709s
Sep  1 11:44:25.087: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 18.013061545s
Sep  1 11:44:27.090: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 20.01572208s
Sep  1 11:44:29.088: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=false. Elapsed: 22.01399582s
Sep  1 11:44:31.089: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01464015s
STEP: Saw pod success 09/01/23 11:44:31.089
Sep  1 11:44:31.089: INFO: Pod "pod-subpath-test-configmap-r8gj" satisfied condition "Succeeded or Failed"
Sep  1 11:44:31.092: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-subpath-test-configmap-r8gj container test-container-subpath-configmap-r8gj: <nil>
STEP: delete the pod 09/01/23 11:44:31.099
Sep  1 11:44:31.115: INFO: Waiting for pod pod-subpath-test-configmap-r8gj to disappear
Sep  1 11:44:31.119: INFO: Pod pod-subpath-test-configmap-r8gj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-r8gj 09/01/23 11:44:31.12
Sep  1 11:44:31.120: INFO: Deleting pod "pod-subpath-test-configmap-r8gj" in namespace "subpath-856"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  1 11:44:31.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-856" for this suite. 09/01/23 11:44:31.129
------------------------------
• [SLOW TEST] [24.135 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:44:07.002
    Sep  1 11:44:07.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename subpath 09/01/23 11:44:07.003
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:44:07.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:44:07.041
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/01/23 11:44:07.046
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-r8gj 09/01/23 11:44:07.062
    STEP: Creating a pod to test atomic-volume-subpath 09/01/23 11:44:07.062
    Sep  1 11:44:07.074: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-r8gj" in namespace "subpath-856" to be "Succeeded or Failed"
    Sep  1 11:44:07.083: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.735854ms
    Sep  1 11:44:09.086: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 2.012400391s
    Sep  1 11:44:11.088: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 4.01427401s
    Sep  1 11:44:13.089: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 6.014948847s
    Sep  1 11:44:15.089: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 8.01553608s
    Sep  1 11:44:17.087: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 10.012855929s
    Sep  1 11:44:19.086: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 12.012471001s
    Sep  1 11:44:21.086: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 14.012589996s
    Sep  1 11:44:23.087: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 16.013230709s
    Sep  1 11:44:25.087: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 18.013061545s
    Sep  1 11:44:27.090: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=true. Elapsed: 20.01572208s
    Sep  1 11:44:29.088: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Running", Reason="", readiness=false. Elapsed: 22.01399582s
    Sep  1 11:44:31.089: INFO: Pod "pod-subpath-test-configmap-r8gj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01464015s
    STEP: Saw pod success 09/01/23 11:44:31.089
    Sep  1 11:44:31.089: INFO: Pod "pod-subpath-test-configmap-r8gj" satisfied condition "Succeeded or Failed"
    Sep  1 11:44:31.092: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-subpath-test-configmap-r8gj container test-container-subpath-configmap-r8gj: <nil>
    STEP: delete the pod 09/01/23 11:44:31.099
    Sep  1 11:44:31.115: INFO: Waiting for pod pod-subpath-test-configmap-r8gj to disappear
    Sep  1 11:44:31.119: INFO: Pod pod-subpath-test-configmap-r8gj no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-r8gj 09/01/23 11:44:31.12
    Sep  1 11:44:31.120: INFO: Deleting pod "pod-subpath-test-configmap-r8gj" in namespace "subpath-856"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:44:31.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-856" for this suite. 09/01/23 11:44:31.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:44:31.146
Sep  1 11:44:31.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename secrets 09/01/23 11:44:31.148
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:44:31.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:44:31.191
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-7b25fe83-daf9-4cfc-a7c4-c430e5417d70 09/01/23 11:44:31.194
STEP: Creating a pod to test consume secrets 09/01/23 11:44:31.198
Sep  1 11:44:31.207: INFO: Waiting up to 5m0s for pod "pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b" in namespace "secrets-5067" to be "Succeeded or Failed"
Sep  1 11:44:31.214: INFO: Pod "pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.944293ms
Sep  1 11:44:33.218: INFO: Pod "pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011087006s
Sep  1 11:44:35.218: INFO: Pod "pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011085417s
STEP: Saw pod success 09/01/23 11:44:35.218
Sep  1 11:44:35.218: INFO: Pod "pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b" satisfied condition "Succeeded or Failed"
Sep  1 11:44:35.221: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b container secret-volume-test: <nil>
STEP: delete the pod 09/01/23 11:44:35.228
Sep  1 11:44:35.238: INFO: Waiting for pod pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b to disappear
Sep  1 11:44:35.242: INFO: Pod pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  1 11:44:35.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5067" for this suite. 09/01/23 11:44:35.247
------------------------------
• [4.109 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:44:31.146
    Sep  1 11:44:31.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename secrets 09/01/23 11:44:31.148
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:44:31.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:44:31.191
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-7b25fe83-daf9-4cfc-a7c4-c430e5417d70 09/01/23 11:44:31.194
    STEP: Creating a pod to test consume secrets 09/01/23 11:44:31.198
    Sep  1 11:44:31.207: INFO: Waiting up to 5m0s for pod "pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b" in namespace "secrets-5067" to be "Succeeded or Failed"
    Sep  1 11:44:31.214: INFO: Pod "pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.944293ms
    Sep  1 11:44:33.218: INFO: Pod "pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011087006s
    Sep  1 11:44:35.218: INFO: Pod "pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011085417s
    STEP: Saw pod success 09/01/23 11:44:35.218
    Sep  1 11:44:35.218: INFO: Pod "pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b" satisfied condition "Succeeded or Failed"
    Sep  1 11:44:35.221: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b container secret-volume-test: <nil>
    STEP: delete the pod 09/01/23 11:44:35.228
    Sep  1 11:44:35.238: INFO: Waiting for pod pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b to disappear
    Sep  1 11:44:35.242: INFO: Pod pod-secrets-a671fc2a-ec5a-4ede-ad58-939a224ae34b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:44:35.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5067" for this suite. 09/01/23 11:44:35.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:44:35.258
Sep  1 11:44:35.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 11:44:35.26
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:44:35.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:44:35.292
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 09/01/23 11:44:35.296
Sep  1 11:44:35.303: INFO: Waiting up to 5m0s for pod "downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d" in namespace "downward-api-7027" to be "Succeeded or Failed"
Sep  1 11:44:35.306: INFO: Pod "downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.289218ms
Sep  1 11:44:37.311: INFO: Pod "downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007926113s
Sep  1 11:44:39.311: INFO: Pod "downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007491984s
STEP: Saw pod success 09/01/23 11:44:39.311
Sep  1 11:44:39.311: INFO: Pod "downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d" satisfied condition "Succeeded or Failed"
Sep  1 11:44:39.314: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d container dapi-container: <nil>
STEP: delete the pod 09/01/23 11:44:39.321
Sep  1 11:44:39.332: INFO: Waiting for pod downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d to disappear
Sep  1 11:44:39.335: INFO: Pod downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  1 11:44:39.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7027" for this suite. 09/01/23 11:44:39.339
------------------------------
• [4.087 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:44:35.258
    Sep  1 11:44:35.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 11:44:35.26
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:44:35.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:44:35.292
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 09/01/23 11:44:35.296
    Sep  1 11:44:35.303: INFO: Waiting up to 5m0s for pod "downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d" in namespace "downward-api-7027" to be "Succeeded or Failed"
    Sep  1 11:44:35.306: INFO: Pod "downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.289218ms
    Sep  1 11:44:37.311: INFO: Pod "downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007926113s
    Sep  1 11:44:39.311: INFO: Pod "downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007491984s
    STEP: Saw pod success 09/01/23 11:44:39.311
    Sep  1 11:44:39.311: INFO: Pod "downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d" satisfied condition "Succeeded or Failed"
    Sep  1 11:44:39.314: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d container dapi-container: <nil>
    STEP: delete the pod 09/01/23 11:44:39.321
    Sep  1 11:44:39.332: INFO: Waiting for pod downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d to disappear
    Sep  1 11:44:39.335: INFO: Pod downward-api-0a07732d-6136-439d-91b4-e9dda7f7e51d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:44:39.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7027" for this suite. 09/01/23 11:44:39.339
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:44:39.347
Sep  1 11:44:39.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename statefulset 09/01/23 11:44:39.349
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:44:39.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:44:39.373
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3134 09/01/23 11:44:39.377
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 09/01/23 11:44:39.386
STEP: Creating pod with conflicting port in namespace statefulset-3134 09/01/23 11:44:39.396
STEP: Waiting until pod test-pod will start running in namespace statefulset-3134 09/01/23 11:44:39.406
Sep  1 11:44:39.406: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3134" to be "running"
Sep  1 11:44:39.408: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535125ms
Sep  1 11:44:41.416: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009801564s
Sep  1 11:44:41.416: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3134 09/01/23 11:44:41.416
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3134 09/01/23 11:44:41.422
Sep  1 11:44:41.437: INFO: Observed stateful pod in namespace: statefulset-3134, name: ss-0, uid: c66ab281-3be8-49c8-a5f0-2fee03fef619, status phase: Pending. Waiting for statefulset controller to delete.
Sep  1 11:44:41.456: INFO: Observed stateful pod in namespace: statefulset-3134, name: ss-0, uid: c66ab281-3be8-49c8-a5f0-2fee03fef619, status phase: Failed. Waiting for statefulset controller to delete.
Sep  1 11:44:41.475: INFO: Observed stateful pod in namespace: statefulset-3134, name: ss-0, uid: c66ab281-3be8-49c8-a5f0-2fee03fef619, status phase: Failed. Waiting for statefulset controller to delete.
Sep  1 11:44:41.479: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3134
STEP: Removing pod with conflicting port in namespace statefulset-3134 09/01/23 11:44:41.479
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3134 and will be in running state 09/01/23 11:44:41.512
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  1 11:44:43.552: INFO: Deleting all statefulset in ns statefulset-3134
Sep  1 11:44:43.555: INFO: Scaling statefulset ss to 0
Sep  1 11:44:53.572: INFO: Waiting for statefulset status.replicas updated to 0
Sep  1 11:44:53.575: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  1 11:44:53.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3134" for this suite. 09/01/23 11:44:53.594
------------------------------
• [SLOW TEST] [14.257 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:44:39.347
    Sep  1 11:44:39.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename statefulset 09/01/23 11:44:39.349
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:44:39.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:44:39.373
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3134 09/01/23 11:44:39.377
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 09/01/23 11:44:39.386
    STEP: Creating pod with conflicting port in namespace statefulset-3134 09/01/23 11:44:39.396
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3134 09/01/23 11:44:39.406
    Sep  1 11:44:39.406: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3134" to be "running"
    Sep  1 11:44:39.408: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535125ms
    Sep  1 11:44:41.416: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009801564s
    Sep  1 11:44:41.416: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3134 09/01/23 11:44:41.416
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3134 09/01/23 11:44:41.422
    Sep  1 11:44:41.437: INFO: Observed stateful pod in namespace: statefulset-3134, name: ss-0, uid: c66ab281-3be8-49c8-a5f0-2fee03fef619, status phase: Pending. Waiting for statefulset controller to delete.
    Sep  1 11:44:41.456: INFO: Observed stateful pod in namespace: statefulset-3134, name: ss-0, uid: c66ab281-3be8-49c8-a5f0-2fee03fef619, status phase: Failed. Waiting for statefulset controller to delete.
    Sep  1 11:44:41.475: INFO: Observed stateful pod in namespace: statefulset-3134, name: ss-0, uid: c66ab281-3be8-49c8-a5f0-2fee03fef619, status phase: Failed. Waiting for statefulset controller to delete.
    Sep  1 11:44:41.479: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3134
    STEP: Removing pod with conflicting port in namespace statefulset-3134 09/01/23 11:44:41.479
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3134 and will be in running state 09/01/23 11:44:41.512
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  1 11:44:43.552: INFO: Deleting all statefulset in ns statefulset-3134
    Sep  1 11:44:43.555: INFO: Scaling statefulset ss to 0
    Sep  1 11:44:53.572: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  1 11:44:53.575: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:44:53.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3134" for this suite. 09/01/23 11:44:53.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:44:53.617
Sep  1 11:44:53.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename init-container 09/01/23 11:44:53.618
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:44:53.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:44:53.639
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 09/01/23 11:44:53.644
Sep  1 11:44:53.645: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:44:56.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1958" for this suite. 09/01/23 11:44:56.887
------------------------------
• [3.279 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:44:53.617
    Sep  1 11:44:53.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename init-container 09/01/23 11:44:53.618
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:44:53.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:44:53.639
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 09/01/23 11:44:53.644
    Sep  1 11:44:53.645: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:44:56.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1958" for this suite. 09/01/23 11:44:56.887
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:44:56.902
Sep  1 11:44:56.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename var-expansion 09/01/23 11:44:56.904
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:44:56.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:44:56.924
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Sep  1 11:44:56.938: INFO: Waiting up to 2m0s for pod "var-expansion-c31d8029-4c0d-49bf-bd04-070623b3b552" in namespace "var-expansion-2309" to be "container 0 failed with reason CreateContainerConfigError"
Sep  1 11:44:56.945: INFO: Pod "var-expansion-c31d8029-4c0d-49bf-bd04-070623b3b552": Phase="Pending", Reason="", readiness=false. Elapsed: 6.93497ms
Sep  1 11:44:58.949: INFO: Pod "var-expansion-c31d8029-4c0d-49bf-bd04-070623b3b552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011294184s
Sep  1 11:44:58.949: INFO: Pod "var-expansion-c31d8029-4c0d-49bf-bd04-070623b3b552" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep  1 11:44:58.949: INFO: Deleting pod "var-expansion-c31d8029-4c0d-49bf-bd04-070623b3b552" in namespace "var-expansion-2309"
Sep  1 11:44:58.960: INFO: Wait up to 5m0s for pod "var-expansion-c31d8029-4c0d-49bf-bd04-070623b3b552" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  1 11:45:00.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2309" for this suite. 09/01/23 11:45:00.982
------------------------------
• [4.088 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:44:56.902
    Sep  1 11:44:56.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename var-expansion 09/01/23 11:44:56.904
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:44:56.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:44:56.924
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Sep  1 11:44:56.938: INFO: Waiting up to 2m0s for pod "var-expansion-c31d8029-4c0d-49bf-bd04-070623b3b552" in namespace "var-expansion-2309" to be "container 0 failed with reason CreateContainerConfigError"
    Sep  1 11:44:56.945: INFO: Pod "var-expansion-c31d8029-4c0d-49bf-bd04-070623b3b552": Phase="Pending", Reason="", readiness=false. Elapsed: 6.93497ms
    Sep  1 11:44:58.949: INFO: Pod "var-expansion-c31d8029-4c0d-49bf-bd04-070623b3b552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011294184s
    Sep  1 11:44:58.949: INFO: Pod "var-expansion-c31d8029-4c0d-49bf-bd04-070623b3b552" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep  1 11:44:58.949: INFO: Deleting pod "var-expansion-c31d8029-4c0d-49bf-bd04-070623b3b552" in namespace "var-expansion-2309"
    Sep  1 11:44:58.960: INFO: Wait up to 5m0s for pod "var-expansion-c31d8029-4c0d-49bf-bd04-070623b3b552" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:45:00.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2309" for this suite. 09/01/23 11:45:00.982
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:45:00.993
Sep  1 11:45:00.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename deployment 09/01/23 11:45:00.995
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:45:01.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:45:01.017
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 09/01/23 11:45:01.026
STEP: waiting for Deployment to be created 09/01/23 11:45:01.034
STEP: waiting for all Replicas to be Ready 09/01/23 11:45:01.039
Sep  1 11:45:01.041: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  1 11:45:01.042: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  1 11:45:01.054: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  1 11:45:01.054: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  1 11:45:01.080: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  1 11:45:01.080: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  1 11:45:01.131: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  1 11:45:01.131: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  1 11:45:02.930: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  1 11:45:02.930: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  1 11:45:03.060: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 09/01/23 11:45:03.06
W0901 11:45:03.068095      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  1 11:45:03.070: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 09/01/23 11:45:03.07
Sep  1 11:45:03.072: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
Sep  1 11:45:03.072: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
Sep  1 11:45:03.073: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
Sep  1 11:45:03.073: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
Sep  1 11:45:03.073: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
Sep  1 11:45:03.073: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
Sep  1 11:45:03.074: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
Sep  1 11:45:03.074: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
Sep  1 11:45:03.074: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
Sep  1 11:45:03.074: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
Sep  1 11:45:03.074: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:03.074: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:03.075: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:03.075: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:03.085: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:03.085: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:03.123: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:03.124: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:03.145: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
Sep  1 11:45:03.145: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
Sep  1 11:45:03.157: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
Sep  1 11:45:03.157: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
Sep  1 11:45:04.955: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:04.955: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:04.982: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
STEP: listing Deployments 09/01/23 11:45:04.983
Sep  1 11:45:04.989: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 09/01/23 11:45:04.989
Sep  1 11:45:05.008: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 09/01/23 11:45:05.008
Sep  1 11:45:05.021: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  1 11:45:05.025: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  1 11:45:05.059: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  1 11:45:05.084: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  1 11:45:05.102: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  1 11:45:06.977: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  1 11:45:07.006: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  1 11:45:07.021: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  1 11:45:07.037: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  1 11:45:07.059: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  1 11:45:08.126: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 09/01/23 11:45:08.184
STEP: fetching the DeploymentStatus 09/01/23 11:45:08.194
Sep  1 11:45:08.207: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 3
STEP: deleting the Deployment 09/01/23 11:45:08.208
Sep  1 11:45:08.219: INFO: observed event type MODIFIED
Sep  1 11:45:08.220: INFO: observed event type MODIFIED
Sep  1 11:45:08.220: INFO: observed event type MODIFIED
Sep  1 11:45:08.220: INFO: observed event type MODIFIED
Sep  1 11:45:08.221: INFO: observed event type MODIFIED
Sep  1 11:45:08.221: INFO: observed event type MODIFIED
Sep  1 11:45:08.221: INFO: observed event type MODIFIED
Sep  1 11:45:08.221: INFO: observed event type MODIFIED
Sep  1 11:45:08.222: INFO: observed event type MODIFIED
Sep  1 11:45:08.222: INFO: observed event type MODIFIED
Sep  1 11:45:08.222: INFO: observed event type MODIFIED
Sep  1 11:45:08.222: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  1 11:45:08.227: INFO: Log out all the ReplicaSets if there is no deployment created
Sep  1 11:45:08.231: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-9770  07e2eb90-eb55-48c0-bfe9-1b059ceb9761 48064 2 2023-09-01 11:45:05 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e40194e1-3dc4-47d6-b72f-9b1304383e7d 0xc0006456d7 0xc0006456d8}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:45:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e40194e1-3dc4-47d6-b72f-9b1304383e7d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:45:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000645760 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Sep  1 11:45:08.238: INFO: pod: "test-deployment-7b7876f9d6-5xlm7":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-5xlm7 test-deployment-7b7876f9d6- deployment-9770  a1af1fa5-7621-4db9-ac3f-5d8a65cd755f 48025 0 2023-09-01 11:45:05 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 07e2eb90-eb55-48c0-bfe9-1b059ceb9761 0xc000645c17 0xc000645c18}] [] [{kube-controller-manager Update v1 2023-09-01 11:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e2eb90-eb55-48c0-bfe9-1b059ceb9761\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:45:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s77ks,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s77ks,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.16,StartTime:2023-09-01 11:45:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:45:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://14ab4cd0f1af803de4a3bea7fe8822528b7b48e965d945395e59c291e2576aff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  1 11:45:08.239: INFO: pod: "test-deployment-7b7876f9d6-rbckz":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-rbckz test-deployment-7b7876f9d6- deployment-9770  b8e81854-42ba-475b-b4ee-f4058655b819 48063 0 2023-09-01 11:45:07 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 07e2eb90-eb55-48c0-bfe9-1b059ceb9761 0xc000645e07 0xc000645e08}] [] [{kube-controller-manager Update v1 2023-09-01 11:45:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e2eb90-eb55-48c0-bfe9-1b059ceb9761\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:45:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tp8p6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tp8p6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.1.3,StartTime:2023-09-01 11:45:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:45:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8d30594b6151f1f1144be1935f5eaa28d4c4faa9615d5f2e5d6de5062d3dc79b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  1 11:45:08.240: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-9770  81262fa0-0731-49ac-9afe-c0ea7cd7da16 48072 4 2023-09-01 11:45:03 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e40194e1-3dc4-47d6-b72f-9b1304383e7d 0xc0006457c7 0xc0006457c8}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:45:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e40194e1-3dc4-47d6-b72f-9b1304383e7d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:45:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000645850 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Sep  1 11:45:08.244: INFO: pod: "test-deployment-7df74c55ff-9kmh4":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-9kmh4 test-deployment-7df74c55ff- deployment-9770  fa23aa77-8937-437f-9f3d-76516868ae13 48068 0 2023-09-01 11:45:03 +0000 UTC 2023-09-01 11:45:09 +0000 UTC 0xc00da6f178 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 81262fa0-0731-49ac-9afe-c0ea7cd7da16 0xc00da6f1a7 0xc00da6f1a8}] [] [{kube-controller-manager Update v1 2023-09-01 11:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81262fa0-0731-49ac-9afe-c0ea7cd7da16\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:45:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkp8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkp8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.32,StartTime:2023-09-01 11:45:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:45:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://e057c65e31e58e0dff42699ebe49132b57afa8b0f969a7459e8045f08b354aad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  1 11:45:08.244: INFO: pod: "test-deployment-7df74c55ff-zd2vl":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-zd2vl test-deployment-7df74c55ff- deployment-9770  b51d1eb5-b461-49da-a8db-5541dcb72090 48047 0 2023-09-01 11:45:05 +0000 UTC 2023-09-01 11:45:07 +0000 UTC 0xc00da6f360 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 81262fa0-0731-49ac-9afe-c0ea7cd7da16 0xc00da6f397 0xc00da6f398}] [] [{kube-controller-manager Update v1 2023-09-01 11:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81262fa0-0731-49ac-9afe-c0ea7cd7da16\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:45:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6sjq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6sjq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.1.198,StartTime:2023-09-01 11:45:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:45:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://d3db20420b7d842af9feed3738898da562a4493f22612a4456bb96d0b8410486,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  1 11:45:08.245: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-9770  eb9a99ca-825d-4f2b-b220-dd2d4590d91e 47966 3 2023-09-01 11:45:01 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e40194e1-3dc4-47d6-b72f-9b1304383e7d 0xc0006458b7 0xc0006458b8}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:45:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e40194e1-3dc4-47d6-b72f-9b1304383e7d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:45:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000645940 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  1 11:45:08.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9770" for this suite. 09/01/23 11:45:08.256
------------------------------
• [SLOW TEST] [7.269 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:45:00.993
    Sep  1 11:45:00.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename deployment 09/01/23 11:45:00.995
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:45:01.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:45:01.017
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 09/01/23 11:45:01.026
    STEP: waiting for Deployment to be created 09/01/23 11:45:01.034
    STEP: waiting for all Replicas to be Ready 09/01/23 11:45:01.039
    Sep  1 11:45:01.041: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  1 11:45:01.042: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  1 11:45:01.054: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  1 11:45:01.054: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  1 11:45:01.080: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  1 11:45:01.080: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  1 11:45:01.131: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  1 11:45:01.131: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  1 11:45:02.930: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep  1 11:45:02.930: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep  1 11:45:03.060: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 09/01/23 11:45:03.06
    W0901 11:45:03.068095      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  1 11:45:03.070: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 09/01/23 11:45:03.07
    Sep  1 11:45:03.072: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
    Sep  1 11:45:03.072: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
    Sep  1 11:45:03.073: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
    Sep  1 11:45:03.073: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
    Sep  1 11:45:03.073: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
    Sep  1 11:45:03.073: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
    Sep  1 11:45:03.074: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
    Sep  1 11:45:03.074: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 0
    Sep  1 11:45:03.074: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
    Sep  1 11:45:03.074: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
    Sep  1 11:45:03.074: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:03.074: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:03.075: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:03.075: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:03.085: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:03.085: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:03.123: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:03.124: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:03.145: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
    Sep  1 11:45:03.145: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
    Sep  1 11:45:03.157: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
    Sep  1 11:45:03.157: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
    Sep  1 11:45:04.955: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:04.955: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:04.982: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
    STEP: listing Deployments 09/01/23 11:45:04.983
    Sep  1 11:45:04.989: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 09/01/23 11:45:04.989
    Sep  1 11:45:05.008: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 09/01/23 11:45:05.008
    Sep  1 11:45:05.021: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  1 11:45:05.025: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  1 11:45:05.059: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  1 11:45:05.084: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  1 11:45:05.102: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  1 11:45:06.977: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  1 11:45:07.006: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  1 11:45:07.021: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  1 11:45:07.037: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  1 11:45:07.059: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  1 11:45:08.126: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 09/01/23 11:45:08.184
    STEP: fetching the DeploymentStatus 09/01/23 11:45:08.194
    Sep  1 11:45:08.207: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
    Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
    Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
    Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
    Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 1
    Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 2
    Sep  1 11:45:08.208: INFO: observed Deployment test-deployment in namespace deployment-9770 with ReadyReplicas 3
    STEP: deleting the Deployment 09/01/23 11:45:08.208
    Sep  1 11:45:08.219: INFO: observed event type MODIFIED
    Sep  1 11:45:08.220: INFO: observed event type MODIFIED
    Sep  1 11:45:08.220: INFO: observed event type MODIFIED
    Sep  1 11:45:08.220: INFO: observed event type MODIFIED
    Sep  1 11:45:08.221: INFO: observed event type MODIFIED
    Sep  1 11:45:08.221: INFO: observed event type MODIFIED
    Sep  1 11:45:08.221: INFO: observed event type MODIFIED
    Sep  1 11:45:08.221: INFO: observed event type MODIFIED
    Sep  1 11:45:08.222: INFO: observed event type MODIFIED
    Sep  1 11:45:08.222: INFO: observed event type MODIFIED
    Sep  1 11:45:08.222: INFO: observed event type MODIFIED
    Sep  1 11:45:08.222: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  1 11:45:08.227: INFO: Log out all the ReplicaSets if there is no deployment created
    Sep  1 11:45:08.231: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-9770  07e2eb90-eb55-48c0-bfe9-1b059ceb9761 48064 2 2023-09-01 11:45:05 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e40194e1-3dc4-47d6-b72f-9b1304383e7d 0xc0006456d7 0xc0006456d8}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:45:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e40194e1-3dc4-47d6-b72f-9b1304383e7d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:45:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000645760 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Sep  1 11:45:08.238: INFO: pod: "test-deployment-7b7876f9d6-5xlm7":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-5xlm7 test-deployment-7b7876f9d6- deployment-9770  a1af1fa5-7621-4db9-ac3f-5d8a65cd755f 48025 0 2023-09-01 11:45:05 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 07e2eb90-eb55-48c0-bfe9-1b059ceb9761 0xc000645c17 0xc000645c18}] [] [{kube-controller-manager Update v1 2023-09-01 11:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e2eb90-eb55-48c0-bfe9-1b059ceb9761\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:45:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s77ks,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s77ks,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.16,StartTime:2023-09-01 11:45:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:45:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://14ab4cd0f1af803de4a3bea7fe8822528b7b48e965d945395e59c291e2576aff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  1 11:45:08.239: INFO: pod: "test-deployment-7b7876f9d6-rbckz":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-rbckz test-deployment-7b7876f9d6- deployment-9770  b8e81854-42ba-475b-b4ee-f4058655b819 48063 0 2023-09-01 11:45:07 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 07e2eb90-eb55-48c0-bfe9-1b059ceb9761 0xc000645e07 0xc000645e08}] [] [{kube-controller-manager Update v1 2023-09-01 11:45:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e2eb90-eb55-48c0-bfe9-1b059ceb9761\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:45:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tp8p6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tp8p6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.1.3,StartTime:2023-09-01 11:45:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:45:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8d30594b6151f1f1144be1935f5eaa28d4c4faa9615d5f2e5d6de5062d3dc79b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  1 11:45:08.240: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-9770  81262fa0-0731-49ac-9afe-c0ea7cd7da16 48072 4 2023-09-01 11:45:03 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e40194e1-3dc4-47d6-b72f-9b1304383e7d 0xc0006457c7 0xc0006457c8}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:45:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e40194e1-3dc4-47d6-b72f-9b1304383e7d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:45:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000645850 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Sep  1 11:45:08.244: INFO: pod: "test-deployment-7df74c55ff-9kmh4":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-9kmh4 test-deployment-7df74c55ff- deployment-9770  fa23aa77-8937-437f-9f3d-76516868ae13 48068 0 2023-09-01 11:45:03 +0000 UTC 2023-09-01 11:45:09 +0000 UTC 0xc00da6f178 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 81262fa0-0731-49ac-9afe-c0ea7cd7da16 0xc00da6f1a7 0xc00da6f1a8}] [] [{kube-controller-manager Update v1 2023-09-01 11:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81262fa0-0731-49ac-9afe-c0ea7cd7da16\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:45:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkp8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkp8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.0.32,StartTime:2023-09-01 11:45:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:45:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://e057c65e31e58e0dff42699ebe49132b57afa8b0f969a7459e8045f08b354aad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  1 11:45:08.244: INFO: pod: "test-deployment-7df74c55ff-zd2vl":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-zd2vl test-deployment-7df74c55ff- deployment-9770  b51d1eb5-b461-49da-a8db-5541dcb72090 48047 0 2023-09-01 11:45:05 +0000 UTC 2023-09-01 11:45:07 +0000 UTC 0xc00da6f360 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 81262fa0-0731-49ac-9afe-c0ea7cd7da16 0xc00da6f397 0xc00da6f398}] [] [{kube-controller-manager Update v1 2023-09-01 11:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81262fa0-0731-49ac-9afe-c0ea7cd7da16\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-01 11:45:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6sjq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6sjq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-01 11:45:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.1.198,StartTime:2023-09-01 11:45:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-01 11:45:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://d3db20420b7d842af9feed3738898da562a4493f22612a4456bb96d0b8410486,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  1 11:45:08.245: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-9770  eb9a99ca-825d-4f2b-b220-dd2d4590d91e 47966 3 2023-09-01 11:45:01 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e40194e1-3dc4-47d6-b72f-9b1304383e7d 0xc0006458b7 0xc0006458b8}] [] [{kube-controller-manager Update apps/v1 2023-09-01 11:45:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e40194e1-3dc4-47d6-b72f-9b1304383e7d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-01 11:45:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000645940 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:45:08.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9770" for this suite. 09/01/23 11:45:08.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:45:08.274
Sep  1 11:45:08.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pod-network-test 09/01/23 11:45:08.275
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:45:08.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:45:08.307
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-1935 09/01/23 11:45:08.311
STEP: creating a selector 09/01/23 11:45:08.311
STEP: Creating the service pods in kubernetes 09/01/23 11:45:08.311
Sep  1 11:45:08.312: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  1 11:45:08.345: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1935" to be "running and ready"
Sep  1 11:45:08.357: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017288ms
Sep  1 11:45:08.357: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:45:10.362: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016584581s
Sep  1 11:45:10.362: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:45:12.361: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016491833s
Sep  1 11:45:12.362: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:45:14.365: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019793271s
Sep  1 11:45:14.365: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:45:16.361: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016066429s
Sep  1 11:45:16.361: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:45:18.362: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017294891s
Sep  1 11:45:18.362: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:45:20.361: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.01618064s
Sep  1 11:45:20.361: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:45:22.361: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015916987s
Sep  1 11:45:22.361: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:45:24.362: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01658456s
Sep  1 11:45:24.362: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:45:26.361: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015784292s
Sep  1 11:45:26.361: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:45:28.362: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.017167881s
Sep  1 11:45:28.362: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  1 11:45:30.361: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015978992s
Sep  1 11:45:30.361: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  1 11:45:30.361: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  1 11:45:30.364: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1935" to be "running and ready"
Sep  1 11:45:30.366: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.452177ms
Sep  1 11:45:30.367: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  1 11:45:30.367: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/01/23 11:45:30.369
Sep  1 11:45:30.375: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1935" to be "running"
Sep  1 11:45:30.378: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.446599ms
Sep  1 11:45:32.382: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006652765s
Sep  1 11:45:32.382: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  1 11:45:32.384: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  1 11:45:32.384: INFO: Breadth first check of 10.10.0.178 on host 172.16.0.3...
Sep  1 11:45:32.387: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.55:9080/dial?request=hostname&protocol=http&host=10.10.0.178&port=8083&tries=1'] Namespace:pod-network-test-1935 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:45:32.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:45:32.388: INFO: ExecWithOptions: Clientset creation
Sep  1 11:45:32.388: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1935/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.0.178%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  1 11:45:32.494: INFO: Waiting for responses: map[]
Sep  1 11:45:32.494: INFO: reached 10.10.0.178 after 0/1 tries
Sep  1 11:45:32.494: INFO: Breadth first check of 10.10.1.114 on host 172.16.0.4...
Sep  1 11:45:32.497: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.55:9080/dial?request=hostname&protocol=http&host=10.10.1.114&port=8083&tries=1'] Namespace:pod-network-test-1935 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:45:32.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:45:32.498: INFO: ExecWithOptions: Clientset creation
Sep  1 11:45:32.498: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1935/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.1.114%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  1 11:45:32.606: INFO: Waiting for responses: map[]
Sep  1 11:45:32.606: INFO: reached 10.10.1.114 after 0/1 tries
Sep  1 11:45:32.606: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  1 11:45:32.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1935" for this suite. 09/01/23 11:45:32.611
------------------------------
• [SLOW TEST] [24.343 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:45:08.274
    Sep  1 11:45:08.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pod-network-test 09/01/23 11:45:08.275
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:45:08.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:45:08.307
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-1935 09/01/23 11:45:08.311
    STEP: creating a selector 09/01/23 11:45:08.311
    STEP: Creating the service pods in kubernetes 09/01/23 11:45:08.311
    Sep  1 11:45:08.312: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  1 11:45:08.345: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1935" to be "running and ready"
    Sep  1 11:45:08.357: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017288ms
    Sep  1 11:45:08.357: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:45:10.362: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016584581s
    Sep  1 11:45:10.362: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:45:12.361: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016491833s
    Sep  1 11:45:12.362: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:45:14.365: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019793271s
    Sep  1 11:45:14.365: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:45:16.361: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016066429s
    Sep  1 11:45:16.361: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:45:18.362: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017294891s
    Sep  1 11:45:18.362: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:45:20.361: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.01618064s
    Sep  1 11:45:20.361: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:45:22.361: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015916987s
    Sep  1 11:45:22.361: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:45:24.362: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01658456s
    Sep  1 11:45:24.362: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:45:26.361: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015784292s
    Sep  1 11:45:26.361: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:45:28.362: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.017167881s
    Sep  1 11:45:28.362: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  1 11:45:30.361: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015978992s
    Sep  1 11:45:30.361: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  1 11:45:30.361: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  1 11:45:30.364: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1935" to be "running and ready"
    Sep  1 11:45:30.366: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.452177ms
    Sep  1 11:45:30.367: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  1 11:45:30.367: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/01/23 11:45:30.369
    Sep  1 11:45:30.375: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1935" to be "running"
    Sep  1 11:45:30.378: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.446599ms
    Sep  1 11:45:32.382: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006652765s
    Sep  1 11:45:32.382: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  1 11:45:32.384: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  1 11:45:32.384: INFO: Breadth first check of 10.10.0.178 on host 172.16.0.3...
    Sep  1 11:45:32.387: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.55:9080/dial?request=hostname&protocol=http&host=10.10.0.178&port=8083&tries=1'] Namespace:pod-network-test-1935 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:45:32.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:45:32.388: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:45:32.388: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1935/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.0.178%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  1 11:45:32.494: INFO: Waiting for responses: map[]
    Sep  1 11:45:32.494: INFO: reached 10.10.0.178 after 0/1 tries
    Sep  1 11:45:32.494: INFO: Breadth first check of 10.10.1.114 on host 172.16.0.4...
    Sep  1 11:45:32.497: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.55:9080/dial?request=hostname&protocol=http&host=10.10.1.114&port=8083&tries=1'] Namespace:pod-network-test-1935 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:45:32.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:45:32.498: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:45:32.498: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1935/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.1.114%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  1 11:45:32.606: INFO: Waiting for responses: map[]
    Sep  1 11:45:32.606: INFO: reached 10.10.1.114 after 0/1 tries
    Sep  1 11:45:32.606: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:45:32.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1935" for this suite. 09/01/23 11:45:32.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:45:32.62
Sep  1 11:45:32.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename aggregator 09/01/23 11:45:32.621
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:45:32.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:45:32.644
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Sep  1 11:45:32.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 09/01/23 11:45:32.661
Sep  1 11:45:32.990: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  1 11:45:35.067: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 11:45:37.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 11:45:39.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 11:45:41.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 11:45:43.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 11:45:45.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 11:45:47.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 11:45:49.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 11:45:51.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 11:45:53.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 11:45:55.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  1 11:45:57.724: INFO: Waited 644.930045ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 09/01/23 11:45:57.887
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/01/23 11:45:57.924
STEP: List APIServices 09/01/23 11:45:57.979
Sep  1 11:45:58.030: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Sep  1 11:45:58.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-2289" for this suite. 09/01/23 11:45:58.875
------------------------------
• [SLOW TEST] [26.308 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:45:32.62
    Sep  1 11:45:32.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename aggregator 09/01/23 11:45:32.621
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:45:32.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:45:32.644
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Sep  1 11:45:32.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 09/01/23 11:45:32.661
    Sep  1 11:45:32.990: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Sep  1 11:45:35.067: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 11:45:37.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 11:45:39.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 11:45:41.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 11:45:43.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 11:45:45.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 11:45:47.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 11:45:49.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 11:45:51.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 11:45:53.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 11:45:55.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 45, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 45, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  1 11:45:57.724: INFO: Waited 644.930045ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 09/01/23 11:45:57.887
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/01/23 11:45:57.924
    STEP: List APIServices 09/01/23 11:45:57.979
    Sep  1 11:45:58.030: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:45:58.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-2289" for this suite. 09/01/23 11:45:58.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:45:58.93
Sep  1 11:45:58.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 11:45:58.931
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:45:58.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:45:58.952
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 09/01/23 11:45:58.955
Sep  1 11:45:58.967: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5" in namespace "downward-api-872" to be "Succeeded or Failed"
Sep  1 11:45:58.975: INFO: Pod "downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.763546ms
Sep  1 11:46:00.979: INFO: Pod "downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012126906s
Sep  1 11:46:02.979: INFO: Pod "downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5": Phase="Running", Reason="", readiness=false. Elapsed: 4.012340712s
Sep  1 11:46:04.978: INFO: Pod "downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011362528s
STEP: Saw pod success 09/01/23 11:46:04.978
Sep  1 11:46:04.979: INFO: Pod "downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5" satisfied condition "Succeeded or Failed"
Sep  1 11:46:04.982: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5 container client-container: <nil>
STEP: delete the pod 09/01/23 11:46:04.99
Sep  1 11:46:05.003: INFO: Waiting for pod downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5 to disappear
Sep  1 11:46:05.006: INFO: Pod downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  1 11:46:05.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-872" for this suite. 09/01/23 11:46:05.011
------------------------------
• [SLOW TEST] [6.087 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:45:58.93
    Sep  1 11:45:58.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 11:45:58.931
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:45:58.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:45:58.952
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 09/01/23 11:45:58.955
    Sep  1 11:45:58.967: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5" in namespace "downward-api-872" to be "Succeeded or Failed"
    Sep  1 11:45:58.975: INFO: Pod "downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.763546ms
    Sep  1 11:46:00.979: INFO: Pod "downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012126906s
    Sep  1 11:46:02.979: INFO: Pod "downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5": Phase="Running", Reason="", readiness=false. Elapsed: 4.012340712s
    Sep  1 11:46:04.978: INFO: Pod "downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011362528s
    STEP: Saw pod success 09/01/23 11:46:04.978
    Sep  1 11:46:04.979: INFO: Pod "downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5" satisfied condition "Succeeded or Failed"
    Sep  1 11:46:04.982: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5 container client-container: <nil>
    STEP: delete the pod 09/01/23 11:46:04.99
    Sep  1 11:46:05.003: INFO: Waiting for pod downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5 to disappear
    Sep  1 11:46:05.006: INFO: Pod downwardapi-volume-41b2bd56-0d4e-4b69-a90f-d181b541dcc5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:46:05.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-872" for this suite. 09/01/23 11:46:05.011
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:46:05.018
Sep  1 11:46:05.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename secrets 09/01/23 11:46:05.019
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:05.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:05.051
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-56b642f2-9f59-4436-a775-f98cf2cf7e94 09/01/23 11:46:05.058
STEP: Creating a pod to test consume secrets 09/01/23 11:46:05.063
Sep  1 11:46:05.071: INFO: Waiting up to 5m0s for pod "pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca" in namespace "secrets-9504" to be "Succeeded or Failed"
Sep  1 11:46:05.075: INFO: Pod "pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.963471ms
Sep  1 11:46:07.080: INFO: Pod "pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008694064s
Sep  1 11:46:09.081: INFO: Pod "pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0094906s
STEP: Saw pod success 09/01/23 11:46:09.081
Sep  1 11:46:09.081: INFO: Pod "pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca" satisfied condition "Succeeded or Failed"
Sep  1 11:46:09.084: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca container secret-volume-test: <nil>
STEP: delete the pod 09/01/23 11:46:09.089
Sep  1 11:46:09.104: INFO: Waiting for pod pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca to disappear
Sep  1 11:46:09.107: INFO: Pod pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  1 11:46:09.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9504" for this suite. 09/01/23 11:46:09.112
------------------------------
• [4.100 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:46:05.018
    Sep  1 11:46:05.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename secrets 09/01/23 11:46:05.019
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:05.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:05.051
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-56b642f2-9f59-4436-a775-f98cf2cf7e94 09/01/23 11:46:05.058
    STEP: Creating a pod to test consume secrets 09/01/23 11:46:05.063
    Sep  1 11:46:05.071: INFO: Waiting up to 5m0s for pod "pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca" in namespace "secrets-9504" to be "Succeeded or Failed"
    Sep  1 11:46:05.075: INFO: Pod "pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.963471ms
    Sep  1 11:46:07.080: INFO: Pod "pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008694064s
    Sep  1 11:46:09.081: INFO: Pod "pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0094906s
    STEP: Saw pod success 09/01/23 11:46:09.081
    Sep  1 11:46:09.081: INFO: Pod "pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca" satisfied condition "Succeeded or Failed"
    Sep  1 11:46:09.084: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca container secret-volume-test: <nil>
    STEP: delete the pod 09/01/23 11:46:09.089
    Sep  1 11:46:09.104: INFO: Waiting for pod pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca to disappear
    Sep  1 11:46:09.107: INFO: Pod pod-secrets-aadb87cb-5918-4fb5-9e0e-6b6712c222ca no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:46:09.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9504" for this suite. 09/01/23 11:46:09.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:46:09.124
Sep  1 11:46:09.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 11:46:09.126
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:09.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:09.15
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-a1863797-5ffe-4ab3-a09c-c22796ee1031 09/01/23 11:46:09.153
STEP: Creating a pod to test consume configMaps 09/01/23 11:46:09.16
Sep  1 11:46:09.170: INFO: Waiting up to 5m0s for pod "pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c" in namespace "configmap-287" to be "Succeeded or Failed"
Sep  1 11:46:09.175: INFO: Pod "pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.371249ms
Sep  1 11:46:11.179: INFO: Pod "pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008525463s
Sep  1 11:46:13.179: INFO: Pod "pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00808002s
STEP: Saw pod success 09/01/23 11:46:13.179
Sep  1 11:46:13.179: INFO: Pod "pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c" satisfied condition "Succeeded or Failed"
Sep  1 11:46:13.182: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c container agnhost-container: <nil>
STEP: delete the pod 09/01/23 11:46:13.188
Sep  1 11:46:13.200: INFO: Waiting for pod pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c to disappear
Sep  1 11:46:13.203: INFO: Pod pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:46:13.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-287" for this suite. 09/01/23 11:46:13.208
------------------------------
• [4.097 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:46:09.124
    Sep  1 11:46:09.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 11:46:09.126
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:09.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:09.15
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-a1863797-5ffe-4ab3-a09c-c22796ee1031 09/01/23 11:46:09.153
    STEP: Creating a pod to test consume configMaps 09/01/23 11:46:09.16
    Sep  1 11:46:09.170: INFO: Waiting up to 5m0s for pod "pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c" in namespace "configmap-287" to be "Succeeded or Failed"
    Sep  1 11:46:09.175: INFO: Pod "pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.371249ms
    Sep  1 11:46:11.179: INFO: Pod "pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008525463s
    Sep  1 11:46:13.179: INFO: Pod "pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00808002s
    STEP: Saw pod success 09/01/23 11:46:13.179
    Sep  1 11:46:13.179: INFO: Pod "pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c" satisfied condition "Succeeded or Failed"
    Sep  1 11:46:13.182: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 11:46:13.188
    Sep  1 11:46:13.200: INFO: Waiting for pod pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c to disappear
    Sep  1 11:46:13.203: INFO: Pod pod-configmaps-735cd71f-d3a4-4eff-8f1f-d7e700b3da1c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:46:13.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-287" for this suite. 09/01/23 11:46:13.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:46:13.233
Sep  1 11:46:13.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename resourcequota 09/01/23 11:46:13.235
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:13.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:13.257
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 09/01/23 11:46:13.26
STEP: Creating a ResourceQuota 09/01/23 11:46:18.302
STEP: Ensuring resource quota status is calculated 09/01/23 11:46:18.318
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  1 11:46:20.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3356" for this suite. 09/01/23 11:46:20.327
------------------------------
• [SLOW TEST] [7.100 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:46:13.233
    Sep  1 11:46:13.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename resourcequota 09/01/23 11:46:13.235
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:13.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:13.257
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 09/01/23 11:46:13.26
    STEP: Creating a ResourceQuota 09/01/23 11:46:18.302
    STEP: Ensuring resource quota status is calculated 09/01/23 11:46:18.318
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:46:20.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3356" for this suite. 09/01/23 11:46:20.327
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:46:20.335
Sep  1 11:46:20.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename proxy 09/01/23 11:46:20.337
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:20.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:20.357
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Sep  1 11:46:20.361: INFO: Creating pod...
Sep  1 11:46:20.370: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7470" to be "running"
Sep  1 11:46:20.375: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.377867ms
Sep  1 11:46:22.379: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008917028s
Sep  1 11:46:22.379: INFO: Pod "agnhost" satisfied condition "running"
Sep  1 11:46:22.379: INFO: Creating service...
Sep  1 11:46:22.391: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/pods/agnhost/proxy/some/path/with/DELETE
Sep  1 11:46:22.402: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  1 11:46:22.402: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/pods/agnhost/proxy/some/path/with/GET
Sep  1 11:46:22.411: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep  1 11:46:22.412: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/pods/agnhost/proxy/some/path/with/HEAD
Sep  1 11:46:22.418: INFO: http.Client request:HEAD | StatusCode:200
Sep  1 11:46:22.418: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/pods/agnhost/proxy/some/path/with/OPTIONS
Sep  1 11:46:22.430: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  1 11:46:22.431: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/pods/agnhost/proxy/some/path/with/PATCH
Sep  1 11:46:22.437: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  1 11:46:22.437: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/pods/agnhost/proxy/some/path/with/POST
Sep  1 11:46:22.443: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  1 11:46:22.443: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/pods/agnhost/proxy/some/path/with/PUT
Sep  1 11:46:22.450: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  1 11:46:22.450: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/services/test-service/proxy/some/path/with/DELETE
Sep  1 11:46:22.457: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  1 11:46:22.458: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/services/test-service/proxy/some/path/with/GET
Sep  1 11:46:22.464: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep  1 11:46:22.464: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/services/test-service/proxy/some/path/with/HEAD
Sep  1 11:46:22.472: INFO: http.Client request:HEAD | StatusCode:200
Sep  1 11:46:22.472: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/services/test-service/proxy/some/path/with/OPTIONS
Sep  1 11:46:22.480: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  1 11:46:22.480: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/services/test-service/proxy/some/path/with/PATCH
Sep  1 11:46:22.486: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  1 11:46:22.486: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/services/test-service/proxy/some/path/with/POST
Sep  1 11:46:22.493: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  1 11:46:22.494: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/services/test-service/proxy/some/path/with/PUT
Sep  1 11:46:22.504: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  1 11:46:22.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7470" for this suite. 09/01/23 11:46:22.515
------------------------------
• [2.195 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:46:20.335
    Sep  1 11:46:20.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename proxy 09/01/23 11:46:20.337
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:20.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:20.357
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Sep  1 11:46:20.361: INFO: Creating pod...
    Sep  1 11:46:20.370: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7470" to be "running"
    Sep  1 11:46:20.375: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.377867ms
    Sep  1 11:46:22.379: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008917028s
    Sep  1 11:46:22.379: INFO: Pod "agnhost" satisfied condition "running"
    Sep  1 11:46:22.379: INFO: Creating service...
    Sep  1 11:46:22.391: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/pods/agnhost/proxy/some/path/with/DELETE
    Sep  1 11:46:22.402: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  1 11:46:22.402: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/pods/agnhost/proxy/some/path/with/GET
    Sep  1 11:46:22.411: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep  1 11:46:22.412: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/pods/agnhost/proxy/some/path/with/HEAD
    Sep  1 11:46:22.418: INFO: http.Client request:HEAD | StatusCode:200
    Sep  1 11:46:22.418: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/pods/agnhost/proxy/some/path/with/OPTIONS
    Sep  1 11:46:22.430: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  1 11:46:22.431: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/pods/agnhost/proxy/some/path/with/PATCH
    Sep  1 11:46:22.437: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  1 11:46:22.437: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/pods/agnhost/proxy/some/path/with/POST
    Sep  1 11:46:22.443: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  1 11:46:22.443: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/pods/agnhost/proxy/some/path/with/PUT
    Sep  1 11:46:22.450: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  1 11:46:22.450: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/services/test-service/proxy/some/path/with/DELETE
    Sep  1 11:46:22.457: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  1 11:46:22.458: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/services/test-service/proxy/some/path/with/GET
    Sep  1 11:46:22.464: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep  1 11:46:22.464: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/services/test-service/proxy/some/path/with/HEAD
    Sep  1 11:46:22.472: INFO: http.Client request:HEAD | StatusCode:200
    Sep  1 11:46:22.472: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/services/test-service/proxy/some/path/with/OPTIONS
    Sep  1 11:46:22.480: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  1 11:46:22.480: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/services/test-service/proxy/some/path/with/PATCH
    Sep  1 11:46:22.486: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  1 11:46:22.486: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/services/test-service/proxy/some/path/with/POST
    Sep  1 11:46:22.493: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  1 11:46:22.494: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7470/services/test-service/proxy/some/path/with/PUT
    Sep  1 11:46:22.504: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:46:22.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7470" for this suite. 09/01/23 11:46:22.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:46:22.548
Sep  1 11:46:22.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename daemonsets 09/01/23 11:46:22.55
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:22.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:22.578
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
Sep  1 11:46:22.619: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 09/01/23 11:46:22.629
Sep  1 11:46:22.643: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:46:22.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 11:46:22.649: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:46:23.653: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:46:23.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 11:46:23.657: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:46:24.654: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:46:24.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  1 11:46:24.657: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 09/01/23 11:46:24.669
STEP: Check that daemon pods images are updated. 09/01/23 11:46:24.683
Sep  1 11:46:24.688: INFO: Wrong image for pod: daemon-set-g4fbq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  1 11:46:24.688: INFO: Wrong image for pod: daemon-set-z2md8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  1 11:46:24.694: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:46:25.698: INFO: Wrong image for pod: daemon-set-z2md8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  1 11:46:25.702: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:46:26.698: INFO: Wrong image for pod: daemon-set-z2md8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  1 11:46:26.702: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:46:27.699: INFO: Pod daemon-set-sh6pn is not available
Sep  1 11:46:27.699: INFO: Wrong image for pod: daemon-set-z2md8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  1 11:46:27.704: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:46:28.699: INFO: Pod daemon-set-sh6pn is not available
Sep  1 11:46:28.699: INFO: Wrong image for pod: daemon-set-z2md8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  1 11:46:28.703: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:46:29.702: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:46:30.701: INFO: Pod daemon-set-cst64 is not available
Sep  1 11:46:30.705: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 09/01/23 11:46:30.748
Sep  1 11:46:30.753: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:46:30.757: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 11:46:30.757: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:46:31.762: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:46:31.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 11:46:31.766: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:46:32.762: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:46:32.765: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  1 11:46:32.766: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/01/23 11:46:32.78
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8935, will wait for the garbage collector to delete the pods 09/01/23 11:46:32.781
Sep  1 11:46:32.840: INFO: Deleting DaemonSet.extensions daemon-set took: 6.119103ms
Sep  1 11:46:32.941: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.66096ms
Sep  1 11:46:35.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 11:46:35.546: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  1 11:46:35.549: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48988"},"items":null}

Sep  1 11:46:35.552: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48988"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:46:35.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8935" for this suite. 09/01/23 11:46:35.566
------------------------------
• [SLOW TEST] [13.025 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:46:22.548
    Sep  1 11:46:22.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename daemonsets 09/01/23 11:46:22.55
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:22.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:22.578
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:385
    Sep  1 11:46:22.619: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 09/01/23 11:46:22.629
    Sep  1 11:46:22.643: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:46:22.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 11:46:22.649: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:46:23.653: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:46:23.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 11:46:23.657: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:46:24.654: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:46:24.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  1 11:46:24.657: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 09/01/23 11:46:24.669
    STEP: Check that daemon pods images are updated. 09/01/23 11:46:24.683
    Sep  1 11:46:24.688: INFO: Wrong image for pod: daemon-set-g4fbq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  1 11:46:24.688: INFO: Wrong image for pod: daemon-set-z2md8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  1 11:46:24.694: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:46:25.698: INFO: Wrong image for pod: daemon-set-z2md8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  1 11:46:25.702: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:46:26.698: INFO: Wrong image for pod: daemon-set-z2md8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  1 11:46:26.702: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:46:27.699: INFO: Pod daemon-set-sh6pn is not available
    Sep  1 11:46:27.699: INFO: Wrong image for pod: daemon-set-z2md8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  1 11:46:27.704: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:46:28.699: INFO: Pod daemon-set-sh6pn is not available
    Sep  1 11:46:28.699: INFO: Wrong image for pod: daemon-set-z2md8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  1 11:46:28.703: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:46:29.702: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:46:30.701: INFO: Pod daemon-set-cst64 is not available
    Sep  1 11:46:30.705: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 09/01/23 11:46:30.748
    Sep  1 11:46:30.753: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:46:30.757: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 11:46:30.757: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:46:31.762: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:46:31.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 11:46:31.766: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:46:32.762: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:46:32.765: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  1 11:46:32.766: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/01/23 11:46:32.78
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8935, will wait for the garbage collector to delete the pods 09/01/23 11:46:32.781
    Sep  1 11:46:32.840: INFO: Deleting DaemonSet.extensions daemon-set took: 6.119103ms
    Sep  1 11:46:32.941: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.66096ms
    Sep  1 11:46:35.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 11:46:35.546: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  1 11:46:35.549: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48988"},"items":null}

    Sep  1 11:46:35.552: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48988"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:46:35.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8935" for this suite. 09/01/23 11:46:35.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:46:35.581
Sep  1 11:46:35.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 11:46:35.582
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:35.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:35.605
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 09/01/23 11:46:35.609
Sep  1 11:46:35.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 create -f -'
Sep  1 11:46:38.928: INFO: stderr: ""
Sep  1 11:46:38.928: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/01/23 11:46:38.928
Sep  1 11:46:38.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  1 11:46:39.047: INFO: stderr: ""
Sep  1 11:46:39.047: INFO: stdout: "update-demo-nautilus-jkxf6 update-demo-nautilus-k2wkz "
Sep  1 11:46:39.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods update-demo-nautilus-jkxf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  1 11:46:39.148: INFO: stderr: ""
Sep  1 11:46:39.148: INFO: stdout: ""
Sep  1 11:46:39.148: INFO: update-demo-nautilus-jkxf6 is created but not running
Sep  1 11:46:44.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  1 11:46:44.244: INFO: stderr: ""
Sep  1 11:46:44.244: INFO: stdout: "update-demo-nautilus-jkxf6 update-demo-nautilus-k2wkz "
Sep  1 11:46:44.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods update-demo-nautilus-jkxf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  1 11:46:44.338: INFO: stderr: ""
Sep  1 11:46:44.338: INFO: stdout: "true"
Sep  1 11:46:44.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods update-demo-nautilus-jkxf6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  1 11:46:44.432: INFO: stderr: ""
Sep  1 11:46:44.432: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  1 11:46:44.432: INFO: validating pod update-demo-nautilus-jkxf6
Sep  1 11:46:44.438: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  1 11:46:44.439: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  1 11:46:44.439: INFO: update-demo-nautilus-jkxf6 is verified up and running
Sep  1 11:46:44.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods update-demo-nautilus-k2wkz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  1 11:46:44.533: INFO: stderr: ""
Sep  1 11:46:44.533: INFO: stdout: "true"
Sep  1 11:46:44.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods update-demo-nautilus-k2wkz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  1 11:46:44.627: INFO: stderr: ""
Sep  1 11:46:44.627: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  1 11:46:44.627: INFO: validating pod update-demo-nautilus-k2wkz
Sep  1 11:46:44.634: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  1 11:46:44.634: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  1 11:46:44.634: INFO: update-demo-nautilus-k2wkz is verified up and running
STEP: using delete to clean up resources 09/01/23 11:46:44.634
Sep  1 11:46:44.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 delete --grace-period=0 --force -f -'
Sep  1 11:46:44.735: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  1 11:46:44.735: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  1 11:46:44.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get rc,svc -l name=update-demo --no-headers'
Sep  1 11:46:44.906: INFO: stderr: "No resources found in kubectl-9369 namespace.\n"
Sep  1 11:46:44.906: INFO: stdout: ""
Sep  1 11:46:44.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  1 11:46:45.089: INFO: stderr: ""
Sep  1 11:46:45.089: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 11:46:45.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9369" for this suite. 09/01/23 11:46:45.098
------------------------------
• [SLOW TEST] [9.524 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:46:35.581
    Sep  1 11:46:35.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 11:46:35.582
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:35.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:35.605
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 09/01/23 11:46:35.609
    Sep  1 11:46:35.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 create -f -'
    Sep  1 11:46:38.928: INFO: stderr: ""
    Sep  1 11:46:38.928: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/01/23 11:46:38.928
    Sep  1 11:46:38.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  1 11:46:39.047: INFO: stderr: ""
    Sep  1 11:46:39.047: INFO: stdout: "update-demo-nautilus-jkxf6 update-demo-nautilus-k2wkz "
    Sep  1 11:46:39.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods update-demo-nautilus-jkxf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  1 11:46:39.148: INFO: stderr: ""
    Sep  1 11:46:39.148: INFO: stdout: ""
    Sep  1 11:46:39.148: INFO: update-demo-nautilus-jkxf6 is created but not running
    Sep  1 11:46:44.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  1 11:46:44.244: INFO: stderr: ""
    Sep  1 11:46:44.244: INFO: stdout: "update-demo-nautilus-jkxf6 update-demo-nautilus-k2wkz "
    Sep  1 11:46:44.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods update-demo-nautilus-jkxf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  1 11:46:44.338: INFO: stderr: ""
    Sep  1 11:46:44.338: INFO: stdout: "true"
    Sep  1 11:46:44.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods update-demo-nautilus-jkxf6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  1 11:46:44.432: INFO: stderr: ""
    Sep  1 11:46:44.432: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  1 11:46:44.432: INFO: validating pod update-demo-nautilus-jkxf6
    Sep  1 11:46:44.438: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  1 11:46:44.439: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  1 11:46:44.439: INFO: update-demo-nautilus-jkxf6 is verified up and running
    Sep  1 11:46:44.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods update-demo-nautilus-k2wkz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  1 11:46:44.533: INFO: stderr: ""
    Sep  1 11:46:44.533: INFO: stdout: "true"
    Sep  1 11:46:44.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods update-demo-nautilus-k2wkz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  1 11:46:44.627: INFO: stderr: ""
    Sep  1 11:46:44.627: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  1 11:46:44.627: INFO: validating pod update-demo-nautilus-k2wkz
    Sep  1 11:46:44.634: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  1 11:46:44.634: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  1 11:46:44.634: INFO: update-demo-nautilus-k2wkz is verified up and running
    STEP: using delete to clean up resources 09/01/23 11:46:44.634
    Sep  1 11:46:44.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 delete --grace-period=0 --force -f -'
    Sep  1 11:46:44.735: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  1 11:46:44.735: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep  1 11:46:44.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get rc,svc -l name=update-demo --no-headers'
    Sep  1 11:46:44.906: INFO: stderr: "No resources found in kubectl-9369 namespace.\n"
    Sep  1 11:46:44.906: INFO: stdout: ""
    Sep  1 11:46:44.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-9369 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  1 11:46:45.089: INFO: stderr: ""
    Sep  1 11:46:45.089: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:46:45.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9369" for this suite. 09/01/23 11:46:45.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:46:45.107
Sep  1 11:46:45.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename gc 09/01/23 11:46:45.108
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:45.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:45.13
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 09/01/23 11:46:45.134
STEP: Wait for the Deployment to create new ReplicaSet 09/01/23 11:46:45.14
STEP: delete the deployment 09/01/23 11:46:45.661
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/01/23 11:46:45.67
STEP: Gathering metrics 09/01/23 11:46:46.191
Sep  1 11:46:46.224: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
Sep  1 11:46:46.227: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.619017ms
Sep  1 11:46:46.228: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
Sep  1 11:46:46.228: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
Sep  1 11:46:46.608: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  1 11:46:46.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5727" for this suite. 09/01/23 11:46:46.613
------------------------------
• [1.513 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:46:45.107
    Sep  1 11:46:45.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename gc 09/01/23 11:46:45.108
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:45.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:45.13
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 09/01/23 11:46:45.134
    STEP: Wait for the Deployment to create new ReplicaSet 09/01/23 11:46:45.14
    STEP: delete the deployment 09/01/23 11:46:45.661
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/01/23 11:46:45.67
    STEP: Gathering metrics 09/01/23 11:46:46.191
    Sep  1 11:46:46.224: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
    Sep  1 11:46:46.227: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.619017ms
    Sep  1 11:46:46.228: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
    Sep  1 11:46:46.228: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
    Sep  1 11:46:46.608: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:46:46.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5727" for this suite. 09/01/23 11:46:46.613
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:46:46.622
Sep  1 11:46:46.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename podtemplate 09/01/23 11:46:46.624
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:46.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:46.645
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 09/01/23 11:46:46.648
Sep  1 11:46:46.652: INFO: created test-podtemplate-1
Sep  1 11:46:46.657: INFO: created test-podtemplate-2
Sep  1 11:46:46.664: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 09/01/23 11:46:46.664
STEP: delete collection of pod templates 09/01/23 11:46:46.668
Sep  1 11:46:46.668: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 09/01/23 11:46:46.685
Sep  1 11:46:46.685: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  1 11:46:46.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-2271" for this suite. 09/01/23 11:46:46.694
------------------------------
• [0.078 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:46:46.622
    Sep  1 11:46:46.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename podtemplate 09/01/23 11:46:46.624
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:46.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:46.645
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 09/01/23 11:46:46.648
    Sep  1 11:46:46.652: INFO: created test-podtemplate-1
    Sep  1 11:46:46.657: INFO: created test-podtemplate-2
    Sep  1 11:46:46.664: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 09/01/23 11:46:46.664
    STEP: delete collection of pod templates 09/01/23 11:46:46.668
    Sep  1 11:46:46.668: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 09/01/23 11:46:46.685
    Sep  1 11:46:46.685: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:46:46.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-2271" for this suite. 09/01/23 11:46:46.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:46:46.714
Sep  1 11:46:46.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename sysctl 09/01/23 11:46:46.717
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:46.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:46.739
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/01/23 11:46:46.743
STEP: Watching for error events or started pod 09/01/23 11:46:46.75
STEP: Waiting for pod completion 09/01/23 11:46:48.756
Sep  1 11:46:48.756: INFO: Waiting up to 3m0s for pod "sysctl-ef31f9ca-cbb9-4480-93d1-6b425a26941d" in namespace "sysctl-5380" to be "completed"
Sep  1 11:46:48.760: INFO: Pod "sysctl-ef31f9ca-cbb9-4480-93d1-6b425a26941d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.681454ms
Sep  1 11:46:50.764: INFO: Pod "sysctl-ef31f9ca-cbb9-4480-93d1-6b425a26941d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007734162s
Sep  1 11:46:50.764: INFO: Pod "sysctl-ef31f9ca-cbb9-4480-93d1-6b425a26941d" satisfied condition "completed"
STEP: Checking that the pod succeeded 09/01/23 11:46:50.769
STEP: Getting logs from the pod 09/01/23 11:46:50.769
STEP: Checking that the sysctl is actually updated 09/01/23 11:46:50.788
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:46:50.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-5380" for this suite. 09/01/23 11:46:50.806
------------------------------
• [4.104 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:46:46.714
    Sep  1 11:46:46.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename sysctl 09/01/23 11:46:46.717
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:46.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:46.739
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/01/23 11:46:46.743
    STEP: Watching for error events or started pod 09/01/23 11:46:46.75
    STEP: Waiting for pod completion 09/01/23 11:46:48.756
    Sep  1 11:46:48.756: INFO: Waiting up to 3m0s for pod "sysctl-ef31f9ca-cbb9-4480-93d1-6b425a26941d" in namespace "sysctl-5380" to be "completed"
    Sep  1 11:46:48.760: INFO: Pod "sysctl-ef31f9ca-cbb9-4480-93d1-6b425a26941d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.681454ms
    Sep  1 11:46:50.764: INFO: Pod "sysctl-ef31f9ca-cbb9-4480-93d1-6b425a26941d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007734162s
    Sep  1 11:46:50.764: INFO: Pod "sysctl-ef31f9ca-cbb9-4480-93d1-6b425a26941d" satisfied condition "completed"
    STEP: Checking that the pod succeeded 09/01/23 11:46:50.769
    STEP: Getting logs from the pod 09/01/23 11:46:50.769
    STEP: Checking that the sysctl is actually updated 09/01/23 11:46:50.788
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:46:50.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-5380" for this suite. 09/01/23 11:46:50.806
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:46:50.829
Sep  1 11:46:50.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 11:46:50.834
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:50.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:51.014
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 11:46:51.106
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:46:52.029
STEP: Deploying the webhook pod 09/01/23 11:46:52.049
STEP: Wait for the deployment to be ready 09/01/23 11:46:52.069
Sep  1 11:46:52.089: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  1 11:46:54.102: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 46, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 46, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 46, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 46, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/01/23 11:46:56.107
STEP: Verifying the service has paired with the endpoint 09/01/23 11:46:56.124
Sep  1 11:46:57.127: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 09/01/23 11:46:57.131
STEP: Creating a configMap that does not comply to the validation webhook rules 09/01/23 11:46:57.187
STEP: Updating a validating webhook configuration's rules to not include the create operation 09/01/23 11:46:57.211
STEP: Creating a configMap that does not comply to the validation webhook rules 09/01/23 11:46:57.225
STEP: Patching a validating webhook configuration's rules to include the create operation 09/01/23 11:46:57.239
STEP: Creating a configMap that does not comply to the validation webhook rules 09/01/23 11:46:57.249
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:46:57.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-56" for this suite. 09/01/23 11:46:57.358
STEP: Destroying namespace "webhook-56-markers" for this suite. 09/01/23 11:46:57.372
------------------------------
• [SLOW TEST] [6.559 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:46:50.829
    Sep  1 11:46:50.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 11:46:50.834
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:50.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:51.014
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 11:46:51.106
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:46:52.029
    STEP: Deploying the webhook pod 09/01/23 11:46:52.049
    STEP: Wait for the deployment to be ready 09/01/23 11:46:52.069
    Sep  1 11:46:52.089: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  1 11:46:54.102: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 46, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 46, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 46, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 46, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/01/23 11:46:56.107
    STEP: Verifying the service has paired with the endpoint 09/01/23 11:46:56.124
    Sep  1 11:46:57.127: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 09/01/23 11:46:57.131
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/01/23 11:46:57.187
    STEP: Updating a validating webhook configuration's rules to not include the create operation 09/01/23 11:46:57.211
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/01/23 11:46:57.225
    STEP: Patching a validating webhook configuration's rules to include the create operation 09/01/23 11:46:57.239
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/01/23 11:46:57.249
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:46:57.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-56" for this suite. 09/01/23 11:46:57.358
    STEP: Destroying namespace "webhook-56-markers" for this suite. 09/01/23 11:46:57.372
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:46:57.435
Sep  1 11:46:57.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 11:46:57.441
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:57.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:57.485
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 11:46:57.519
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:46:58.488
STEP: Deploying the webhook pod 09/01/23 11:46:58.5
STEP: Wait for the deployment to be ready 09/01/23 11:46:58.523
Sep  1 11:46:58.552: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  1 11:47:00.577: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 46, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 46, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 46, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 46, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/01/23 11:47:02.583
STEP: Verifying the service has paired with the endpoint 09/01/23 11:47:02.6
Sep  1 11:47:03.603: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Sep  1 11:47:03.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7197-crds.webhook.example.com via the AdmissionRegistration API 09/01/23 11:47:04.132
STEP: Creating a custom resource that should be mutated by the webhook 09/01/23 11:47:04.174
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:47:06.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7504" for this suite. 09/01/23 11:47:07.003
STEP: Destroying namespace "webhook-7504-markers" for this suite. 09/01/23 11:47:07.02
------------------------------
• [SLOW TEST] [9.628 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:46:57.435
    Sep  1 11:46:57.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 11:46:57.441
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:46:57.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:46:57.485
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 11:46:57.519
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 11:46:58.488
    STEP: Deploying the webhook pod 09/01/23 11:46:58.5
    STEP: Wait for the deployment to be ready 09/01/23 11:46:58.523
    Sep  1 11:46:58.552: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  1 11:47:00.577: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 1, 11, 46, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 46, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 1, 11, 46, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 1, 11, 46, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/01/23 11:47:02.583
    STEP: Verifying the service has paired with the endpoint 09/01/23 11:47:02.6
    Sep  1 11:47:03.603: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Sep  1 11:47:03.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7197-crds.webhook.example.com via the AdmissionRegistration API 09/01/23 11:47:04.132
    STEP: Creating a custom resource that should be mutated by the webhook 09/01/23 11:47:04.174
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:47:06.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7504" for this suite. 09/01/23 11:47:07.003
    STEP: Destroying namespace "webhook-7504-markers" for this suite. 09/01/23 11:47:07.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:47:07.074
Sep  1 11:47:07.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 11:47:07.076
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:47:07.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:47:07.129
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-4949 09/01/23 11:47:07.141
STEP: creating service affinity-nodeport in namespace services-4949 09/01/23 11:47:07.141
STEP: creating replication controller affinity-nodeport in namespace services-4949 09/01/23 11:47:07.164
I0901 11:47:07.189775      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4949, replica count: 3
I0901 11:47:10.241703      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  1 11:47:10.251: INFO: Creating new exec pod
Sep  1 11:47:10.256: INFO: Waiting up to 5m0s for pod "execpod-affinityr26wd" in namespace "services-4949" to be "running"
Sep  1 11:47:10.259: INFO: Pod "execpod-affinityr26wd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119164ms
Sep  1 11:47:12.263: INFO: Pod "execpod-affinityr26wd": Phase="Running", Reason="", readiness=true. Elapsed: 2.00695047s
Sep  1 11:47:12.263: INFO: Pod "execpod-affinityr26wd" satisfied condition "running"
Sep  1 11:47:13.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-4949 exec execpod-affinityr26wd -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Sep  1 11:47:13.585: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep  1 11:47:13.585: INFO: stdout: ""
Sep  1 11:47:13.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-4949 exec execpod-affinityr26wd -- /bin/sh -x -c nc -v -z -w 2 10.99.8.85 80'
Sep  1 11:47:13.790: INFO: stderr: "+ nc -v -z -w 2 10.99.8.85 80\nConnection to 10.99.8.85 80 port [tcp/http] succeeded!\n"
Sep  1 11:47:13.790: INFO: stdout: ""
Sep  1 11:47:13.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-4949 exec execpod-affinityr26wd -- /bin/sh -x -c nc -v -z -w 2 172.16.0.3 32173'
Sep  1 11:47:13.995: INFO: stderr: "+ nc -v -z -w 2 172.16.0.3 32173\nConnection to 172.16.0.3 32173 port [tcp/*] succeeded!\n"
Sep  1 11:47:13.995: INFO: stdout: ""
Sep  1 11:47:13.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-4949 exec execpod-affinityr26wd -- /bin/sh -x -c nc -v -z -w 2 172.16.0.4 32173'
Sep  1 11:47:14.190: INFO: stderr: "+ nc -v -z -w 2 172.16.0.4 32173\nConnection to 172.16.0.4 32173 port [tcp/*] succeeded!\n"
Sep  1 11:47:14.190: INFO: stdout: ""
Sep  1 11:47:14.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-4949 exec execpod-affinityr26wd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.3:32173/ ; done'
Sep  1 11:47:14.797: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n"
Sep  1 11:47:14.797: INFO: stdout: "\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn"
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
Sep  1 11:47:14.797: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4949, will wait for the garbage collector to delete the pods 09/01/23 11:47:14.811
Sep  1 11:47:14.873: INFO: Deleting ReplicationController affinity-nodeport took: 5.745427ms
Sep  1 11:47:14.975: INFO: Terminating ReplicationController affinity-nodeport pods took: 102.445787ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 11:47:16.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4949" for this suite. 09/01/23 11:47:16.911
------------------------------
• [SLOW TEST] [9.844 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:47:07.074
    Sep  1 11:47:07.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 11:47:07.076
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:47:07.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:47:07.129
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-4949 09/01/23 11:47:07.141
    STEP: creating service affinity-nodeport in namespace services-4949 09/01/23 11:47:07.141
    STEP: creating replication controller affinity-nodeport in namespace services-4949 09/01/23 11:47:07.164
    I0901 11:47:07.189775      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4949, replica count: 3
    I0901 11:47:10.241703      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  1 11:47:10.251: INFO: Creating new exec pod
    Sep  1 11:47:10.256: INFO: Waiting up to 5m0s for pod "execpod-affinityr26wd" in namespace "services-4949" to be "running"
    Sep  1 11:47:10.259: INFO: Pod "execpod-affinityr26wd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119164ms
    Sep  1 11:47:12.263: INFO: Pod "execpod-affinityr26wd": Phase="Running", Reason="", readiness=true. Elapsed: 2.00695047s
    Sep  1 11:47:12.263: INFO: Pod "execpod-affinityr26wd" satisfied condition "running"
    Sep  1 11:47:13.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-4949 exec execpod-affinityr26wd -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Sep  1 11:47:13.585: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Sep  1 11:47:13.585: INFO: stdout: ""
    Sep  1 11:47:13.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-4949 exec execpod-affinityr26wd -- /bin/sh -x -c nc -v -z -w 2 10.99.8.85 80'
    Sep  1 11:47:13.790: INFO: stderr: "+ nc -v -z -w 2 10.99.8.85 80\nConnection to 10.99.8.85 80 port [tcp/http] succeeded!\n"
    Sep  1 11:47:13.790: INFO: stdout: ""
    Sep  1 11:47:13.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-4949 exec execpod-affinityr26wd -- /bin/sh -x -c nc -v -z -w 2 172.16.0.3 32173'
    Sep  1 11:47:13.995: INFO: stderr: "+ nc -v -z -w 2 172.16.0.3 32173\nConnection to 172.16.0.3 32173 port [tcp/*] succeeded!\n"
    Sep  1 11:47:13.995: INFO: stdout: ""
    Sep  1 11:47:13.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-4949 exec execpod-affinityr26wd -- /bin/sh -x -c nc -v -z -w 2 172.16.0.4 32173'
    Sep  1 11:47:14.190: INFO: stderr: "+ nc -v -z -w 2 172.16.0.4 32173\nConnection to 172.16.0.4 32173 port [tcp/*] succeeded!\n"
    Sep  1 11:47:14.190: INFO: stdout: ""
    Sep  1 11:47:14.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-4949 exec execpod-affinityr26wd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.3:32173/ ; done'
    Sep  1 11:47:14.797: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.3:32173/\n"
    Sep  1 11:47:14.797: INFO: stdout: "\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn\naffinity-nodeport-4zmmn"
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Received response from host: affinity-nodeport-4zmmn
    Sep  1 11:47:14.797: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-4949, will wait for the garbage collector to delete the pods 09/01/23 11:47:14.811
    Sep  1 11:47:14.873: INFO: Deleting ReplicationController affinity-nodeport took: 5.745427ms
    Sep  1 11:47:14.975: INFO: Terminating ReplicationController affinity-nodeport pods took: 102.445787ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:47:16.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4949" for this suite. 09/01/23 11:47:16.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:47:16.922
Sep  1 11:47:16.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 11:47:16.925
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:47:16.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:47:16.959
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 09/01/23 11:47:16.966
Sep  1 11:47:16.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: mark a version not serverd 09/01/23 11:47:26.125
STEP: check the unserved version gets removed 09/01/23 11:47:26.167
STEP: check the other version is not changed 09/01/23 11:47:31.245
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:47:38.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4615" for this suite. 09/01/23 11:47:38.308
------------------------------
• [SLOW TEST] [21.393 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:47:16.922
    Sep  1 11:47:16.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename crd-publish-openapi 09/01/23 11:47:16.925
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:47:16.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:47:16.959
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 09/01/23 11:47:16.966
    Sep  1 11:47:16.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: mark a version not serverd 09/01/23 11:47:26.125
    STEP: check the unserved version gets removed 09/01/23 11:47:26.167
    STEP: check the other version is not changed 09/01/23 11:47:31.245
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:47:38.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4615" for this suite. 09/01/23 11:47:38.308
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:47:38.316
Sep  1 11:47:38.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename resourcequota 09/01/23 11:47:38.32
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:47:38.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:47:38.345
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 09/01/23 11:47:38.349
STEP: Creating a ResourceQuota 09/01/23 11:47:43.354
STEP: Ensuring resource quota status is calculated 09/01/23 11:47:43.361
STEP: Creating a Service 09/01/23 11:47:45.365
STEP: Creating a NodePort Service 09/01/23 11:47:45.392
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/01/23 11:47:45.439
STEP: Ensuring resource quota status captures service creation 09/01/23 11:47:45.507
STEP: Deleting Services 09/01/23 11:47:47.512
STEP: Ensuring resource quota status released usage 09/01/23 11:47:47.565
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  1 11:47:49.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8508" for this suite. 09/01/23 11:47:49.575
------------------------------
• [SLOW TEST] [11.268 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:47:38.316
    Sep  1 11:47:38.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename resourcequota 09/01/23 11:47:38.32
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:47:38.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:47:38.345
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 09/01/23 11:47:38.349
    STEP: Creating a ResourceQuota 09/01/23 11:47:43.354
    STEP: Ensuring resource quota status is calculated 09/01/23 11:47:43.361
    STEP: Creating a Service 09/01/23 11:47:45.365
    STEP: Creating a NodePort Service 09/01/23 11:47:45.392
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/01/23 11:47:45.439
    STEP: Ensuring resource quota status captures service creation 09/01/23 11:47:45.507
    STEP: Deleting Services 09/01/23 11:47:47.512
    STEP: Ensuring resource quota status released usage 09/01/23 11:47:47.565
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:47:49.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8508" for this suite. 09/01/23 11:47:49.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:47:49.587
Sep  1 11:47:49.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename daemonsets 09/01/23 11:47:49.589
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:47:49.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:47:49.625
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
STEP: Creating simple DaemonSet "daemon-set" 09/01/23 11:47:49.644
STEP: Check that daemon pods launch on every node of the cluster. 09/01/23 11:47:49.65
Sep  1 11:47:49.655: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:47:49.659: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 11:47:49.659: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:47:50.664: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:47:50.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 11:47:50.669: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:47:51.664: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:47:51.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  1 11:47:51.669: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 09/01/23 11:47:51.675
Sep  1 11:47:51.704: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:47:51.707: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 11:47:51.707: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:47:52.713: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:47:52.716: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 11:47:52.716: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:47:53.787: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:47:53.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 11:47:53.870: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:47:54.798: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:47:54.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 11:47:54.864: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:47:55.712: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:47:55.718: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 11:47:55.718: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:47:56.712: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:47:56.715: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 11:47:56.716: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 11:47:57.713: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 11:47:57.725: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  1 11:47:57.725: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/01/23 11:47:57.728
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2575, will wait for the garbage collector to delete the pods 09/01/23 11:47:57.729
Sep  1 11:47:57.788: INFO: Deleting DaemonSet.extensions daemon-set took: 6.324053ms
Sep  1 11:47:57.890: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.206187ms
Sep  1 11:47:59.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 11:47:59.894: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  1 11:47:59.897: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"50020"},"items":null}

Sep  1 11:47:59.900: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"50020"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:47:59.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2575" for this suite. 09/01/23 11:47:59.913
------------------------------
• [SLOW TEST] [10.332 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:47:49.587
    Sep  1 11:47:49.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename daemonsets 09/01/23 11:47:49.589
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:47:49.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:47:49.625
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:177
    STEP: Creating simple DaemonSet "daemon-set" 09/01/23 11:47:49.644
    STEP: Check that daemon pods launch on every node of the cluster. 09/01/23 11:47:49.65
    Sep  1 11:47:49.655: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:47:49.659: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 11:47:49.659: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:47:50.664: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:47:50.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 11:47:50.669: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:47:51.664: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:47:51.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  1 11:47:51.669: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 09/01/23 11:47:51.675
    Sep  1 11:47:51.704: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:47:51.707: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 11:47:51.707: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:47:52.713: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:47:52.716: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 11:47:52.716: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:47:53.787: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:47:53.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 11:47:53.870: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:47:54.798: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:47:54.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 11:47:54.864: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:47:55.712: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:47:55.718: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 11:47:55.718: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:47:56.712: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:47:56.715: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 11:47:56.716: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 11:47:57.713: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 11:47:57.725: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  1 11:47:57.725: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/01/23 11:47:57.728
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2575, will wait for the garbage collector to delete the pods 09/01/23 11:47:57.729
    Sep  1 11:47:57.788: INFO: Deleting DaemonSet.extensions daemon-set took: 6.324053ms
    Sep  1 11:47:57.890: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.206187ms
    Sep  1 11:47:59.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 11:47:59.894: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  1 11:47:59.897: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"50020"},"items":null}

    Sep  1 11:47:59.900: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"50020"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:47:59.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2575" for this suite. 09/01/23 11:47:59.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:47:59.925
Sep  1 11:47:59.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename resourcequota 09/01/23 11:47:59.927
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:47:59.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:47:59.948
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 09/01/23 11:47:59.951
STEP: Ensuring ResourceQuota status is calculated 09/01/23 11:47:59.957
STEP: Creating a ResourceQuota with not best effort scope 09/01/23 11:48:01.962
STEP: Ensuring ResourceQuota status is calculated 09/01/23 11:48:01.968
STEP: Creating a best-effort pod 09/01/23 11:48:03.972
STEP: Ensuring resource quota with best effort scope captures the pod usage 09/01/23 11:48:03.987
STEP: Ensuring resource quota with not best effort ignored the pod usage 09/01/23 11:48:05.99
STEP: Deleting the pod 09/01/23 11:48:07.994
STEP: Ensuring resource quota status released the pod usage 09/01/23 11:48:08.007
STEP: Creating a not best-effort pod 09/01/23 11:48:10.011
STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/01/23 11:48:10.02
STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/01/23 11:48:12.025
STEP: Deleting the pod 09/01/23 11:48:14.029
STEP: Ensuring resource quota status released the pod usage 09/01/23 11:48:14.04
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  1 11:48:16.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2598" for this suite. 09/01/23 11:48:16.048
------------------------------
• [SLOW TEST] [16.129 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:47:59.925
    Sep  1 11:47:59.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename resourcequota 09/01/23 11:47:59.927
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:47:59.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:47:59.948
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 09/01/23 11:47:59.951
    STEP: Ensuring ResourceQuota status is calculated 09/01/23 11:47:59.957
    STEP: Creating a ResourceQuota with not best effort scope 09/01/23 11:48:01.962
    STEP: Ensuring ResourceQuota status is calculated 09/01/23 11:48:01.968
    STEP: Creating a best-effort pod 09/01/23 11:48:03.972
    STEP: Ensuring resource quota with best effort scope captures the pod usage 09/01/23 11:48:03.987
    STEP: Ensuring resource quota with not best effort ignored the pod usage 09/01/23 11:48:05.99
    STEP: Deleting the pod 09/01/23 11:48:07.994
    STEP: Ensuring resource quota status released the pod usage 09/01/23 11:48:08.007
    STEP: Creating a not best-effort pod 09/01/23 11:48:10.011
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/01/23 11:48:10.02
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/01/23 11:48:12.025
    STEP: Deleting the pod 09/01/23 11:48:14.029
    STEP: Ensuring resource quota status released the pod usage 09/01/23 11:48:14.04
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:48:16.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2598" for this suite. 09/01/23 11:48:16.048
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:48:16.059
Sep  1 11:48:16.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename ephemeral-containers-test 09/01/23 11:48:16.061
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:48:16.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:48:16.079
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 09/01/23 11:48:16.083
Sep  1 11:48:16.091: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4760" to be "running and ready"
Sep  1 11:48:16.099: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.965974ms
Sep  1 11:48:16.099: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:48:18.102: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011220597s
Sep  1 11:48:18.102: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Sep  1 11:48:18.102: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 09/01/23 11:48:18.105
Sep  1 11:48:18.142: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4760" to be "container debugger running"
Sep  1 11:48:18.146: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.564512ms
Sep  1 11:48:20.151: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008252257s
Sep  1 11:48:22.152: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009143547s
Sep  1 11:48:22.152: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 09/01/23 11:48:22.152
Sep  1 11:48:22.152: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4760 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  1 11:48:22.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
Sep  1 11:48:22.154: INFO: ExecWithOptions: Clientset creation
Sep  1 11:48:22.154: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4760/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Sep  1 11:48:22.258: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:48:22.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-4760" for this suite. 09/01/23 11:48:22.28
------------------------------
• [SLOW TEST] [6.226 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:48:16.059
    Sep  1 11:48:16.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename ephemeral-containers-test 09/01/23 11:48:16.061
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:48:16.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:48:16.079
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 09/01/23 11:48:16.083
    Sep  1 11:48:16.091: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4760" to be "running and ready"
    Sep  1 11:48:16.099: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.965974ms
    Sep  1 11:48:16.099: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:48:18.102: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011220597s
    Sep  1 11:48:18.102: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Sep  1 11:48:18.102: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 09/01/23 11:48:18.105
    Sep  1 11:48:18.142: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4760" to be "container debugger running"
    Sep  1 11:48:18.146: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.564512ms
    Sep  1 11:48:20.151: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008252257s
    Sep  1 11:48:22.152: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009143547s
    Sep  1 11:48:22.152: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 09/01/23 11:48:22.152
    Sep  1 11:48:22.152: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4760 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  1 11:48:22.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    Sep  1 11:48:22.154: INFO: ExecWithOptions: Clientset creation
    Sep  1 11:48:22.154: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4760/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Sep  1 11:48:22.258: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:48:22.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-4760" for this suite. 09/01/23 11:48:22.28
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:48:22.286
Sep  1 11:48:22.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:48:22.288
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:48:22.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:48:22.309
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-ae38bb79-1fbc-4e62-b3af-81c97db387da 09/01/23 11:48:22.312
STEP: Creating a pod to test consume secrets 09/01/23 11:48:22.318
Sep  1 11:48:22.326: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7" in namespace "projected-3321" to be "Succeeded or Failed"
Sep  1 11:48:22.332: INFO: Pod "pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.116791ms
Sep  1 11:48:24.336: INFO: Pod "pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009148524s
Sep  1 11:48:26.335: INFO: Pod "pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008918263s
STEP: Saw pod success 09/01/23 11:48:26.335
Sep  1 11:48:26.336: INFO: Pod "pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7" satisfied condition "Succeeded or Failed"
Sep  1 11:48:26.339: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/01/23 11:48:26.345
Sep  1 11:48:26.356: INFO: Waiting for pod pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7 to disappear
Sep  1 11:48:26.359: INFO: Pod pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  1 11:48:26.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3321" for this suite. 09/01/23 11:48:26.364
------------------------------
• [4.084 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:48:22.286
    Sep  1 11:48:22.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:48:22.288
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:48:22.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:48:22.309
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-ae38bb79-1fbc-4e62-b3af-81c97db387da 09/01/23 11:48:22.312
    STEP: Creating a pod to test consume secrets 09/01/23 11:48:22.318
    Sep  1 11:48:22.326: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7" in namespace "projected-3321" to be "Succeeded or Failed"
    Sep  1 11:48:22.332: INFO: Pod "pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.116791ms
    Sep  1 11:48:24.336: INFO: Pod "pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009148524s
    Sep  1 11:48:26.335: INFO: Pod "pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008918263s
    STEP: Saw pod success 09/01/23 11:48:26.335
    Sep  1 11:48:26.336: INFO: Pod "pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7" satisfied condition "Succeeded or Failed"
    Sep  1 11:48:26.339: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/01/23 11:48:26.345
    Sep  1 11:48:26.356: INFO: Waiting for pod pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7 to disappear
    Sep  1 11:48:26.359: INFO: Pod pod-projected-secrets-4cb7a6d8-61a5-41a3-9c32-4ef6a18331a7 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:48:26.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3321" for this suite. 09/01/23 11:48:26.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:48:26.38
Sep  1 11:48:26.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 11:48:26.381
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:48:26.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:48:26.399
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/01/23 11:48:26.404
Sep  1 11:48:26.411: INFO: Waiting up to 5m0s for pod "pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2" in namespace "emptydir-1783" to be "Succeeded or Failed"
Sep  1 11:48:26.415: INFO: Pod "pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.326694ms
Sep  1 11:48:28.419: INFO: Pod "pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006868127s
Sep  1 11:48:30.419: INFO: Pod "pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007232911s
STEP: Saw pod success 09/01/23 11:48:30.419
Sep  1 11:48:30.419: INFO: Pod "pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2" satisfied condition "Succeeded or Failed"
Sep  1 11:48:30.422: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2 container test-container: <nil>
STEP: delete the pod 09/01/23 11:48:30.428
Sep  1 11:48:30.437: INFO: Waiting for pod pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2 to disappear
Sep  1 11:48:30.439: INFO: Pod pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 11:48:30.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1783" for this suite. 09/01/23 11:48:30.443
------------------------------
• [4.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:48:26.38
    Sep  1 11:48:26.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 11:48:26.381
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:48:26.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:48:26.399
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/01/23 11:48:26.404
    Sep  1 11:48:26.411: INFO: Waiting up to 5m0s for pod "pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2" in namespace "emptydir-1783" to be "Succeeded or Failed"
    Sep  1 11:48:26.415: INFO: Pod "pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.326694ms
    Sep  1 11:48:28.419: INFO: Pod "pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006868127s
    Sep  1 11:48:30.419: INFO: Pod "pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007232911s
    STEP: Saw pod success 09/01/23 11:48:30.419
    Sep  1 11:48:30.419: INFO: Pod "pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2" satisfied condition "Succeeded or Failed"
    Sep  1 11:48:30.422: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2 container test-container: <nil>
    STEP: delete the pod 09/01/23 11:48:30.428
    Sep  1 11:48:30.437: INFO: Waiting for pod pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2 to disappear
    Sep  1 11:48:30.439: INFO: Pod pod-b2cd1fbb-c834-4ad0-a296-175f5d671bb2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:48:30.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1783" for this suite. 09/01/23 11:48:30.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:48:30.449
Sep  1 11:48:30.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename svc-latency 09/01/23 11:48:30.451
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:48:30.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:48:30.469
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Sep  1 11:48:30.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7813 09/01/23 11:48:30.593
I0901 11:48:30.602479      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7813, replica count: 1
I0901 11:48:31.654073      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0901 11:48:32.654406      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  1 11:48:32.882: INFO: Created: latency-svc-d9ddk
Sep  1 11:48:32.900: INFO: Got endpoints: latency-svc-d9ddk [97.640455ms]
Sep  1 11:48:32.926: INFO: Created: latency-svc-4kwzk
Sep  1 11:48:32.942: INFO: Got endpoints: latency-svc-4kwzk [41.332318ms]
Sep  1 11:48:32.950: INFO: Created: latency-svc-ghg27
Sep  1 11:48:32.961: INFO: Got endpoints: latency-svc-ghg27 [59.146858ms]
Sep  1 11:48:32.994: INFO: Created: latency-svc-xncvt
Sep  1 11:48:33.005: INFO: Got endpoints: latency-svc-xncvt [102.864374ms]
Sep  1 11:48:33.016: INFO: Created: latency-svc-wmlcx
Sep  1 11:48:33.022: INFO: Got endpoints: latency-svc-wmlcx [119.564088ms]
Sep  1 11:48:33.065: INFO: Created: latency-svc-j7ssm
Sep  1 11:48:33.074: INFO: Got endpoints: latency-svc-j7ssm [171.36969ms]
Sep  1 11:48:33.083: INFO: Created: latency-svc-6kh79
Sep  1 11:48:33.095: INFO: Got endpoints: latency-svc-6kh79 [192.585101ms]
Sep  1 11:48:33.116: INFO: Created: latency-svc-t8mlq
Sep  1 11:48:33.126: INFO: Got endpoints: latency-svc-t8mlq [223.273317ms]
Sep  1 11:48:33.136: INFO: Created: latency-svc-cdjbp
Sep  1 11:48:33.149: INFO: Got endpoints: latency-svc-cdjbp [246.123996ms]
Sep  1 11:48:33.153: INFO: Created: latency-svc-w9rq4
Sep  1 11:48:33.194: INFO: Got endpoints: latency-svc-w9rq4 [290.812774ms]
Sep  1 11:48:33.212: INFO: Created: latency-svc-9n6lf
Sep  1 11:48:33.223: INFO: Got endpoints: latency-svc-9n6lf [319.802939ms]
Sep  1 11:48:33.243: INFO: Created: latency-svc-6f9j2
Sep  1 11:48:33.253: INFO: Got endpoints: latency-svc-6f9j2 [349.588621ms]
Sep  1 11:48:33.260: INFO: Created: latency-svc-kq9bx
Sep  1 11:48:33.272: INFO: Got endpoints: latency-svc-kq9bx [368.979794ms]
Sep  1 11:48:33.281: INFO: Created: latency-svc-kvb6m
Sep  1 11:48:33.289: INFO: Got endpoints: latency-svc-kvb6m [385.514104ms]
Sep  1 11:48:33.302: INFO: Created: latency-svc-g25np
Sep  1 11:48:33.317: INFO: Got endpoints: latency-svc-g25np [413.687838ms]
Sep  1 11:48:33.328: INFO: Created: latency-svc-lbcmg
Sep  1 11:48:33.346: INFO: Got endpoints: latency-svc-lbcmg [441.992026ms]
Sep  1 11:48:33.354: INFO: Created: latency-svc-jt5tv
Sep  1 11:48:33.371: INFO: Got endpoints: latency-svc-jt5tv [428.055256ms]
Sep  1 11:48:33.376: INFO: Created: latency-svc-mmkvv
Sep  1 11:48:33.393: INFO: Got endpoints: latency-svc-mmkvv [431.691365ms]
Sep  1 11:48:33.406: INFO: Created: latency-svc-flcc9
Sep  1 11:48:33.422: INFO: Got endpoints: latency-svc-flcc9 [416.190931ms]
Sep  1 11:48:33.432: INFO: Created: latency-svc-2qwq6
Sep  1 11:48:33.440: INFO: Got endpoints: latency-svc-2qwq6 [417.498597ms]
Sep  1 11:48:33.473: INFO: Created: latency-svc-6knld
Sep  1 11:48:33.478: INFO: Got endpoints: latency-svc-6knld [403.96761ms]
Sep  1 11:48:33.491: INFO: Created: latency-svc-ldhjs
Sep  1 11:48:33.504: INFO: Got endpoints: latency-svc-ldhjs [407.825842ms]
Sep  1 11:48:33.514: INFO: Created: latency-svc-7zx4d
Sep  1 11:48:33.522: INFO: Got endpoints: latency-svc-7zx4d [395.467006ms]
Sep  1 11:48:33.541: INFO: Created: latency-svc-vbdr8
Sep  1 11:48:33.552: INFO: Got endpoints: latency-svc-vbdr8 [403.393225ms]
Sep  1 11:48:33.567: INFO: Created: latency-svc-87xhf
Sep  1 11:48:33.583: INFO: Got endpoints: latency-svc-87xhf [389.128319ms]
Sep  1 11:48:33.601: INFO: Created: latency-svc-twrfn
Sep  1 11:48:33.615: INFO: Got endpoints: latency-svc-twrfn [392.144751ms]
Sep  1 11:48:33.625: INFO: Created: latency-svc-ztb2s
Sep  1 11:48:33.634: INFO: Got endpoints: latency-svc-ztb2s [381.321469ms]
Sep  1 11:48:33.642: INFO: Created: latency-svc-x8fgd
Sep  1 11:48:33.655: INFO: Got endpoints: latency-svc-x8fgd [382.794386ms]
Sep  1 11:48:33.673: INFO: Created: latency-svc-bbq92
Sep  1 11:48:33.680: INFO: Got endpoints: latency-svc-bbq92 [362.456029ms]
Sep  1 11:48:33.690: INFO: Created: latency-svc-5l9b9
Sep  1 11:48:33.698: INFO: Got endpoints: latency-svc-5l9b9 [352.419033ms]
Sep  1 11:48:33.709: INFO: Created: latency-svc-xz54j
Sep  1 11:48:33.718: INFO: Got endpoints: latency-svc-xz54j [347.24662ms]
Sep  1 11:48:33.727: INFO: Created: latency-svc-zbbcw
Sep  1 11:48:33.733: INFO: Got endpoints: latency-svc-zbbcw [340.271154ms]
Sep  1 11:48:33.741: INFO: Created: latency-svc-s6zpr
Sep  1 11:48:33.751: INFO: Got endpoints: latency-svc-s6zpr [327.994585ms]
Sep  1 11:48:33.756: INFO: Created: latency-svc-5kfvl
Sep  1 11:48:33.765: INFO: Got endpoints: latency-svc-5kfvl [325.365774ms]
Sep  1 11:48:33.783: INFO: Created: latency-svc-vgl9x
Sep  1 11:48:33.793: INFO: Got endpoints: latency-svc-vgl9x [314.353671ms]
Sep  1 11:48:33.798: INFO: Created: latency-svc-wg9wb
Sep  1 11:48:33.811: INFO: Got endpoints: latency-svc-wg9wb [306.895087ms]
Sep  1 11:48:33.817: INFO: Created: latency-svc-7fnvp
Sep  1 11:48:33.831: INFO: Got endpoints: latency-svc-7fnvp [309.600604ms]
Sep  1 11:48:33.831: INFO: Created: latency-svc-jldxz
Sep  1 11:48:33.837: INFO: Got endpoints: latency-svc-jldxz [284.681735ms]
Sep  1 11:48:33.849: INFO: Created: latency-svc-z9mdz
Sep  1 11:48:33.857: INFO: Got endpoints: latency-svc-z9mdz [273.920267ms]
Sep  1 11:48:33.866: INFO: Created: latency-svc-fxsqk
Sep  1 11:48:33.882: INFO: Got endpoints: latency-svc-fxsqk [266.262758ms]
Sep  1 11:48:33.905: INFO: Created: latency-svc-9qmv6
Sep  1 11:48:33.906: INFO: Created: latency-svc-w6hh5
Sep  1 11:48:33.926: INFO: Got endpoints: latency-svc-w6hh5 [291.773661ms]
Sep  1 11:48:33.934: INFO: Got endpoints: latency-svc-9qmv6 [645.088642ms]
Sep  1 11:48:33.959: INFO: Created: latency-svc-tfh9g
Sep  1 11:48:33.971: INFO: Got endpoints: latency-svc-tfh9g [315.445415ms]
Sep  1 11:48:33.981: INFO: Created: latency-svc-vvv2g
Sep  1 11:48:33.985: INFO: Got endpoints: latency-svc-vvv2g [304.805704ms]
Sep  1 11:48:34.006: INFO: Created: latency-svc-vjdjd
Sep  1 11:48:34.015: INFO: Got endpoints: latency-svc-vjdjd [316.684845ms]
Sep  1 11:48:34.050: INFO: Created: latency-svc-s2d94
Sep  1 11:48:34.077: INFO: Got endpoints: latency-svc-s2d94 [359.282981ms]
Sep  1 11:48:34.085: INFO: Created: latency-svc-qtsdv
Sep  1 11:48:34.109: INFO: Got endpoints: latency-svc-qtsdv [375.562246ms]
Sep  1 11:48:34.122: INFO: Created: latency-svc-vzfvw
Sep  1 11:48:34.128: INFO: Got endpoints: latency-svc-vzfvw [376.850313ms]
Sep  1 11:48:34.153: INFO: Created: latency-svc-jww5j
Sep  1 11:48:34.166: INFO: Got endpoints: latency-svc-jww5j [400.327521ms]
Sep  1 11:48:34.178: INFO: Created: latency-svc-5k9x2
Sep  1 11:48:34.184: INFO: Got endpoints: latency-svc-5k9x2 [390.972642ms]
Sep  1 11:48:34.201: INFO: Created: latency-svc-kvtrk
Sep  1 11:48:34.203: INFO: Got endpoints: latency-svc-kvtrk [392.053638ms]
Sep  1 11:48:34.220: INFO: Created: latency-svc-lw6ml
Sep  1 11:48:34.235: INFO: Got endpoints: latency-svc-lw6ml [403.11439ms]
Sep  1 11:48:34.236: INFO: Created: latency-svc-xff6d
Sep  1 11:48:34.245: INFO: Got endpoints: latency-svc-xff6d [407.906728ms]
Sep  1 11:48:34.255: INFO: Created: latency-svc-rhlqj
Sep  1 11:48:34.271: INFO: Got endpoints: latency-svc-rhlqj [414.142562ms]
Sep  1 11:48:34.280: INFO: Created: latency-svc-fh28l
Sep  1 11:48:34.285: INFO: Got endpoints: latency-svc-fh28l [402.473597ms]
Sep  1 11:48:34.297: INFO: Created: latency-svc-dx47b
Sep  1 11:48:34.311: INFO: Got endpoints: latency-svc-dx47b [384.926118ms]
Sep  1 11:48:34.314: INFO: Created: latency-svc-8r4pc
Sep  1 11:48:34.332: INFO: Got endpoints: latency-svc-8r4pc [397.882522ms]
Sep  1 11:48:34.349: INFO: Created: latency-svc-xxbgc
Sep  1 11:48:34.357: INFO: Got endpoints: latency-svc-xxbgc [371.1218ms]
Sep  1 11:48:34.366: INFO: Created: latency-svc-8xcsx
Sep  1 11:48:34.376: INFO: Got endpoints: latency-svc-8xcsx [360.172586ms]
Sep  1 11:48:34.385: INFO: Created: latency-svc-2w96c
Sep  1 11:48:34.397: INFO: Got endpoints: latency-svc-2w96c [319.20496ms]
Sep  1 11:48:34.404: INFO: Created: latency-svc-26z7c
Sep  1 11:48:34.442: INFO: Created: latency-svc-nwqcq
Sep  1 11:48:34.445: INFO: Got endpoints: latency-svc-26z7c [335.804262ms]
Sep  1 11:48:34.463: INFO: Created: latency-svc-v2q5q
Sep  1 11:48:34.474: INFO: Created: latency-svc-5bnr2
Sep  1 11:48:34.489: INFO: Created: latency-svc-px6lg
Sep  1 11:48:34.496: INFO: Got endpoints: latency-svc-nwqcq [368.189795ms]
Sep  1 11:48:34.505: INFO: Created: latency-svc-vm88q
Sep  1 11:48:34.519: INFO: Created: latency-svc-6fs72
Sep  1 11:48:34.530: INFO: Created: latency-svc-dv82b
Sep  1 11:48:34.540: INFO: Got endpoints: latency-svc-v2q5q [373.941013ms]
Sep  1 11:48:34.561: INFO: Created: latency-svc-4kgbp
Sep  1 11:48:34.570: INFO: Created: latency-svc-hxlln
Sep  1 11:48:34.578: INFO: Created: latency-svc-q4w98
Sep  1 11:48:34.593: INFO: Created: latency-svc-7zv6c
Sep  1 11:48:34.596: INFO: Got endpoints: latency-svc-5bnr2 [411.643288ms]
Sep  1 11:48:34.609: INFO: Created: latency-svc-nhj6h
Sep  1 11:48:34.624: INFO: Created: latency-svc-thj9r
Sep  1 11:48:34.629: INFO: Created: latency-svc-qwf94
Sep  1 11:48:34.641: INFO: Got endpoints: latency-svc-px6lg [438.474929ms]
Sep  1 11:48:34.648: INFO: Created: latency-svc-762lc
Sep  1 11:48:34.668: INFO: Created: latency-svc-j42c4
Sep  1 11:48:34.680: INFO: Created: latency-svc-tqltc
Sep  1 11:48:34.694: INFO: Created: latency-svc-5fj5d
Sep  1 11:48:34.697: INFO: Got endpoints: latency-svc-vm88q [462.341084ms]
Sep  1 11:48:34.705: INFO: Created: latency-svc-n2hql
Sep  1 11:48:34.718: INFO: Created: latency-svc-z2njg
Sep  1 11:48:34.743: INFO: Got endpoints: latency-svc-6fs72 [497.790335ms]
Sep  1 11:48:34.762: INFO: Created: latency-svc-r7lxl
Sep  1 11:48:34.793: INFO: Got endpoints: latency-svc-dv82b [520.456832ms]
Sep  1 11:48:34.832: INFO: Created: latency-svc-mmxwm
Sep  1 11:48:34.842: INFO: Got endpoints: latency-svc-4kgbp [556.217139ms]
Sep  1 11:48:34.861: INFO: Created: latency-svc-2rxjk
Sep  1 11:48:34.891: INFO: Got endpoints: latency-svc-hxlln [919.065302ms]
Sep  1 11:48:34.912: INFO: Created: latency-svc-7gclf
Sep  1 11:48:34.943: INFO: Got endpoints: latency-svc-q4w98 [632.057543ms]
Sep  1 11:48:34.968: INFO: Created: latency-svc-h828m
Sep  1 11:48:35.000: INFO: Got endpoints: latency-svc-7zv6c [666.502133ms]
Sep  1 11:48:35.019: INFO: Created: latency-svc-pp2q5
Sep  1 11:48:35.045: INFO: Got endpoints: latency-svc-nhj6h [688.161851ms]
Sep  1 11:48:35.068: INFO: Created: latency-svc-nwfxd
Sep  1 11:48:35.096: INFO: Got endpoints: latency-svc-thj9r [720.21816ms]
Sep  1 11:48:35.114: INFO: Created: latency-svc-d462p
Sep  1 11:48:35.139: INFO: Got endpoints: latency-svc-qwf94 [741.615697ms]
Sep  1 11:48:35.155: INFO: Created: latency-svc-bjm2m
Sep  1 11:48:35.192: INFO: Got endpoints: latency-svc-762lc [746.834858ms]
Sep  1 11:48:35.213: INFO: Created: latency-svc-mlx6c
Sep  1 11:48:35.245: INFO: Got endpoints: latency-svc-j42c4 [749.048363ms]
Sep  1 11:48:35.263: INFO: Created: latency-svc-bg4fn
Sep  1 11:48:35.294: INFO: Got endpoints: latency-svc-tqltc [753.044738ms]
Sep  1 11:48:35.311: INFO: Created: latency-svc-4587v
Sep  1 11:48:35.347: INFO: Got endpoints: latency-svc-5fj5d [750.837149ms]
Sep  1 11:48:35.364: INFO: Created: latency-svc-j6lcr
Sep  1 11:48:35.389: INFO: Got endpoints: latency-svc-n2hql [747.499957ms]
Sep  1 11:48:35.416: INFO: Created: latency-svc-vqvhq
Sep  1 11:48:35.440: INFO: Got endpoints: latency-svc-z2njg [742.338288ms]
Sep  1 11:48:35.456: INFO: Created: latency-svc-sldwf
Sep  1 11:48:35.493: INFO: Got endpoints: latency-svc-r7lxl [749.879648ms]
Sep  1 11:48:35.514: INFO: Created: latency-svc-9r7nt
Sep  1 11:48:35.546: INFO: Got endpoints: latency-svc-mmxwm [752.797961ms]
Sep  1 11:48:35.564: INFO: Created: latency-svc-kl6pt
Sep  1 11:48:35.594: INFO: Got endpoints: latency-svc-2rxjk [752.239284ms]
Sep  1 11:48:35.611: INFO: Created: latency-svc-zjjv5
Sep  1 11:48:35.641: INFO: Got endpoints: latency-svc-7gclf [749.852026ms]
Sep  1 11:48:35.666: INFO: Created: latency-svc-cp5pp
Sep  1 11:48:35.693: INFO: Got endpoints: latency-svc-h828m [749.92519ms]
Sep  1 11:48:35.713: INFO: Created: latency-svc-vp447
Sep  1 11:48:35.740: INFO: Got endpoints: latency-svc-pp2q5 [740.019646ms]
Sep  1 11:48:35.769: INFO: Created: latency-svc-c4zvx
Sep  1 11:48:35.792: INFO: Got endpoints: latency-svc-nwfxd [745.905246ms]
Sep  1 11:48:35.810: INFO: Created: latency-svc-7dw72
Sep  1 11:48:35.843: INFO: Got endpoints: latency-svc-d462p [746.486977ms]
Sep  1 11:48:35.862: INFO: Created: latency-svc-8swft
Sep  1 11:48:35.889: INFO: Got endpoints: latency-svc-bjm2m [749.799959ms]
Sep  1 11:48:35.913: INFO: Created: latency-svc-5tcgv
Sep  1 11:48:35.949: INFO: Got endpoints: latency-svc-mlx6c [756.950439ms]
Sep  1 11:48:35.975: INFO: Created: latency-svc-h7szd
Sep  1 11:48:35.993: INFO: Got endpoints: latency-svc-bg4fn [747.704254ms]
Sep  1 11:48:36.015: INFO: Created: latency-svc-sc49t
Sep  1 11:48:36.042: INFO: Got endpoints: latency-svc-4587v [748.512012ms]
Sep  1 11:48:36.069: INFO: Created: latency-svc-kl6vq
Sep  1 11:48:36.099: INFO: Got endpoints: latency-svc-j6lcr [752.357652ms]
Sep  1 11:48:36.123: INFO: Created: latency-svc-8sxcq
Sep  1 11:48:36.148: INFO: Got endpoints: latency-svc-vqvhq [758.323465ms]
Sep  1 11:48:36.197: INFO: Created: latency-svc-2dr98
Sep  1 11:48:36.218: INFO: Got endpoints: latency-svc-sldwf [777.791752ms]
Sep  1 11:48:36.256: INFO: Got endpoints: latency-svc-9r7nt [762.486241ms]
Sep  1 11:48:36.257: INFO: Created: latency-svc-p49jh
Sep  1 11:48:36.301: INFO: Got endpoints: latency-svc-kl6pt [755.611098ms]
Sep  1 11:48:36.308: INFO: Created: latency-svc-kjlzw
Sep  1 11:48:36.343: INFO: Got endpoints: latency-svc-zjjv5 [748.987432ms]
Sep  1 11:48:36.356: INFO: Created: latency-svc-jwlgs
Sep  1 11:48:36.377: INFO: Created: latency-svc-fcqx5
Sep  1 11:48:36.397: INFO: Got endpoints: latency-svc-cp5pp [756.471662ms]
Sep  1 11:48:36.426: INFO: Created: latency-svc-xg8ts
Sep  1 11:48:36.447: INFO: Got endpoints: latency-svc-vp447 [753.252303ms]
Sep  1 11:48:36.474: INFO: Created: latency-svc-5l228
Sep  1 11:48:36.500: INFO: Got endpoints: latency-svc-c4zvx [760.034795ms]
Sep  1 11:48:36.570: INFO: Got endpoints: latency-svc-7dw72 [778.653916ms]
Sep  1 11:48:36.571: INFO: Created: latency-svc-gd6rs
Sep  1 11:48:36.594: INFO: Got endpoints: latency-svc-8swft [750.996933ms]
Sep  1 11:48:36.602: INFO: Created: latency-svc-2vwn8
Sep  1 11:48:36.628: INFO: Created: latency-svc-lvkxt
Sep  1 11:48:36.695: INFO: Got endpoints: latency-svc-5tcgv [805.461806ms]
Sep  1 11:48:36.759: INFO: Got endpoints: latency-svc-h7szd [809.535551ms]
Sep  1 11:48:36.807: INFO: Got endpoints: latency-svc-sc49t [813.353241ms]
Sep  1 11:48:36.838: INFO: Got endpoints: latency-svc-kl6vq [795.488784ms]
Sep  1 11:48:36.838: INFO: Created: latency-svc-v8kbx
Sep  1 11:48:36.909: INFO: Created: latency-svc-xcqwv
Sep  1 11:48:36.922: INFO: Got endpoints: latency-svc-8sxcq [822.185751ms]
Sep  1 11:48:36.937: INFO: Got endpoints: latency-svc-2dr98 [789.288178ms]
Sep  1 11:48:36.995: INFO: Created: latency-svc-kn9dw
Sep  1 11:48:37.002: INFO: Got endpoints: latency-svc-p49jh [784.638059ms]
Sep  1 11:48:37.018: INFO: Created: latency-svc-26rg4
Sep  1 11:48:37.041: INFO: Got endpoints: latency-svc-kjlzw [784.679076ms]
Sep  1 11:48:37.091: INFO: Got endpoints: latency-svc-jwlgs [788.709899ms]
Sep  1 11:48:37.104: INFO: Created: latency-svc-vvksq
Sep  1 11:48:37.109: INFO: Got endpoints: latency-svc-fcqx5 [766.212679ms]
Sep  1 11:48:37.198: INFO: Created: latency-svc-r92pd
Sep  1 11:48:37.200: INFO: Got endpoints: latency-svc-xg8ts [802.256377ms]
Sep  1 11:48:37.241: INFO: Got endpoints: latency-svc-5l228 [793.986362ms]
Sep  1 11:48:37.268: INFO: Created: latency-svc-pdvvt
Sep  1 11:48:37.317: INFO: Created: latency-svc-c6jr8
Sep  1 11:48:37.332: INFO: Got endpoints: latency-svc-gd6rs [831.414282ms]
Sep  1 11:48:37.356: INFO: Got endpoints: latency-svc-2vwn8 [785.521446ms]
Sep  1 11:48:37.419: INFO: Got endpoints: latency-svc-lvkxt [824.869806ms]
Sep  1 11:48:37.419: INFO: Created: latency-svc-d4jdm
Sep  1 11:48:37.462: INFO: Got endpoints: latency-svc-v8kbx [767.472941ms]
Sep  1 11:48:37.463: INFO: Got endpoints: latency-svc-xcqwv [703.560865ms]
Sep  1 11:48:37.508: INFO: Created: latency-svc-nmj8z
Sep  1 11:48:37.537: INFO: Got endpoints: latency-svc-kn9dw [729.141618ms]
Sep  1 11:48:37.572: INFO: Got endpoints: latency-svc-26rg4 [733.485361ms]
Sep  1 11:48:37.590: INFO: Created: latency-svc-w5lgz
Sep  1 11:48:37.608: INFO: Got endpoints: latency-svc-vvksq [685.908761ms]
Sep  1 11:48:37.630: INFO: Created: latency-svc-xlgrn
Sep  1 11:48:37.678: INFO: Created: latency-svc-qplrl
Sep  1 11:48:37.680: INFO: Got endpoints: latency-svc-r92pd [743.14097ms]
Sep  1 11:48:37.713: INFO: Got endpoints: latency-svc-pdvvt [709.718004ms]
Sep  1 11:48:37.731: INFO: Created: latency-svc-g86tn
Sep  1 11:48:37.752: INFO: Got endpoints: latency-svc-c6jr8 [711.20954ms]
Sep  1 11:48:37.768: INFO: Created: latency-svc-ffqpl
Sep  1 11:48:37.782: INFO: Created: latency-svc-d2tjb
Sep  1 11:48:37.801: INFO: Got endpoints: latency-svc-d4jdm [709.964705ms]
Sep  1 11:48:37.826: INFO: Created: latency-svc-sqqdt
Sep  1 11:48:37.838: INFO: Created: latency-svc-nflkg
Sep  1 11:48:37.844: INFO: Got endpoints: latency-svc-nmj8z [734.846088ms]
Sep  1 11:48:37.863: INFO: Created: latency-svc-hwbd8
Sep  1 11:48:37.922: INFO: Got endpoints: latency-svc-w5lgz [722.430353ms]
Sep  1 11:48:37.928: INFO: Created: latency-svc-sgwb5
Sep  1 11:48:37.949: INFO: Created: latency-svc-qbkjz
Sep  1 11:48:37.950: INFO: Got endpoints: latency-svc-xlgrn [709.71899ms]
Sep  1 11:48:37.966: INFO: Created: latency-svc-cf2k9
Sep  1 11:48:37.979: INFO: Created: latency-svc-8bdlc
Sep  1 11:48:37.994: INFO: Got endpoints: latency-svc-qplrl [662.033759ms]
Sep  1 11:48:38.005: INFO: Created: latency-svc-kbv54
Sep  1 11:48:38.034: INFO: Created: latency-svc-bjggp
Sep  1 11:48:38.045: INFO: Got endpoints: latency-svc-g86tn [688.981391ms]
Sep  1 11:48:38.048: INFO: Created: latency-svc-c8gdh
Sep  1 11:48:38.073: INFO: Created: latency-svc-stgtp
Sep  1 11:48:38.078: INFO: Created: latency-svc-2vrjq
Sep  1 11:48:38.094: INFO: Got endpoints: latency-svc-ffqpl [674.327299ms]
Sep  1 11:48:38.103: INFO: Created: latency-svc-7fgft
Sep  1 11:48:38.116: INFO: Created: latency-svc-q2ldh
Sep  1 11:48:38.145: INFO: Got endpoints: latency-svc-d2tjb [682.101168ms]
Sep  1 11:48:38.162: INFO: Created: latency-svc-wjvwc
Sep  1 11:48:38.197: INFO: Got endpoints: latency-svc-sqqdt [734.595433ms]
Sep  1 11:48:38.216: INFO: Created: latency-svc-rczgn
Sep  1 11:48:38.249: INFO: Got endpoints: latency-svc-nflkg [712.076641ms]
Sep  1 11:48:38.277: INFO: Created: latency-svc-g46qq
Sep  1 11:48:38.290: INFO: Got endpoints: latency-svc-hwbd8 [718.607871ms]
Sep  1 11:48:38.308: INFO: Created: latency-svc-l4w5f
Sep  1 11:48:38.339: INFO: Got endpoints: latency-svc-sgwb5 [731.412514ms]
Sep  1 11:48:38.356: INFO: Created: latency-svc-2scjl
Sep  1 11:48:38.392: INFO: Got endpoints: latency-svc-qbkjz [711.934823ms]
Sep  1 11:48:38.406: INFO: Created: latency-svc-gm922
Sep  1 11:48:38.440: INFO: Got endpoints: latency-svc-cf2k9 [727.251313ms]
Sep  1 11:48:38.460: INFO: Created: latency-svc-9mmlp
Sep  1 11:48:38.491: INFO: Got endpoints: latency-svc-8bdlc [737.468769ms]
Sep  1 11:48:38.506: INFO: Created: latency-svc-mmtpg
Sep  1 11:48:38.544: INFO: Got endpoints: latency-svc-kbv54 [743.00869ms]
Sep  1 11:48:38.558: INFO: Created: latency-svc-58q47
Sep  1 11:48:38.589: INFO: Got endpoints: latency-svc-bjggp [744.959304ms]
Sep  1 11:48:38.607: INFO: Created: latency-svc-c92cf
Sep  1 11:48:38.639: INFO: Got endpoints: latency-svc-c8gdh [716.525151ms]
Sep  1 11:48:38.661: INFO: Created: latency-svc-lbxn9
Sep  1 11:48:38.693: INFO: Got endpoints: latency-svc-stgtp [741.779368ms]
Sep  1 11:48:38.711: INFO: Created: latency-svc-gfdgn
Sep  1 11:48:38.742: INFO: Got endpoints: latency-svc-2vrjq [747.229086ms]
Sep  1 11:48:38.757: INFO: Created: latency-svc-vx52p
Sep  1 11:48:38.793: INFO: Got endpoints: latency-svc-7fgft [746.694688ms]
Sep  1 11:48:38.809: INFO: Created: latency-svc-lswq8
Sep  1 11:48:38.839: INFO: Got endpoints: latency-svc-q2ldh [745.740846ms]
Sep  1 11:48:38.856: INFO: Created: latency-svc-s7rsh
Sep  1 11:48:38.890: INFO: Got endpoints: latency-svc-wjvwc [745.029815ms]
Sep  1 11:48:38.941: INFO: Created: latency-svc-l5zr4
Sep  1 11:48:38.945: INFO: Got endpoints: latency-svc-rczgn [747.600438ms]
Sep  1 11:48:38.968: INFO: Created: latency-svc-mqlnw
Sep  1 11:48:39.001: INFO: Got endpoints: latency-svc-g46qq [751.5592ms]
Sep  1 11:48:39.022: INFO: Created: latency-svc-k2vtl
Sep  1 11:48:39.043: INFO: Got endpoints: latency-svc-l4w5f [752.46665ms]
Sep  1 11:48:39.074: INFO: Created: latency-svc-dhrmj
Sep  1 11:48:39.107: INFO: Got endpoints: latency-svc-2scjl [767.001691ms]
Sep  1 11:48:39.129: INFO: Created: latency-svc-dhpxs
Sep  1 11:48:39.142: INFO: Got endpoints: latency-svc-gm922 [749.868256ms]
Sep  1 11:48:39.178: INFO: Created: latency-svc-9qr5p
Sep  1 11:48:39.194: INFO: Got endpoints: latency-svc-9mmlp [753.566126ms]
Sep  1 11:48:39.217: INFO: Created: latency-svc-244zg
Sep  1 11:48:39.240: INFO: Got endpoints: latency-svc-mmtpg [748.813331ms]
Sep  1 11:48:39.255: INFO: Created: latency-svc-tnpzf
Sep  1 11:48:39.290: INFO: Got endpoints: latency-svc-58q47 [745.692192ms]
Sep  1 11:48:39.307: INFO: Created: latency-svc-sldvw
Sep  1 11:48:39.340: INFO: Got endpoints: latency-svc-c92cf [750.65101ms]
Sep  1 11:48:39.355: INFO: Created: latency-svc-dkxsn
Sep  1 11:48:39.390: INFO: Got endpoints: latency-svc-lbxn9 [750.254785ms]
Sep  1 11:48:39.410: INFO: Created: latency-svc-tgvdw
Sep  1 11:48:39.441: INFO: Got endpoints: latency-svc-gfdgn [748.505188ms]
Sep  1 11:48:39.461: INFO: Created: latency-svc-k89dl
Sep  1 11:48:39.492: INFO: Got endpoints: latency-svc-vx52p [750.132065ms]
Sep  1 11:48:39.510: INFO: Created: latency-svc-24jpz
Sep  1 11:48:39.542: INFO: Got endpoints: latency-svc-lswq8 [748.747004ms]
Sep  1 11:48:39.558: INFO: Created: latency-svc-vjqrg
Sep  1 11:48:39.594: INFO: Got endpoints: latency-svc-s7rsh [755.142112ms]
Sep  1 11:48:39.610: INFO: Created: latency-svc-6rljs
Sep  1 11:48:39.640: INFO: Got endpoints: latency-svc-l5zr4 [749.988899ms]
Sep  1 11:48:39.655: INFO: Created: latency-svc-7wfw5
Sep  1 11:48:39.690: INFO: Got endpoints: latency-svc-mqlnw [744.551218ms]
Sep  1 11:48:39.705: INFO: Created: latency-svc-xdzpz
Sep  1 11:48:39.742: INFO: Got endpoints: latency-svc-k2vtl [741.070927ms]
Sep  1 11:48:39.763: INFO: Created: latency-svc-k9fmn
Sep  1 11:48:39.789: INFO: Got endpoints: latency-svc-dhrmj [745.507083ms]
Sep  1 11:48:39.806: INFO: Created: latency-svc-rw7cg
Sep  1 11:48:39.841: INFO: Got endpoints: latency-svc-dhpxs [734.191151ms]
Sep  1 11:48:39.860: INFO: Created: latency-svc-4sndj
Sep  1 11:48:39.892: INFO: Got endpoints: latency-svc-9qr5p [749.180656ms]
Sep  1 11:48:39.925: INFO: Created: latency-svc-2j2rs
Sep  1 11:48:39.943: INFO: Got endpoints: latency-svc-244zg [749.029101ms]
Sep  1 11:48:39.970: INFO: Created: latency-svc-rpvwf
Sep  1 11:48:39.999: INFO: Got endpoints: latency-svc-tnpzf [758.864945ms]
Sep  1 11:48:40.017: INFO: Created: latency-svc-md9n2
Sep  1 11:48:40.045: INFO: Got endpoints: latency-svc-sldvw [755.109626ms]
Sep  1 11:48:40.078: INFO: Created: latency-svc-5nmhk
Sep  1 11:48:40.096: INFO: Got endpoints: latency-svc-dkxsn [755.371225ms]
Sep  1 11:48:40.121: INFO: Created: latency-svc-l2x7v
Sep  1 11:48:40.142: INFO: Got endpoints: latency-svc-tgvdw [752.454524ms]
Sep  1 11:48:40.157: INFO: Created: latency-svc-2f8wf
Sep  1 11:48:40.190: INFO: Got endpoints: latency-svc-k89dl [748.175372ms]
Sep  1 11:48:40.223: INFO: Created: latency-svc-9thqq
Sep  1 11:48:40.242: INFO: Got endpoints: latency-svc-24jpz [749.866774ms]
Sep  1 11:48:40.258: INFO: Created: latency-svc-psz4g
Sep  1 11:48:40.289: INFO: Got endpoints: latency-svc-vjqrg [747.295247ms]
Sep  1 11:48:40.305: INFO: Created: latency-svc-4ccp8
Sep  1 11:48:40.340: INFO: Got endpoints: latency-svc-6rljs [745.451867ms]
Sep  1 11:48:40.354: INFO: Created: latency-svc-jjw4j
Sep  1 11:48:40.391: INFO: Got endpoints: latency-svc-7wfw5 [751.186309ms]
Sep  1 11:48:40.407: INFO: Created: latency-svc-tctrp
Sep  1 11:48:40.443: INFO: Got endpoints: latency-svc-xdzpz [753.014219ms]
Sep  1 11:48:40.459: INFO: Created: latency-svc-gpdkz
Sep  1 11:48:40.493: INFO: Got endpoints: latency-svc-k9fmn [751.068819ms]
Sep  1 11:48:40.509: INFO: Created: latency-svc-rtmnz
Sep  1 11:48:40.539: INFO: Got endpoints: latency-svc-rw7cg [749.359802ms]
Sep  1 11:48:40.556: INFO: Created: latency-svc-fz5q4
Sep  1 11:48:40.591: INFO: Got endpoints: latency-svc-4sndj [749.458429ms]
Sep  1 11:48:40.606: INFO: Created: latency-svc-lhxsb
Sep  1 11:48:40.641: INFO: Got endpoints: latency-svc-2j2rs [749.370528ms]
Sep  1 11:48:40.656: INFO: Created: latency-svc-7bj5l
Sep  1 11:48:40.690: INFO: Got endpoints: latency-svc-rpvwf [745.935683ms]
Sep  1 11:48:40.718: INFO: Created: latency-svc-wtq9t
Sep  1 11:48:40.740: INFO: Got endpoints: latency-svc-md9n2 [741.495608ms]
Sep  1 11:48:40.789: INFO: Got endpoints: latency-svc-5nmhk [743.429589ms]
Sep  1 11:48:40.842: INFO: Got endpoints: latency-svc-l2x7v [745.536966ms]
Sep  1 11:48:40.892: INFO: Got endpoints: latency-svc-2f8wf [749.454127ms]
Sep  1 11:48:40.946: INFO: Got endpoints: latency-svc-9thqq [756.050227ms]
Sep  1 11:48:40.994: INFO: Got endpoints: latency-svc-psz4g [752.016068ms]
Sep  1 11:48:41.047: INFO: Got endpoints: latency-svc-4ccp8 [757.929995ms]
Sep  1 11:48:41.096: INFO: Got endpoints: latency-svc-jjw4j [755.546601ms]
Sep  1 11:48:41.141: INFO: Got endpoints: latency-svc-tctrp [749.759706ms]
Sep  1 11:48:41.190: INFO: Got endpoints: latency-svc-gpdkz [746.429066ms]
Sep  1 11:48:41.239: INFO: Got endpoints: latency-svc-rtmnz [745.472121ms]
Sep  1 11:48:41.293: INFO: Got endpoints: latency-svc-fz5q4 [753.814404ms]
Sep  1 11:48:41.340: INFO: Got endpoints: latency-svc-lhxsb [748.817745ms]
Sep  1 11:48:41.393: INFO: Got endpoints: latency-svc-7bj5l [751.786426ms]
Sep  1 11:48:41.447: INFO: Got endpoints: latency-svc-wtq9t [756.327917ms]
Sep  1 11:48:41.448: INFO: Latencies: [41.332318ms 59.146858ms 102.864374ms 119.564088ms 171.36969ms 192.585101ms 223.273317ms 246.123996ms 266.262758ms 273.920267ms 284.681735ms 290.812774ms 291.773661ms 304.805704ms 306.895087ms 309.600604ms 314.353671ms 315.445415ms 316.684845ms 319.20496ms 319.802939ms 325.365774ms 327.994585ms 335.804262ms 340.271154ms 347.24662ms 349.588621ms 352.419033ms 359.282981ms 360.172586ms 362.456029ms 368.189795ms 368.979794ms 371.1218ms 373.941013ms 375.562246ms 376.850313ms 381.321469ms 382.794386ms 384.926118ms 385.514104ms 389.128319ms 390.972642ms 392.053638ms 392.144751ms 395.467006ms 397.882522ms 400.327521ms 402.473597ms 403.11439ms 403.393225ms 403.96761ms 407.825842ms 407.906728ms 411.643288ms 413.687838ms 414.142562ms 416.190931ms 417.498597ms 428.055256ms 431.691365ms 438.474929ms 441.992026ms 462.341084ms 497.790335ms 520.456832ms 556.217139ms 632.057543ms 645.088642ms 662.033759ms 666.502133ms 674.327299ms 682.101168ms 685.908761ms 688.161851ms 688.981391ms 703.560865ms 709.718004ms 709.71899ms 709.964705ms 711.20954ms 711.934823ms 712.076641ms 716.525151ms 718.607871ms 720.21816ms 722.430353ms 727.251313ms 729.141618ms 731.412514ms 733.485361ms 734.191151ms 734.595433ms 734.846088ms 737.468769ms 740.019646ms 741.070927ms 741.495608ms 741.615697ms 741.779368ms 742.338288ms 743.00869ms 743.14097ms 743.429589ms 744.551218ms 744.959304ms 745.029815ms 745.451867ms 745.472121ms 745.507083ms 745.536966ms 745.692192ms 745.740846ms 745.905246ms 745.935683ms 746.429066ms 746.486977ms 746.694688ms 746.834858ms 747.229086ms 747.295247ms 747.499957ms 747.600438ms 747.704254ms 748.175372ms 748.505188ms 748.512012ms 748.747004ms 748.813331ms 748.817745ms 748.987432ms 749.029101ms 749.048363ms 749.180656ms 749.359802ms 749.370528ms 749.454127ms 749.458429ms 749.759706ms 749.799959ms 749.852026ms 749.866774ms 749.868256ms 749.879648ms 749.92519ms 749.988899ms 750.132065ms 750.254785ms 750.65101ms 750.837149ms 750.996933ms 751.068819ms 751.186309ms 751.5592ms 751.786426ms 752.016068ms 752.239284ms 752.357652ms 752.454524ms 752.46665ms 752.797961ms 753.014219ms 753.044738ms 753.252303ms 753.566126ms 753.814404ms 755.109626ms 755.142112ms 755.371225ms 755.546601ms 755.611098ms 756.050227ms 756.327917ms 756.471662ms 756.950439ms 757.929995ms 758.323465ms 758.864945ms 760.034795ms 762.486241ms 766.212679ms 767.001691ms 767.472941ms 777.791752ms 778.653916ms 784.638059ms 784.679076ms 785.521446ms 788.709899ms 789.288178ms 793.986362ms 795.488784ms 802.256377ms 805.461806ms 809.535551ms 813.353241ms 822.185751ms 824.869806ms 831.414282ms 919.065302ms]
Sep  1 11:48:41.448: INFO: 50 %ile: 742.338288ms
Sep  1 11:48:41.448: INFO: 90 %ile: 766.212679ms
Sep  1 11:48:41.448: INFO: 99 %ile: 831.414282ms
Sep  1 11:48:41.448: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Sep  1 11:48:41.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-7813" for this suite. 09/01/23 11:48:41.459
------------------------------
• [SLOW TEST] [11.018 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:48:30.449
    Sep  1 11:48:30.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename svc-latency 09/01/23 11:48:30.451
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:48:30.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:48:30.469
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Sep  1 11:48:30.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7813 09/01/23 11:48:30.593
    I0901 11:48:30.602479      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7813, replica count: 1
    I0901 11:48:31.654073      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0901 11:48:32.654406      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  1 11:48:32.882: INFO: Created: latency-svc-d9ddk
    Sep  1 11:48:32.900: INFO: Got endpoints: latency-svc-d9ddk [97.640455ms]
    Sep  1 11:48:32.926: INFO: Created: latency-svc-4kwzk
    Sep  1 11:48:32.942: INFO: Got endpoints: latency-svc-4kwzk [41.332318ms]
    Sep  1 11:48:32.950: INFO: Created: latency-svc-ghg27
    Sep  1 11:48:32.961: INFO: Got endpoints: latency-svc-ghg27 [59.146858ms]
    Sep  1 11:48:32.994: INFO: Created: latency-svc-xncvt
    Sep  1 11:48:33.005: INFO: Got endpoints: latency-svc-xncvt [102.864374ms]
    Sep  1 11:48:33.016: INFO: Created: latency-svc-wmlcx
    Sep  1 11:48:33.022: INFO: Got endpoints: latency-svc-wmlcx [119.564088ms]
    Sep  1 11:48:33.065: INFO: Created: latency-svc-j7ssm
    Sep  1 11:48:33.074: INFO: Got endpoints: latency-svc-j7ssm [171.36969ms]
    Sep  1 11:48:33.083: INFO: Created: latency-svc-6kh79
    Sep  1 11:48:33.095: INFO: Got endpoints: latency-svc-6kh79 [192.585101ms]
    Sep  1 11:48:33.116: INFO: Created: latency-svc-t8mlq
    Sep  1 11:48:33.126: INFO: Got endpoints: latency-svc-t8mlq [223.273317ms]
    Sep  1 11:48:33.136: INFO: Created: latency-svc-cdjbp
    Sep  1 11:48:33.149: INFO: Got endpoints: latency-svc-cdjbp [246.123996ms]
    Sep  1 11:48:33.153: INFO: Created: latency-svc-w9rq4
    Sep  1 11:48:33.194: INFO: Got endpoints: latency-svc-w9rq4 [290.812774ms]
    Sep  1 11:48:33.212: INFO: Created: latency-svc-9n6lf
    Sep  1 11:48:33.223: INFO: Got endpoints: latency-svc-9n6lf [319.802939ms]
    Sep  1 11:48:33.243: INFO: Created: latency-svc-6f9j2
    Sep  1 11:48:33.253: INFO: Got endpoints: latency-svc-6f9j2 [349.588621ms]
    Sep  1 11:48:33.260: INFO: Created: latency-svc-kq9bx
    Sep  1 11:48:33.272: INFO: Got endpoints: latency-svc-kq9bx [368.979794ms]
    Sep  1 11:48:33.281: INFO: Created: latency-svc-kvb6m
    Sep  1 11:48:33.289: INFO: Got endpoints: latency-svc-kvb6m [385.514104ms]
    Sep  1 11:48:33.302: INFO: Created: latency-svc-g25np
    Sep  1 11:48:33.317: INFO: Got endpoints: latency-svc-g25np [413.687838ms]
    Sep  1 11:48:33.328: INFO: Created: latency-svc-lbcmg
    Sep  1 11:48:33.346: INFO: Got endpoints: latency-svc-lbcmg [441.992026ms]
    Sep  1 11:48:33.354: INFO: Created: latency-svc-jt5tv
    Sep  1 11:48:33.371: INFO: Got endpoints: latency-svc-jt5tv [428.055256ms]
    Sep  1 11:48:33.376: INFO: Created: latency-svc-mmkvv
    Sep  1 11:48:33.393: INFO: Got endpoints: latency-svc-mmkvv [431.691365ms]
    Sep  1 11:48:33.406: INFO: Created: latency-svc-flcc9
    Sep  1 11:48:33.422: INFO: Got endpoints: latency-svc-flcc9 [416.190931ms]
    Sep  1 11:48:33.432: INFO: Created: latency-svc-2qwq6
    Sep  1 11:48:33.440: INFO: Got endpoints: latency-svc-2qwq6 [417.498597ms]
    Sep  1 11:48:33.473: INFO: Created: latency-svc-6knld
    Sep  1 11:48:33.478: INFO: Got endpoints: latency-svc-6knld [403.96761ms]
    Sep  1 11:48:33.491: INFO: Created: latency-svc-ldhjs
    Sep  1 11:48:33.504: INFO: Got endpoints: latency-svc-ldhjs [407.825842ms]
    Sep  1 11:48:33.514: INFO: Created: latency-svc-7zx4d
    Sep  1 11:48:33.522: INFO: Got endpoints: latency-svc-7zx4d [395.467006ms]
    Sep  1 11:48:33.541: INFO: Created: latency-svc-vbdr8
    Sep  1 11:48:33.552: INFO: Got endpoints: latency-svc-vbdr8 [403.393225ms]
    Sep  1 11:48:33.567: INFO: Created: latency-svc-87xhf
    Sep  1 11:48:33.583: INFO: Got endpoints: latency-svc-87xhf [389.128319ms]
    Sep  1 11:48:33.601: INFO: Created: latency-svc-twrfn
    Sep  1 11:48:33.615: INFO: Got endpoints: latency-svc-twrfn [392.144751ms]
    Sep  1 11:48:33.625: INFO: Created: latency-svc-ztb2s
    Sep  1 11:48:33.634: INFO: Got endpoints: latency-svc-ztb2s [381.321469ms]
    Sep  1 11:48:33.642: INFO: Created: latency-svc-x8fgd
    Sep  1 11:48:33.655: INFO: Got endpoints: latency-svc-x8fgd [382.794386ms]
    Sep  1 11:48:33.673: INFO: Created: latency-svc-bbq92
    Sep  1 11:48:33.680: INFO: Got endpoints: latency-svc-bbq92 [362.456029ms]
    Sep  1 11:48:33.690: INFO: Created: latency-svc-5l9b9
    Sep  1 11:48:33.698: INFO: Got endpoints: latency-svc-5l9b9 [352.419033ms]
    Sep  1 11:48:33.709: INFO: Created: latency-svc-xz54j
    Sep  1 11:48:33.718: INFO: Got endpoints: latency-svc-xz54j [347.24662ms]
    Sep  1 11:48:33.727: INFO: Created: latency-svc-zbbcw
    Sep  1 11:48:33.733: INFO: Got endpoints: latency-svc-zbbcw [340.271154ms]
    Sep  1 11:48:33.741: INFO: Created: latency-svc-s6zpr
    Sep  1 11:48:33.751: INFO: Got endpoints: latency-svc-s6zpr [327.994585ms]
    Sep  1 11:48:33.756: INFO: Created: latency-svc-5kfvl
    Sep  1 11:48:33.765: INFO: Got endpoints: latency-svc-5kfvl [325.365774ms]
    Sep  1 11:48:33.783: INFO: Created: latency-svc-vgl9x
    Sep  1 11:48:33.793: INFO: Got endpoints: latency-svc-vgl9x [314.353671ms]
    Sep  1 11:48:33.798: INFO: Created: latency-svc-wg9wb
    Sep  1 11:48:33.811: INFO: Got endpoints: latency-svc-wg9wb [306.895087ms]
    Sep  1 11:48:33.817: INFO: Created: latency-svc-7fnvp
    Sep  1 11:48:33.831: INFO: Got endpoints: latency-svc-7fnvp [309.600604ms]
    Sep  1 11:48:33.831: INFO: Created: latency-svc-jldxz
    Sep  1 11:48:33.837: INFO: Got endpoints: latency-svc-jldxz [284.681735ms]
    Sep  1 11:48:33.849: INFO: Created: latency-svc-z9mdz
    Sep  1 11:48:33.857: INFO: Got endpoints: latency-svc-z9mdz [273.920267ms]
    Sep  1 11:48:33.866: INFO: Created: latency-svc-fxsqk
    Sep  1 11:48:33.882: INFO: Got endpoints: latency-svc-fxsqk [266.262758ms]
    Sep  1 11:48:33.905: INFO: Created: latency-svc-9qmv6
    Sep  1 11:48:33.906: INFO: Created: latency-svc-w6hh5
    Sep  1 11:48:33.926: INFO: Got endpoints: latency-svc-w6hh5 [291.773661ms]
    Sep  1 11:48:33.934: INFO: Got endpoints: latency-svc-9qmv6 [645.088642ms]
    Sep  1 11:48:33.959: INFO: Created: latency-svc-tfh9g
    Sep  1 11:48:33.971: INFO: Got endpoints: latency-svc-tfh9g [315.445415ms]
    Sep  1 11:48:33.981: INFO: Created: latency-svc-vvv2g
    Sep  1 11:48:33.985: INFO: Got endpoints: latency-svc-vvv2g [304.805704ms]
    Sep  1 11:48:34.006: INFO: Created: latency-svc-vjdjd
    Sep  1 11:48:34.015: INFO: Got endpoints: latency-svc-vjdjd [316.684845ms]
    Sep  1 11:48:34.050: INFO: Created: latency-svc-s2d94
    Sep  1 11:48:34.077: INFO: Got endpoints: latency-svc-s2d94 [359.282981ms]
    Sep  1 11:48:34.085: INFO: Created: latency-svc-qtsdv
    Sep  1 11:48:34.109: INFO: Got endpoints: latency-svc-qtsdv [375.562246ms]
    Sep  1 11:48:34.122: INFO: Created: latency-svc-vzfvw
    Sep  1 11:48:34.128: INFO: Got endpoints: latency-svc-vzfvw [376.850313ms]
    Sep  1 11:48:34.153: INFO: Created: latency-svc-jww5j
    Sep  1 11:48:34.166: INFO: Got endpoints: latency-svc-jww5j [400.327521ms]
    Sep  1 11:48:34.178: INFO: Created: latency-svc-5k9x2
    Sep  1 11:48:34.184: INFO: Got endpoints: latency-svc-5k9x2 [390.972642ms]
    Sep  1 11:48:34.201: INFO: Created: latency-svc-kvtrk
    Sep  1 11:48:34.203: INFO: Got endpoints: latency-svc-kvtrk [392.053638ms]
    Sep  1 11:48:34.220: INFO: Created: latency-svc-lw6ml
    Sep  1 11:48:34.235: INFO: Got endpoints: latency-svc-lw6ml [403.11439ms]
    Sep  1 11:48:34.236: INFO: Created: latency-svc-xff6d
    Sep  1 11:48:34.245: INFO: Got endpoints: latency-svc-xff6d [407.906728ms]
    Sep  1 11:48:34.255: INFO: Created: latency-svc-rhlqj
    Sep  1 11:48:34.271: INFO: Got endpoints: latency-svc-rhlqj [414.142562ms]
    Sep  1 11:48:34.280: INFO: Created: latency-svc-fh28l
    Sep  1 11:48:34.285: INFO: Got endpoints: latency-svc-fh28l [402.473597ms]
    Sep  1 11:48:34.297: INFO: Created: latency-svc-dx47b
    Sep  1 11:48:34.311: INFO: Got endpoints: latency-svc-dx47b [384.926118ms]
    Sep  1 11:48:34.314: INFO: Created: latency-svc-8r4pc
    Sep  1 11:48:34.332: INFO: Got endpoints: latency-svc-8r4pc [397.882522ms]
    Sep  1 11:48:34.349: INFO: Created: latency-svc-xxbgc
    Sep  1 11:48:34.357: INFO: Got endpoints: latency-svc-xxbgc [371.1218ms]
    Sep  1 11:48:34.366: INFO: Created: latency-svc-8xcsx
    Sep  1 11:48:34.376: INFO: Got endpoints: latency-svc-8xcsx [360.172586ms]
    Sep  1 11:48:34.385: INFO: Created: latency-svc-2w96c
    Sep  1 11:48:34.397: INFO: Got endpoints: latency-svc-2w96c [319.20496ms]
    Sep  1 11:48:34.404: INFO: Created: latency-svc-26z7c
    Sep  1 11:48:34.442: INFO: Created: latency-svc-nwqcq
    Sep  1 11:48:34.445: INFO: Got endpoints: latency-svc-26z7c [335.804262ms]
    Sep  1 11:48:34.463: INFO: Created: latency-svc-v2q5q
    Sep  1 11:48:34.474: INFO: Created: latency-svc-5bnr2
    Sep  1 11:48:34.489: INFO: Created: latency-svc-px6lg
    Sep  1 11:48:34.496: INFO: Got endpoints: latency-svc-nwqcq [368.189795ms]
    Sep  1 11:48:34.505: INFO: Created: latency-svc-vm88q
    Sep  1 11:48:34.519: INFO: Created: latency-svc-6fs72
    Sep  1 11:48:34.530: INFO: Created: latency-svc-dv82b
    Sep  1 11:48:34.540: INFO: Got endpoints: latency-svc-v2q5q [373.941013ms]
    Sep  1 11:48:34.561: INFO: Created: latency-svc-4kgbp
    Sep  1 11:48:34.570: INFO: Created: latency-svc-hxlln
    Sep  1 11:48:34.578: INFO: Created: latency-svc-q4w98
    Sep  1 11:48:34.593: INFO: Created: latency-svc-7zv6c
    Sep  1 11:48:34.596: INFO: Got endpoints: latency-svc-5bnr2 [411.643288ms]
    Sep  1 11:48:34.609: INFO: Created: latency-svc-nhj6h
    Sep  1 11:48:34.624: INFO: Created: latency-svc-thj9r
    Sep  1 11:48:34.629: INFO: Created: latency-svc-qwf94
    Sep  1 11:48:34.641: INFO: Got endpoints: latency-svc-px6lg [438.474929ms]
    Sep  1 11:48:34.648: INFO: Created: latency-svc-762lc
    Sep  1 11:48:34.668: INFO: Created: latency-svc-j42c4
    Sep  1 11:48:34.680: INFO: Created: latency-svc-tqltc
    Sep  1 11:48:34.694: INFO: Created: latency-svc-5fj5d
    Sep  1 11:48:34.697: INFO: Got endpoints: latency-svc-vm88q [462.341084ms]
    Sep  1 11:48:34.705: INFO: Created: latency-svc-n2hql
    Sep  1 11:48:34.718: INFO: Created: latency-svc-z2njg
    Sep  1 11:48:34.743: INFO: Got endpoints: latency-svc-6fs72 [497.790335ms]
    Sep  1 11:48:34.762: INFO: Created: latency-svc-r7lxl
    Sep  1 11:48:34.793: INFO: Got endpoints: latency-svc-dv82b [520.456832ms]
    Sep  1 11:48:34.832: INFO: Created: latency-svc-mmxwm
    Sep  1 11:48:34.842: INFO: Got endpoints: latency-svc-4kgbp [556.217139ms]
    Sep  1 11:48:34.861: INFO: Created: latency-svc-2rxjk
    Sep  1 11:48:34.891: INFO: Got endpoints: latency-svc-hxlln [919.065302ms]
    Sep  1 11:48:34.912: INFO: Created: latency-svc-7gclf
    Sep  1 11:48:34.943: INFO: Got endpoints: latency-svc-q4w98 [632.057543ms]
    Sep  1 11:48:34.968: INFO: Created: latency-svc-h828m
    Sep  1 11:48:35.000: INFO: Got endpoints: latency-svc-7zv6c [666.502133ms]
    Sep  1 11:48:35.019: INFO: Created: latency-svc-pp2q5
    Sep  1 11:48:35.045: INFO: Got endpoints: latency-svc-nhj6h [688.161851ms]
    Sep  1 11:48:35.068: INFO: Created: latency-svc-nwfxd
    Sep  1 11:48:35.096: INFO: Got endpoints: latency-svc-thj9r [720.21816ms]
    Sep  1 11:48:35.114: INFO: Created: latency-svc-d462p
    Sep  1 11:48:35.139: INFO: Got endpoints: latency-svc-qwf94 [741.615697ms]
    Sep  1 11:48:35.155: INFO: Created: latency-svc-bjm2m
    Sep  1 11:48:35.192: INFO: Got endpoints: latency-svc-762lc [746.834858ms]
    Sep  1 11:48:35.213: INFO: Created: latency-svc-mlx6c
    Sep  1 11:48:35.245: INFO: Got endpoints: latency-svc-j42c4 [749.048363ms]
    Sep  1 11:48:35.263: INFO: Created: latency-svc-bg4fn
    Sep  1 11:48:35.294: INFO: Got endpoints: latency-svc-tqltc [753.044738ms]
    Sep  1 11:48:35.311: INFO: Created: latency-svc-4587v
    Sep  1 11:48:35.347: INFO: Got endpoints: latency-svc-5fj5d [750.837149ms]
    Sep  1 11:48:35.364: INFO: Created: latency-svc-j6lcr
    Sep  1 11:48:35.389: INFO: Got endpoints: latency-svc-n2hql [747.499957ms]
    Sep  1 11:48:35.416: INFO: Created: latency-svc-vqvhq
    Sep  1 11:48:35.440: INFO: Got endpoints: latency-svc-z2njg [742.338288ms]
    Sep  1 11:48:35.456: INFO: Created: latency-svc-sldwf
    Sep  1 11:48:35.493: INFO: Got endpoints: latency-svc-r7lxl [749.879648ms]
    Sep  1 11:48:35.514: INFO: Created: latency-svc-9r7nt
    Sep  1 11:48:35.546: INFO: Got endpoints: latency-svc-mmxwm [752.797961ms]
    Sep  1 11:48:35.564: INFO: Created: latency-svc-kl6pt
    Sep  1 11:48:35.594: INFO: Got endpoints: latency-svc-2rxjk [752.239284ms]
    Sep  1 11:48:35.611: INFO: Created: latency-svc-zjjv5
    Sep  1 11:48:35.641: INFO: Got endpoints: latency-svc-7gclf [749.852026ms]
    Sep  1 11:48:35.666: INFO: Created: latency-svc-cp5pp
    Sep  1 11:48:35.693: INFO: Got endpoints: latency-svc-h828m [749.92519ms]
    Sep  1 11:48:35.713: INFO: Created: latency-svc-vp447
    Sep  1 11:48:35.740: INFO: Got endpoints: latency-svc-pp2q5 [740.019646ms]
    Sep  1 11:48:35.769: INFO: Created: latency-svc-c4zvx
    Sep  1 11:48:35.792: INFO: Got endpoints: latency-svc-nwfxd [745.905246ms]
    Sep  1 11:48:35.810: INFO: Created: latency-svc-7dw72
    Sep  1 11:48:35.843: INFO: Got endpoints: latency-svc-d462p [746.486977ms]
    Sep  1 11:48:35.862: INFO: Created: latency-svc-8swft
    Sep  1 11:48:35.889: INFO: Got endpoints: latency-svc-bjm2m [749.799959ms]
    Sep  1 11:48:35.913: INFO: Created: latency-svc-5tcgv
    Sep  1 11:48:35.949: INFO: Got endpoints: latency-svc-mlx6c [756.950439ms]
    Sep  1 11:48:35.975: INFO: Created: latency-svc-h7szd
    Sep  1 11:48:35.993: INFO: Got endpoints: latency-svc-bg4fn [747.704254ms]
    Sep  1 11:48:36.015: INFO: Created: latency-svc-sc49t
    Sep  1 11:48:36.042: INFO: Got endpoints: latency-svc-4587v [748.512012ms]
    Sep  1 11:48:36.069: INFO: Created: latency-svc-kl6vq
    Sep  1 11:48:36.099: INFO: Got endpoints: latency-svc-j6lcr [752.357652ms]
    Sep  1 11:48:36.123: INFO: Created: latency-svc-8sxcq
    Sep  1 11:48:36.148: INFO: Got endpoints: latency-svc-vqvhq [758.323465ms]
    Sep  1 11:48:36.197: INFO: Created: latency-svc-2dr98
    Sep  1 11:48:36.218: INFO: Got endpoints: latency-svc-sldwf [777.791752ms]
    Sep  1 11:48:36.256: INFO: Got endpoints: latency-svc-9r7nt [762.486241ms]
    Sep  1 11:48:36.257: INFO: Created: latency-svc-p49jh
    Sep  1 11:48:36.301: INFO: Got endpoints: latency-svc-kl6pt [755.611098ms]
    Sep  1 11:48:36.308: INFO: Created: latency-svc-kjlzw
    Sep  1 11:48:36.343: INFO: Got endpoints: latency-svc-zjjv5 [748.987432ms]
    Sep  1 11:48:36.356: INFO: Created: latency-svc-jwlgs
    Sep  1 11:48:36.377: INFO: Created: latency-svc-fcqx5
    Sep  1 11:48:36.397: INFO: Got endpoints: latency-svc-cp5pp [756.471662ms]
    Sep  1 11:48:36.426: INFO: Created: latency-svc-xg8ts
    Sep  1 11:48:36.447: INFO: Got endpoints: latency-svc-vp447 [753.252303ms]
    Sep  1 11:48:36.474: INFO: Created: latency-svc-5l228
    Sep  1 11:48:36.500: INFO: Got endpoints: latency-svc-c4zvx [760.034795ms]
    Sep  1 11:48:36.570: INFO: Got endpoints: latency-svc-7dw72 [778.653916ms]
    Sep  1 11:48:36.571: INFO: Created: latency-svc-gd6rs
    Sep  1 11:48:36.594: INFO: Got endpoints: latency-svc-8swft [750.996933ms]
    Sep  1 11:48:36.602: INFO: Created: latency-svc-2vwn8
    Sep  1 11:48:36.628: INFO: Created: latency-svc-lvkxt
    Sep  1 11:48:36.695: INFO: Got endpoints: latency-svc-5tcgv [805.461806ms]
    Sep  1 11:48:36.759: INFO: Got endpoints: latency-svc-h7szd [809.535551ms]
    Sep  1 11:48:36.807: INFO: Got endpoints: latency-svc-sc49t [813.353241ms]
    Sep  1 11:48:36.838: INFO: Got endpoints: latency-svc-kl6vq [795.488784ms]
    Sep  1 11:48:36.838: INFO: Created: latency-svc-v8kbx
    Sep  1 11:48:36.909: INFO: Created: latency-svc-xcqwv
    Sep  1 11:48:36.922: INFO: Got endpoints: latency-svc-8sxcq [822.185751ms]
    Sep  1 11:48:36.937: INFO: Got endpoints: latency-svc-2dr98 [789.288178ms]
    Sep  1 11:48:36.995: INFO: Created: latency-svc-kn9dw
    Sep  1 11:48:37.002: INFO: Got endpoints: latency-svc-p49jh [784.638059ms]
    Sep  1 11:48:37.018: INFO: Created: latency-svc-26rg4
    Sep  1 11:48:37.041: INFO: Got endpoints: latency-svc-kjlzw [784.679076ms]
    Sep  1 11:48:37.091: INFO: Got endpoints: latency-svc-jwlgs [788.709899ms]
    Sep  1 11:48:37.104: INFO: Created: latency-svc-vvksq
    Sep  1 11:48:37.109: INFO: Got endpoints: latency-svc-fcqx5 [766.212679ms]
    Sep  1 11:48:37.198: INFO: Created: latency-svc-r92pd
    Sep  1 11:48:37.200: INFO: Got endpoints: latency-svc-xg8ts [802.256377ms]
    Sep  1 11:48:37.241: INFO: Got endpoints: latency-svc-5l228 [793.986362ms]
    Sep  1 11:48:37.268: INFO: Created: latency-svc-pdvvt
    Sep  1 11:48:37.317: INFO: Created: latency-svc-c6jr8
    Sep  1 11:48:37.332: INFO: Got endpoints: latency-svc-gd6rs [831.414282ms]
    Sep  1 11:48:37.356: INFO: Got endpoints: latency-svc-2vwn8 [785.521446ms]
    Sep  1 11:48:37.419: INFO: Got endpoints: latency-svc-lvkxt [824.869806ms]
    Sep  1 11:48:37.419: INFO: Created: latency-svc-d4jdm
    Sep  1 11:48:37.462: INFO: Got endpoints: latency-svc-v8kbx [767.472941ms]
    Sep  1 11:48:37.463: INFO: Got endpoints: latency-svc-xcqwv [703.560865ms]
    Sep  1 11:48:37.508: INFO: Created: latency-svc-nmj8z
    Sep  1 11:48:37.537: INFO: Got endpoints: latency-svc-kn9dw [729.141618ms]
    Sep  1 11:48:37.572: INFO: Got endpoints: latency-svc-26rg4 [733.485361ms]
    Sep  1 11:48:37.590: INFO: Created: latency-svc-w5lgz
    Sep  1 11:48:37.608: INFO: Got endpoints: latency-svc-vvksq [685.908761ms]
    Sep  1 11:48:37.630: INFO: Created: latency-svc-xlgrn
    Sep  1 11:48:37.678: INFO: Created: latency-svc-qplrl
    Sep  1 11:48:37.680: INFO: Got endpoints: latency-svc-r92pd [743.14097ms]
    Sep  1 11:48:37.713: INFO: Got endpoints: latency-svc-pdvvt [709.718004ms]
    Sep  1 11:48:37.731: INFO: Created: latency-svc-g86tn
    Sep  1 11:48:37.752: INFO: Got endpoints: latency-svc-c6jr8 [711.20954ms]
    Sep  1 11:48:37.768: INFO: Created: latency-svc-ffqpl
    Sep  1 11:48:37.782: INFO: Created: latency-svc-d2tjb
    Sep  1 11:48:37.801: INFO: Got endpoints: latency-svc-d4jdm [709.964705ms]
    Sep  1 11:48:37.826: INFO: Created: latency-svc-sqqdt
    Sep  1 11:48:37.838: INFO: Created: latency-svc-nflkg
    Sep  1 11:48:37.844: INFO: Got endpoints: latency-svc-nmj8z [734.846088ms]
    Sep  1 11:48:37.863: INFO: Created: latency-svc-hwbd8
    Sep  1 11:48:37.922: INFO: Got endpoints: latency-svc-w5lgz [722.430353ms]
    Sep  1 11:48:37.928: INFO: Created: latency-svc-sgwb5
    Sep  1 11:48:37.949: INFO: Created: latency-svc-qbkjz
    Sep  1 11:48:37.950: INFO: Got endpoints: latency-svc-xlgrn [709.71899ms]
    Sep  1 11:48:37.966: INFO: Created: latency-svc-cf2k9
    Sep  1 11:48:37.979: INFO: Created: latency-svc-8bdlc
    Sep  1 11:48:37.994: INFO: Got endpoints: latency-svc-qplrl [662.033759ms]
    Sep  1 11:48:38.005: INFO: Created: latency-svc-kbv54
    Sep  1 11:48:38.034: INFO: Created: latency-svc-bjggp
    Sep  1 11:48:38.045: INFO: Got endpoints: latency-svc-g86tn [688.981391ms]
    Sep  1 11:48:38.048: INFO: Created: latency-svc-c8gdh
    Sep  1 11:48:38.073: INFO: Created: latency-svc-stgtp
    Sep  1 11:48:38.078: INFO: Created: latency-svc-2vrjq
    Sep  1 11:48:38.094: INFO: Got endpoints: latency-svc-ffqpl [674.327299ms]
    Sep  1 11:48:38.103: INFO: Created: latency-svc-7fgft
    Sep  1 11:48:38.116: INFO: Created: latency-svc-q2ldh
    Sep  1 11:48:38.145: INFO: Got endpoints: latency-svc-d2tjb [682.101168ms]
    Sep  1 11:48:38.162: INFO: Created: latency-svc-wjvwc
    Sep  1 11:48:38.197: INFO: Got endpoints: latency-svc-sqqdt [734.595433ms]
    Sep  1 11:48:38.216: INFO: Created: latency-svc-rczgn
    Sep  1 11:48:38.249: INFO: Got endpoints: latency-svc-nflkg [712.076641ms]
    Sep  1 11:48:38.277: INFO: Created: latency-svc-g46qq
    Sep  1 11:48:38.290: INFO: Got endpoints: latency-svc-hwbd8 [718.607871ms]
    Sep  1 11:48:38.308: INFO: Created: latency-svc-l4w5f
    Sep  1 11:48:38.339: INFO: Got endpoints: latency-svc-sgwb5 [731.412514ms]
    Sep  1 11:48:38.356: INFO: Created: latency-svc-2scjl
    Sep  1 11:48:38.392: INFO: Got endpoints: latency-svc-qbkjz [711.934823ms]
    Sep  1 11:48:38.406: INFO: Created: latency-svc-gm922
    Sep  1 11:48:38.440: INFO: Got endpoints: latency-svc-cf2k9 [727.251313ms]
    Sep  1 11:48:38.460: INFO: Created: latency-svc-9mmlp
    Sep  1 11:48:38.491: INFO: Got endpoints: latency-svc-8bdlc [737.468769ms]
    Sep  1 11:48:38.506: INFO: Created: latency-svc-mmtpg
    Sep  1 11:48:38.544: INFO: Got endpoints: latency-svc-kbv54 [743.00869ms]
    Sep  1 11:48:38.558: INFO: Created: latency-svc-58q47
    Sep  1 11:48:38.589: INFO: Got endpoints: latency-svc-bjggp [744.959304ms]
    Sep  1 11:48:38.607: INFO: Created: latency-svc-c92cf
    Sep  1 11:48:38.639: INFO: Got endpoints: latency-svc-c8gdh [716.525151ms]
    Sep  1 11:48:38.661: INFO: Created: latency-svc-lbxn9
    Sep  1 11:48:38.693: INFO: Got endpoints: latency-svc-stgtp [741.779368ms]
    Sep  1 11:48:38.711: INFO: Created: latency-svc-gfdgn
    Sep  1 11:48:38.742: INFO: Got endpoints: latency-svc-2vrjq [747.229086ms]
    Sep  1 11:48:38.757: INFO: Created: latency-svc-vx52p
    Sep  1 11:48:38.793: INFO: Got endpoints: latency-svc-7fgft [746.694688ms]
    Sep  1 11:48:38.809: INFO: Created: latency-svc-lswq8
    Sep  1 11:48:38.839: INFO: Got endpoints: latency-svc-q2ldh [745.740846ms]
    Sep  1 11:48:38.856: INFO: Created: latency-svc-s7rsh
    Sep  1 11:48:38.890: INFO: Got endpoints: latency-svc-wjvwc [745.029815ms]
    Sep  1 11:48:38.941: INFO: Created: latency-svc-l5zr4
    Sep  1 11:48:38.945: INFO: Got endpoints: latency-svc-rczgn [747.600438ms]
    Sep  1 11:48:38.968: INFO: Created: latency-svc-mqlnw
    Sep  1 11:48:39.001: INFO: Got endpoints: latency-svc-g46qq [751.5592ms]
    Sep  1 11:48:39.022: INFO: Created: latency-svc-k2vtl
    Sep  1 11:48:39.043: INFO: Got endpoints: latency-svc-l4w5f [752.46665ms]
    Sep  1 11:48:39.074: INFO: Created: latency-svc-dhrmj
    Sep  1 11:48:39.107: INFO: Got endpoints: latency-svc-2scjl [767.001691ms]
    Sep  1 11:48:39.129: INFO: Created: latency-svc-dhpxs
    Sep  1 11:48:39.142: INFO: Got endpoints: latency-svc-gm922 [749.868256ms]
    Sep  1 11:48:39.178: INFO: Created: latency-svc-9qr5p
    Sep  1 11:48:39.194: INFO: Got endpoints: latency-svc-9mmlp [753.566126ms]
    Sep  1 11:48:39.217: INFO: Created: latency-svc-244zg
    Sep  1 11:48:39.240: INFO: Got endpoints: latency-svc-mmtpg [748.813331ms]
    Sep  1 11:48:39.255: INFO: Created: latency-svc-tnpzf
    Sep  1 11:48:39.290: INFO: Got endpoints: latency-svc-58q47 [745.692192ms]
    Sep  1 11:48:39.307: INFO: Created: latency-svc-sldvw
    Sep  1 11:48:39.340: INFO: Got endpoints: latency-svc-c92cf [750.65101ms]
    Sep  1 11:48:39.355: INFO: Created: latency-svc-dkxsn
    Sep  1 11:48:39.390: INFO: Got endpoints: latency-svc-lbxn9 [750.254785ms]
    Sep  1 11:48:39.410: INFO: Created: latency-svc-tgvdw
    Sep  1 11:48:39.441: INFO: Got endpoints: latency-svc-gfdgn [748.505188ms]
    Sep  1 11:48:39.461: INFO: Created: latency-svc-k89dl
    Sep  1 11:48:39.492: INFO: Got endpoints: latency-svc-vx52p [750.132065ms]
    Sep  1 11:48:39.510: INFO: Created: latency-svc-24jpz
    Sep  1 11:48:39.542: INFO: Got endpoints: latency-svc-lswq8 [748.747004ms]
    Sep  1 11:48:39.558: INFO: Created: latency-svc-vjqrg
    Sep  1 11:48:39.594: INFO: Got endpoints: latency-svc-s7rsh [755.142112ms]
    Sep  1 11:48:39.610: INFO: Created: latency-svc-6rljs
    Sep  1 11:48:39.640: INFO: Got endpoints: latency-svc-l5zr4 [749.988899ms]
    Sep  1 11:48:39.655: INFO: Created: latency-svc-7wfw5
    Sep  1 11:48:39.690: INFO: Got endpoints: latency-svc-mqlnw [744.551218ms]
    Sep  1 11:48:39.705: INFO: Created: latency-svc-xdzpz
    Sep  1 11:48:39.742: INFO: Got endpoints: latency-svc-k2vtl [741.070927ms]
    Sep  1 11:48:39.763: INFO: Created: latency-svc-k9fmn
    Sep  1 11:48:39.789: INFO: Got endpoints: latency-svc-dhrmj [745.507083ms]
    Sep  1 11:48:39.806: INFO: Created: latency-svc-rw7cg
    Sep  1 11:48:39.841: INFO: Got endpoints: latency-svc-dhpxs [734.191151ms]
    Sep  1 11:48:39.860: INFO: Created: latency-svc-4sndj
    Sep  1 11:48:39.892: INFO: Got endpoints: latency-svc-9qr5p [749.180656ms]
    Sep  1 11:48:39.925: INFO: Created: latency-svc-2j2rs
    Sep  1 11:48:39.943: INFO: Got endpoints: latency-svc-244zg [749.029101ms]
    Sep  1 11:48:39.970: INFO: Created: latency-svc-rpvwf
    Sep  1 11:48:39.999: INFO: Got endpoints: latency-svc-tnpzf [758.864945ms]
    Sep  1 11:48:40.017: INFO: Created: latency-svc-md9n2
    Sep  1 11:48:40.045: INFO: Got endpoints: latency-svc-sldvw [755.109626ms]
    Sep  1 11:48:40.078: INFO: Created: latency-svc-5nmhk
    Sep  1 11:48:40.096: INFO: Got endpoints: latency-svc-dkxsn [755.371225ms]
    Sep  1 11:48:40.121: INFO: Created: latency-svc-l2x7v
    Sep  1 11:48:40.142: INFO: Got endpoints: latency-svc-tgvdw [752.454524ms]
    Sep  1 11:48:40.157: INFO: Created: latency-svc-2f8wf
    Sep  1 11:48:40.190: INFO: Got endpoints: latency-svc-k89dl [748.175372ms]
    Sep  1 11:48:40.223: INFO: Created: latency-svc-9thqq
    Sep  1 11:48:40.242: INFO: Got endpoints: latency-svc-24jpz [749.866774ms]
    Sep  1 11:48:40.258: INFO: Created: latency-svc-psz4g
    Sep  1 11:48:40.289: INFO: Got endpoints: latency-svc-vjqrg [747.295247ms]
    Sep  1 11:48:40.305: INFO: Created: latency-svc-4ccp8
    Sep  1 11:48:40.340: INFO: Got endpoints: latency-svc-6rljs [745.451867ms]
    Sep  1 11:48:40.354: INFO: Created: latency-svc-jjw4j
    Sep  1 11:48:40.391: INFO: Got endpoints: latency-svc-7wfw5 [751.186309ms]
    Sep  1 11:48:40.407: INFO: Created: latency-svc-tctrp
    Sep  1 11:48:40.443: INFO: Got endpoints: latency-svc-xdzpz [753.014219ms]
    Sep  1 11:48:40.459: INFO: Created: latency-svc-gpdkz
    Sep  1 11:48:40.493: INFO: Got endpoints: latency-svc-k9fmn [751.068819ms]
    Sep  1 11:48:40.509: INFO: Created: latency-svc-rtmnz
    Sep  1 11:48:40.539: INFO: Got endpoints: latency-svc-rw7cg [749.359802ms]
    Sep  1 11:48:40.556: INFO: Created: latency-svc-fz5q4
    Sep  1 11:48:40.591: INFO: Got endpoints: latency-svc-4sndj [749.458429ms]
    Sep  1 11:48:40.606: INFO: Created: latency-svc-lhxsb
    Sep  1 11:48:40.641: INFO: Got endpoints: latency-svc-2j2rs [749.370528ms]
    Sep  1 11:48:40.656: INFO: Created: latency-svc-7bj5l
    Sep  1 11:48:40.690: INFO: Got endpoints: latency-svc-rpvwf [745.935683ms]
    Sep  1 11:48:40.718: INFO: Created: latency-svc-wtq9t
    Sep  1 11:48:40.740: INFO: Got endpoints: latency-svc-md9n2 [741.495608ms]
    Sep  1 11:48:40.789: INFO: Got endpoints: latency-svc-5nmhk [743.429589ms]
    Sep  1 11:48:40.842: INFO: Got endpoints: latency-svc-l2x7v [745.536966ms]
    Sep  1 11:48:40.892: INFO: Got endpoints: latency-svc-2f8wf [749.454127ms]
    Sep  1 11:48:40.946: INFO: Got endpoints: latency-svc-9thqq [756.050227ms]
    Sep  1 11:48:40.994: INFO: Got endpoints: latency-svc-psz4g [752.016068ms]
    Sep  1 11:48:41.047: INFO: Got endpoints: latency-svc-4ccp8 [757.929995ms]
    Sep  1 11:48:41.096: INFO: Got endpoints: latency-svc-jjw4j [755.546601ms]
    Sep  1 11:48:41.141: INFO: Got endpoints: latency-svc-tctrp [749.759706ms]
    Sep  1 11:48:41.190: INFO: Got endpoints: latency-svc-gpdkz [746.429066ms]
    Sep  1 11:48:41.239: INFO: Got endpoints: latency-svc-rtmnz [745.472121ms]
    Sep  1 11:48:41.293: INFO: Got endpoints: latency-svc-fz5q4 [753.814404ms]
    Sep  1 11:48:41.340: INFO: Got endpoints: latency-svc-lhxsb [748.817745ms]
    Sep  1 11:48:41.393: INFO: Got endpoints: latency-svc-7bj5l [751.786426ms]
    Sep  1 11:48:41.447: INFO: Got endpoints: latency-svc-wtq9t [756.327917ms]
    Sep  1 11:48:41.448: INFO: Latencies: [41.332318ms 59.146858ms 102.864374ms 119.564088ms 171.36969ms 192.585101ms 223.273317ms 246.123996ms 266.262758ms 273.920267ms 284.681735ms 290.812774ms 291.773661ms 304.805704ms 306.895087ms 309.600604ms 314.353671ms 315.445415ms 316.684845ms 319.20496ms 319.802939ms 325.365774ms 327.994585ms 335.804262ms 340.271154ms 347.24662ms 349.588621ms 352.419033ms 359.282981ms 360.172586ms 362.456029ms 368.189795ms 368.979794ms 371.1218ms 373.941013ms 375.562246ms 376.850313ms 381.321469ms 382.794386ms 384.926118ms 385.514104ms 389.128319ms 390.972642ms 392.053638ms 392.144751ms 395.467006ms 397.882522ms 400.327521ms 402.473597ms 403.11439ms 403.393225ms 403.96761ms 407.825842ms 407.906728ms 411.643288ms 413.687838ms 414.142562ms 416.190931ms 417.498597ms 428.055256ms 431.691365ms 438.474929ms 441.992026ms 462.341084ms 497.790335ms 520.456832ms 556.217139ms 632.057543ms 645.088642ms 662.033759ms 666.502133ms 674.327299ms 682.101168ms 685.908761ms 688.161851ms 688.981391ms 703.560865ms 709.718004ms 709.71899ms 709.964705ms 711.20954ms 711.934823ms 712.076641ms 716.525151ms 718.607871ms 720.21816ms 722.430353ms 727.251313ms 729.141618ms 731.412514ms 733.485361ms 734.191151ms 734.595433ms 734.846088ms 737.468769ms 740.019646ms 741.070927ms 741.495608ms 741.615697ms 741.779368ms 742.338288ms 743.00869ms 743.14097ms 743.429589ms 744.551218ms 744.959304ms 745.029815ms 745.451867ms 745.472121ms 745.507083ms 745.536966ms 745.692192ms 745.740846ms 745.905246ms 745.935683ms 746.429066ms 746.486977ms 746.694688ms 746.834858ms 747.229086ms 747.295247ms 747.499957ms 747.600438ms 747.704254ms 748.175372ms 748.505188ms 748.512012ms 748.747004ms 748.813331ms 748.817745ms 748.987432ms 749.029101ms 749.048363ms 749.180656ms 749.359802ms 749.370528ms 749.454127ms 749.458429ms 749.759706ms 749.799959ms 749.852026ms 749.866774ms 749.868256ms 749.879648ms 749.92519ms 749.988899ms 750.132065ms 750.254785ms 750.65101ms 750.837149ms 750.996933ms 751.068819ms 751.186309ms 751.5592ms 751.786426ms 752.016068ms 752.239284ms 752.357652ms 752.454524ms 752.46665ms 752.797961ms 753.014219ms 753.044738ms 753.252303ms 753.566126ms 753.814404ms 755.109626ms 755.142112ms 755.371225ms 755.546601ms 755.611098ms 756.050227ms 756.327917ms 756.471662ms 756.950439ms 757.929995ms 758.323465ms 758.864945ms 760.034795ms 762.486241ms 766.212679ms 767.001691ms 767.472941ms 777.791752ms 778.653916ms 784.638059ms 784.679076ms 785.521446ms 788.709899ms 789.288178ms 793.986362ms 795.488784ms 802.256377ms 805.461806ms 809.535551ms 813.353241ms 822.185751ms 824.869806ms 831.414282ms 919.065302ms]
    Sep  1 11:48:41.448: INFO: 50 %ile: 742.338288ms
    Sep  1 11:48:41.448: INFO: 90 %ile: 766.212679ms
    Sep  1 11:48:41.448: INFO: 99 %ile: 831.414282ms
    Sep  1 11:48:41.448: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:48:41.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-7813" for this suite. 09/01/23 11:48:41.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:48:41.512
Sep  1 11:48:41.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 11:48:41.513
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:48:41.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:48:41.535
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-2249 09/01/23 11:48:41.539
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2249 to expose endpoints map[] 09/01/23 11:48:41.552
Sep  1 11:48:41.555: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Sep  1 11:48:42.561: INFO: successfully validated that service multi-endpoint-test in namespace services-2249 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2249 09/01/23 11:48:42.562
Sep  1 11:48:42.569: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2249" to be "running and ready"
Sep  1 11:48:42.574: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079162ms
Sep  1 11:48:42.574: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:48:44.577: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007787581s
Sep  1 11:48:44.577: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  1 11:48:44.577: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2249 to expose endpoints map[pod1:[100]] 09/01/23 11:48:44.58
Sep  1 11:48:44.588: INFO: successfully validated that service multi-endpoint-test in namespace services-2249 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2249 09/01/23 11:48:44.588
Sep  1 11:48:44.594: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2249" to be "running and ready"
Sep  1 11:48:44.597: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.472033ms
Sep  1 11:48:44.597: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:48:46.602: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00790745s
Sep  1 11:48:46.602: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  1 11:48:46.602: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2249 to expose endpoints map[pod1:[100] pod2:[101]] 09/01/23 11:48:46.607
Sep  1 11:48:46.623: INFO: successfully validated that service multi-endpoint-test in namespace services-2249 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 09/01/23 11:48:46.623
Sep  1 11:48:46.623: INFO: Creating new exec pod
Sep  1 11:48:46.632: INFO: Waiting up to 5m0s for pod "execpodlj9zm" in namespace "services-2249" to be "running"
Sep  1 11:48:46.638: INFO: Pod "execpodlj9zm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.191214ms
Sep  1 11:48:48.693: INFO: Pod "execpodlj9zm": Phase="Running", Reason="", readiness=true. Elapsed: 2.060756688s
Sep  1 11:48:48.694: INFO: Pod "execpodlj9zm" satisfied condition "running"
Sep  1 11:48:49.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-2249 exec execpodlj9zm -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Sep  1 11:48:49.926: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Sep  1 11:48:49.926: INFO: stdout: ""
Sep  1 11:48:49.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-2249 exec execpodlj9zm -- /bin/sh -x -c nc -v -z -w 2 10.103.44.214 80'
Sep  1 11:48:50.207: INFO: stderr: "+ nc -v -z -w 2 10.103.44.214 80\nConnection to 10.103.44.214 80 port [tcp/http] succeeded!\n"
Sep  1 11:48:50.207: INFO: stdout: ""
Sep  1 11:48:50.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-2249 exec execpodlj9zm -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Sep  1 11:48:50.432: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Sep  1 11:48:50.432: INFO: stdout: ""
Sep  1 11:48:50.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-2249 exec execpodlj9zm -- /bin/sh -x -c nc -v -z -w 2 10.103.44.214 81'
Sep  1 11:48:50.647: INFO: stderr: "+ nc -v -z -w 2 10.103.44.214 81\nConnection to 10.103.44.214 81 port [tcp/*] succeeded!\n"
Sep  1 11:48:50.647: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-2249 09/01/23 11:48:50.647
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2249 to expose endpoints map[pod2:[101]] 09/01/23 11:48:50.675
Sep  1 11:48:54.706: INFO: Unexpected endpoints: found map[69e6bdf4-3d38-43d8-8dbf-6fd2a9943348:[101] 7034e803-871c-4b7b-b3d1-610e6ce1f21d:[100]], expected map[pod2:[101]], will retry
Sep  1 11:48:59.703: INFO: Unexpected endpoints: found map[69e6bdf4-3d38-43d8-8dbf-6fd2a9943348:[101] 7034e803-871c-4b7b-b3d1-610e6ce1f21d:[100]], expected map[pod2:[101]], will retry
Sep  1 11:49:01.708: INFO: successfully validated that service multi-endpoint-test in namespace services-2249 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2249 09/01/23 11:49:01.709
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2249 to expose endpoints map[] 09/01/23 11:49:01.726
Sep  1 11:49:02.777: INFO: successfully validated that service multi-endpoint-test in namespace services-2249 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 11:49:02.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2249" for this suite. 09/01/23 11:49:02.816
------------------------------
• [SLOW TEST] [21.317 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:48:41.512
    Sep  1 11:48:41.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 11:48:41.513
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:48:41.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:48:41.535
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-2249 09/01/23 11:48:41.539
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2249 to expose endpoints map[] 09/01/23 11:48:41.552
    Sep  1 11:48:41.555: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Sep  1 11:48:42.561: INFO: successfully validated that service multi-endpoint-test in namespace services-2249 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2249 09/01/23 11:48:42.562
    Sep  1 11:48:42.569: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2249" to be "running and ready"
    Sep  1 11:48:42.574: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079162ms
    Sep  1 11:48:42.574: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:48:44.577: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007787581s
    Sep  1 11:48:44.577: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  1 11:48:44.577: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2249 to expose endpoints map[pod1:[100]] 09/01/23 11:48:44.58
    Sep  1 11:48:44.588: INFO: successfully validated that service multi-endpoint-test in namespace services-2249 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-2249 09/01/23 11:48:44.588
    Sep  1 11:48:44.594: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2249" to be "running and ready"
    Sep  1 11:48:44.597: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.472033ms
    Sep  1 11:48:44.597: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:48:46.602: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00790745s
    Sep  1 11:48:46.602: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  1 11:48:46.602: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2249 to expose endpoints map[pod1:[100] pod2:[101]] 09/01/23 11:48:46.607
    Sep  1 11:48:46.623: INFO: successfully validated that service multi-endpoint-test in namespace services-2249 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 09/01/23 11:48:46.623
    Sep  1 11:48:46.623: INFO: Creating new exec pod
    Sep  1 11:48:46.632: INFO: Waiting up to 5m0s for pod "execpodlj9zm" in namespace "services-2249" to be "running"
    Sep  1 11:48:46.638: INFO: Pod "execpodlj9zm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.191214ms
    Sep  1 11:48:48.693: INFO: Pod "execpodlj9zm": Phase="Running", Reason="", readiness=true. Elapsed: 2.060756688s
    Sep  1 11:48:48.694: INFO: Pod "execpodlj9zm" satisfied condition "running"
    Sep  1 11:48:49.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-2249 exec execpodlj9zm -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Sep  1 11:48:49.926: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Sep  1 11:48:49.926: INFO: stdout: ""
    Sep  1 11:48:49.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-2249 exec execpodlj9zm -- /bin/sh -x -c nc -v -z -w 2 10.103.44.214 80'
    Sep  1 11:48:50.207: INFO: stderr: "+ nc -v -z -w 2 10.103.44.214 80\nConnection to 10.103.44.214 80 port [tcp/http] succeeded!\n"
    Sep  1 11:48:50.207: INFO: stdout: ""
    Sep  1 11:48:50.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-2249 exec execpodlj9zm -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Sep  1 11:48:50.432: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Sep  1 11:48:50.432: INFO: stdout: ""
    Sep  1 11:48:50.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-2249 exec execpodlj9zm -- /bin/sh -x -c nc -v -z -w 2 10.103.44.214 81'
    Sep  1 11:48:50.647: INFO: stderr: "+ nc -v -z -w 2 10.103.44.214 81\nConnection to 10.103.44.214 81 port [tcp/*] succeeded!\n"
    Sep  1 11:48:50.647: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-2249 09/01/23 11:48:50.647
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2249 to expose endpoints map[pod2:[101]] 09/01/23 11:48:50.675
    Sep  1 11:48:54.706: INFO: Unexpected endpoints: found map[69e6bdf4-3d38-43d8-8dbf-6fd2a9943348:[101] 7034e803-871c-4b7b-b3d1-610e6ce1f21d:[100]], expected map[pod2:[101]], will retry
    Sep  1 11:48:59.703: INFO: Unexpected endpoints: found map[69e6bdf4-3d38-43d8-8dbf-6fd2a9943348:[101] 7034e803-871c-4b7b-b3d1-610e6ce1f21d:[100]], expected map[pod2:[101]], will retry
    Sep  1 11:49:01.708: INFO: successfully validated that service multi-endpoint-test in namespace services-2249 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-2249 09/01/23 11:49:01.709
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2249 to expose endpoints map[] 09/01/23 11:49:01.726
    Sep  1 11:49:02.777: INFO: successfully validated that service multi-endpoint-test in namespace services-2249 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:49:02.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2249" for this suite. 09/01/23 11:49:02.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:49:02.833
Sep  1 11:49:02.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 11:49:02.836
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:49:02.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:49:02.869
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-49f8aec2-5112-4334-b0d6-dc319f145ebd 09/01/23 11:49:02.874
STEP: Creating a pod to test consume configMaps 09/01/23 11:49:02.881
Sep  1 11:49:02.896: INFO: Waiting up to 5m0s for pod "pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3" in namespace "configmap-398" to be "Succeeded or Failed"
Sep  1 11:49:02.902: INFO: Pod "pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.180987ms
Sep  1 11:49:04.907: INFO: Pod "pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010466273s
Sep  1 11:49:06.907: INFO: Pod "pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010474816s
STEP: Saw pod success 09/01/23 11:49:06.907
Sep  1 11:49:06.908: INFO: Pod "pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3" satisfied condition "Succeeded or Failed"
Sep  1 11:49:06.910: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3 container configmap-volume-test: <nil>
STEP: delete the pod 09/01/23 11:49:06.917
Sep  1 11:49:06.933: INFO: Waiting for pod pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3 to disappear
Sep  1 11:49:06.936: INFO: Pod pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:49:06.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-398" for this suite. 09/01/23 11:49:06.941
------------------------------
• [4.115 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:49:02.833
    Sep  1 11:49:02.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 11:49:02.836
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:49:02.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:49:02.869
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-49f8aec2-5112-4334-b0d6-dc319f145ebd 09/01/23 11:49:02.874
    STEP: Creating a pod to test consume configMaps 09/01/23 11:49:02.881
    Sep  1 11:49:02.896: INFO: Waiting up to 5m0s for pod "pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3" in namespace "configmap-398" to be "Succeeded or Failed"
    Sep  1 11:49:02.902: INFO: Pod "pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.180987ms
    Sep  1 11:49:04.907: INFO: Pod "pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010466273s
    Sep  1 11:49:06.907: INFO: Pod "pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010474816s
    STEP: Saw pod success 09/01/23 11:49:06.907
    Sep  1 11:49:06.908: INFO: Pod "pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3" satisfied condition "Succeeded or Failed"
    Sep  1 11:49:06.910: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3 container configmap-volume-test: <nil>
    STEP: delete the pod 09/01/23 11:49:06.917
    Sep  1 11:49:06.933: INFO: Waiting for pod pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3 to disappear
    Sep  1 11:49:06.936: INFO: Pod pod-configmaps-d4e99bbf-e405-43ed-8fab-394b643056b3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:49:06.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-398" for this suite. 09/01/23 11:49:06.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:49:06.953
Sep  1 11:49:06.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 11:49:06.956
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:49:06.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:49:06.977
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 09/01/23 11:49:06.981
Sep  1 11:49:07.013: INFO: Waiting up to 5m0s for pod "labelsupdatebe46a969-6e9e-4e7f-8b23-41660c642fd9" in namespace "downward-api-3959" to be "running and ready"
Sep  1 11:49:07.016: INFO: Pod "labelsupdatebe46a969-6e9e-4e7f-8b23-41660c642fd9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.18985ms
Sep  1 11:49:07.016: INFO: The phase of Pod labelsupdatebe46a969-6e9e-4e7f-8b23-41660c642fd9 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:49:09.024: INFO: Pod "labelsupdatebe46a969-6e9e-4e7f-8b23-41660c642fd9": Phase="Running", Reason="", readiness=true. Elapsed: 2.01073117s
Sep  1 11:49:09.024: INFO: The phase of Pod labelsupdatebe46a969-6e9e-4e7f-8b23-41660c642fd9 is Running (Ready = true)
Sep  1 11:49:09.024: INFO: Pod "labelsupdatebe46a969-6e9e-4e7f-8b23-41660c642fd9" satisfied condition "running and ready"
Sep  1 11:49:09.594: INFO: Successfully updated pod "labelsupdatebe46a969-6e9e-4e7f-8b23-41660c642fd9"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  1 11:49:13.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3959" for this suite. 09/01/23 11:49:13.687
------------------------------
• [SLOW TEST] [6.739 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:49:06.953
    Sep  1 11:49:06.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 11:49:06.956
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:49:06.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:49:06.977
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 09/01/23 11:49:06.981
    Sep  1 11:49:07.013: INFO: Waiting up to 5m0s for pod "labelsupdatebe46a969-6e9e-4e7f-8b23-41660c642fd9" in namespace "downward-api-3959" to be "running and ready"
    Sep  1 11:49:07.016: INFO: Pod "labelsupdatebe46a969-6e9e-4e7f-8b23-41660c642fd9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.18985ms
    Sep  1 11:49:07.016: INFO: The phase of Pod labelsupdatebe46a969-6e9e-4e7f-8b23-41660c642fd9 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:49:09.024: INFO: Pod "labelsupdatebe46a969-6e9e-4e7f-8b23-41660c642fd9": Phase="Running", Reason="", readiness=true. Elapsed: 2.01073117s
    Sep  1 11:49:09.024: INFO: The phase of Pod labelsupdatebe46a969-6e9e-4e7f-8b23-41660c642fd9 is Running (Ready = true)
    Sep  1 11:49:09.024: INFO: Pod "labelsupdatebe46a969-6e9e-4e7f-8b23-41660c642fd9" satisfied condition "running and ready"
    Sep  1 11:49:09.594: INFO: Successfully updated pod "labelsupdatebe46a969-6e9e-4e7f-8b23-41660c642fd9"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:49:13.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3959" for this suite. 09/01/23 11:49:13.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:49:13.695
Sep  1 11:49:13.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename replicaset 09/01/23 11:49:13.698
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:49:13.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:49:13.721
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 09/01/23 11:49:13.725
STEP: Verify that the required pods have come up 09/01/23 11:49:13.73
Sep  1 11:49:13.734: INFO: Pod name sample-pod: Found 0 pods out of 3
Sep  1 11:49:18.739: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 09/01/23 11:49:18.739
Sep  1 11:49:18.743: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 09/01/23 11:49:18.743
STEP: DeleteCollection of the ReplicaSets 09/01/23 11:49:18.748
STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/01/23 11:49:18.762
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  1 11:49:18.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7733" for this suite. 09/01/23 11:49:18.777
------------------------------
• [SLOW TEST] [5.109 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:49:13.695
    Sep  1 11:49:13.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename replicaset 09/01/23 11:49:13.698
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:49:13.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:49:13.721
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 09/01/23 11:49:13.725
    STEP: Verify that the required pods have come up 09/01/23 11:49:13.73
    Sep  1 11:49:13.734: INFO: Pod name sample-pod: Found 0 pods out of 3
    Sep  1 11:49:18.739: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 09/01/23 11:49:18.739
    Sep  1 11:49:18.743: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 09/01/23 11:49:18.743
    STEP: DeleteCollection of the ReplicaSets 09/01/23 11:49:18.748
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/01/23 11:49:18.762
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:49:18.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7733" for this suite. 09/01/23 11:49:18.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:49:18.805
Sep  1 11:49:18.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubelet-test 09/01/23 11:49:18.81
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:49:18.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:49:18.841
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 09/01/23 11:49:18.868
Sep  1 11:49:18.868: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases3eb8f6e5-d177-408e-9051-6e7f43fe948c" in namespace "kubelet-test-2095" to be "completed"
Sep  1 11:49:18.876: INFO: Pod "agnhost-host-aliases3eb8f6e5-d177-408e-9051-6e7f43fe948c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.540615ms
Sep  1 11:49:20.880: INFO: Pod "agnhost-host-aliases3eb8f6e5-d177-408e-9051-6e7f43fe948c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012572033s
Sep  1 11:49:22.881: INFO: Pod "agnhost-host-aliases3eb8f6e5-d177-408e-9051-6e7f43fe948c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013339815s
Sep  1 11:49:22.881: INFO: Pod "agnhost-host-aliases3eb8f6e5-d177-408e-9051-6e7f43fe948c" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  1 11:49:22.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2095" for this suite. 09/01/23 11:49:22.893
------------------------------
• [4.098 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:49:18.805
    Sep  1 11:49:18.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubelet-test 09/01/23 11:49:18.81
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:49:18.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:49:18.841
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 09/01/23 11:49:18.868
    Sep  1 11:49:18.868: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases3eb8f6e5-d177-408e-9051-6e7f43fe948c" in namespace "kubelet-test-2095" to be "completed"
    Sep  1 11:49:18.876: INFO: Pod "agnhost-host-aliases3eb8f6e5-d177-408e-9051-6e7f43fe948c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.540615ms
    Sep  1 11:49:20.880: INFO: Pod "agnhost-host-aliases3eb8f6e5-d177-408e-9051-6e7f43fe948c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012572033s
    Sep  1 11:49:22.881: INFO: Pod "agnhost-host-aliases3eb8f6e5-d177-408e-9051-6e7f43fe948c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013339815s
    Sep  1 11:49:22.881: INFO: Pod "agnhost-host-aliases3eb8f6e5-d177-408e-9051-6e7f43fe948c" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:49:22.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2095" for this suite. 09/01/23 11:49:22.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:49:22.907
Sep  1 11:49:22.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename var-expansion 09/01/23 11:49:22.91
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:49:22.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:49:22.931
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 09/01/23 11:49:22.935
Sep  1 11:49:22.945: INFO: Waiting up to 5m0s for pod "var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f" in namespace "var-expansion-2155" to be "Succeeded or Failed"
Sep  1 11:49:22.951: INFO: Pod "var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.715334ms
Sep  1 11:49:24.955: INFO: Pod "var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009834866s
Sep  1 11:49:26.956: INFO: Pod "var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010858757s
STEP: Saw pod success 09/01/23 11:49:26.956
Sep  1 11:49:26.956: INFO: Pod "var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f" satisfied condition "Succeeded or Failed"
Sep  1 11:49:26.959: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f container dapi-container: <nil>
STEP: delete the pod 09/01/23 11:49:26.964
Sep  1 11:49:26.975: INFO: Waiting for pod var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f to disappear
Sep  1 11:49:26.979: INFO: Pod var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  1 11:49:26.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2155" for this suite. 09/01/23 11:49:26.983
------------------------------
• [4.082 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:49:22.907
    Sep  1 11:49:22.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename var-expansion 09/01/23 11:49:22.91
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:49:22.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:49:22.931
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 09/01/23 11:49:22.935
    Sep  1 11:49:22.945: INFO: Waiting up to 5m0s for pod "var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f" in namespace "var-expansion-2155" to be "Succeeded or Failed"
    Sep  1 11:49:22.951: INFO: Pod "var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.715334ms
    Sep  1 11:49:24.955: INFO: Pod "var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009834866s
    Sep  1 11:49:26.956: INFO: Pod "var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010858757s
    STEP: Saw pod success 09/01/23 11:49:26.956
    Sep  1 11:49:26.956: INFO: Pod "var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f" satisfied condition "Succeeded or Failed"
    Sep  1 11:49:26.959: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f container dapi-container: <nil>
    STEP: delete the pod 09/01/23 11:49:26.964
    Sep  1 11:49:26.975: INFO: Waiting for pod var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f to disappear
    Sep  1 11:49:26.979: INFO: Pod var-expansion-c8a0d8cf-b6bb-44aa-ba5d-3db8cba0823f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:49:26.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2155" for this suite. 09/01/23 11:49:26.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:49:26.991
Sep  1 11:49:26.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename cronjob 09/01/23 11:49:26.992
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:49:27.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:49:27.015
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 09/01/23 11:49:27.019
STEP: Ensuring no jobs are scheduled 09/01/23 11:49:27.025
STEP: Ensuring no job exists by listing jobs explicitly 09/01/23 11:54:27.032
STEP: Removing cronjob 09/01/23 11:54:27.068
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  1 11:54:27.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7996" for this suite. 09/01/23 11:54:27.078
------------------------------
• [SLOW TEST] [300.092 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:49:26.991
    Sep  1 11:49:26.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename cronjob 09/01/23 11:49:26.992
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:49:27.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:49:27.015
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 09/01/23 11:49:27.019
    STEP: Ensuring no jobs are scheduled 09/01/23 11:49:27.025
    STEP: Ensuring no job exists by listing jobs explicitly 09/01/23 11:54:27.032
    STEP: Removing cronjob 09/01/23 11:54:27.068
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:54:27.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7996" for this suite. 09/01/23 11:54:27.078
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:54:27.085
Sep  1 11:54:27.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 11:54:27.087
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:54:27.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:54:27.111
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6 09/01/23 11:54:27.114
STEP: changing the ExternalName service to type=NodePort 09/01/23 11:54:27.12
STEP: creating replication controller externalname-service in namespace services-6 09/01/23 11:54:27.147
I0901 11:54:27.155655      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6, replica count: 2
I0901 11:54:30.207856      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  1 11:54:30.208: INFO: Creating new exec pod
Sep  1 11:54:30.237: INFO: Waiting up to 5m0s for pod "execpodgz9q5" in namespace "services-6" to be "running"
Sep  1 11:54:30.257: INFO: Pod "execpodgz9q5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.796495ms
Sep  1 11:54:32.260: INFO: Pod "execpodgz9q5": Phase="Running", Reason="", readiness=true. Elapsed: 2.022473106s
Sep  1 11:54:32.260: INFO: Pod "execpodgz9q5" satisfied condition "running"
Sep  1 11:54:33.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6 exec execpodgz9q5 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Sep  1 11:54:33.489: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  1 11:54:33.489: INFO: stdout: ""
Sep  1 11:54:33.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6 exec execpodgz9q5 -- /bin/sh -x -c nc -v -z -w 2 10.111.169.85 80'
Sep  1 11:54:33.691: INFO: stderr: "+ nc -v -z -w 2 10.111.169.85 80\nConnection to 10.111.169.85 80 port [tcp/http] succeeded!\n"
Sep  1 11:54:33.691: INFO: stdout: ""
Sep  1 11:54:33.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6 exec execpodgz9q5 -- /bin/sh -x -c nc -v -z -w 2 172.16.0.3 32060'
Sep  1 11:54:33.895: INFO: stderr: "+ nc -v -z -w 2 172.16.0.3 32060\nConnection to 172.16.0.3 32060 port [tcp/*] succeeded!\n"
Sep  1 11:54:33.895: INFO: stdout: ""
Sep  1 11:54:33.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6 exec execpodgz9q5 -- /bin/sh -x -c nc -v -z -w 2 172.16.0.4 32060'
Sep  1 11:54:34.090: INFO: stderr: "+ nc -v -z -w 2 172.16.0.4 32060\nConnection to 172.16.0.4 32060 port [tcp/*] succeeded!\n"
Sep  1 11:54:34.090: INFO: stdout: ""
Sep  1 11:54:34.090: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 11:54:34.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6" for this suite. 09/01/23 11:54:34.188
------------------------------
• [SLOW TEST] [7.111 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:54:27.085
    Sep  1 11:54:27.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 11:54:27.087
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:54:27.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:54:27.111
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6 09/01/23 11:54:27.114
    STEP: changing the ExternalName service to type=NodePort 09/01/23 11:54:27.12
    STEP: creating replication controller externalname-service in namespace services-6 09/01/23 11:54:27.147
    I0901 11:54:27.155655      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6, replica count: 2
    I0901 11:54:30.207856      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  1 11:54:30.208: INFO: Creating new exec pod
    Sep  1 11:54:30.237: INFO: Waiting up to 5m0s for pod "execpodgz9q5" in namespace "services-6" to be "running"
    Sep  1 11:54:30.257: INFO: Pod "execpodgz9q5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.796495ms
    Sep  1 11:54:32.260: INFO: Pod "execpodgz9q5": Phase="Running", Reason="", readiness=true. Elapsed: 2.022473106s
    Sep  1 11:54:32.260: INFO: Pod "execpodgz9q5" satisfied condition "running"
    Sep  1 11:54:33.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6 exec execpodgz9q5 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Sep  1 11:54:33.489: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  1 11:54:33.489: INFO: stdout: ""
    Sep  1 11:54:33.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6 exec execpodgz9q5 -- /bin/sh -x -c nc -v -z -w 2 10.111.169.85 80'
    Sep  1 11:54:33.691: INFO: stderr: "+ nc -v -z -w 2 10.111.169.85 80\nConnection to 10.111.169.85 80 port [tcp/http] succeeded!\n"
    Sep  1 11:54:33.691: INFO: stdout: ""
    Sep  1 11:54:33.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6 exec execpodgz9q5 -- /bin/sh -x -c nc -v -z -w 2 172.16.0.3 32060'
    Sep  1 11:54:33.895: INFO: stderr: "+ nc -v -z -w 2 172.16.0.3 32060\nConnection to 172.16.0.3 32060 port [tcp/*] succeeded!\n"
    Sep  1 11:54:33.895: INFO: stdout: ""
    Sep  1 11:54:33.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=services-6 exec execpodgz9q5 -- /bin/sh -x -c nc -v -z -w 2 172.16.0.4 32060'
    Sep  1 11:54:34.090: INFO: stderr: "+ nc -v -z -w 2 172.16.0.4 32060\nConnection to 172.16.0.4 32060 port [tcp/*] succeeded!\n"
    Sep  1 11:54:34.090: INFO: stdout: ""
    Sep  1 11:54:34.090: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:54:34.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6" for this suite. 09/01/23 11:54:34.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:54:34.197
Sep  1 11:54:34.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 11:54:34.199
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:54:34.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:54:34.228
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-7a409128-126f-46d9-abf4-ec94b0eaceef 09/01/23 11:54:34.235
STEP: Creating a pod to test consume configMaps 09/01/23 11:54:34.242
Sep  1 11:54:34.256: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379" in namespace "projected-9823" to be "Succeeded or Failed"
Sep  1 11:54:34.280: INFO: Pod "pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379": Phase="Pending", Reason="", readiness=false. Elapsed: 24.035089ms
Sep  1 11:54:36.287: INFO: Pod "pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031693753s
Sep  1 11:54:38.288: INFO: Pod "pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031750155s
STEP: Saw pod success 09/01/23 11:54:38.288
Sep  1 11:54:38.288: INFO: Pod "pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379" satisfied condition "Succeeded or Failed"
Sep  1 11:54:38.291: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379 container agnhost-container: <nil>
STEP: delete the pod 09/01/23 11:54:38.311
Sep  1 11:54:38.326: INFO: Waiting for pod pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379 to disappear
Sep  1 11:54:38.329: INFO: Pod pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:54:38.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9823" for this suite. 09/01/23 11:54:38.334
------------------------------
• [4.143 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:54:34.197
    Sep  1 11:54:34.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 11:54:34.199
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:54:34.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:54:34.228
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-7a409128-126f-46d9-abf4-ec94b0eaceef 09/01/23 11:54:34.235
    STEP: Creating a pod to test consume configMaps 09/01/23 11:54:34.242
    Sep  1 11:54:34.256: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379" in namespace "projected-9823" to be "Succeeded or Failed"
    Sep  1 11:54:34.280: INFO: Pod "pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379": Phase="Pending", Reason="", readiness=false. Elapsed: 24.035089ms
    Sep  1 11:54:36.287: INFO: Pod "pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031693753s
    Sep  1 11:54:38.288: INFO: Pod "pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031750155s
    STEP: Saw pod success 09/01/23 11:54:38.288
    Sep  1 11:54:38.288: INFO: Pod "pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379" satisfied condition "Succeeded or Failed"
    Sep  1 11:54:38.291: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379 container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 11:54:38.311
    Sep  1 11:54:38.326: INFO: Waiting for pod pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379 to disappear
    Sep  1 11:54:38.329: INFO: Pod pod-projected-configmaps-90a14d82-48f1-483b-a949-342821869379 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:54:38.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9823" for this suite. 09/01/23 11:54:38.334
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:54:38.341
Sep  1 11:54:38.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename replicaset 09/01/23 11:54:38.343
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:54:38.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:54:38.366
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Sep  1 11:54:38.396: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  1 11:54:43.406: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/01/23 11:54:43.406
STEP: Scaling up "test-rs" replicaset  09/01/23 11:54:43.406
Sep  1 11:54:43.417: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 09/01/23 11:54:43.417
W0901 11:54:43.431600      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  1 11:54:43.434: INFO: observed ReplicaSet test-rs in namespace replicaset-647 with ReadyReplicas 1, AvailableReplicas 1
Sep  1 11:54:43.455: INFO: observed ReplicaSet test-rs in namespace replicaset-647 with ReadyReplicas 1, AvailableReplicas 1
Sep  1 11:54:43.476: INFO: observed ReplicaSet test-rs in namespace replicaset-647 with ReadyReplicas 1, AvailableReplicas 1
Sep  1 11:54:43.485: INFO: observed ReplicaSet test-rs in namespace replicaset-647 with ReadyReplicas 1, AvailableReplicas 1
Sep  1 11:54:44.579: INFO: observed ReplicaSet test-rs in namespace replicaset-647 with ReadyReplicas 2, AvailableReplicas 2
Sep  1 11:54:45.123: INFO: observed Replicaset test-rs in namespace replicaset-647 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  1 11:54:45.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-647" for this suite. 09/01/23 11:54:45.127
------------------------------
• [SLOW TEST] [6.793 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:54:38.341
    Sep  1 11:54:38.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename replicaset 09/01/23 11:54:38.343
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:54:38.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:54:38.366
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Sep  1 11:54:38.396: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  1 11:54:43.406: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/01/23 11:54:43.406
    STEP: Scaling up "test-rs" replicaset  09/01/23 11:54:43.406
    Sep  1 11:54:43.417: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 09/01/23 11:54:43.417
    W0901 11:54:43.431600      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  1 11:54:43.434: INFO: observed ReplicaSet test-rs in namespace replicaset-647 with ReadyReplicas 1, AvailableReplicas 1
    Sep  1 11:54:43.455: INFO: observed ReplicaSet test-rs in namespace replicaset-647 with ReadyReplicas 1, AvailableReplicas 1
    Sep  1 11:54:43.476: INFO: observed ReplicaSet test-rs in namespace replicaset-647 with ReadyReplicas 1, AvailableReplicas 1
    Sep  1 11:54:43.485: INFO: observed ReplicaSet test-rs in namespace replicaset-647 with ReadyReplicas 1, AvailableReplicas 1
    Sep  1 11:54:44.579: INFO: observed ReplicaSet test-rs in namespace replicaset-647 with ReadyReplicas 2, AvailableReplicas 2
    Sep  1 11:54:45.123: INFO: observed Replicaset test-rs in namespace replicaset-647 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:54:45.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-647" for this suite. 09/01/23 11:54:45.127
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:54:45.139
Sep  1 11:54:45.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 11:54:45.141
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:54:45.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:54:45.163
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 09/01/23 11:54:45.167
Sep  1 11:54:45.175: INFO: Waiting up to 5m0s for pod "annotationupdatee4b1d060-fd13-44ea-8858-9f8edd87e60d" in namespace "downward-api-2187" to be "running and ready"
Sep  1 11:54:45.179: INFO: Pod "annotationupdatee4b1d060-fd13-44ea-8858-9f8edd87e60d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.445032ms
Sep  1 11:54:45.179: INFO: The phase of Pod annotationupdatee4b1d060-fd13-44ea-8858-9f8edd87e60d is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:54:47.183: INFO: Pod "annotationupdatee4b1d060-fd13-44ea-8858-9f8edd87e60d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007483663s
Sep  1 11:54:47.183: INFO: The phase of Pod annotationupdatee4b1d060-fd13-44ea-8858-9f8edd87e60d is Running (Ready = true)
Sep  1 11:54:47.183: INFO: Pod "annotationupdatee4b1d060-fd13-44ea-8858-9f8edd87e60d" satisfied condition "running and ready"
Sep  1 11:54:47.708: INFO: Successfully updated pod "annotationupdatee4b1d060-fd13-44ea-8858-9f8edd87e60d"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  1 11:54:51.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2187" for this suite. 09/01/23 11:54:51.75
------------------------------
• [SLOW TEST] [6.618 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:54:45.139
    Sep  1 11:54:45.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 11:54:45.141
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:54:45.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:54:45.163
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 09/01/23 11:54:45.167
    Sep  1 11:54:45.175: INFO: Waiting up to 5m0s for pod "annotationupdatee4b1d060-fd13-44ea-8858-9f8edd87e60d" in namespace "downward-api-2187" to be "running and ready"
    Sep  1 11:54:45.179: INFO: Pod "annotationupdatee4b1d060-fd13-44ea-8858-9f8edd87e60d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.445032ms
    Sep  1 11:54:45.179: INFO: The phase of Pod annotationupdatee4b1d060-fd13-44ea-8858-9f8edd87e60d is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:54:47.183: INFO: Pod "annotationupdatee4b1d060-fd13-44ea-8858-9f8edd87e60d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007483663s
    Sep  1 11:54:47.183: INFO: The phase of Pod annotationupdatee4b1d060-fd13-44ea-8858-9f8edd87e60d is Running (Ready = true)
    Sep  1 11:54:47.183: INFO: Pod "annotationupdatee4b1d060-fd13-44ea-8858-9f8edd87e60d" satisfied condition "running and ready"
    Sep  1 11:54:47.708: INFO: Successfully updated pod "annotationupdatee4b1d060-fd13-44ea-8858-9f8edd87e60d"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:54:51.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2187" for this suite. 09/01/23 11:54:51.75
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:54:51.76
Sep  1 11:54:51.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename taint-single-pod 09/01/23 11:54:51.761
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:54:51.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:54:51.782
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Sep  1 11:54:51.785: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  1 11:55:51.831: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Sep  1 11:55:51.836: INFO: Starting informer...
STEP: Starting pod... 09/01/23 11:55:51.836
Sep  1 11:55:52.059: INFO: Pod is running on k8s-worker-1.c.operations-lab.internal. Tainting Node
STEP: Trying to apply a taint on the Node 09/01/23 11:55:52.059
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/01/23 11:55:52.115
STEP: Waiting short time to make sure Pod is queued for deletion 09/01/23 11:55:52.12
Sep  1 11:55:52.120: INFO: Pod wasn't evicted. Proceeding
Sep  1 11:55:52.120: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/01/23 11:55:52.143
STEP: Waiting some time to make sure that toleration time passed. 09/01/23 11:55:52.153
Sep  1 11:57:07.157: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:57:07.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-5540" for this suite. 09/01/23 11:57:07.163
------------------------------
• [SLOW TEST] [135.416 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:54:51.76
    Sep  1 11:54:51.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename taint-single-pod 09/01/23 11:54:51.761
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:54:51.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:54:51.782
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Sep  1 11:54:51.785: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  1 11:55:51.831: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Sep  1 11:55:51.836: INFO: Starting informer...
    STEP: Starting pod... 09/01/23 11:55:51.836
    Sep  1 11:55:52.059: INFO: Pod is running on k8s-worker-1.c.operations-lab.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 09/01/23 11:55:52.059
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/01/23 11:55:52.115
    STEP: Waiting short time to make sure Pod is queued for deletion 09/01/23 11:55:52.12
    Sep  1 11:55:52.120: INFO: Pod wasn't evicted. Proceeding
    Sep  1 11:55:52.120: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/01/23 11:55:52.143
    STEP: Waiting some time to make sure that toleration time passed. 09/01/23 11:55:52.153
    Sep  1 11:57:07.157: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:57:07.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-5540" for this suite. 09/01/23 11:57:07.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:57:07.181
Sep  1 11:57:07.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename pods 09/01/23 11:57:07.183
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:07.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:07.211
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Sep  1 11:57:07.225: INFO: Waiting up to 5m0s for pod "server-envvars-efc5e801-9561-45b0-b83d-537c95d6a697" in namespace "pods-9171" to be "running and ready"
Sep  1 11:57:07.228: INFO: Pod "server-envvars-efc5e801-9561-45b0-b83d-537c95d6a697": Phase="Pending", Reason="", readiness=false. Elapsed: 3.174919ms
Sep  1 11:57:07.228: INFO: The phase of Pod server-envvars-efc5e801-9561-45b0-b83d-537c95d6a697 is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:57:09.234: INFO: Pod "server-envvars-efc5e801-9561-45b0-b83d-537c95d6a697": Phase="Running", Reason="", readiness=true. Elapsed: 2.00892448s
Sep  1 11:57:09.234: INFO: The phase of Pod server-envvars-efc5e801-9561-45b0-b83d-537c95d6a697 is Running (Ready = true)
Sep  1 11:57:09.234: INFO: Pod "server-envvars-efc5e801-9561-45b0-b83d-537c95d6a697" satisfied condition "running and ready"
Sep  1 11:57:09.267: INFO: Waiting up to 5m0s for pod "client-envvars-af711d16-6421-417d-a70b-8236f87a02eb" in namespace "pods-9171" to be "Succeeded or Failed"
Sep  1 11:57:09.277: INFO: Pod "client-envvars-af711d16-6421-417d-a70b-8236f87a02eb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.603676ms
Sep  1 11:57:11.283: INFO: Pod "client-envvars-af711d16-6421-417d-a70b-8236f87a02eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016253792s
Sep  1 11:57:13.282: INFO: Pod "client-envvars-af711d16-6421-417d-a70b-8236f87a02eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015569516s
STEP: Saw pod success 09/01/23 11:57:13.283
Sep  1 11:57:13.283: INFO: Pod "client-envvars-af711d16-6421-417d-a70b-8236f87a02eb" satisfied condition "Succeeded or Failed"
Sep  1 11:57:13.286: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod client-envvars-af711d16-6421-417d-a70b-8236f87a02eb container env3cont: <nil>
STEP: delete the pod 09/01/23 11:57:13.303
Sep  1 11:57:13.320: INFO: Waiting for pod client-envvars-af711d16-6421-417d-a70b-8236f87a02eb to disappear
Sep  1 11:57:13.323: INFO: Pod client-envvars-af711d16-6421-417d-a70b-8236f87a02eb no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  1 11:57:13.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9171" for this suite. 09/01/23 11:57:13.327
------------------------------
• [SLOW TEST] [6.157 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:57:07.181
    Sep  1 11:57:07.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename pods 09/01/23 11:57:07.183
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:07.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:07.211
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Sep  1 11:57:07.225: INFO: Waiting up to 5m0s for pod "server-envvars-efc5e801-9561-45b0-b83d-537c95d6a697" in namespace "pods-9171" to be "running and ready"
    Sep  1 11:57:07.228: INFO: Pod "server-envvars-efc5e801-9561-45b0-b83d-537c95d6a697": Phase="Pending", Reason="", readiness=false. Elapsed: 3.174919ms
    Sep  1 11:57:07.228: INFO: The phase of Pod server-envvars-efc5e801-9561-45b0-b83d-537c95d6a697 is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:57:09.234: INFO: Pod "server-envvars-efc5e801-9561-45b0-b83d-537c95d6a697": Phase="Running", Reason="", readiness=true. Elapsed: 2.00892448s
    Sep  1 11:57:09.234: INFO: The phase of Pod server-envvars-efc5e801-9561-45b0-b83d-537c95d6a697 is Running (Ready = true)
    Sep  1 11:57:09.234: INFO: Pod "server-envvars-efc5e801-9561-45b0-b83d-537c95d6a697" satisfied condition "running and ready"
    Sep  1 11:57:09.267: INFO: Waiting up to 5m0s for pod "client-envvars-af711d16-6421-417d-a70b-8236f87a02eb" in namespace "pods-9171" to be "Succeeded or Failed"
    Sep  1 11:57:09.277: INFO: Pod "client-envvars-af711d16-6421-417d-a70b-8236f87a02eb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.603676ms
    Sep  1 11:57:11.283: INFO: Pod "client-envvars-af711d16-6421-417d-a70b-8236f87a02eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016253792s
    Sep  1 11:57:13.282: INFO: Pod "client-envvars-af711d16-6421-417d-a70b-8236f87a02eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015569516s
    STEP: Saw pod success 09/01/23 11:57:13.283
    Sep  1 11:57:13.283: INFO: Pod "client-envvars-af711d16-6421-417d-a70b-8236f87a02eb" satisfied condition "Succeeded or Failed"
    Sep  1 11:57:13.286: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod client-envvars-af711d16-6421-417d-a70b-8236f87a02eb container env3cont: <nil>
    STEP: delete the pod 09/01/23 11:57:13.303
    Sep  1 11:57:13.320: INFO: Waiting for pod client-envvars-af711d16-6421-417d-a70b-8236f87a02eb to disappear
    Sep  1 11:57:13.323: INFO: Pod client-envvars-af711d16-6421-417d-a70b-8236f87a02eb no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:57:13.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9171" for this suite. 09/01/23 11:57:13.327
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:57:13.338
Sep  1 11:57:13.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 11:57:13.341
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:13.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:13.37
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 09/01/23 11:57:13.376
Sep  1 11:57:13.376: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep  1 11:57:13.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 create -f -'
Sep  1 11:57:14.272: INFO: stderr: ""
Sep  1 11:57:14.272: INFO: stdout: "service/agnhost-replica created\n"
Sep  1 11:57:14.273: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep  1 11:57:14.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 create -f -'
Sep  1 11:57:15.023: INFO: stderr: ""
Sep  1 11:57:15.023: INFO: stdout: "service/agnhost-primary created\n"
Sep  1 11:57:15.071: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  1 11:57:15.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 create -f -'
Sep  1 11:57:15.710: INFO: stderr: ""
Sep  1 11:57:15.711: INFO: stdout: "service/frontend created\n"
Sep  1 11:57:15.769: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep  1 11:57:15.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 create -f -'
Sep  1 11:57:16.354: INFO: stderr: ""
Sep  1 11:57:16.354: INFO: stdout: "deployment.apps/frontend created\n"
Sep  1 11:57:16.354: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  1 11:57:16.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 create -f -'
Sep  1 11:57:17.580: INFO: stderr: ""
Sep  1 11:57:17.580: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep  1 11:57:17.580: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  1 11:57:17.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 create -f -'
Sep  1 11:57:19.178: INFO: stderr: ""
Sep  1 11:57:19.178: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 09/01/23 11:57:19.178
Sep  1 11:57:19.178: INFO: Waiting for all frontend pods to be Running.
Sep  1 11:57:19.229: INFO: Waiting for frontend to serve content.
Sep  1 11:57:22.574: INFO: Trying to add a new entry to the guestbook.
Sep  1 11:57:22.621: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 09/01/23 11:57:22.633
Sep  1 11:57:22.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 delete --grace-period=0 --force -f -'
Sep  1 11:57:22.765: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  1 11:57:22.765: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 09/01/23 11:57:22.765
Sep  1 11:57:22.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 delete --grace-period=0 --force -f -'
Sep  1 11:57:22.900: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  1 11:57:22.900: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/01/23 11:57:22.9
Sep  1 11:57:22.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 delete --grace-period=0 --force -f -'
Sep  1 11:57:23.040: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  1 11:57:23.040: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/01/23 11:57:23.04
Sep  1 11:57:23.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 delete --grace-period=0 --force -f -'
Sep  1 11:57:23.133: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  1 11:57:23.133: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/01/23 11:57:23.133
Sep  1 11:57:23.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 delete --grace-period=0 --force -f -'
Sep  1 11:57:23.308: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  1 11:57:23.308: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/01/23 11:57:23.309
Sep  1 11:57:23.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 delete --grace-period=0 --force -f -'
Sep  1 11:57:23.543: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  1 11:57:23.543: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 11:57:23.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6447" for this suite. 09/01/23 11:57:23.553
------------------------------
• [SLOW TEST] [10.225 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:57:13.338
    Sep  1 11:57:13.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 11:57:13.341
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:13.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:13.37
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 09/01/23 11:57:13.376
    Sep  1 11:57:13.376: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Sep  1 11:57:13.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 create -f -'
    Sep  1 11:57:14.272: INFO: stderr: ""
    Sep  1 11:57:14.272: INFO: stdout: "service/agnhost-replica created\n"
    Sep  1 11:57:14.273: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Sep  1 11:57:14.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 create -f -'
    Sep  1 11:57:15.023: INFO: stderr: ""
    Sep  1 11:57:15.023: INFO: stdout: "service/agnhost-primary created\n"
    Sep  1 11:57:15.071: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Sep  1 11:57:15.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 create -f -'
    Sep  1 11:57:15.710: INFO: stderr: ""
    Sep  1 11:57:15.711: INFO: stdout: "service/frontend created\n"
    Sep  1 11:57:15.769: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Sep  1 11:57:15.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 create -f -'
    Sep  1 11:57:16.354: INFO: stderr: ""
    Sep  1 11:57:16.354: INFO: stdout: "deployment.apps/frontend created\n"
    Sep  1 11:57:16.354: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep  1 11:57:16.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 create -f -'
    Sep  1 11:57:17.580: INFO: stderr: ""
    Sep  1 11:57:17.580: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Sep  1 11:57:17.580: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep  1 11:57:17.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 create -f -'
    Sep  1 11:57:19.178: INFO: stderr: ""
    Sep  1 11:57:19.178: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 09/01/23 11:57:19.178
    Sep  1 11:57:19.178: INFO: Waiting for all frontend pods to be Running.
    Sep  1 11:57:19.229: INFO: Waiting for frontend to serve content.
    Sep  1 11:57:22.574: INFO: Trying to add a new entry to the guestbook.
    Sep  1 11:57:22.621: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 09/01/23 11:57:22.633
    Sep  1 11:57:22.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 delete --grace-period=0 --force -f -'
    Sep  1 11:57:22.765: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  1 11:57:22.765: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 09/01/23 11:57:22.765
    Sep  1 11:57:22.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 delete --grace-period=0 --force -f -'
    Sep  1 11:57:22.900: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  1 11:57:22.900: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/01/23 11:57:22.9
    Sep  1 11:57:22.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 delete --grace-period=0 --force -f -'
    Sep  1 11:57:23.040: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  1 11:57:23.040: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/01/23 11:57:23.04
    Sep  1 11:57:23.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 delete --grace-period=0 --force -f -'
    Sep  1 11:57:23.133: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  1 11:57:23.133: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/01/23 11:57:23.133
    Sep  1 11:57:23.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 delete --grace-period=0 --force -f -'
    Sep  1 11:57:23.308: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  1 11:57:23.308: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/01/23 11:57:23.309
    Sep  1 11:57:23.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-6447 delete --grace-period=0 --force -f -'
    Sep  1 11:57:23.543: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  1 11:57:23.543: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:57:23.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6447" for this suite. 09/01/23 11:57:23.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:57:23.566
Sep  1 11:57:23.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 11:57:23.609
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:23.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:23.639
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Sep  1 11:57:23.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-4265 version'
Sep  1 11:57:23.833: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Sep  1 11:57:23.833: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:50:44Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:43:07Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 11:57:23.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4265" for this suite. 09/01/23 11:57:23.839
------------------------------
• [0.281 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:57:23.566
    Sep  1 11:57:23.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 11:57:23.609
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:23.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:23.639
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Sep  1 11:57:23.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-4265 version'
    Sep  1 11:57:23.833: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Sep  1 11:57:23.833: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:50:44Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:43:07Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:57:23.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4265" for this suite. 09/01/23 11:57:23.839
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:57:23.848
Sep  1 11:57:23.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 11:57:23.859
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:23.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:23.897
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 09/01/23 11:57:23.95
STEP: waiting for available Endpoint 09/01/23 11:57:23.965
STEP: listing all Endpoints 09/01/23 11:57:23.967
STEP: updating the Endpoint 09/01/23 11:57:23.981
STEP: fetching the Endpoint 09/01/23 11:57:23.997
STEP: patching the Endpoint 09/01/23 11:57:24.134
STEP: fetching the Endpoint 09/01/23 11:57:24.146
STEP: deleting the Endpoint by Collection 09/01/23 11:57:24.149
STEP: waiting for Endpoint deletion 09/01/23 11:57:24.162
STEP: fetching the Endpoint 09/01/23 11:57:24.165
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 11:57:24.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7538" for this suite. 09/01/23 11:57:24.176
------------------------------
• [0.337 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:57:23.848
    Sep  1 11:57:23.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 11:57:23.859
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:23.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:23.897
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 09/01/23 11:57:23.95
    STEP: waiting for available Endpoint 09/01/23 11:57:23.965
    STEP: listing all Endpoints 09/01/23 11:57:23.967
    STEP: updating the Endpoint 09/01/23 11:57:23.981
    STEP: fetching the Endpoint 09/01/23 11:57:23.997
    STEP: patching the Endpoint 09/01/23 11:57:24.134
    STEP: fetching the Endpoint 09/01/23 11:57:24.146
    STEP: deleting the Endpoint by Collection 09/01/23 11:57:24.149
    STEP: waiting for Endpoint deletion 09/01/23 11:57:24.162
    STEP: fetching the Endpoint 09/01/23 11:57:24.165
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:57:24.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7538" for this suite. 09/01/23 11:57:24.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:57:24.196
Sep  1 11:57:24.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename watch 09/01/23 11:57:24.198
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:24.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:24.221
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 09/01/23 11:57:24.228
STEP: creating a new configmap 09/01/23 11:57:24.231
STEP: modifying the configmap once 09/01/23 11:57:24.24
STEP: changing the label value of the configmap 09/01/23 11:57:24.267
STEP: Expecting to observe a delete notification for the watched object 09/01/23 11:57:24.296
Sep  1 11:57:24.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  cb8237f3-b92f-4b48-bd1c-bfaaecdcf54c 55764 0 2023-09-01 11:57:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-01 11:57:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  1 11:57:24.297: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  cb8237f3-b92f-4b48-bd1c-bfaaecdcf54c 55767 0 2023-09-01 11:57:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-01 11:57:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  1 11:57:24.299: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  cb8237f3-b92f-4b48-bd1c-bfaaecdcf54c 55771 0 2023-09-01 11:57:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-01 11:57:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 09/01/23 11:57:24.302
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/01/23 11:57:24.313
STEP: changing the label value of the configmap back 09/01/23 11:57:34.315
STEP: modifying the configmap a third time 09/01/23 11:57:34.323
STEP: deleting the configmap 09/01/23 11:57:34.33
STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/01/23 11:57:34.335
Sep  1 11:57:34.336: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  cb8237f3-b92f-4b48-bd1c-bfaaecdcf54c 55895 0 2023-09-01 11:57:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-01 11:57:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  1 11:57:34.336: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  cb8237f3-b92f-4b48-bd1c-bfaaecdcf54c 55896 0 2023-09-01 11:57:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-01 11:57:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  1 11:57:34.337: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  cb8237f3-b92f-4b48-bd1c-bfaaecdcf54c 55897 0 2023-09-01 11:57:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-01 11:57:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  1 11:57:34.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-599" for this suite. 09/01/23 11:57:34.341
------------------------------
• [SLOW TEST] [10.155 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:57:24.196
    Sep  1 11:57:24.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename watch 09/01/23 11:57:24.198
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:24.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:24.221
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 09/01/23 11:57:24.228
    STEP: creating a new configmap 09/01/23 11:57:24.231
    STEP: modifying the configmap once 09/01/23 11:57:24.24
    STEP: changing the label value of the configmap 09/01/23 11:57:24.267
    STEP: Expecting to observe a delete notification for the watched object 09/01/23 11:57:24.296
    Sep  1 11:57:24.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  cb8237f3-b92f-4b48-bd1c-bfaaecdcf54c 55764 0 2023-09-01 11:57:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-01 11:57:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  1 11:57:24.297: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  cb8237f3-b92f-4b48-bd1c-bfaaecdcf54c 55767 0 2023-09-01 11:57:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-01 11:57:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  1 11:57:24.299: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  cb8237f3-b92f-4b48-bd1c-bfaaecdcf54c 55771 0 2023-09-01 11:57:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-01 11:57:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 09/01/23 11:57:24.302
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/01/23 11:57:24.313
    STEP: changing the label value of the configmap back 09/01/23 11:57:34.315
    STEP: modifying the configmap a third time 09/01/23 11:57:34.323
    STEP: deleting the configmap 09/01/23 11:57:34.33
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/01/23 11:57:34.335
    Sep  1 11:57:34.336: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  cb8237f3-b92f-4b48-bd1c-bfaaecdcf54c 55895 0 2023-09-01 11:57:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-01 11:57:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  1 11:57:34.336: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  cb8237f3-b92f-4b48-bd1c-bfaaecdcf54c 55896 0 2023-09-01 11:57:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-01 11:57:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  1 11:57:34.337: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-599  cb8237f3-b92f-4b48-bd1c-bfaaecdcf54c 55897 0 2023-09-01 11:57:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-01 11:57:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:57:34.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-599" for this suite. 09/01/23 11:57:34.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:57:34.359
Sep  1 11:57:34.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 11:57:34.361
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:34.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:34.38
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 09/01/23 11:57:34.383
Sep  1 11:57:34.392: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd" in namespace "downward-api-4568" to be "Succeeded or Failed"
Sep  1 11:57:34.396: INFO: Pod "downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.231093ms
Sep  1 11:57:36.400: INFO: Pod "downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008031448s
Sep  1 11:57:38.400: INFO: Pod "downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008201944s
STEP: Saw pod success 09/01/23 11:57:38.4
Sep  1 11:57:38.400: INFO: Pod "downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd" satisfied condition "Succeeded or Failed"
Sep  1 11:57:38.404: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd container client-container: <nil>
STEP: delete the pod 09/01/23 11:57:38.41
Sep  1 11:57:38.422: INFO: Waiting for pod downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd to disappear
Sep  1 11:57:38.425: INFO: Pod downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  1 11:57:38.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4568" for this suite. 09/01/23 11:57:38.43
------------------------------
• [4.079 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:57:34.359
    Sep  1 11:57:34.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 11:57:34.361
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:34.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:34.38
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 09/01/23 11:57:34.383
    Sep  1 11:57:34.392: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd" in namespace "downward-api-4568" to be "Succeeded or Failed"
    Sep  1 11:57:34.396: INFO: Pod "downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.231093ms
    Sep  1 11:57:36.400: INFO: Pod "downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008031448s
    Sep  1 11:57:38.400: INFO: Pod "downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008201944s
    STEP: Saw pod success 09/01/23 11:57:38.4
    Sep  1 11:57:38.400: INFO: Pod "downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd" satisfied condition "Succeeded or Failed"
    Sep  1 11:57:38.404: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd container client-container: <nil>
    STEP: delete the pod 09/01/23 11:57:38.41
    Sep  1 11:57:38.422: INFO: Waiting for pod downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd to disappear
    Sep  1 11:57:38.425: INFO: Pod downwardapi-volume-4e94deeb-5fc0-48fb-9148-f42b796981cd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:57:38.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4568" for this suite. 09/01/23 11:57:38.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:57:38.443
Sep  1 11:57:38.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 11:57:38.445
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:38.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:38.469
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-0557bec2-e4be-4e9d-883f-4bfb02a4ac35 09/01/23 11:57:38.472
STEP: Creating a pod to test consume configMaps 09/01/23 11:57:38.479
Sep  1 11:57:38.487: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9" in namespace "configmap-5211" to be "Succeeded or Failed"
Sep  1 11:57:38.492: INFO: Pod "pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.146411ms
Sep  1 11:57:40.495: INFO: Pod "pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007597381s
Sep  1 11:57:42.496: INFO: Pod "pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007705352s
STEP: Saw pod success 09/01/23 11:57:42.496
Sep  1 11:57:42.496: INFO: Pod "pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9" satisfied condition "Succeeded or Failed"
Sep  1 11:57:42.499: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9 container agnhost-container: <nil>
STEP: delete the pod 09/01/23 11:57:42.505
Sep  1 11:57:42.516: INFO: Waiting for pod pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9 to disappear
Sep  1 11:57:42.519: INFO: Pod pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 11:57:42.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5211" for this suite. 09/01/23 11:57:42.524
------------------------------
• [4.086 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:57:38.443
    Sep  1 11:57:38.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 11:57:38.445
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:38.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:38.469
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-0557bec2-e4be-4e9d-883f-4bfb02a4ac35 09/01/23 11:57:38.472
    STEP: Creating a pod to test consume configMaps 09/01/23 11:57:38.479
    Sep  1 11:57:38.487: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9" in namespace "configmap-5211" to be "Succeeded or Failed"
    Sep  1 11:57:38.492: INFO: Pod "pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.146411ms
    Sep  1 11:57:40.495: INFO: Pod "pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007597381s
    Sep  1 11:57:42.496: INFO: Pod "pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007705352s
    STEP: Saw pod success 09/01/23 11:57:42.496
    Sep  1 11:57:42.496: INFO: Pod "pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9" satisfied condition "Succeeded or Failed"
    Sep  1 11:57:42.499: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9 container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 11:57:42.505
    Sep  1 11:57:42.516: INFO: Waiting for pod pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9 to disappear
    Sep  1 11:57:42.519: INFO: Pod pod-configmaps-7f44dfb8-7a82-4ae3-b319-a97b9ead84e9 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:57:42.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5211" for this suite. 09/01/23 11:57:42.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:57:42.537
Sep  1 11:57:42.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename discovery 09/01/23 11:57:42.539
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:42.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:42.559
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 09/01/23 11:57:42.625
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Sep  1 11:57:42.794: INFO: Checking APIGroup: apiregistration.k8s.io
Sep  1 11:57:42.795: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep  1 11:57:42.795: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Sep  1 11:57:42.795: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep  1 11:57:42.795: INFO: Checking APIGroup: apps
Sep  1 11:57:42.796: INFO: PreferredVersion.GroupVersion: apps/v1
Sep  1 11:57:42.796: INFO: Versions found [{apps/v1 v1}]
Sep  1 11:57:42.796: INFO: apps/v1 matches apps/v1
Sep  1 11:57:42.796: INFO: Checking APIGroup: events.k8s.io
Sep  1 11:57:42.797: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep  1 11:57:42.797: INFO: Versions found [{events.k8s.io/v1 v1}]
Sep  1 11:57:42.797: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep  1 11:57:42.797: INFO: Checking APIGroup: authentication.k8s.io
Sep  1 11:57:42.797: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep  1 11:57:42.797: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Sep  1 11:57:42.797: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep  1 11:57:42.797: INFO: Checking APIGroup: authorization.k8s.io
Sep  1 11:57:42.798: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep  1 11:57:42.798: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Sep  1 11:57:42.798: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep  1 11:57:42.798: INFO: Checking APIGroup: autoscaling
Sep  1 11:57:42.799: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Sep  1 11:57:42.799: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Sep  1 11:57:42.799: INFO: autoscaling/v2 matches autoscaling/v2
Sep  1 11:57:42.799: INFO: Checking APIGroup: batch
Sep  1 11:57:42.801: INFO: PreferredVersion.GroupVersion: batch/v1
Sep  1 11:57:42.801: INFO: Versions found [{batch/v1 v1}]
Sep  1 11:57:42.801: INFO: batch/v1 matches batch/v1
Sep  1 11:57:42.801: INFO: Checking APIGroup: certificates.k8s.io
Sep  1 11:57:42.802: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep  1 11:57:42.802: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Sep  1 11:57:42.802: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep  1 11:57:42.802: INFO: Checking APIGroup: networking.k8s.io
Sep  1 11:57:42.803: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep  1 11:57:42.803: INFO: Versions found [{networking.k8s.io/v1 v1}]
Sep  1 11:57:42.803: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep  1 11:57:42.803: INFO: Checking APIGroup: policy
Sep  1 11:57:42.804: INFO: PreferredVersion.GroupVersion: policy/v1
Sep  1 11:57:42.804: INFO: Versions found [{policy/v1 v1}]
Sep  1 11:57:42.804: INFO: policy/v1 matches policy/v1
Sep  1 11:57:42.804: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep  1 11:57:42.805: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep  1 11:57:42.805: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Sep  1 11:57:42.805: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep  1 11:57:42.805: INFO: Checking APIGroup: storage.k8s.io
Sep  1 11:57:42.806: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep  1 11:57:42.806: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep  1 11:57:42.806: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep  1 11:57:42.806: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep  1 11:57:42.806: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep  1 11:57:42.806: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Sep  1 11:57:42.806: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep  1 11:57:42.806: INFO: Checking APIGroup: apiextensions.k8s.io
Sep  1 11:57:42.807: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep  1 11:57:42.807: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Sep  1 11:57:42.807: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep  1 11:57:42.807: INFO: Checking APIGroup: scheduling.k8s.io
Sep  1 11:57:42.808: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep  1 11:57:42.808: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Sep  1 11:57:42.808: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep  1 11:57:42.808: INFO: Checking APIGroup: coordination.k8s.io
Sep  1 11:57:42.809: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep  1 11:57:42.809: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Sep  1 11:57:42.809: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep  1 11:57:42.809: INFO: Checking APIGroup: node.k8s.io
Sep  1 11:57:42.810: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Sep  1 11:57:42.810: INFO: Versions found [{node.k8s.io/v1 v1}]
Sep  1 11:57:42.810: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Sep  1 11:57:42.810: INFO: Checking APIGroup: discovery.k8s.io
Sep  1 11:57:42.811: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Sep  1 11:57:42.811: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Sep  1 11:57:42.811: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Sep  1 11:57:42.811: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep  1 11:57:42.811: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Sep  1 11:57:42.811: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Sep  1 11:57:42.811: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Sep  1 11:57:42.811: INFO: Checking APIGroup: acme.cert-manager.io
Sep  1 11:57:42.813: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Sep  1 11:57:42.813: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
Sep  1 11:57:42.813: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Sep  1 11:57:42.813: INFO: Checking APIGroup: cert-manager.io
Sep  1 11:57:42.814: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Sep  1 11:57:42.814: INFO: Versions found [{cert-manager.io/v1 v1}]
Sep  1 11:57:42.814: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Sep  1 11:57:42.814: INFO: Checking APIGroup: kyverno.io
Sep  1 11:57:42.815: INFO: PreferredVersion.GroupVersion: kyverno.io/v1
Sep  1 11:57:42.815: INFO: Versions found [{kyverno.io/v1 v1} {kyverno.io/v2beta1 v2beta1} {kyverno.io/v1beta1 v1beta1} {kyverno.io/v2alpha1 v2alpha1} {kyverno.io/v1alpha2 v1alpha2}]
Sep  1 11:57:42.815: INFO: kyverno.io/v1 matches kyverno.io/v1
Sep  1 11:57:42.815: INFO: Checking APIGroup: monitoring.coreos.com
Sep  1 11:57:42.816: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Sep  1 11:57:42.816: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Sep  1 11:57:42.816: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Sep  1 11:57:42.816: INFO: Checking APIGroup: velero.io
Sep  1 11:57:42.817: INFO: PreferredVersion.GroupVersion: velero.io/v1
Sep  1 11:57:42.817: INFO: Versions found [{velero.io/v1 v1}]
Sep  1 11:57:42.817: INFO: velero.io/v1 matches velero.io/v1
Sep  1 11:57:42.817: INFO: Checking APIGroup: kube-green.com
Sep  1 11:57:42.818: INFO: PreferredVersion.GroupVersion: kube-green.com/v1alpha1
Sep  1 11:57:42.818: INFO: Versions found [{kube-green.com/v1alpha1 v1alpha1}]
Sep  1 11:57:42.818: INFO: kube-green.com/v1alpha1 matches kube-green.com/v1alpha1
Sep  1 11:57:42.818: INFO: Checking APIGroup: logging-extensions.banzaicloud.io
Sep  1 11:57:42.820: INFO: PreferredVersion.GroupVersion: logging-extensions.banzaicloud.io/v1alpha1
Sep  1 11:57:42.820: INFO: Versions found [{logging-extensions.banzaicloud.io/v1alpha1 v1alpha1}]
Sep  1 11:57:42.820: INFO: logging-extensions.banzaicloud.io/v1alpha1 matches logging-extensions.banzaicloud.io/v1alpha1
Sep  1 11:57:42.820: INFO: Checking APIGroup: logging.banzaicloud.io
Sep  1 11:57:42.820: INFO: PreferredVersion.GroupVersion: logging.banzaicloud.io/v1beta1
Sep  1 11:57:42.820: INFO: Versions found [{logging.banzaicloud.io/v1beta1 v1beta1} {logging.banzaicloud.io/v1alpha1 v1alpha1}]
Sep  1 11:57:42.820: INFO: logging.banzaicloud.io/v1beta1 matches logging.banzaicloud.io/v1beta1
Sep  1 11:57:42.820: INFO: Checking APIGroup: traefik.containo.us
Sep  1 11:57:42.821: INFO: PreferredVersion.GroupVersion: traefik.containo.us/v1alpha1
Sep  1 11:57:42.821: INFO: Versions found [{traefik.containo.us/v1alpha1 v1alpha1}]
Sep  1 11:57:42.821: INFO: traefik.containo.us/v1alpha1 matches traefik.containo.us/v1alpha1
Sep  1 11:57:42.821: INFO: Checking APIGroup: traefik.io
Sep  1 11:57:42.822: INFO: PreferredVersion.GroupVersion: traefik.io/v1alpha1
Sep  1 11:57:42.822: INFO: Versions found [{traefik.io/v1alpha1 v1alpha1}]
Sep  1 11:57:42.822: INFO: traefik.io/v1alpha1 matches traefik.io/v1alpha1
Sep  1 11:57:42.822: INFO: Checking APIGroup: wgpolicyk8s.io
Sep  1 11:57:42.823: INFO: PreferredVersion.GroupVersion: wgpolicyk8s.io/v1alpha2
Sep  1 11:57:42.823: INFO: Versions found [{wgpolicyk8s.io/v1alpha2 v1alpha2}]
Sep  1 11:57:42.823: INFO: wgpolicyk8s.io/v1alpha2 matches wgpolicyk8s.io/v1alpha2
Sep  1 11:57:42.823: INFO: Checking APIGroup: rbacmanager.reactiveops.io
Sep  1 11:57:42.824: INFO: PreferredVersion.GroupVersion: rbacmanager.reactiveops.io/v1beta1
Sep  1 11:57:42.824: INFO: Versions found [{rbacmanager.reactiveops.io/v1beta1 v1beta1}]
Sep  1 11:57:42.824: INFO: rbacmanager.reactiveops.io/v1beta1 matches rbacmanager.reactiveops.io/v1beta1
Sep  1 11:57:42.824: INFO: Checking APIGroup: cilium.io
Sep  1 11:57:42.825: INFO: PreferredVersion.GroupVersion: cilium.io/v2
Sep  1 11:57:42.825: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
Sep  1 11:57:42.825: INFO: cilium.io/v2 matches cilium.io/v2
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Sep  1 11:57:42.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-8921" for this suite. 09/01/23 11:57:42.833
------------------------------
• [0.303 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:57:42.537
    Sep  1 11:57:42.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename discovery 09/01/23 11:57:42.539
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:42.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:42.559
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 09/01/23 11:57:42.625
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Sep  1 11:57:42.794: INFO: Checking APIGroup: apiregistration.k8s.io
    Sep  1 11:57:42.795: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Sep  1 11:57:42.795: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Sep  1 11:57:42.795: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Sep  1 11:57:42.795: INFO: Checking APIGroup: apps
    Sep  1 11:57:42.796: INFO: PreferredVersion.GroupVersion: apps/v1
    Sep  1 11:57:42.796: INFO: Versions found [{apps/v1 v1}]
    Sep  1 11:57:42.796: INFO: apps/v1 matches apps/v1
    Sep  1 11:57:42.796: INFO: Checking APIGroup: events.k8s.io
    Sep  1 11:57:42.797: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Sep  1 11:57:42.797: INFO: Versions found [{events.k8s.io/v1 v1}]
    Sep  1 11:57:42.797: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Sep  1 11:57:42.797: INFO: Checking APIGroup: authentication.k8s.io
    Sep  1 11:57:42.797: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Sep  1 11:57:42.797: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Sep  1 11:57:42.797: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Sep  1 11:57:42.797: INFO: Checking APIGroup: authorization.k8s.io
    Sep  1 11:57:42.798: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Sep  1 11:57:42.798: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Sep  1 11:57:42.798: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Sep  1 11:57:42.798: INFO: Checking APIGroup: autoscaling
    Sep  1 11:57:42.799: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Sep  1 11:57:42.799: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Sep  1 11:57:42.799: INFO: autoscaling/v2 matches autoscaling/v2
    Sep  1 11:57:42.799: INFO: Checking APIGroup: batch
    Sep  1 11:57:42.801: INFO: PreferredVersion.GroupVersion: batch/v1
    Sep  1 11:57:42.801: INFO: Versions found [{batch/v1 v1}]
    Sep  1 11:57:42.801: INFO: batch/v1 matches batch/v1
    Sep  1 11:57:42.801: INFO: Checking APIGroup: certificates.k8s.io
    Sep  1 11:57:42.802: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Sep  1 11:57:42.802: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Sep  1 11:57:42.802: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Sep  1 11:57:42.802: INFO: Checking APIGroup: networking.k8s.io
    Sep  1 11:57:42.803: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Sep  1 11:57:42.803: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Sep  1 11:57:42.803: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Sep  1 11:57:42.803: INFO: Checking APIGroup: policy
    Sep  1 11:57:42.804: INFO: PreferredVersion.GroupVersion: policy/v1
    Sep  1 11:57:42.804: INFO: Versions found [{policy/v1 v1}]
    Sep  1 11:57:42.804: INFO: policy/v1 matches policy/v1
    Sep  1 11:57:42.804: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Sep  1 11:57:42.805: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Sep  1 11:57:42.805: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Sep  1 11:57:42.805: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Sep  1 11:57:42.805: INFO: Checking APIGroup: storage.k8s.io
    Sep  1 11:57:42.806: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Sep  1 11:57:42.806: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Sep  1 11:57:42.806: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Sep  1 11:57:42.806: INFO: Checking APIGroup: admissionregistration.k8s.io
    Sep  1 11:57:42.806: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Sep  1 11:57:42.806: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Sep  1 11:57:42.806: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Sep  1 11:57:42.806: INFO: Checking APIGroup: apiextensions.k8s.io
    Sep  1 11:57:42.807: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Sep  1 11:57:42.807: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Sep  1 11:57:42.807: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Sep  1 11:57:42.807: INFO: Checking APIGroup: scheduling.k8s.io
    Sep  1 11:57:42.808: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Sep  1 11:57:42.808: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Sep  1 11:57:42.808: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Sep  1 11:57:42.808: INFO: Checking APIGroup: coordination.k8s.io
    Sep  1 11:57:42.809: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Sep  1 11:57:42.809: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Sep  1 11:57:42.809: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Sep  1 11:57:42.809: INFO: Checking APIGroup: node.k8s.io
    Sep  1 11:57:42.810: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Sep  1 11:57:42.810: INFO: Versions found [{node.k8s.io/v1 v1}]
    Sep  1 11:57:42.810: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Sep  1 11:57:42.810: INFO: Checking APIGroup: discovery.k8s.io
    Sep  1 11:57:42.811: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Sep  1 11:57:42.811: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Sep  1 11:57:42.811: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Sep  1 11:57:42.811: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Sep  1 11:57:42.811: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Sep  1 11:57:42.811: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Sep  1 11:57:42.811: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Sep  1 11:57:42.811: INFO: Checking APIGroup: acme.cert-manager.io
    Sep  1 11:57:42.813: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
    Sep  1 11:57:42.813: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
    Sep  1 11:57:42.813: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
    Sep  1 11:57:42.813: INFO: Checking APIGroup: cert-manager.io
    Sep  1 11:57:42.814: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
    Sep  1 11:57:42.814: INFO: Versions found [{cert-manager.io/v1 v1}]
    Sep  1 11:57:42.814: INFO: cert-manager.io/v1 matches cert-manager.io/v1
    Sep  1 11:57:42.814: INFO: Checking APIGroup: kyverno.io
    Sep  1 11:57:42.815: INFO: PreferredVersion.GroupVersion: kyverno.io/v1
    Sep  1 11:57:42.815: INFO: Versions found [{kyverno.io/v1 v1} {kyverno.io/v2beta1 v2beta1} {kyverno.io/v1beta1 v1beta1} {kyverno.io/v2alpha1 v2alpha1} {kyverno.io/v1alpha2 v1alpha2}]
    Sep  1 11:57:42.815: INFO: kyverno.io/v1 matches kyverno.io/v1
    Sep  1 11:57:42.815: INFO: Checking APIGroup: monitoring.coreos.com
    Sep  1 11:57:42.816: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Sep  1 11:57:42.816: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Sep  1 11:57:42.816: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Sep  1 11:57:42.816: INFO: Checking APIGroup: velero.io
    Sep  1 11:57:42.817: INFO: PreferredVersion.GroupVersion: velero.io/v1
    Sep  1 11:57:42.817: INFO: Versions found [{velero.io/v1 v1}]
    Sep  1 11:57:42.817: INFO: velero.io/v1 matches velero.io/v1
    Sep  1 11:57:42.817: INFO: Checking APIGroup: kube-green.com
    Sep  1 11:57:42.818: INFO: PreferredVersion.GroupVersion: kube-green.com/v1alpha1
    Sep  1 11:57:42.818: INFO: Versions found [{kube-green.com/v1alpha1 v1alpha1}]
    Sep  1 11:57:42.818: INFO: kube-green.com/v1alpha1 matches kube-green.com/v1alpha1
    Sep  1 11:57:42.818: INFO: Checking APIGroup: logging-extensions.banzaicloud.io
    Sep  1 11:57:42.820: INFO: PreferredVersion.GroupVersion: logging-extensions.banzaicloud.io/v1alpha1
    Sep  1 11:57:42.820: INFO: Versions found [{logging-extensions.banzaicloud.io/v1alpha1 v1alpha1}]
    Sep  1 11:57:42.820: INFO: logging-extensions.banzaicloud.io/v1alpha1 matches logging-extensions.banzaicloud.io/v1alpha1
    Sep  1 11:57:42.820: INFO: Checking APIGroup: logging.banzaicloud.io
    Sep  1 11:57:42.820: INFO: PreferredVersion.GroupVersion: logging.banzaicloud.io/v1beta1
    Sep  1 11:57:42.820: INFO: Versions found [{logging.banzaicloud.io/v1beta1 v1beta1} {logging.banzaicloud.io/v1alpha1 v1alpha1}]
    Sep  1 11:57:42.820: INFO: logging.banzaicloud.io/v1beta1 matches logging.banzaicloud.io/v1beta1
    Sep  1 11:57:42.820: INFO: Checking APIGroup: traefik.containo.us
    Sep  1 11:57:42.821: INFO: PreferredVersion.GroupVersion: traefik.containo.us/v1alpha1
    Sep  1 11:57:42.821: INFO: Versions found [{traefik.containo.us/v1alpha1 v1alpha1}]
    Sep  1 11:57:42.821: INFO: traefik.containo.us/v1alpha1 matches traefik.containo.us/v1alpha1
    Sep  1 11:57:42.821: INFO: Checking APIGroup: traefik.io
    Sep  1 11:57:42.822: INFO: PreferredVersion.GroupVersion: traefik.io/v1alpha1
    Sep  1 11:57:42.822: INFO: Versions found [{traefik.io/v1alpha1 v1alpha1}]
    Sep  1 11:57:42.822: INFO: traefik.io/v1alpha1 matches traefik.io/v1alpha1
    Sep  1 11:57:42.822: INFO: Checking APIGroup: wgpolicyk8s.io
    Sep  1 11:57:42.823: INFO: PreferredVersion.GroupVersion: wgpolicyk8s.io/v1alpha2
    Sep  1 11:57:42.823: INFO: Versions found [{wgpolicyk8s.io/v1alpha2 v1alpha2}]
    Sep  1 11:57:42.823: INFO: wgpolicyk8s.io/v1alpha2 matches wgpolicyk8s.io/v1alpha2
    Sep  1 11:57:42.823: INFO: Checking APIGroup: rbacmanager.reactiveops.io
    Sep  1 11:57:42.824: INFO: PreferredVersion.GroupVersion: rbacmanager.reactiveops.io/v1beta1
    Sep  1 11:57:42.824: INFO: Versions found [{rbacmanager.reactiveops.io/v1beta1 v1beta1}]
    Sep  1 11:57:42.824: INFO: rbacmanager.reactiveops.io/v1beta1 matches rbacmanager.reactiveops.io/v1beta1
    Sep  1 11:57:42.824: INFO: Checking APIGroup: cilium.io
    Sep  1 11:57:42.825: INFO: PreferredVersion.GroupVersion: cilium.io/v2
    Sep  1 11:57:42.825: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
    Sep  1 11:57:42.825: INFO: cilium.io/v2 matches cilium.io/v2
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:57:42.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-8921" for this suite. 09/01/23 11:57:42.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:57:42.852
Sep  1 11:57:42.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename dns 09/01/23 11:57:42.853
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:42.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:42.88
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9441.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9441.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 09/01/23 11:57:42.888
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9441.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9441.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 09/01/23 11:57:42.929
STEP: creating a pod to probe /etc/hosts 09/01/23 11:57:42.929
STEP: submitting the pod to kubernetes 09/01/23 11:57:42.929
Sep  1 11:57:42.969: INFO: Waiting up to 15m0s for pod "dns-test-58c70ef7-f2bb-4b9e-85ae-79f555c5696a" in namespace "dns-9441" to be "running"
Sep  1 11:57:42.985: INFO: Pod "dns-test-58c70ef7-f2bb-4b9e-85ae-79f555c5696a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.817495ms
Sep  1 11:57:44.990: INFO: Pod "dns-test-58c70ef7-f2bb-4b9e-85ae-79f555c5696a": Phase="Running", Reason="", readiness=true. Elapsed: 2.020850369s
Sep  1 11:57:44.990: INFO: Pod "dns-test-58c70ef7-f2bb-4b9e-85ae-79f555c5696a" satisfied condition "running"
STEP: retrieving the pod 09/01/23 11:57:44.99
STEP: looking for the results for each expected name from probers 09/01/23 11:57:44.993
Sep  1 11:57:45.025: INFO: DNS probes using dns-9441/dns-test-58c70ef7-f2bb-4b9e-85ae-79f555c5696a succeeded

STEP: deleting the pod 09/01/23 11:57:45.025
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  1 11:57:45.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9441" for this suite. 09/01/23 11:57:45.057
------------------------------
• [2.242 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:57:42.852
    Sep  1 11:57:42.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename dns 09/01/23 11:57:42.853
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:42.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:42.88
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9441.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9441.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     09/01/23 11:57:42.888
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9441.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9441.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     09/01/23 11:57:42.929
    STEP: creating a pod to probe /etc/hosts 09/01/23 11:57:42.929
    STEP: submitting the pod to kubernetes 09/01/23 11:57:42.929
    Sep  1 11:57:42.969: INFO: Waiting up to 15m0s for pod "dns-test-58c70ef7-f2bb-4b9e-85ae-79f555c5696a" in namespace "dns-9441" to be "running"
    Sep  1 11:57:42.985: INFO: Pod "dns-test-58c70ef7-f2bb-4b9e-85ae-79f555c5696a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.817495ms
    Sep  1 11:57:44.990: INFO: Pod "dns-test-58c70ef7-f2bb-4b9e-85ae-79f555c5696a": Phase="Running", Reason="", readiness=true. Elapsed: 2.020850369s
    Sep  1 11:57:44.990: INFO: Pod "dns-test-58c70ef7-f2bb-4b9e-85ae-79f555c5696a" satisfied condition "running"
    STEP: retrieving the pod 09/01/23 11:57:44.99
    STEP: looking for the results for each expected name from probers 09/01/23 11:57:44.993
    Sep  1 11:57:45.025: INFO: DNS probes using dns-9441/dns-test-58c70ef7-f2bb-4b9e-85ae-79f555c5696a succeeded

    STEP: deleting the pod 09/01/23 11:57:45.025
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:57:45.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9441" for this suite. 09/01/23 11:57:45.057
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:57:45.101
Sep  1 11:57:45.101: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 11:57:45.102
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:45.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:45.144
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 11:57:45.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2553" for this suite. 09/01/23 11:57:45.157
------------------------------
• [0.064 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:57:45.101
    Sep  1 11:57:45.101: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 11:57:45.102
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:45.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:45.144
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:57:45.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2553" for this suite. 09/01/23 11:57:45.157
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:57:45.168
Sep  1 11:57:45.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-probe 09/01/23 11:57:45.17
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:45.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:45.192
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  1 11:58:45.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9830" for this suite. 09/01/23 11:58:45.213
------------------------------
• [SLOW TEST] [60.052 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:57:45.168
    Sep  1 11:57:45.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-probe 09/01/23 11:57:45.17
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:57:45.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:57:45.192
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:58:45.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9830" for this suite. 09/01/23 11:58:45.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:58:45.223
Sep  1 11:58:45.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename kubectl 09/01/23 11:58:45.225
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:58:45.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:58:45.246
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 09/01/23 11:58:45.25
Sep  1 11:58:45.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-3595 api-versions'
Sep  1 11:58:45.520: INFO: stderr: ""
Sep  1 11:58:45.520: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nkube-green.com/v1alpha1\nkyverno.io/v1\nkyverno.io/v1alpha2\nkyverno.io/v1beta1\nkyverno.io/v2alpha1\nkyverno.io/v2beta1\nlogging-extensions.banzaicloud.io/v1alpha1\nlogging.banzaicloud.io/v1alpha1\nlogging.banzaicloud.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nrbacmanager.reactiveops.io/v1beta1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntraefik.containo.us/v1alpha1\ntraefik.io/v1alpha1\nv1\nvelero.io/v1\nwgpolicyk8s.io/v1alpha2\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  1 11:58:45.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3595" for this suite. 09/01/23 11:58:45.524
------------------------------
• [0.307 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:58:45.223
    Sep  1 11:58:45.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename kubectl 09/01/23 11:58:45.225
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:58:45.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:58:45.246
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 09/01/23 11:58:45.25
    Sep  1 11:58:45.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=kubectl-3595 api-versions'
    Sep  1 11:58:45.520: INFO: stderr: ""
    Sep  1 11:58:45.520: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nkube-green.com/v1alpha1\nkyverno.io/v1\nkyverno.io/v1alpha2\nkyverno.io/v1beta1\nkyverno.io/v2alpha1\nkyverno.io/v2beta1\nlogging-extensions.banzaicloud.io/v1alpha1\nlogging.banzaicloud.io/v1alpha1\nlogging.banzaicloud.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nrbacmanager.reactiveops.io/v1beta1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntraefik.containo.us/v1alpha1\ntraefik.io/v1alpha1\nv1\nvelero.io/v1\nwgpolicyk8s.io/v1alpha2\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:58:45.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3595" for this suite. 09/01/23 11:58:45.524
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:58:45.531
Sep  1 11:58:45.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename subpath 09/01/23 11:58:45.533
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:58:45.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:58:45.555
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/01/23 11:58:45.558
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-zztb 09/01/23 11:58:45.572
STEP: Creating a pod to test atomic-volume-subpath 09/01/23 11:58:45.572
Sep  1 11:58:45.581: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-zztb" in namespace "subpath-6221" to be "Succeeded or Failed"
Sep  1 11:58:45.585: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.820371ms
Sep  1 11:58:47.589: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007882331s
Sep  1 11:58:49.594: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 4.012925885s
Sep  1 11:58:51.590: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 6.009030089s
Sep  1 11:58:53.589: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 8.007540518s
Sep  1 11:58:55.590: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 10.008820418s
Sep  1 11:58:57.589: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 12.007877217s
Sep  1 11:58:59.590: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 14.008798893s
Sep  1 11:59:01.589: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 16.007856306s
Sep  1 11:59:03.589: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 18.007723199s
Sep  1 11:59:05.589: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 20.008032608s
Sep  1 11:59:07.591: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=false. Elapsed: 22.009477676s
Sep  1 11:59:09.590: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008432316s
STEP: Saw pod success 09/01/23 11:59:09.59
Sep  1 11:59:09.590: INFO: Pod "pod-subpath-test-downwardapi-zztb" satisfied condition "Succeeded or Failed"
Sep  1 11:59:09.593: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-subpath-test-downwardapi-zztb container test-container-subpath-downwardapi-zztb: <nil>
STEP: delete the pod 09/01/23 11:59:09.601
Sep  1 11:59:09.617: INFO: Waiting for pod pod-subpath-test-downwardapi-zztb to disappear
Sep  1 11:59:09.620: INFO: Pod pod-subpath-test-downwardapi-zztb no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-zztb 09/01/23 11:59:09.62
Sep  1 11:59:09.620: INFO: Deleting pod "pod-subpath-test-downwardapi-zztb" in namespace "subpath-6221"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  1 11:59:09.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6221" for this suite. 09/01/23 11:59:09.629
------------------------------
• [SLOW TEST] [24.105 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:58:45.531
    Sep  1 11:58:45.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename subpath 09/01/23 11:58:45.533
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:58:45.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:58:45.555
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/01/23 11:58:45.558
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-zztb 09/01/23 11:58:45.572
    STEP: Creating a pod to test atomic-volume-subpath 09/01/23 11:58:45.572
    Sep  1 11:58:45.581: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-zztb" in namespace "subpath-6221" to be "Succeeded or Failed"
    Sep  1 11:58:45.585: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.820371ms
    Sep  1 11:58:47.589: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007882331s
    Sep  1 11:58:49.594: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 4.012925885s
    Sep  1 11:58:51.590: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 6.009030089s
    Sep  1 11:58:53.589: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 8.007540518s
    Sep  1 11:58:55.590: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 10.008820418s
    Sep  1 11:58:57.589: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 12.007877217s
    Sep  1 11:58:59.590: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 14.008798893s
    Sep  1 11:59:01.589: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 16.007856306s
    Sep  1 11:59:03.589: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 18.007723199s
    Sep  1 11:59:05.589: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=true. Elapsed: 20.008032608s
    Sep  1 11:59:07.591: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Running", Reason="", readiness=false. Elapsed: 22.009477676s
    Sep  1 11:59:09.590: INFO: Pod "pod-subpath-test-downwardapi-zztb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008432316s
    STEP: Saw pod success 09/01/23 11:59:09.59
    Sep  1 11:59:09.590: INFO: Pod "pod-subpath-test-downwardapi-zztb" satisfied condition "Succeeded or Failed"
    Sep  1 11:59:09.593: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-subpath-test-downwardapi-zztb container test-container-subpath-downwardapi-zztb: <nil>
    STEP: delete the pod 09/01/23 11:59:09.601
    Sep  1 11:59:09.617: INFO: Waiting for pod pod-subpath-test-downwardapi-zztb to disappear
    Sep  1 11:59:09.620: INFO: Pod pod-subpath-test-downwardapi-zztb no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-zztb 09/01/23 11:59:09.62
    Sep  1 11:59:09.620: INFO: Deleting pod "pod-subpath-test-downwardapi-zztb" in namespace "subpath-6221"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:59:09.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6221" for this suite. 09/01/23 11:59:09.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:59:09.643
Sep  1 11:59:09.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename gc 09/01/23 11:59:09.644
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:59:09.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:59:09.667
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 09/01/23 11:59:09.673
STEP: delete the rc 09/01/23 11:59:14.683
STEP: wait for all pods to be garbage collected 09/01/23 11:59:14.69
STEP: Gathering metrics 09/01/23 11:59:19.697
Sep  1 11:59:19.723: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
Sep  1 11:59:19.727: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.167968ms
Sep  1 11:59:19.728: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
Sep  1 11:59:19.728: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
Sep  1 11:59:19.854: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  1 11:59:19.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9061" for this suite. 09/01/23 11:59:19.859
------------------------------
• [SLOW TEST] [10.225 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:59:09.643
    Sep  1 11:59:09.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename gc 09/01/23 11:59:09.644
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:59:09.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:59:09.667
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 09/01/23 11:59:09.673
    STEP: delete the rc 09/01/23 11:59:14.683
    STEP: wait for all pods to be garbage collected 09/01/23 11:59:14.69
    STEP: Gathering metrics 09/01/23 11:59:19.697
    Sep  1 11:59:19.723: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
    Sep  1 11:59:19.727: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.167968ms
    Sep  1 11:59:19.728: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
    Sep  1 11:59:19.728: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
    Sep  1 11:59:19.854: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:59:19.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9061" for this suite. 09/01/23 11:59:19.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:59:19.868
Sep  1 11:59:19.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-lifecycle-hook 09/01/23 11:59:19.87
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:59:19.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:59:19.892
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/01/23 11:59:19.898
Sep  1 11:59:19.907: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7151" to be "running and ready"
Sep  1 11:59:19.910: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.43373ms
Sep  1 11:59:19.910: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:59:21.914: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007283676s
Sep  1 11:59:21.914: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  1 11:59:21.914: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 09/01/23 11:59:21.917
Sep  1 11:59:21.922: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7151" to be "running and ready"
Sep  1 11:59:21.925: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.130742ms
Sep  1 11:59:21.925: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:59:23.929: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007081867s
Sep  1 11:59:23.929: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Sep  1 11:59:23.929: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/01/23 11:59:23.932
Sep  1 11:59:23.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  1 11:59:23.949: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  1 11:59:25.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  1 11:59:25.954: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  1 11:59:27.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  1 11:59:27.954: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 09/01/23 11:59:27.954
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  1 11:59:27.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7151" for this suite. 09/01/23 11:59:27.966
------------------------------
• [SLOW TEST] [8.106 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:59:19.868
    Sep  1 11:59:19.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/01/23 11:59:19.87
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:59:19.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:59:19.892
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/01/23 11:59:19.898
    Sep  1 11:59:19.907: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7151" to be "running and ready"
    Sep  1 11:59:19.910: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.43373ms
    Sep  1 11:59:19.910: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:59:21.914: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007283676s
    Sep  1 11:59:21.914: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  1 11:59:21.914: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 09/01/23 11:59:21.917
    Sep  1 11:59:21.922: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7151" to be "running and ready"
    Sep  1 11:59:21.925: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.130742ms
    Sep  1 11:59:21.925: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:59:23.929: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007081867s
    Sep  1 11:59:23.929: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Sep  1 11:59:23.929: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/01/23 11:59:23.932
    Sep  1 11:59:23.940: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  1 11:59:23.949: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep  1 11:59:25.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  1 11:59:25.954: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep  1 11:59:27.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  1 11:59:27.954: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 09/01/23 11:59:27.954
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:59:27.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7151" for this suite. 09/01/23 11:59:27.966
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:59:27.98
Sep  1 11:59:27.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename crd-webhook 09/01/23 11:59:27.982
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:59:27.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:59:28.004
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/01/23 11:59:28.008
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/01/23 11:59:29.027
STEP: Deploying the custom resource conversion webhook pod 09/01/23 11:59:29.034
STEP: Wait for the deployment to be ready 09/01/23 11:59:29.052
Sep  1 11:59:29.058: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 09/01/23 11:59:31.069
STEP: Verifying the service has paired with the endpoint 09/01/23 11:59:31.096
Sep  1 11:59:32.097: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Sep  1 11:59:32.101: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Creating a v1 custom resource 09/01/23 11:59:35.04
STEP: Create a v2 custom resource 09/01/23 11:59:35.179
STEP: List CRs in v1 09/01/23 11:59:35.323
STEP: List CRs in v2 09/01/23 11:59:35.335
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 11:59:35.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-7320" for this suite. 09/01/23 11:59:35.984
------------------------------
• [SLOW TEST] [8.039 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:59:27.98
    Sep  1 11:59:27.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename crd-webhook 09/01/23 11:59:27.982
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:59:27.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:59:28.004
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/01/23 11:59:28.008
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/01/23 11:59:29.027
    STEP: Deploying the custom resource conversion webhook pod 09/01/23 11:59:29.034
    STEP: Wait for the deployment to be ready 09/01/23 11:59:29.052
    Sep  1 11:59:29.058: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 09/01/23 11:59:31.069
    STEP: Verifying the service has paired with the endpoint 09/01/23 11:59:31.096
    Sep  1 11:59:32.097: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Sep  1 11:59:32.101: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Creating a v1 custom resource 09/01/23 11:59:35.04
    STEP: Create a v2 custom resource 09/01/23 11:59:35.179
    STEP: List CRs in v1 09/01/23 11:59:35.323
    STEP: List CRs in v2 09/01/23 11:59:35.335
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:59:35.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-7320" for this suite. 09/01/23 11:59:35.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:59:36.033
Sep  1 11:59:36.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-lifecycle-hook 09/01/23 11:59:36.035
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:59:36.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:59:36.089
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/01/23 11:59:36.11
Sep  1 11:59:36.122: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8652" to be "running and ready"
Sep  1 11:59:36.137: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.616757ms
Sep  1 11:59:36.137: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:59:38.144: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021575102s
Sep  1 11:59:38.144: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:59:40.177: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.054115093s
Sep  1 11:59:40.177: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  1 11:59:40.177: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 09/01/23 11:59:40.198
Sep  1 11:59:40.206: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8652" to be "running and ready"
Sep  1 11:59:40.214: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.545879ms
Sep  1 11:59:40.214: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  1 11:59:42.218: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012768756s
Sep  1 11:59:42.218: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Sep  1 11:59:42.218: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/01/23 11:59:42.221
STEP: delete the pod with lifecycle hook 09/01/23 11:59:42.23
Sep  1 11:59:42.239: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  1 11:59:42.253: INFO: Pod pod-with-poststart-http-hook still exists
Sep  1 11:59:44.254: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  1 11:59:44.257: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  1 11:59:44.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-8652" for this suite. 09/01/23 11:59:44.263
------------------------------
• [SLOW TEST] [8.238 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:59:36.033
    Sep  1 11:59:36.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/01/23 11:59:36.035
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:59:36.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:59:36.089
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/01/23 11:59:36.11
    Sep  1 11:59:36.122: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8652" to be "running and ready"
    Sep  1 11:59:36.137: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.616757ms
    Sep  1 11:59:36.137: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:59:38.144: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021575102s
    Sep  1 11:59:38.144: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:59:40.177: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.054115093s
    Sep  1 11:59:40.177: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  1 11:59:40.177: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 09/01/23 11:59:40.198
    Sep  1 11:59:40.206: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8652" to be "running and ready"
    Sep  1 11:59:40.214: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.545879ms
    Sep  1 11:59:40.214: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 11:59:42.218: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012768756s
    Sep  1 11:59:42.218: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Sep  1 11:59:42.218: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/01/23 11:59:42.221
    STEP: delete the pod with lifecycle hook 09/01/23 11:59:42.23
    Sep  1 11:59:42.239: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  1 11:59:42.253: INFO: Pod pod-with-poststart-http-hook still exists
    Sep  1 11:59:44.254: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  1 11:59:44.257: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  1 11:59:44.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-8652" for this suite. 09/01/23 11:59:44.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 11:59:44.276
Sep  1 11:59:44.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename subpath 09/01/23 11:59:44.278
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:59:44.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:59:44.301
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/01/23 11:59:44.304
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-knw4 09/01/23 11:59:44.314
STEP: Creating a pod to test atomic-volume-subpath 09/01/23 11:59:44.314
Sep  1 11:59:44.329: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-knw4" in namespace "subpath-6787" to be "Succeeded or Failed"
Sep  1 11:59:44.335: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.898156ms
Sep  1 11:59:46.340: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010413606s
Sep  1 11:59:48.340: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 4.010489128s
Sep  1 11:59:50.342: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 6.011979773s
Sep  1 11:59:52.340: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 8.010403186s
Sep  1 11:59:54.340: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 10.010083484s
Sep  1 11:59:56.341: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 12.011261607s
Sep  1 11:59:58.340: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 14.010582429s
Sep  1 12:00:00.339: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 16.009922017s
Sep  1 12:00:02.341: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 18.011115998s
Sep  1 12:00:04.339: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 20.009679038s
Sep  1 12:00:06.340: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=false. Elapsed: 22.010834136s
Sep  1 12:00:08.340: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010303116s
STEP: Saw pod success 09/01/23 12:00:08.34
Sep  1 12:00:08.341: INFO: Pod "pod-subpath-test-secret-knw4" satisfied condition "Succeeded or Failed"
Sep  1 12:00:08.343: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-subpath-test-secret-knw4 container test-container-subpath-secret-knw4: <nil>
STEP: delete the pod 09/01/23 12:00:08.35
Sep  1 12:00:08.364: INFO: Waiting for pod pod-subpath-test-secret-knw4 to disappear
Sep  1 12:00:08.367: INFO: Pod pod-subpath-test-secret-knw4 no longer exists
STEP: Deleting pod pod-subpath-test-secret-knw4 09/01/23 12:00:08.367
Sep  1 12:00:08.367: INFO: Deleting pod "pod-subpath-test-secret-knw4" in namespace "subpath-6787"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  1 12:00:08.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6787" for this suite. 09/01/23 12:00:08.375
------------------------------
• [SLOW TEST] [24.106 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 11:59:44.276
    Sep  1 11:59:44.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename subpath 09/01/23 11:59:44.278
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 11:59:44.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 11:59:44.301
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/01/23 11:59:44.304
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-knw4 09/01/23 11:59:44.314
    STEP: Creating a pod to test atomic-volume-subpath 09/01/23 11:59:44.314
    Sep  1 11:59:44.329: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-knw4" in namespace "subpath-6787" to be "Succeeded or Failed"
    Sep  1 11:59:44.335: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.898156ms
    Sep  1 11:59:46.340: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010413606s
    Sep  1 11:59:48.340: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 4.010489128s
    Sep  1 11:59:50.342: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 6.011979773s
    Sep  1 11:59:52.340: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 8.010403186s
    Sep  1 11:59:54.340: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 10.010083484s
    Sep  1 11:59:56.341: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 12.011261607s
    Sep  1 11:59:58.340: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 14.010582429s
    Sep  1 12:00:00.339: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 16.009922017s
    Sep  1 12:00:02.341: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 18.011115998s
    Sep  1 12:00:04.339: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=true. Elapsed: 20.009679038s
    Sep  1 12:00:06.340: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Running", Reason="", readiness=false. Elapsed: 22.010834136s
    Sep  1 12:00:08.340: INFO: Pod "pod-subpath-test-secret-knw4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010303116s
    STEP: Saw pod success 09/01/23 12:00:08.34
    Sep  1 12:00:08.341: INFO: Pod "pod-subpath-test-secret-knw4" satisfied condition "Succeeded or Failed"
    Sep  1 12:00:08.343: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-subpath-test-secret-knw4 container test-container-subpath-secret-knw4: <nil>
    STEP: delete the pod 09/01/23 12:00:08.35
    Sep  1 12:00:08.364: INFO: Waiting for pod pod-subpath-test-secret-knw4 to disappear
    Sep  1 12:00:08.367: INFO: Pod pod-subpath-test-secret-knw4 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-knw4 09/01/23 12:00:08.367
    Sep  1 12:00:08.367: INFO: Deleting pod "pod-subpath-test-secret-knw4" in namespace "subpath-6787"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:00:08.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6787" for this suite. 09/01/23 12:00:08.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:00:08.393
Sep  1 12:00:08.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename endpointslice 09/01/23 12:00:08.394
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:00:08.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:00:08.419
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 09/01/23 12:00:13.535
STEP: referencing matching pods with named port 09/01/23 12:00:18.578
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/01/23 12:00:23.588
STEP: recreating EndpointSlices after they've been deleted 09/01/23 12:00:28.596
Sep  1 12:00:28.624: INFO: EndpointSlice for Service endpointslice-6121/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  1 12:00:38.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6121" for this suite. 09/01/23 12:00:38.662
------------------------------
• [SLOW TEST] [30.276 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:00:08.393
    Sep  1 12:00:08.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename endpointslice 09/01/23 12:00:08.394
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:00:08.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:00:08.419
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 09/01/23 12:00:13.535
    STEP: referencing matching pods with named port 09/01/23 12:00:18.578
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/01/23 12:00:23.588
    STEP: recreating EndpointSlices after they've been deleted 09/01/23 12:00:28.596
    Sep  1 12:00:28.624: INFO: EndpointSlice for Service endpointslice-6121/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:00:38.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6121" for this suite. 09/01/23 12:00:38.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:00:38.676
Sep  1 12:00:38.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename cronjob 09/01/23 12:00:38.678
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:00:38.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:00:38.699
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 09/01/23 12:00:38.703
STEP: Ensuring more than one job is running at a time 09/01/23 12:00:38.71
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/01/23 12:02:00.715
STEP: Removing cronjob 09/01/23 12:02:00.721
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  1 12:02:00.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6880" for this suite. 09/01/23 12:02:00.737
------------------------------
• [SLOW TEST] [82.098 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:00:38.676
    Sep  1 12:00:38.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename cronjob 09/01/23 12:00:38.678
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:00:38.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:00:38.699
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 09/01/23 12:00:38.703
    STEP: Ensuring more than one job is running at a time 09/01/23 12:00:38.71
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/01/23 12:02:00.715
    STEP: Removing cronjob 09/01/23 12:02:00.721
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:02:00.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6880" for this suite. 09/01/23 12:02:00.737
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:02:00.788
Sep  1 12:02:00.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename emptydir 09/01/23 12:02:00.795
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:00.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:00.836
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 09/01/23 12:02:00.84
Sep  1 12:02:00.851: INFO: Waiting up to 5m0s for pod "pod-ae4fb811-4fbe-47cb-b890-cf147845401e" in namespace "emptydir-9619" to be "Succeeded or Failed"
Sep  1 12:02:00.856: INFO: Pod "pod-ae4fb811-4fbe-47cb-b890-cf147845401e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.321934ms
Sep  1 12:02:02.860: INFO: Pod "pod-ae4fb811-4fbe-47cb-b890-cf147845401e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008944765s
Sep  1 12:02:04.862: INFO: Pod "pod-ae4fb811-4fbe-47cb-b890-cf147845401e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010206442s
STEP: Saw pod success 09/01/23 12:02:04.862
Sep  1 12:02:04.862: INFO: Pod "pod-ae4fb811-4fbe-47cb-b890-cf147845401e" satisfied condition "Succeeded or Failed"
Sep  1 12:02:04.866: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-ae4fb811-4fbe-47cb-b890-cf147845401e container test-container: <nil>
STEP: delete the pod 09/01/23 12:02:04.886
Sep  1 12:02:04.906: INFO: Waiting for pod pod-ae4fb811-4fbe-47cb-b890-cf147845401e to disappear
Sep  1 12:02:04.910: INFO: Pod pod-ae4fb811-4fbe-47cb-b890-cf147845401e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  1 12:02:04.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9619" for this suite. 09/01/23 12:02:04.915
------------------------------
• [4.135 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:02:00.788
    Sep  1 12:02:00.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename emptydir 09/01/23 12:02:00.795
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:00.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:00.836
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/01/23 12:02:00.84
    Sep  1 12:02:00.851: INFO: Waiting up to 5m0s for pod "pod-ae4fb811-4fbe-47cb-b890-cf147845401e" in namespace "emptydir-9619" to be "Succeeded or Failed"
    Sep  1 12:02:00.856: INFO: Pod "pod-ae4fb811-4fbe-47cb-b890-cf147845401e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.321934ms
    Sep  1 12:02:02.860: INFO: Pod "pod-ae4fb811-4fbe-47cb-b890-cf147845401e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008944765s
    Sep  1 12:02:04.862: INFO: Pod "pod-ae4fb811-4fbe-47cb-b890-cf147845401e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010206442s
    STEP: Saw pod success 09/01/23 12:02:04.862
    Sep  1 12:02:04.862: INFO: Pod "pod-ae4fb811-4fbe-47cb-b890-cf147845401e" satisfied condition "Succeeded or Failed"
    Sep  1 12:02:04.866: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-ae4fb811-4fbe-47cb-b890-cf147845401e container test-container: <nil>
    STEP: delete the pod 09/01/23 12:02:04.886
    Sep  1 12:02:04.906: INFO: Waiting for pod pod-ae4fb811-4fbe-47cb-b890-cf147845401e to disappear
    Sep  1 12:02:04.910: INFO: Pod pod-ae4fb811-4fbe-47cb-b890-cf147845401e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:02:04.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9619" for this suite. 09/01/23 12:02:04.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:02:04.928
Sep  1 12:02:04.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename daemonsets 09/01/23 12:02:04.93
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:04.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:04.966
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
STEP: Creating a simple DaemonSet "daemon-set" 09/01/23 12:02:04.99
STEP: Check that daemon pods launch on every node of the cluster. 09/01/23 12:02:04.998
Sep  1 12:02:05.010: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 12:02:05.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 12:02:05.016: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 12:02:06.021: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 12:02:06.029: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 12:02:06.029: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 12:02:07.021: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 12:02:07.025: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  1 12:02:07.026: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/01/23 12:02:07.029
Sep  1 12:02:07.051: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 12:02:07.058: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 12:02:07.058: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 12:02:08.066: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 12:02:08.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  1 12:02:08.070: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Sep  1 12:02:09.065: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  1 12:02:09.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  1 12:02:09.069: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 09/01/23 12:02:09.069
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 09/01/23 12:02:09.082
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8310, will wait for the garbage collector to delete the pods 09/01/23 12:02:09.082
Sep  1 12:02:09.144: INFO: Deleting DaemonSet.extensions daemon-set took: 7.188976ms
Sep  1 12:02:09.244: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.188012ms
Sep  1 12:02:11.953: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  1 12:02:11.953: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  1 12:02:11.963: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"58077"},"items":null}

Sep  1 12:02:11.973: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"58077"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  1 12:02:11.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8310" for this suite. 09/01/23 12:02:12.01
------------------------------
• [SLOW TEST] [7.097 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:02:04.928
    Sep  1 12:02:04.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename daemonsets 09/01/23 12:02:04.93
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:04.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:04.966
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:305
    STEP: Creating a simple DaemonSet "daemon-set" 09/01/23 12:02:04.99
    STEP: Check that daemon pods launch on every node of the cluster. 09/01/23 12:02:04.998
    Sep  1 12:02:05.010: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 12:02:05.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 12:02:05.016: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 12:02:06.021: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 12:02:06.029: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 12:02:06.029: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 12:02:07.021: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 12:02:07.025: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  1 12:02:07.026: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/01/23 12:02:07.029
    Sep  1 12:02:07.051: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 12:02:07.058: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 12:02:07.058: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 12:02:08.066: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 12:02:08.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  1 12:02:08.070: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Sep  1 12:02:09.065: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  1 12:02:09.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  1 12:02:09.069: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 09/01/23 12:02:09.069
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 09/01/23 12:02:09.082
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8310, will wait for the garbage collector to delete the pods 09/01/23 12:02:09.082
    Sep  1 12:02:09.144: INFO: Deleting DaemonSet.extensions daemon-set took: 7.188976ms
    Sep  1 12:02:09.244: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.188012ms
    Sep  1 12:02:11.953: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  1 12:02:11.953: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  1 12:02:11.963: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"58077"},"items":null}

    Sep  1 12:02:11.973: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"58077"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:02:11.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8310" for this suite. 09/01/23 12:02:12.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:02:12.033
Sep  1 12:02:12.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename webhook 09/01/23 12:02:12.036
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:12.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:12.102
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/01/23 12:02:12.147
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 12:02:12.777
STEP: Deploying the webhook pod 09/01/23 12:02:12.786
STEP: Wait for the deployment to be ready 09/01/23 12:02:12.802
Sep  1 12:02:12.829: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/01/23 12:02:14.839
STEP: Verifying the service has paired with the endpoint 09/01/23 12:02:14.854
Sep  1 12:02:15.854: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Sep  1 12:02:15.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8290-crds.webhook.example.com via the AdmissionRegistration API 09/01/23 12:02:16.37
STEP: Creating a custom resource that should be mutated by the webhook 09/01/23 12:02:16.393
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 12:02:18.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9008" for this suite. 09/01/23 12:02:19.066
STEP: Destroying namespace "webhook-9008-markers" for this suite. 09/01/23 12:02:19.092
------------------------------
• [SLOW TEST] [7.081 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:02:12.033
    Sep  1 12:02:12.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename webhook 09/01/23 12:02:12.036
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:12.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:12.102
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/01/23 12:02:12.147
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/01/23 12:02:12.777
    STEP: Deploying the webhook pod 09/01/23 12:02:12.786
    STEP: Wait for the deployment to be ready 09/01/23 12:02:12.802
    Sep  1 12:02:12.829: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/01/23 12:02:14.839
    STEP: Verifying the service has paired with the endpoint 09/01/23 12:02:14.854
    Sep  1 12:02:15.854: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Sep  1 12:02:15.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8290-crds.webhook.example.com via the AdmissionRegistration API 09/01/23 12:02:16.37
    STEP: Creating a custom resource that should be mutated by the webhook 09/01/23 12:02:16.393
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:02:18.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9008" for this suite. 09/01/23 12:02:19.066
    STEP: Destroying namespace "webhook-9008-markers" for this suite. 09/01/23 12:02:19.092
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:02:19.143
Sep  1 12:02:19.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename ingressclass 09/01/23 12:02:19.144
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:19.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:19.189
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 09/01/23 12:02:19.207
STEP: getting /apis/networking.k8s.io 09/01/23 12:02:19.248
STEP: getting /apis/networking.k8s.iov1 09/01/23 12:02:19.253
STEP: creating 09/01/23 12:02:19.257
STEP: getting 09/01/23 12:02:19.316
STEP: listing 09/01/23 12:02:19.325
STEP: watching 09/01/23 12:02:19.331
Sep  1 12:02:19.331: INFO: starting watch
STEP: patching 09/01/23 12:02:19.334
STEP: updating 09/01/23 12:02:19.342
Sep  1 12:02:19.349: INFO: waiting for watch events with expected annotations
Sep  1 12:02:19.349: INFO: saw patched and updated annotations
STEP: deleting 09/01/23 12:02:19.349
STEP: deleting a collection 09/01/23 12:02:19.366
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Sep  1 12:02:19.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-3500" for this suite. 09/01/23 12:02:19.394
------------------------------
• [0.266 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:02:19.143
    Sep  1 12:02:19.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename ingressclass 09/01/23 12:02:19.144
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:19.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:19.189
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 09/01/23 12:02:19.207
    STEP: getting /apis/networking.k8s.io 09/01/23 12:02:19.248
    STEP: getting /apis/networking.k8s.iov1 09/01/23 12:02:19.253
    STEP: creating 09/01/23 12:02:19.257
    STEP: getting 09/01/23 12:02:19.316
    STEP: listing 09/01/23 12:02:19.325
    STEP: watching 09/01/23 12:02:19.331
    Sep  1 12:02:19.331: INFO: starting watch
    STEP: patching 09/01/23 12:02:19.334
    STEP: updating 09/01/23 12:02:19.342
    Sep  1 12:02:19.349: INFO: waiting for watch events with expected annotations
    Sep  1 12:02:19.349: INFO: saw patched and updated annotations
    STEP: deleting 09/01/23 12:02:19.349
    STEP: deleting a collection 09/01/23 12:02:19.366
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:02:19.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-3500" for this suite. 09/01/23 12:02:19.394
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:02:19.412
Sep  1 12:02:19.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename resourcequota 09/01/23 12:02:19.422
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:19.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:19.448
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 09/01/23 12:02:19.452
STEP: Creating a ResourceQuota 09/01/23 12:02:24.532
STEP: Ensuring resource quota status is calculated 09/01/23 12:02:24.544
STEP: Creating a Pod that fits quota 09/01/23 12:02:26.549
STEP: Ensuring ResourceQuota status captures the pod usage 09/01/23 12:02:26.568
STEP: Not allowing a pod to be created that exceeds remaining quota 09/01/23 12:02:28.573
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/01/23 12:02:28.576
STEP: Ensuring a pod cannot update its resource requirements 09/01/23 12:02:28.58
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/01/23 12:02:28.588
STEP: Deleting the pod 09/01/23 12:02:30.592
STEP: Ensuring resource quota status released the pod usage 09/01/23 12:02:30.61
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  1 12:02:32.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3617" for this suite. 09/01/23 12:02:32.62
------------------------------
• [SLOW TEST] [13.216 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:02:19.412
    Sep  1 12:02:19.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename resourcequota 09/01/23 12:02:19.422
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:19.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:19.448
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 09/01/23 12:02:19.452
    STEP: Creating a ResourceQuota 09/01/23 12:02:24.532
    STEP: Ensuring resource quota status is calculated 09/01/23 12:02:24.544
    STEP: Creating a Pod that fits quota 09/01/23 12:02:26.549
    STEP: Ensuring ResourceQuota status captures the pod usage 09/01/23 12:02:26.568
    STEP: Not allowing a pod to be created that exceeds remaining quota 09/01/23 12:02:28.573
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/01/23 12:02:28.576
    STEP: Ensuring a pod cannot update its resource requirements 09/01/23 12:02:28.58
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/01/23 12:02:28.588
    STEP: Deleting the pod 09/01/23 12:02:30.592
    STEP: Ensuring resource quota status released the pod usage 09/01/23 12:02:30.61
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:02:32.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3617" for this suite. 09/01/23 12:02:32.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:02:32.633
Sep  1 12:02:32.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename init-container 09/01/23 12:02:32.635
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:32.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:32.663
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 09/01/23 12:02:32.667
Sep  1 12:02:32.667: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  1 12:02:37.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9870" for this suite. 09/01/23 12:02:37.908
------------------------------
• [SLOW TEST] [5.286 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:02:32.633
    Sep  1 12:02:32.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename init-container 09/01/23 12:02:32.635
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:32.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:32.663
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 09/01/23 12:02:32.667
    Sep  1 12:02:32.667: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:02:37.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9870" for this suite. 09/01/23 12:02:37.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:02:37.921
Sep  1 12:02:37.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename watch 09/01/23 12:02:37.923
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:37.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:37.962
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 09/01/23 12:02:37.967
STEP: creating a new configmap 09/01/23 12:02:37.969
STEP: modifying the configmap once 09/01/23 12:02:37.976
STEP: closing the watch once it receives two notifications 09/01/23 12:02:37.988
Sep  1 12:02:37.988: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9390  190ce6b8-f5b0-4166-bbc9-8b610f2f1cbf 58383 0 2023-09-01 12:02:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-01 12:02:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  1 12:02:37.988: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9390  190ce6b8-f5b0-4166-bbc9-8b610f2f1cbf 58384 0 2023-09-01 12:02:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-01 12:02:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 09/01/23 12:02:37.989
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/01/23 12:02:38.003
STEP: deleting the configmap 09/01/23 12:02:38.005
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/01/23 12:02:38.015
Sep  1 12:02:38.015: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9390  190ce6b8-f5b0-4166-bbc9-8b610f2f1cbf 58385 0 2023-09-01 12:02:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-01 12:02:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  1 12:02:38.016: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9390  190ce6b8-f5b0-4166-bbc9-8b610f2f1cbf 58386 0 2023-09-01 12:02:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-01 12:02:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  1 12:02:38.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9390" for this suite. 09/01/23 12:02:38.022
------------------------------
• [0.111 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:02:37.921
    Sep  1 12:02:37.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename watch 09/01/23 12:02:37.923
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:37.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:37.962
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 09/01/23 12:02:37.967
    STEP: creating a new configmap 09/01/23 12:02:37.969
    STEP: modifying the configmap once 09/01/23 12:02:37.976
    STEP: closing the watch once it receives two notifications 09/01/23 12:02:37.988
    Sep  1 12:02:37.988: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9390  190ce6b8-f5b0-4166-bbc9-8b610f2f1cbf 58383 0 2023-09-01 12:02:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-01 12:02:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  1 12:02:37.988: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9390  190ce6b8-f5b0-4166-bbc9-8b610f2f1cbf 58384 0 2023-09-01 12:02:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-01 12:02:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 09/01/23 12:02:37.989
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/01/23 12:02:38.003
    STEP: deleting the configmap 09/01/23 12:02:38.005
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/01/23 12:02:38.015
    Sep  1 12:02:38.015: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9390  190ce6b8-f5b0-4166-bbc9-8b610f2f1cbf 58385 0 2023-09-01 12:02:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-01 12:02:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  1 12:02:38.016: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9390  190ce6b8-f5b0-4166-bbc9-8b610f2f1cbf 58386 0 2023-09-01 12:02:37 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-01 12:02:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:02:38.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9390" for this suite. 09/01/23 12:02:38.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:02:38.034
Sep  1 12:02:38.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename statefulset 09/01/23 12:02:38.036
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:38.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:38.069
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2531 09/01/23 12:02:38.073
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 09/01/23 12:02:38.087
Sep  1 12:02:38.104: INFO: Found 0 stateful pods, waiting for 3
Sep  1 12:02:48.109: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  1 12:02:48.109: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  1 12:02:48.109: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  1 12:02:48.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-2531 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  1 12:02:48.375: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  1 12:02:48.375: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  1 12:02:48.375: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/01/23 12:02:58.388
Sep  1 12:02:58.409: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/01/23 12:02:58.409
STEP: Updating Pods in reverse ordinal order 09/01/23 12:03:08.43
Sep  1 12:03:08.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-2531 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  1 12:03:08.664: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  1 12:03:08.664: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  1 12:03:08.664: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 09/01/23 12:03:28.686
Sep  1 12:03:28.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-2531 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  1 12:03:28.886: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  1 12:03:28.887: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  1 12:03:28.887: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  1 12:03:38.972: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 09/01/23 12:03:38.979
Sep  1 12:03:38.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-2531 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  1 12:03:39.201: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  1 12:03:39.202: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  1 12:03:39.202: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  1 12:03:49.223: INFO: Deleting all statefulset in ns statefulset-2531
Sep  1 12:03:49.226: INFO: Scaling statefulset ss2 to 0
Sep  1 12:03:59.245: INFO: Waiting for statefulset status.replicas updated to 0
Sep  1 12:03:59.248: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  1 12:03:59.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2531" for this suite. 09/01/23 12:03:59.304
------------------------------
• [SLOW TEST] [81.277 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:02:38.034
    Sep  1 12:02:38.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename statefulset 09/01/23 12:02:38.036
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:02:38.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:02:38.069
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2531 09/01/23 12:02:38.073
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 09/01/23 12:02:38.087
    Sep  1 12:02:38.104: INFO: Found 0 stateful pods, waiting for 3
    Sep  1 12:02:48.109: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  1 12:02:48.109: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  1 12:02:48.109: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Sep  1 12:02:48.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-2531 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  1 12:02:48.375: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  1 12:02:48.375: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  1 12:02:48.375: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/01/23 12:02:58.388
    Sep  1 12:02:58.409: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/01/23 12:02:58.409
    STEP: Updating Pods in reverse ordinal order 09/01/23 12:03:08.43
    Sep  1 12:03:08.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-2531 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  1 12:03:08.664: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  1 12:03:08.664: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  1 12:03:08.664: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 09/01/23 12:03:28.686
    Sep  1 12:03:28.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-2531 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  1 12:03:28.886: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  1 12:03:28.887: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  1 12:03:28.887: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  1 12:03:38.972: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 09/01/23 12:03:38.979
    Sep  1 12:03:38.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3273326990 --namespace=statefulset-2531 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  1 12:03:39.201: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  1 12:03:39.202: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  1 12:03:39.202: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  1 12:03:49.223: INFO: Deleting all statefulset in ns statefulset-2531
    Sep  1 12:03:49.226: INFO: Scaling statefulset ss2 to 0
    Sep  1 12:03:59.245: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  1 12:03:59.248: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:03:59.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2531" for this suite. 09/01/23 12:03:59.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:03:59.32
Sep  1 12:03:59.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename services 09/01/23 12:03:59.322
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:03:59.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:03:59.347
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 09/01/23 12:03:59.355
STEP: watching for the Service to be added 09/01/23 12:03:59.372
Sep  1 12:03:59.374: INFO: Found Service test-service-h6s2f in namespace services-803 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Sep  1 12:03:59.374: INFO: Service test-service-h6s2f created
STEP: Getting /status 09/01/23 12:03:59.375
Sep  1 12:03:59.378: INFO: Service test-service-h6s2f has LoadBalancer: {[]}
STEP: patching the ServiceStatus 09/01/23 12:03:59.378
STEP: watching for the Service to be patched 09/01/23 12:03:59.386
Sep  1 12:03:59.391: INFO: observed Service test-service-h6s2f in namespace services-803 with annotations: map[] & LoadBalancer: {[]}
Sep  1 12:03:59.391: INFO: Found Service test-service-h6s2f in namespace services-803 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Sep  1 12:03:59.391: INFO: Service test-service-h6s2f has service status patched
STEP: updating the ServiceStatus 09/01/23 12:03:59.391
Sep  1 12:03:59.411: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 09/01/23 12:03:59.412
Sep  1 12:03:59.415: INFO: Observed Service test-service-h6s2f in namespace services-803 with annotations: map[] & Conditions: {[]}
Sep  1 12:03:59.415: INFO: Observed event: &Service{ObjectMeta:{test-service-h6s2f  services-803  ba7f9c84-16e6-4ca2-90fc-d2faf99d5d68 59215 0 2023-09-01 12:03:59 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-01 12:03:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-01 12:03:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.104.193.11,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.104.193.11],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Sep  1 12:03:59.415: INFO: Found Service test-service-h6s2f in namespace services-803 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  1 12:03:59.415: INFO: Service test-service-h6s2f has service status updated
STEP: patching the service 09/01/23 12:03:59.415
STEP: watching for the Service to be patched 09/01/23 12:03:59.44
Sep  1 12:03:59.444: INFO: observed Service test-service-h6s2f in namespace services-803 with labels: map[test-service-static:true]
Sep  1 12:03:59.455: INFO: observed Service test-service-h6s2f in namespace services-803 with labels: map[test-service-static:true]
Sep  1 12:03:59.455: INFO: observed Service test-service-h6s2f in namespace services-803 with labels: map[test-service-static:true]
Sep  1 12:03:59.455: INFO: Found Service test-service-h6s2f in namespace services-803 with labels: map[test-service:patched test-service-static:true]
Sep  1 12:03:59.455: INFO: Service test-service-h6s2f patched
STEP: deleting the service 09/01/23 12:03:59.455
STEP: watching for the Service to be deleted 09/01/23 12:03:59.486
Sep  1 12:03:59.489: INFO: Observed event: ADDED
Sep  1 12:03:59.489: INFO: Observed event: MODIFIED
Sep  1 12:03:59.489: INFO: Observed event: MODIFIED
Sep  1 12:03:59.489: INFO: Observed event: MODIFIED
Sep  1 12:03:59.490: INFO: Found Service test-service-h6s2f in namespace services-803 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Sep  1 12:03:59.490: INFO: Service test-service-h6s2f deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  1 12:03:59.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-803" for this suite. 09/01/23 12:03:59.501
------------------------------
• [0.189 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:03:59.32
    Sep  1 12:03:59.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename services 09/01/23 12:03:59.322
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:03:59.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:03:59.347
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 09/01/23 12:03:59.355
    STEP: watching for the Service to be added 09/01/23 12:03:59.372
    Sep  1 12:03:59.374: INFO: Found Service test-service-h6s2f in namespace services-803 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Sep  1 12:03:59.374: INFO: Service test-service-h6s2f created
    STEP: Getting /status 09/01/23 12:03:59.375
    Sep  1 12:03:59.378: INFO: Service test-service-h6s2f has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 09/01/23 12:03:59.378
    STEP: watching for the Service to be patched 09/01/23 12:03:59.386
    Sep  1 12:03:59.391: INFO: observed Service test-service-h6s2f in namespace services-803 with annotations: map[] & LoadBalancer: {[]}
    Sep  1 12:03:59.391: INFO: Found Service test-service-h6s2f in namespace services-803 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Sep  1 12:03:59.391: INFO: Service test-service-h6s2f has service status patched
    STEP: updating the ServiceStatus 09/01/23 12:03:59.391
    Sep  1 12:03:59.411: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 09/01/23 12:03:59.412
    Sep  1 12:03:59.415: INFO: Observed Service test-service-h6s2f in namespace services-803 with annotations: map[] & Conditions: {[]}
    Sep  1 12:03:59.415: INFO: Observed event: &Service{ObjectMeta:{test-service-h6s2f  services-803  ba7f9c84-16e6-4ca2-90fc-d2faf99d5d68 59215 0 2023-09-01 12:03:59 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-01 12:03:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-01 12:03:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.104.193.11,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.104.193.11],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Sep  1 12:03:59.415: INFO: Found Service test-service-h6s2f in namespace services-803 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  1 12:03:59.415: INFO: Service test-service-h6s2f has service status updated
    STEP: patching the service 09/01/23 12:03:59.415
    STEP: watching for the Service to be patched 09/01/23 12:03:59.44
    Sep  1 12:03:59.444: INFO: observed Service test-service-h6s2f in namespace services-803 with labels: map[test-service-static:true]
    Sep  1 12:03:59.455: INFO: observed Service test-service-h6s2f in namespace services-803 with labels: map[test-service-static:true]
    Sep  1 12:03:59.455: INFO: observed Service test-service-h6s2f in namespace services-803 with labels: map[test-service-static:true]
    Sep  1 12:03:59.455: INFO: Found Service test-service-h6s2f in namespace services-803 with labels: map[test-service:patched test-service-static:true]
    Sep  1 12:03:59.455: INFO: Service test-service-h6s2f patched
    STEP: deleting the service 09/01/23 12:03:59.455
    STEP: watching for the Service to be deleted 09/01/23 12:03:59.486
    Sep  1 12:03:59.489: INFO: Observed event: ADDED
    Sep  1 12:03:59.489: INFO: Observed event: MODIFIED
    Sep  1 12:03:59.489: INFO: Observed event: MODIFIED
    Sep  1 12:03:59.489: INFO: Observed event: MODIFIED
    Sep  1 12:03:59.490: INFO: Found Service test-service-h6s2f in namespace services-803 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Sep  1 12:03:59.490: INFO: Service test-service-h6s2f deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:03:59.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-803" for this suite. 09/01/23 12:03:59.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:03:59.519
Sep  1 12:03:59.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 12:03:59.522
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:03:59.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:03:59.563
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-8616/configmap-test-36665b2e-9679-4506-b272-97ba9b08d8e3 09/01/23 12:03:59.577
STEP: Creating a pod to test consume configMaps 09/01/23 12:03:59.589
Sep  1 12:03:59.605: INFO: Waiting up to 5m0s for pod "pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80" in namespace "configmap-8616" to be "Succeeded or Failed"
Sep  1 12:03:59.614: INFO: Pod "pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80": Phase="Pending", Reason="", readiness=false. Elapsed: 9.660406ms
Sep  1 12:04:01.618: INFO: Pod "pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013597823s
Sep  1 12:04:03.618: INFO: Pod "pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013574769s
STEP: Saw pod success 09/01/23 12:04:03.618
Sep  1 12:04:03.619: INFO: Pod "pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80" satisfied condition "Succeeded or Failed"
Sep  1 12:04:03.622: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80 container env-test: <nil>
STEP: delete the pod 09/01/23 12:04:03.642
Sep  1 12:04:03.655: INFO: Waiting for pod pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80 to disappear
Sep  1 12:04:03.658: INFO: Pod pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 12:04:03.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8616" for this suite. 09/01/23 12:04:03.663
------------------------------
• [4.154 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:03:59.519
    Sep  1 12:03:59.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 12:03:59.522
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:03:59.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:03:59.563
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-8616/configmap-test-36665b2e-9679-4506-b272-97ba9b08d8e3 09/01/23 12:03:59.577
    STEP: Creating a pod to test consume configMaps 09/01/23 12:03:59.589
    Sep  1 12:03:59.605: INFO: Waiting up to 5m0s for pod "pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80" in namespace "configmap-8616" to be "Succeeded or Failed"
    Sep  1 12:03:59.614: INFO: Pod "pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80": Phase="Pending", Reason="", readiness=false. Elapsed: 9.660406ms
    Sep  1 12:04:01.618: INFO: Pod "pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013597823s
    Sep  1 12:04:03.618: INFO: Pod "pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013574769s
    STEP: Saw pod success 09/01/23 12:04:03.618
    Sep  1 12:04:03.619: INFO: Pod "pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80" satisfied condition "Succeeded or Failed"
    Sep  1 12:04:03.622: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80 container env-test: <nil>
    STEP: delete the pod 09/01/23 12:04:03.642
    Sep  1 12:04:03.655: INFO: Waiting for pod pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80 to disappear
    Sep  1 12:04:03.658: INFO: Pod pod-configmaps-969c626e-cbae-4e93-81a3-9dd3fc9c8e80 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:04:03.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8616" for this suite. 09/01/23 12:04:03.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:04:03.68
Sep  1 12:04:03.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename limitrange 09/01/23 12:04:03.682
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:04:03.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:04:03.701
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 09/01/23 12:04:03.704
STEP: Setting up watch 09/01/23 12:04:03.744
STEP: Submitting a LimitRange 09/01/23 12:04:03.849
STEP: Verifying LimitRange creation was observed 09/01/23 12:04:03.855
STEP: Fetching the LimitRange to ensure it has proper values 09/01/23 12:04:03.936
Sep  1 12:04:03.943: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  1 12:04:03.943: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 09/01/23 12:04:03.943
STEP: Ensuring Pod has resource requirements applied from LimitRange 09/01/23 12:04:03.95
Sep  1 12:04:03.953: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  1 12:04:03.954: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 09/01/23 12:04:03.954
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/01/23 12:04:03.965
Sep  1 12:04:03.971: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep  1 12:04:03.971: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 09/01/23 12:04:03.971
STEP: Failing to create a Pod with more than max resources 09/01/23 12:04:03.974
STEP: Updating a LimitRange 09/01/23 12:04:03.977
STEP: Verifying LimitRange updating is effective 09/01/23 12:04:03.991
STEP: Creating a Pod with less than former min resources 09/01/23 12:04:05.995
STEP: Failing to create a Pod with more than max resources 09/01/23 12:04:06.003
STEP: Deleting a LimitRange 09/01/23 12:04:06.006
STEP: Verifying the LimitRange was deleted 09/01/23 12:04:06.019
Sep  1 12:04:11.023: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 09/01/23 12:04:11.023
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Sep  1 12:04:11.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-8132" for this suite. 09/01/23 12:04:11.042
------------------------------
• [SLOW TEST] [7.367 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:04:03.68
    Sep  1 12:04:03.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename limitrange 09/01/23 12:04:03.682
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:04:03.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:04:03.701
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 09/01/23 12:04:03.704
    STEP: Setting up watch 09/01/23 12:04:03.744
    STEP: Submitting a LimitRange 09/01/23 12:04:03.849
    STEP: Verifying LimitRange creation was observed 09/01/23 12:04:03.855
    STEP: Fetching the LimitRange to ensure it has proper values 09/01/23 12:04:03.936
    Sep  1 12:04:03.943: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep  1 12:04:03.943: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 09/01/23 12:04:03.943
    STEP: Ensuring Pod has resource requirements applied from LimitRange 09/01/23 12:04:03.95
    Sep  1 12:04:03.953: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep  1 12:04:03.954: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 09/01/23 12:04:03.954
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/01/23 12:04:03.965
    Sep  1 12:04:03.971: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Sep  1 12:04:03.971: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 09/01/23 12:04:03.971
    STEP: Failing to create a Pod with more than max resources 09/01/23 12:04:03.974
    STEP: Updating a LimitRange 09/01/23 12:04:03.977
    STEP: Verifying LimitRange updating is effective 09/01/23 12:04:03.991
    STEP: Creating a Pod with less than former min resources 09/01/23 12:04:05.995
    STEP: Failing to create a Pod with more than max resources 09/01/23 12:04:06.003
    STEP: Deleting a LimitRange 09/01/23 12:04:06.006
    STEP: Verifying the LimitRange was deleted 09/01/23 12:04:06.019
    Sep  1 12:04:11.023: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 09/01/23 12:04:11.023
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:04:11.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-8132" for this suite. 09/01/23 12:04:11.042
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:04:11.048
Sep  1 12:04:11.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename runtimeclass 09/01/23 12:04:11.049
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:04:11.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:04:11.069
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 09/01/23 12:04:11.073
STEP: getting /apis/node.k8s.io 09/01/23 12:04:11.195
STEP: getting /apis/node.k8s.io/v1 09/01/23 12:04:11.196
STEP: creating 09/01/23 12:04:11.197
STEP: watching 09/01/23 12:04:11.214
Sep  1 12:04:11.214: INFO: starting watch
STEP: getting 09/01/23 12:04:11.221
STEP: listing 09/01/23 12:04:11.224
STEP: patching 09/01/23 12:04:11.227
STEP: updating 09/01/23 12:04:11.232
Sep  1 12:04:11.239: INFO: waiting for watch events with expected annotations
STEP: deleting 09/01/23 12:04:11.239
STEP: deleting a collection 09/01/23 12:04:11.25
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  1 12:04:11.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2806" for this suite. 09/01/23 12:04:11.273
------------------------------
• [0.234 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:04:11.048
    Sep  1 12:04:11.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename runtimeclass 09/01/23 12:04:11.049
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:04:11.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:04:11.069
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 09/01/23 12:04:11.073
    STEP: getting /apis/node.k8s.io 09/01/23 12:04:11.195
    STEP: getting /apis/node.k8s.io/v1 09/01/23 12:04:11.196
    STEP: creating 09/01/23 12:04:11.197
    STEP: watching 09/01/23 12:04:11.214
    Sep  1 12:04:11.214: INFO: starting watch
    STEP: getting 09/01/23 12:04:11.221
    STEP: listing 09/01/23 12:04:11.224
    STEP: patching 09/01/23 12:04:11.227
    STEP: updating 09/01/23 12:04:11.232
    Sep  1 12:04:11.239: INFO: waiting for watch events with expected annotations
    STEP: deleting 09/01/23 12:04:11.239
    STEP: deleting a collection 09/01/23 12:04:11.25
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:04:11.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2806" for this suite. 09/01/23 12:04:11.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:04:11.286
Sep  1 12:04:11.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename job 09/01/23 12:04:11.288
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:04:11.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:04:11.31
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 09/01/23 12:04:11.313
STEP: Ensuring active pods == parallelism 09/01/23 12:04:11.319
STEP: delete a job 09/01/23 12:04:13.324
STEP: deleting Job.batch foo in namespace job-8101, will wait for the garbage collector to delete the pods 09/01/23 12:04:13.324
Sep  1 12:04:13.384: INFO: Deleting Job.batch foo took: 5.652026ms
Sep  1 12:04:13.484: INFO: Terminating Job.batch foo pods took: 100.87638ms
STEP: Ensuring job was deleted 09/01/23 12:04:46.385
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  1 12:04:46.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8101" for this suite. 09/01/23 12:04:46.395
------------------------------
• [SLOW TEST] [35.115 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:04:11.286
    Sep  1 12:04:11.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename job 09/01/23 12:04:11.288
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:04:11.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:04:11.31
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 09/01/23 12:04:11.313
    STEP: Ensuring active pods == parallelism 09/01/23 12:04:11.319
    STEP: delete a job 09/01/23 12:04:13.324
    STEP: deleting Job.batch foo in namespace job-8101, will wait for the garbage collector to delete the pods 09/01/23 12:04:13.324
    Sep  1 12:04:13.384: INFO: Deleting Job.batch foo took: 5.652026ms
    Sep  1 12:04:13.484: INFO: Terminating Job.batch foo pods took: 100.87638ms
    STEP: Ensuring job was deleted 09/01/23 12:04:46.385
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:04:46.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8101" for this suite. 09/01/23 12:04:46.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:04:46.403
Sep  1 12:04:46.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename container-runtime 09/01/23 12:04:46.405
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:04:46.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:04:46.428
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 09/01/23 12:04:46.434
STEP: wait for the container to reach Failed 09/01/23 12:04:46.445
STEP: get the container status 09/01/23 12:04:50.465
STEP: the container should be terminated 09/01/23 12:04:50.468
STEP: the termination message should be set 09/01/23 12:04:50.468
Sep  1 12:04:50.469: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/01/23 12:04:50.469
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  1 12:04:50.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8047" for this suite. 09/01/23 12:04:50.488
------------------------------
• [4.092 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:04:46.403
    Sep  1 12:04:46.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename container-runtime 09/01/23 12:04:46.405
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:04:46.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:04:46.428
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 09/01/23 12:04:46.434
    STEP: wait for the container to reach Failed 09/01/23 12:04:46.445
    STEP: get the container status 09/01/23 12:04:50.465
    STEP: the container should be terminated 09/01/23 12:04:50.468
    STEP: the termination message should be set 09/01/23 12:04:50.468
    Sep  1 12:04:50.469: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/01/23 12:04:50.469
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:04:50.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8047" for this suite. 09/01/23 12:04:50.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:04:50.498
Sep  1 12:04:50.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename downward-api 09/01/23 12:04:50.5
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:04:50.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:04:50.519
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 09/01/23 12:04:50.522
Sep  1 12:04:50.531: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7" in namespace "downward-api-4213" to be "Succeeded or Failed"
Sep  1 12:04:50.534: INFO: Pod "downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.787935ms
Sep  1 12:04:52.538: INFO: Pod "downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006397095s
Sep  1 12:04:54.538: INFO: Pod "downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00667284s
STEP: Saw pod success 09/01/23 12:04:54.538
Sep  1 12:04:54.539: INFO: Pod "downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7" satisfied condition "Succeeded or Failed"
Sep  1 12:04:54.542: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7 container client-container: <nil>
STEP: delete the pod 09/01/23 12:04:54.55
Sep  1 12:04:54.565: INFO: Waiting for pod downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7 to disappear
Sep  1 12:04:54.569: INFO: Pod downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  1 12:04:54.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4213" for this suite. 09/01/23 12:04:54.577
------------------------------
• [4.102 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:04:50.498
    Sep  1 12:04:50.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename downward-api 09/01/23 12:04:50.5
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:04:50.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:04:50.519
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 09/01/23 12:04:50.522
    Sep  1 12:04:50.531: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7" in namespace "downward-api-4213" to be "Succeeded or Failed"
    Sep  1 12:04:50.534: INFO: Pod "downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.787935ms
    Sep  1 12:04:52.538: INFO: Pod "downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006397095s
    Sep  1 12:04:54.538: INFO: Pod "downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00667284s
    STEP: Saw pod success 09/01/23 12:04:54.538
    Sep  1 12:04:54.539: INFO: Pod "downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7" satisfied condition "Succeeded or Failed"
    Sep  1 12:04:54.542: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7 container client-container: <nil>
    STEP: delete the pod 09/01/23 12:04:54.55
    Sep  1 12:04:54.565: INFO: Waiting for pod downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7 to disappear
    Sep  1 12:04:54.569: INFO: Pod downwardapi-volume-61dc19d9-ec87-4353-9592-acee405572c7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:04:54.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4213" for this suite. 09/01/23 12:04:54.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:04:54.607
Sep  1 12:04:54.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 12:04:54.609
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:04:54.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:04:54.627
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 09/01/23 12:04:54.631
Sep  1 12:04:54.641: INFO: Waiting up to 5m0s for pod "labelsupdate57b2b02d-a94a-4842-aa6d-be447f36db2b" in namespace "projected-4479" to be "running and ready"
Sep  1 12:04:54.646: INFO: Pod "labelsupdate57b2b02d-a94a-4842-aa6d-be447f36db2b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.044565ms
Sep  1 12:04:54.646: INFO: The phase of Pod labelsupdate57b2b02d-a94a-4842-aa6d-be447f36db2b is Pending, waiting for it to be Running (with Ready = true)
Sep  1 12:04:56.649: INFO: Pod "labelsupdate57b2b02d-a94a-4842-aa6d-be447f36db2b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008355996s
Sep  1 12:04:56.649: INFO: The phase of Pod labelsupdate57b2b02d-a94a-4842-aa6d-be447f36db2b is Running (Ready = true)
Sep  1 12:04:56.649: INFO: Pod "labelsupdate57b2b02d-a94a-4842-aa6d-be447f36db2b" satisfied condition "running and ready"
Sep  1 12:04:57.174: INFO: Successfully updated pod "labelsupdate57b2b02d-a94a-4842-aa6d-be447f36db2b"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  1 12:05:01.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4479" for this suite. 09/01/23 12:05:01.212
------------------------------
• [SLOW TEST] [6.610 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:04:54.607
    Sep  1 12:04:54.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 12:04:54.609
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:04:54.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:04:54.627
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 09/01/23 12:04:54.631
    Sep  1 12:04:54.641: INFO: Waiting up to 5m0s for pod "labelsupdate57b2b02d-a94a-4842-aa6d-be447f36db2b" in namespace "projected-4479" to be "running and ready"
    Sep  1 12:04:54.646: INFO: Pod "labelsupdate57b2b02d-a94a-4842-aa6d-be447f36db2b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.044565ms
    Sep  1 12:04:54.646: INFO: The phase of Pod labelsupdate57b2b02d-a94a-4842-aa6d-be447f36db2b is Pending, waiting for it to be Running (with Ready = true)
    Sep  1 12:04:56.649: INFO: Pod "labelsupdate57b2b02d-a94a-4842-aa6d-be447f36db2b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008355996s
    Sep  1 12:04:56.649: INFO: The phase of Pod labelsupdate57b2b02d-a94a-4842-aa6d-be447f36db2b is Running (Ready = true)
    Sep  1 12:04:56.649: INFO: Pod "labelsupdate57b2b02d-a94a-4842-aa6d-be447f36db2b" satisfied condition "running and ready"
    Sep  1 12:04:57.174: INFO: Successfully updated pod "labelsupdate57b2b02d-a94a-4842-aa6d-be447f36db2b"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:05:01.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4479" for this suite. 09/01/23 12:05:01.212
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:05:01.224
Sep  1 12:05:01.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename svcaccounts 09/01/23 12:05:01.226
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:05:01.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:05:01.247
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Sep  1 12:05:01.266: INFO: Waiting up to 5m0s for pod "pod-service-account-8229b994-a7e1-414d-9d54-2413c6cca578" in namespace "svcaccounts-3612" to be "running"
Sep  1 12:05:01.277: INFO: Pod "pod-service-account-8229b994-a7e1-414d-9d54-2413c6cca578": Phase="Pending", Reason="", readiness=false. Elapsed: 10.812425ms
Sep  1 12:05:03.281: INFO: Pod "pod-service-account-8229b994-a7e1-414d-9d54-2413c6cca578": Phase="Running", Reason="", readiness=true. Elapsed: 2.014682969s
Sep  1 12:05:03.281: INFO: Pod "pod-service-account-8229b994-a7e1-414d-9d54-2413c6cca578" satisfied condition "running"
STEP: reading a file in the container 09/01/23 12:05:03.281
Sep  1 12:05:03.348: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3612 pod-service-account-8229b994-a7e1-414d-9d54-2413c6cca578 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 09/01/23 12:05:03.551
Sep  1 12:05:03.551: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3612 pod-service-account-8229b994-a7e1-414d-9d54-2413c6cca578 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 09/01/23 12:05:03.891
Sep  1 12:05:03.892: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3612 pod-service-account-8229b994-a7e1-414d-9d54-2413c6cca578 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Sep  1 12:05:04.113: INFO: Got root ca configmap in namespace "svcaccounts-3612"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  1 12:05:04.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3612" for this suite. 09/01/23 12:05:04.162
------------------------------
• [2.946 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:05:01.224
    Sep  1 12:05:01.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename svcaccounts 09/01/23 12:05:01.226
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:05:01.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:05:01.247
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Sep  1 12:05:01.266: INFO: Waiting up to 5m0s for pod "pod-service-account-8229b994-a7e1-414d-9d54-2413c6cca578" in namespace "svcaccounts-3612" to be "running"
    Sep  1 12:05:01.277: INFO: Pod "pod-service-account-8229b994-a7e1-414d-9d54-2413c6cca578": Phase="Pending", Reason="", readiness=false. Elapsed: 10.812425ms
    Sep  1 12:05:03.281: INFO: Pod "pod-service-account-8229b994-a7e1-414d-9d54-2413c6cca578": Phase="Running", Reason="", readiness=true. Elapsed: 2.014682969s
    Sep  1 12:05:03.281: INFO: Pod "pod-service-account-8229b994-a7e1-414d-9d54-2413c6cca578" satisfied condition "running"
    STEP: reading a file in the container 09/01/23 12:05:03.281
    Sep  1 12:05:03.348: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3612 pod-service-account-8229b994-a7e1-414d-9d54-2413c6cca578 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 09/01/23 12:05:03.551
    Sep  1 12:05:03.551: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3612 pod-service-account-8229b994-a7e1-414d-9d54-2413c6cca578 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 09/01/23 12:05:03.891
    Sep  1 12:05:03.892: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3612 pod-service-account-8229b994-a7e1-414d-9d54-2413c6cca578 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Sep  1 12:05:04.113: INFO: Got root ca configmap in namespace "svcaccounts-3612"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:05:04.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3612" for this suite. 09/01/23 12:05:04.162
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:05:04.172
Sep  1 12:05:04.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename configmap 09/01/23 12:05:04.174
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:05:04.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:05:04.19
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-bdd662d7-234e-48c0-a1f9-63c78843cf4a 09/01/23 12:05:04.192
STEP: Creating a pod to test consume configMaps 09/01/23 12:05:04.197
Sep  1 12:05:04.206: INFO: Waiting up to 5m0s for pod "pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c" in namespace "configmap-9631" to be "Succeeded or Failed"
Sep  1 12:05:04.209: INFO: Pod "pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.495328ms
Sep  1 12:05:06.214: INFO: Pod "pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007934182s
Sep  1 12:05:08.214: INFO: Pod "pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008260418s
STEP: Saw pod success 09/01/23 12:05:08.214
Sep  1 12:05:08.215: INFO: Pod "pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c" satisfied condition "Succeeded or Failed"
Sep  1 12:05:08.218: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c container agnhost-container: <nil>
STEP: delete the pod 09/01/23 12:05:08.224
Sep  1 12:05:08.241: INFO: Waiting for pod pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c to disappear
Sep  1 12:05:08.245: INFO: Pod pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  1 12:05:08.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9631" for this suite. 09/01/23 12:05:08.249
------------------------------
• [4.085 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:05:04.172
    Sep  1 12:05:04.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename configmap 09/01/23 12:05:04.174
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:05:04.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:05:04.19
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-bdd662d7-234e-48c0-a1f9-63c78843cf4a 09/01/23 12:05:04.192
    STEP: Creating a pod to test consume configMaps 09/01/23 12:05:04.197
    Sep  1 12:05:04.206: INFO: Waiting up to 5m0s for pod "pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c" in namespace "configmap-9631" to be "Succeeded or Failed"
    Sep  1 12:05:04.209: INFO: Pod "pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.495328ms
    Sep  1 12:05:06.214: INFO: Pod "pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007934182s
    Sep  1 12:05:08.214: INFO: Pod "pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008260418s
    STEP: Saw pod success 09/01/23 12:05:08.214
    Sep  1 12:05:08.215: INFO: Pod "pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c" satisfied condition "Succeeded or Failed"
    Sep  1 12:05:08.218: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c container agnhost-container: <nil>
    STEP: delete the pod 09/01/23 12:05:08.224
    Sep  1 12:05:08.241: INFO: Waiting for pod pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c to disappear
    Sep  1 12:05:08.245: INFO: Pod pod-configmaps-c84749d6-6964-48c3-8a66-bbd1c60fe90c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:05:08.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9631" for this suite. 09/01/23 12:05:08.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:05:08.258
Sep  1 12:05:08.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename projected 09/01/23 12:05:08.26
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:05:08.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:05:08.287
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 09/01/23 12:05:08.292
Sep  1 12:05:08.301: INFO: Waiting up to 5m0s for pod "downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3" in namespace "projected-3235" to be "Succeeded or Failed"
Sep  1 12:05:08.306: INFO: Pod "downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.860364ms
Sep  1 12:05:10.311: INFO: Pod "downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009580262s
Sep  1 12:05:12.323: INFO: Pod "downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022050882s
STEP: Saw pod success 09/01/23 12:05:12.323
Sep  1 12:05:12.323: INFO: Pod "downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3" satisfied condition "Succeeded or Failed"
Sep  1 12:05:12.329: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3 container client-container: <nil>
STEP: delete the pod 09/01/23 12:05:12.336
Sep  1 12:05:12.358: INFO: Waiting for pod downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3 to disappear
Sep  1 12:05:12.364: INFO: Pod downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  1 12:05:12.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3235" for this suite. 09/01/23 12:05:12.37
------------------------------
• [4.119 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:05:08.258
    Sep  1 12:05:08.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename projected 09/01/23 12:05:08.26
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:05:08.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:05:08.287
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 09/01/23 12:05:08.292
    Sep  1 12:05:08.301: INFO: Waiting up to 5m0s for pod "downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3" in namespace "projected-3235" to be "Succeeded or Failed"
    Sep  1 12:05:08.306: INFO: Pod "downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.860364ms
    Sep  1 12:05:10.311: INFO: Pod "downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009580262s
    Sep  1 12:05:12.323: INFO: Pod "downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022050882s
    STEP: Saw pod success 09/01/23 12:05:12.323
    Sep  1 12:05:12.323: INFO: Pod "downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3" satisfied condition "Succeeded or Failed"
    Sep  1 12:05:12.329: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3 container client-container: <nil>
    STEP: delete the pod 09/01/23 12:05:12.336
    Sep  1 12:05:12.358: INFO: Waiting for pod downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3 to disappear
    Sep  1 12:05:12.364: INFO: Pod downwardapi-volume-abccd8d7-dd1b-4475-b150-9ca5631453c3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:05:12.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3235" for this suite. 09/01/23 12:05:12.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:05:12.385
Sep  1 12:05:12.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename proxy 09/01/23 12:05:12.387
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:05:12.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:05:12.412
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Sep  1 12:05:12.416: INFO: Creating pod...
Sep  1 12:05:12.456: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8730" to be "running"
Sep  1 12:05:12.461: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.845487ms
Sep  1 12:05:14.466: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009509981s
Sep  1 12:05:14.466: INFO: Pod "agnhost" satisfied condition "running"
Sep  1 12:05:14.466: INFO: Creating service...
Sep  1 12:05:14.487: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/pods/agnhost/proxy?method=DELETE
Sep  1 12:05:14.501: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  1 12:05:14.501: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/pods/agnhost/proxy?method=OPTIONS
Sep  1 12:05:14.507: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  1 12:05:14.507: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/pods/agnhost/proxy?method=PATCH
Sep  1 12:05:14.515: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  1 12:05:14.515: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/pods/agnhost/proxy?method=POST
Sep  1 12:05:14.522: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  1 12:05:14.522: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/pods/agnhost/proxy?method=PUT
Sep  1 12:05:14.528: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  1 12:05:14.528: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/services/e2e-proxy-test-service/proxy?method=DELETE
Sep  1 12:05:14.540: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  1 12:05:14.541: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/services/e2e-proxy-test-service/proxy?method=OPTIONS
Sep  1 12:05:14.548: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  1 12:05:14.548: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/services/e2e-proxy-test-service/proxy?method=PATCH
Sep  1 12:05:14.555: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  1 12:05:14.555: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/services/e2e-proxy-test-service/proxy?method=POST
Sep  1 12:05:14.565: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  1 12:05:14.565: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/services/e2e-proxy-test-service/proxy?method=PUT
Sep  1 12:05:14.571: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  1 12:05:14.571: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/pods/agnhost/proxy?method=GET
Sep  1 12:05:14.579: INFO: http.Client request:GET StatusCode:301
Sep  1 12:05:14.580: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/services/e2e-proxy-test-service/proxy?method=GET
Sep  1 12:05:14.585: INFO: http.Client request:GET StatusCode:301
Sep  1 12:05:14.585: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/pods/agnhost/proxy?method=HEAD
Sep  1 12:05:14.589: INFO: http.Client request:HEAD StatusCode:301
Sep  1 12:05:14.589: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/services/e2e-proxy-test-service/proxy?method=HEAD
Sep  1 12:05:14.594: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  1 12:05:14.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8730" for this suite. 09/01/23 12:05:14.603
------------------------------
• [2.230 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:05:12.385
    Sep  1 12:05:12.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename proxy 09/01/23 12:05:12.387
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:05:12.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:05:12.412
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Sep  1 12:05:12.416: INFO: Creating pod...
    Sep  1 12:05:12.456: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8730" to be "running"
    Sep  1 12:05:12.461: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.845487ms
    Sep  1 12:05:14.466: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009509981s
    Sep  1 12:05:14.466: INFO: Pod "agnhost" satisfied condition "running"
    Sep  1 12:05:14.466: INFO: Creating service...
    Sep  1 12:05:14.487: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/pods/agnhost/proxy?method=DELETE
    Sep  1 12:05:14.501: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  1 12:05:14.501: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/pods/agnhost/proxy?method=OPTIONS
    Sep  1 12:05:14.507: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  1 12:05:14.507: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/pods/agnhost/proxy?method=PATCH
    Sep  1 12:05:14.515: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  1 12:05:14.515: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/pods/agnhost/proxy?method=POST
    Sep  1 12:05:14.522: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  1 12:05:14.522: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/pods/agnhost/proxy?method=PUT
    Sep  1 12:05:14.528: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  1 12:05:14.528: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/services/e2e-proxy-test-service/proxy?method=DELETE
    Sep  1 12:05:14.540: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  1 12:05:14.541: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Sep  1 12:05:14.548: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  1 12:05:14.548: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/services/e2e-proxy-test-service/proxy?method=PATCH
    Sep  1 12:05:14.555: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  1 12:05:14.555: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/services/e2e-proxy-test-service/proxy?method=POST
    Sep  1 12:05:14.565: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  1 12:05:14.565: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/services/e2e-proxy-test-service/proxy?method=PUT
    Sep  1 12:05:14.571: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  1 12:05:14.571: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/pods/agnhost/proxy?method=GET
    Sep  1 12:05:14.579: INFO: http.Client request:GET StatusCode:301
    Sep  1 12:05:14.580: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/services/e2e-proxy-test-service/proxy?method=GET
    Sep  1 12:05:14.585: INFO: http.Client request:GET StatusCode:301
    Sep  1 12:05:14.585: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/pods/agnhost/proxy?method=HEAD
    Sep  1 12:05:14.589: INFO: http.Client request:HEAD StatusCode:301
    Sep  1 12:05:14.589: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8730/services/e2e-proxy-test-service/proxy?method=HEAD
    Sep  1 12:05:14.594: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:05:14.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8730" for this suite. 09/01/23 12:05:14.603
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:05:14.623
Sep  1 12:05:14.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename tables 09/01/23 12:05:14.624
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:05:14.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:05:14.657
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Sep  1 12:05:14.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-597" for this suite. 09/01/23 12:05:14.738
------------------------------
• [0.122 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:05:14.623
    Sep  1 12:05:14.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename tables 09/01/23 12:05:14.624
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:05:14.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:05:14.657
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:05:14.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-597" for this suite. 09/01/23 12:05:14.738
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:05:14.747
Sep  1 12:05:14.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename runtimeclass 09/01/23 12:05:14.749
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:05:14.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:05:14.768
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-4251-delete-me 09/01/23 12:05:14.776
STEP: Waiting for the RuntimeClass to disappear 09/01/23 12:05:14.782
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  1 12:05:14.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4251" for this suite. 09/01/23 12:05:14.794
------------------------------
• [0.052 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:05:14.747
    Sep  1 12:05:14.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename runtimeclass 09/01/23 12:05:14.749
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:05:14.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:05:14.768
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-4251-delete-me 09/01/23 12:05:14.776
    STEP: Waiting for the RuntimeClass to disappear 09/01/23 12:05:14.782
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:05:14.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4251" for this suite. 09/01/23 12:05:14.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:05:14.803
Sep  1 12:05:14.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename statefulset 09/01/23 12:05:14.804
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:05:14.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:05:14.827
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4452 09/01/23 12:05:14.832
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 09/01/23 12:05:14.84
Sep  1 12:05:14.853: INFO: Found 0 stateful pods, waiting for 3
Sep  1 12:05:24.858: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  1 12:05:24.859: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  1 12:05:24.859: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/01/23 12:05:24.869
Sep  1 12:05:24.890: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/01/23 12:05:24.89
STEP: Not applying an update when the partition is greater than the number of replicas 09/01/23 12:05:34.906
STEP: Performing a canary update 09/01/23 12:05:34.906
Sep  1 12:05:34.929: INFO: Updating stateful set ss2
Sep  1 12:05:34.936: INFO: Waiting for Pod statefulset-4452/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 09/01/23 12:05:44.944
Sep  1 12:05:45.038: INFO: Found 1 stateful pods, waiting for 3
Sep  1 12:05:55.076: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  1 12:05:55.076: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  1 12:05:55.076: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 09/01/23 12:05:55.085
Sep  1 12:05:55.106: INFO: Updating stateful set ss2
Sep  1 12:05:55.112: INFO: Waiting for Pod statefulset-4452/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Sep  1 12:06:05.141: INFO: Updating stateful set ss2
Sep  1 12:06:05.149: INFO: Waiting for StatefulSet statefulset-4452/ss2 to complete update
Sep  1 12:06:05.149: INFO: Waiting for Pod statefulset-4452/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  1 12:06:15.157: INFO: Deleting all statefulset in ns statefulset-4452
Sep  1 12:06:15.160: INFO: Scaling statefulset ss2 to 0
Sep  1 12:06:25.186: INFO: Waiting for statefulset status.replicas updated to 0
Sep  1 12:06:25.190: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  1 12:06:25.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4452" for this suite. 09/01/23 12:06:25.217
------------------------------
• [SLOW TEST] [70.432 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:05:14.803
    Sep  1 12:05:14.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename statefulset 09/01/23 12:05:14.804
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:05:14.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:05:14.827
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4452 09/01/23 12:05:14.832
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 09/01/23 12:05:14.84
    Sep  1 12:05:14.853: INFO: Found 0 stateful pods, waiting for 3
    Sep  1 12:05:24.858: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  1 12:05:24.859: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  1 12:05:24.859: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/01/23 12:05:24.869
    Sep  1 12:05:24.890: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/01/23 12:05:24.89
    STEP: Not applying an update when the partition is greater than the number of replicas 09/01/23 12:05:34.906
    STEP: Performing a canary update 09/01/23 12:05:34.906
    Sep  1 12:05:34.929: INFO: Updating stateful set ss2
    Sep  1 12:05:34.936: INFO: Waiting for Pod statefulset-4452/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 09/01/23 12:05:44.944
    Sep  1 12:05:45.038: INFO: Found 1 stateful pods, waiting for 3
    Sep  1 12:05:55.076: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  1 12:05:55.076: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  1 12:05:55.076: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 09/01/23 12:05:55.085
    Sep  1 12:05:55.106: INFO: Updating stateful set ss2
    Sep  1 12:05:55.112: INFO: Waiting for Pod statefulset-4452/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Sep  1 12:06:05.141: INFO: Updating stateful set ss2
    Sep  1 12:06:05.149: INFO: Waiting for StatefulSet statefulset-4452/ss2 to complete update
    Sep  1 12:06:05.149: INFO: Waiting for Pod statefulset-4452/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  1 12:06:15.157: INFO: Deleting all statefulset in ns statefulset-4452
    Sep  1 12:06:15.160: INFO: Scaling statefulset ss2 to 0
    Sep  1 12:06:25.186: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  1 12:06:25.190: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:06:25.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4452" for this suite. 09/01/23 12:06:25.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:06:25.253
Sep  1 12:06:25.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename crd-watch 09/01/23 12:06:25.254
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:06:25.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:06:25.286
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Sep  1 12:06:25.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Creating first CR  09/01/23 12:06:27.858
Sep  1 12:06:27.864: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-01T12:06:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-01T12:06:27Z]] name:name1 resourceVersion:60706 uid:3cc5ea09-4bb6-4bf5-ac12-88f1c607cf59] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 09/01/23 12:06:37.866
Sep  1 12:06:37.874: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-01T12:06:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-01T12:06:37Z]] name:name2 resourceVersion:60817 uid:880fb949-3b36-4c5d-ba85-321781bf62eb] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 09/01/23 12:06:47.875
Sep  1 12:06:47.887: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-01T12:06:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-01T12:06:47Z]] name:name1 resourceVersion:60872 uid:3cc5ea09-4bb6-4bf5-ac12-88f1c607cf59] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 09/01/23 12:06:57.888
Sep  1 12:06:57.895: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-01T12:06:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-01T12:06:57Z]] name:name2 resourceVersion:60927 uid:880fb949-3b36-4c5d-ba85-321781bf62eb] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 09/01/23 12:07:07.896
Sep  1 12:07:07.951: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-01T12:06:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-01T12:06:47Z]] name:name1 resourceVersion:60980 uid:3cc5ea09-4bb6-4bf5-ac12-88f1c607cf59] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 09/01/23 12:07:17.952
Sep  1 12:07:17.960: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-01T12:06:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-01T12:06:57Z]] name:name2 resourceVersion:61047 uid:880fb949-3b36-4c5d-ba85-321781bf62eb] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  1 12:07:28.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-9757" for this suite. 09/01/23 12:07:28.485
------------------------------
• [SLOW TEST] [63.244 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:06:25.253
    Sep  1 12:06:25.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename crd-watch 09/01/23 12:06:25.254
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:06:25.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:06:25.286
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Sep  1 12:06:25.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Creating first CR  09/01/23 12:06:27.858
    Sep  1 12:06:27.864: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-01T12:06:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-01T12:06:27Z]] name:name1 resourceVersion:60706 uid:3cc5ea09-4bb6-4bf5-ac12-88f1c607cf59] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 09/01/23 12:06:37.866
    Sep  1 12:06:37.874: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-01T12:06:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-01T12:06:37Z]] name:name2 resourceVersion:60817 uid:880fb949-3b36-4c5d-ba85-321781bf62eb] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 09/01/23 12:06:47.875
    Sep  1 12:06:47.887: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-01T12:06:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-01T12:06:47Z]] name:name1 resourceVersion:60872 uid:3cc5ea09-4bb6-4bf5-ac12-88f1c607cf59] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 09/01/23 12:06:57.888
    Sep  1 12:06:57.895: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-01T12:06:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-01T12:06:57Z]] name:name2 resourceVersion:60927 uid:880fb949-3b36-4c5d-ba85-321781bf62eb] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 09/01/23 12:07:07.896
    Sep  1 12:07:07.951: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-01T12:06:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-01T12:06:47Z]] name:name1 resourceVersion:60980 uid:3cc5ea09-4bb6-4bf5-ac12-88f1c607cf59] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 09/01/23 12:07:17.952
    Sep  1 12:07:17.960: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-01T12:06:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-01T12:06:57Z]] name:name2 resourceVersion:61047 uid:880fb949-3b36-4c5d-ba85-321781bf62eb] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:07:28.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-9757" for this suite. 09/01/23 12:07:28.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:07:28.499
Sep  1 12:07:28.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename security-context-test 09/01/23 12:07:28.501
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:07:28.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:07:28.534
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Sep  1 12:07:28.547: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-07793612-8612-4435-ba09-7ae0e90ccaee" in namespace "security-context-test-1590" to be "Succeeded or Failed"
Sep  1 12:07:28.552: INFO: Pod "busybox-privileged-false-07793612-8612-4435-ba09-7ae0e90ccaee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.678562ms
Sep  1 12:07:30.557: INFO: Pod "busybox-privileged-false-07793612-8612-4435-ba09-7ae0e90ccaee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010110989s
Sep  1 12:07:32.560: INFO: Pod "busybox-privileged-false-07793612-8612-4435-ba09-7ae0e90ccaee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012362311s
Sep  1 12:07:32.560: INFO: Pod "busybox-privileged-false-07793612-8612-4435-ba09-7ae0e90ccaee" satisfied condition "Succeeded or Failed"
Sep  1 12:07:32.580: INFO: Got logs for pod "busybox-privileged-false-07793612-8612-4435-ba09-7ae0e90ccaee": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  1 12:07:32.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1590" for this suite. 09/01/23 12:07:32.587
------------------------------
• [4.099 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:07:28.499
    Sep  1 12:07:28.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename security-context-test 09/01/23 12:07:28.501
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:07:28.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:07:28.534
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Sep  1 12:07:28.547: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-07793612-8612-4435-ba09-7ae0e90ccaee" in namespace "security-context-test-1590" to be "Succeeded or Failed"
    Sep  1 12:07:28.552: INFO: Pod "busybox-privileged-false-07793612-8612-4435-ba09-7ae0e90ccaee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.678562ms
    Sep  1 12:07:30.557: INFO: Pod "busybox-privileged-false-07793612-8612-4435-ba09-7ae0e90ccaee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010110989s
    Sep  1 12:07:32.560: INFO: Pod "busybox-privileged-false-07793612-8612-4435-ba09-7ae0e90ccaee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012362311s
    Sep  1 12:07:32.560: INFO: Pod "busybox-privileged-false-07793612-8612-4435-ba09-7ae0e90ccaee" satisfied condition "Succeeded or Failed"
    Sep  1 12:07:32.580: INFO: Got logs for pod "busybox-privileged-false-07793612-8612-4435-ba09-7ae0e90ccaee": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:07:32.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1590" for this suite. 09/01/23 12:07:32.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/01/23 12:07:32.598
Sep  1 12:07:32.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
STEP: Building a namespace api object, basename secrets 09/01/23 12:07:32.6
STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:07:32.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:07:32.64
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-137c2774-fac3-485c-ad1f-e19dda5c4baf 09/01/23 12:07:32.643
STEP: Creating a pod to test consume secrets 09/01/23 12:07:32.656
Sep  1 12:07:32.678: INFO: Waiting up to 5m0s for pod "pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573" in namespace "secrets-4956" to be "Succeeded or Failed"
Sep  1 12:07:32.703: INFO: Pod "pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573": Phase="Pending", Reason="", readiness=false. Elapsed: 24.323268ms
Sep  1 12:07:34.707: INFO: Pod "pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028459291s
Sep  1 12:07:36.729: INFO: Pod "pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050480552s
STEP: Saw pod success 09/01/23 12:07:36.729
Sep  1 12:07:36.729: INFO: Pod "pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573" satisfied condition "Succeeded or Failed"
Sep  1 12:07:36.747: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573 container secret-volume-test: <nil>
STEP: delete the pod 09/01/23 12:07:36.829
Sep  1 12:07:36.917: INFO: Waiting for pod pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573 to disappear
Sep  1 12:07:36.929: INFO: Pod pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  1 12:07:36.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4956" for this suite. 09/01/23 12:07:36.952
------------------------------
• [4.415 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/01/23 12:07:32.598
    Sep  1 12:07:32.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3273326990
    STEP: Building a namespace api object, basename secrets 09/01/23 12:07:32.6
    STEP: Waiting for a default service account to be provisioned in namespace 09/01/23 12:07:32.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/01/23 12:07:32.64
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-137c2774-fac3-485c-ad1f-e19dda5c4baf 09/01/23 12:07:32.643
    STEP: Creating a pod to test consume secrets 09/01/23 12:07:32.656
    Sep  1 12:07:32.678: INFO: Waiting up to 5m0s for pod "pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573" in namespace "secrets-4956" to be "Succeeded or Failed"
    Sep  1 12:07:32.703: INFO: Pod "pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573": Phase="Pending", Reason="", readiness=false. Elapsed: 24.323268ms
    Sep  1 12:07:34.707: INFO: Pod "pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028459291s
    Sep  1 12:07:36.729: INFO: Pod "pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050480552s
    STEP: Saw pod success 09/01/23 12:07:36.729
    Sep  1 12:07:36.729: INFO: Pod "pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573" satisfied condition "Succeeded or Failed"
    Sep  1 12:07:36.747: INFO: Trying to get logs from node k8s-worker-1.c.operations-lab.internal pod pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573 container secret-volume-test: <nil>
    STEP: delete the pod 09/01/23 12:07:36.829
    Sep  1 12:07:36.917: INFO: Waiting for pod pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573 to disappear
    Sep  1 12:07:36.929: INFO: Pod pod-secrets-e7945b73-d790-4f68-9d37-45f1220b1573 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  1 12:07:36.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4956" for this suite. 09/01/23 12:07:36.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Sep  1 12:07:37.023: INFO: Running AfterSuite actions on node 1
Sep  1 12:07:37.023: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.001 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Sep  1 12:07:37.023: INFO: Running AfterSuite actions on node 1
    Sep  1 12:07:37.023: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.001 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.393 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5887.814 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h38m8.913138928s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

