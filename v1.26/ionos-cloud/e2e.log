I0103 11:41:33.267848      22 e2e.go:126] Starting e2e run "574f13e1-2ed7-447d-a3ec-af70b4fe9007" on Ginkgo node 1
Jan  3 11:41:33.277: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1704282093 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan  3 11:41:33.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 11:41:33.400: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0103 11:41:33.401098      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jan  3 11:41:33.513: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan  3 11:41:33.609: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan  3 11:41:33.609: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
Jan  3 11:41:33.609: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan  3 11:41:33.632: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan  3 11:41:33.632: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-ionoscloud' (0 seconds elapsed)
Jan  3 11:41:33.632: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Jan  3 11:41:33.632: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan  3 11:41:33.632: INFO: e2e test version: v1.26.9
Jan  3 11:41:33.646: INFO: kube-apiserver version: v1.26.9
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan  3 11:41:33.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 11:41:33.667: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.268 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan  3 11:41:33.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 11:41:33.400: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0103 11:41:33.401098      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Jan  3 11:41:33.513: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan  3 11:41:33.609: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan  3 11:41:33.609: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
    Jan  3 11:41:33.609: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan  3 11:41:33.632: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jan  3 11:41:33.632: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-ionoscloud' (0 seconds elapsed)
    Jan  3 11:41:33.632: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
    Jan  3 11:41:33.632: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jan  3 11:41:33.632: INFO: e2e test version: v1.26.9
    Jan  3 11:41:33.646: INFO: kube-apiserver version: v1.26.9
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan  3 11:41:33.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 11:41:33.667: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:41:33.697
Jan  3 11:41:33.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/24 11:41:33.697
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:41:33.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:41:33.785
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/03/24 11:41:33.834
Jan  3 11:41:33.866: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3286" to be "running and ready"
Jan  3 11:41:33.885: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 19.723426ms
Jan  3 11:41:33.885: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 11:41:35.915: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049256183s
Jan  3 11:41:35.915: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 11:41:37.907: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040930344s
Jan  3 11:41:37.907: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 11:41:39.907: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041258372s
Jan  3 11:41:39.907: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 11:41:41.906: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040485901s
Jan  3 11:41:41.906: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 11:41:43.908: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042769081s
Jan  3 11:41:43.909: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 11:41:45.918: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 12.052162844s
Jan  3 11:41:45.918: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  3 11:41:45.918: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 01/03/24 11:41:45.942
Jan  3 11:41:45.965: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-3286" to be "running and ready"
Jan  3 11:41:45.983: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 18.261879ms
Jan  3 11:41:45.983: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  3 11:41:48.004: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039013877s
Jan  3 11:41:48.004: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  3 11:41:50.007: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.041571128s
Jan  3 11:41:50.007: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan  3 11:41:50.007: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/03/24 11:41:50.049
STEP: delete the pod with lifecycle hook 01/03/24 11:41:50.247
Jan  3 11:41:50.276: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  3 11:41:50.296: INFO: Pod pod-with-poststart-http-hook still exists
Jan  3 11:41:52.297: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  3 11:41:52.317: INFO: Pod pod-with-poststart-http-hook still exists
Jan  3 11:41:54.296: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  3 11:41:54.323: INFO: Pod pod-with-poststart-http-hook still exists
Jan  3 11:41:56.298: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  3 11:41:56.326: INFO: Pod pod-with-poststart-http-hook still exists
Jan  3 11:41:58.297: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  3 11:41:58.317: INFO: Pod pod-with-poststart-http-hook still exists
Jan  3 11:42:00.297: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  3 11:42:00.316: INFO: Pod pod-with-poststart-http-hook still exists
Jan  3 11:42:02.296: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  3 11:42:02.344: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan  3 11:42:02.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-3286" for this suite. 01/03/24 11:42:02.376
------------------------------
• [SLOW TEST] [28.711 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:41:33.697
    Jan  3 11:41:33.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/24 11:41:33.697
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:41:33.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:41:33.785
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/03/24 11:41:33.834
    Jan  3 11:41:33.866: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3286" to be "running and ready"
    Jan  3 11:41:33.885: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 19.723426ms
    Jan  3 11:41:33.885: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 11:41:35.915: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049256183s
    Jan  3 11:41:35.915: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 11:41:37.907: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040930344s
    Jan  3 11:41:37.907: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 11:41:39.907: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041258372s
    Jan  3 11:41:39.907: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 11:41:41.906: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040485901s
    Jan  3 11:41:41.906: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 11:41:43.908: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042769081s
    Jan  3 11:41:43.909: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 11:41:45.918: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 12.052162844s
    Jan  3 11:41:45.918: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  3 11:41:45.918: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 01/03/24 11:41:45.942
    Jan  3 11:41:45.965: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-3286" to be "running and ready"
    Jan  3 11:41:45.983: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 18.261879ms
    Jan  3 11:41:45.983: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 11:41:48.004: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039013877s
    Jan  3 11:41:48.004: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 11:41:50.007: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.041571128s
    Jan  3 11:41:50.007: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan  3 11:41:50.007: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/03/24 11:41:50.049
    STEP: delete the pod with lifecycle hook 01/03/24 11:41:50.247
    Jan  3 11:41:50.276: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  3 11:41:50.296: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  3 11:41:52.297: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  3 11:41:52.317: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  3 11:41:54.296: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  3 11:41:54.323: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  3 11:41:56.298: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  3 11:41:56.326: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  3 11:41:58.297: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  3 11:41:58.317: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  3 11:42:00.297: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  3 11:42:00.316: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  3 11:42:02.296: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  3 11:42:02.344: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:42:02.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-3286" for this suite. 01/03/24 11:42:02.376
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:42:02.412
Jan  3 11:42:02.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename ephemeral-containers-test 01/03/24 11:42:02.415
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:42:02.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:42:02.505
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/03/24 11:42:02.534
Jan  3 11:42:02.562: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7484" to be "running and ready"
Jan  3 11:42:02.582: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.171568ms
Jan  3 11:42:02.582: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  3 11:42:04.614: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051338479s
Jan  3 11:42:04.614: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  3 11:42:06.602: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.039168995s
Jan  3 11:42:06.602: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan  3 11:42:06.602: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/03/24 11:42:06.62
Jan  3 11:42:06.666: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7484" to be "container debugger running"
Jan  3 11:42:06.685: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 19.18081ms
Jan  3 11:42:08.732: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.066017439s
Jan  3 11:42:08.732: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/03/24 11:42:08.732
Jan  3 11:42:08.733: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-7484 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 11:42:08.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 11:42:08.735: INFO: ExecWithOptions: Clientset creation
Jan  3 11:42:08.735: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-7484/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan  3 11:42:09.138: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  3 11:42:09.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-7484" for this suite. 01/03/24 11:42:09.349
------------------------------
• [SLOW TEST] [6.975 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:42:02.412
    Jan  3 11:42:02.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/03/24 11:42:02.415
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:42:02.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:42:02.505
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/03/24 11:42:02.534
    Jan  3 11:42:02.562: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7484" to be "running and ready"
    Jan  3 11:42:02.582: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.171568ms
    Jan  3 11:42:02.582: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 11:42:04.614: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051338479s
    Jan  3 11:42:04.614: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 11:42:06.602: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.039168995s
    Jan  3 11:42:06.602: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan  3 11:42:06.602: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/03/24 11:42:06.62
    Jan  3 11:42:06.666: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7484" to be "container debugger running"
    Jan  3 11:42:06.685: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 19.18081ms
    Jan  3 11:42:08.732: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.066017439s
    Jan  3 11:42:08.732: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/03/24 11:42:08.732
    Jan  3 11:42:08.733: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-7484 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 11:42:08.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 11:42:08.735: INFO: ExecWithOptions: Clientset creation
    Jan  3 11:42:08.735: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-7484/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan  3 11:42:09.138: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:42:09.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-7484" for this suite. 01/03/24 11:42:09.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:42:09.394
Jan  3 11:42:09.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 11:42:09.397
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:42:09.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:42:09.483
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-435648a2-96d8-4002-8bbb-6793b17d9788 01/03/24 11:42:09.539
STEP: Creating the pod 01/03/24 11:42:09.557
Jan  3 11:42:09.598: INFO: Waiting up to 5m0s for pod "pod-configmaps-b34bd024-d5e1-4fb4-ba76-9033ab5ae77d" in namespace "configmap-7642" to be "running and ready"
Jan  3 11:42:09.619: INFO: Pod "pod-configmaps-b34bd024-d5e1-4fb4-ba76-9033ab5ae77d": Phase="Pending", Reason="", readiness=false. Elapsed: 21.627854ms
Jan  3 11:42:09.620: INFO: The phase of Pod pod-configmaps-b34bd024-d5e1-4fb4-ba76-9033ab5ae77d is Pending, waiting for it to be Running (with Ready = true)
Jan  3 11:42:11.641: INFO: Pod "pod-configmaps-b34bd024-d5e1-4fb4-ba76-9033ab5ae77d": Phase="Running", Reason="", readiness=true. Elapsed: 2.043274199s
Jan  3 11:42:11.641: INFO: The phase of Pod pod-configmaps-b34bd024-d5e1-4fb4-ba76-9033ab5ae77d is Running (Ready = true)
Jan  3 11:42:11.641: INFO: Pod "pod-configmaps-b34bd024-d5e1-4fb4-ba76-9033ab5ae77d" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-435648a2-96d8-4002-8bbb-6793b17d9788 01/03/24 11:42:11.708
STEP: waiting to observe update in volume 01/03/24 11:42:11.728
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 11:42:13.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7642" for this suite. 01/03/24 11:42:13.84
------------------------------
• [4.471 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:42:09.394
    Jan  3 11:42:09.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 11:42:09.397
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:42:09.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:42:09.483
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-435648a2-96d8-4002-8bbb-6793b17d9788 01/03/24 11:42:09.539
    STEP: Creating the pod 01/03/24 11:42:09.557
    Jan  3 11:42:09.598: INFO: Waiting up to 5m0s for pod "pod-configmaps-b34bd024-d5e1-4fb4-ba76-9033ab5ae77d" in namespace "configmap-7642" to be "running and ready"
    Jan  3 11:42:09.619: INFO: Pod "pod-configmaps-b34bd024-d5e1-4fb4-ba76-9033ab5ae77d": Phase="Pending", Reason="", readiness=false. Elapsed: 21.627854ms
    Jan  3 11:42:09.620: INFO: The phase of Pod pod-configmaps-b34bd024-d5e1-4fb4-ba76-9033ab5ae77d is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 11:42:11.641: INFO: Pod "pod-configmaps-b34bd024-d5e1-4fb4-ba76-9033ab5ae77d": Phase="Running", Reason="", readiness=true. Elapsed: 2.043274199s
    Jan  3 11:42:11.641: INFO: The phase of Pod pod-configmaps-b34bd024-d5e1-4fb4-ba76-9033ab5ae77d is Running (Ready = true)
    Jan  3 11:42:11.641: INFO: Pod "pod-configmaps-b34bd024-d5e1-4fb4-ba76-9033ab5ae77d" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-435648a2-96d8-4002-8bbb-6793b17d9788 01/03/24 11:42:11.708
    STEP: waiting to observe update in volume 01/03/24 11:42:11.728
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:42:13.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7642" for this suite. 01/03/24 11:42:13.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:42:13.867
Jan  3 11:42:13.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename subpath 01/03/24 11:42:13.869
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:42:13.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:42:13.951
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/03/24 11:42:13.982
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-lhrx 01/03/24 11:42:14.021
STEP: Creating a pod to test atomic-volume-subpath 01/03/24 11:42:14.022
Jan  3 11:42:14.055: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lhrx" in namespace "subpath-6562" to be "Succeeded or Failed"
Jan  3 11:42:14.073: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Pending", Reason="", readiness=false. Elapsed: 17.927208ms
Jan  3 11:42:16.095: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 2.040447846s
Jan  3 11:42:18.108: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 4.052620287s
Jan  3 11:42:20.103: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 6.048301224s
Jan  3 11:42:22.095: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 8.039758678s
Jan  3 11:42:24.094: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 10.039101327s
Jan  3 11:42:26.097: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 12.04204347s
Jan  3 11:42:28.095: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 14.04038047s
Jan  3 11:42:30.120: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 16.065125183s
Jan  3 11:42:32.095: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 18.040169655s
Jan  3 11:42:34.093: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 20.037926784s
Jan  3 11:42:36.095: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 22.040338097s
Jan  3 11:42:38.098: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=false. Elapsed: 24.043568624s
Jan  3 11:42:40.106: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.051371516s
STEP: Saw pod success 01/03/24 11:42:40.106
Jan  3 11:42:40.107: INFO: Pod "pod-subpath-test-configmap-lhrx" satisfied condition "Succeeded or Failed"
Jan  3 11:42:40.128: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-subpath-test-configmap-lhrx container test-container-subpath-configmap-lhrx: <nil>
STEP: delete the pod 01/03/24 11:42:40.169
Jan  3 11:42:40.226: INFO: Waiting for pod pod-subpath-test-configmap-lhrx to disappear
Jan  3 11:42:40.244: INFO: Pod pod-subpath-test-configmap-lhrx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lhrx 01/03/24 11:42:40.244
Jan  3 11:42:40.244: INFO: Deleting pod "pod-subpath-test-configmap-lhrx" in namespace "subpath-6562"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  3 11:42:40.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6562" for this suite. 01/03/24 11:42:40.308
------------------------------
• [SLOW TEST] [26.476 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:42:13.867
    Jan  3 11:42:13.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename subpath 01/03/24 11:42:13.869
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:42:13.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:42:13.951
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/03/24 11:42:13.982
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-lhrx 01/03/24 11:42:14.021
    STEP: Creating a pod to test atomic-volume-subpath 01/03/24 11:42:14.022
    Jan  3 11:42:14.055: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lhrx" in namespace "subpath-6562" to be "Succeeded or Failed"
    Jan  3 11:42:14.073: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Pending", Reason="", readiness=false. Elapsed: 17.927208ms
    Jan  3 11:42:16.095: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 2.040447846s
    Jan  3 11:42:18.108: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 4.052620287s
    Jan  3 11:42:20.103: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 6.048301224s
    Jan  3 11:42:22.095: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 8.039758678s
    Jan  3 11:42:24.094: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 10.039101327s
    Jan  3 11:42:26.097: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 12.04204347s
    Jan  3 11:42:28.095: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 14.04038047s
    Jan  3 11:42:30.120: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 16.065125183s
    Jan  3 11:42:32.095: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 18.040169655s
    Jan  3 11:42:34.093: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 20.037926784s
    Jan  3 11:42:36.095: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=true. Elapsed: 22.040338097s
    Jan  3 11:42:38.098: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Running", Reason="", readiness=false. Elapsed: 24.043568624s
    Jan  3 11:42:40.106: INFO: Pod "pod-subpath-test-configmap-lhrx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.051371516s
    STEP: Saw pod success 01/03/24 11:42:40.106
    Jan  3 11:42:40.107: INFO: Pod "pod-subpath-test-configmap-lhrx" satisfied condition "Succeeded or Failed"
    Jan  3 11:42:40.128: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-subpath-test-configmap-lhrx container test-container-subpath-configmap-lhrx: <nil>
    STEP: delete the pod 01/03/24 11:42:40.169
    Jan  3 11:42:40.226: INFO: Waiting for pod pod-subpath-test-configmap-lhrx to disappear
    Jan  3 11:42:40.244: INFO: Pod pod-subpath-test-configmap-lhrx no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-lhrx 01/03/24 11:42:40.244
    Jan  3 11:42:40.244: INFO: Deleting pod "pod-subpath-test-configmap-lhrx" in namespace "subpath-6562"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:42:40.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6562" for this suite. 01/03/24 11:42:40.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:42:40.355
Jan  3 11:42:40.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 11:42:40.358
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:42:40.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:42:40.44
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-68154daa-1be0-4369-ad62-ff72e507d17b 01/03/24 11:42:40.487
STEP: Creating secret with name s-test-opt-upd-598bb807-b2c3-4386-963e-2029cc5631c5 01/03/24 11:42:40.506
STEP: Creating the pod 01/03/24 11:42:40.528
Jan  3 11:42:40.564: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a081c278-77f5-48b7-ae57-dc33cb83cfb8" in namespace "projected-8483" to be "running and ready"
Jan  3 11:42:40.583: INFO: Pod "pod-projected-secrets-a081c278-77f5-48b7-ae57-dc33cb83cfb8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.254116ms
Jan  3 11:42:40.583: INFO: The phase of Pod pod-projected-secrets-a081c278-77f5-48b7-ae57-dc33cb83cfb8 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 11:42:42.602: INFO: Pod "pod-projected-secrets-a081c278-77f5-48b7-ae57-dc33cb83cfb8": Phase="Running", Reason="", readiness=true. Elapsed: 2.038620403s
Jan  3 11:42:42.602: INFO: The phase of Pod pod-projected-secrets-a081c278-77f5-48b7-ae57-dc33cb83cfb8 is Running (Ready = true)
Jan  3 11:42:42.602: INFO: Pod "pod-projected-secrets-a081c278-77f5-48b7-ae57-dc33cb83cfb8" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-68154daa-1be0-4369-ad62-ff72e507d17b 01/03/24 11:42:42.751
STEP: Updating secret s-test-opt-upd-598bb807-b2c3-4386-963e-2029cc5631c5 01/03/24 11:42:42.773
STEP: Creating secret with name s-test-opt-create-e0e57cbf-c410-4dea-aa1e-86705523f833 01/03/24 11:42:42.792
STEP: waiting to observe update in volume 01/03/24 11:42:42.816
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  3 11:42:44.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8483" for this suite. 01/03/24 11:42:45.013
------------------------------
• [4.720 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:42:40.355
    Jan  3 11:42:40.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 11:42:40.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:42:40.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:42:40.44
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-68154daa-1be0-4369-ad62-ff72e507d17b 01/03/24 11:42:40.487
    STEP: Creating secret with name s-test-opt-upd-598bb807-b2c3-4386-963e-2029cc5631c5 01/03/24 11:42:40.506
    STEP: Creating the pod 01/03/24 11:42:40.528
    Jan  3 11:42:40.564: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a081c278-77f5-48b7-ae57-dc33cb83cfb8" in namespace "projected-8483" to be "running and ready"
    Jan  3 11:42:40.583: INFO: Pod "pod-projected-secrets-a081c278-77f5-48b7-ae57-dc33cb83cfb8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.254116ms
    Jan  3 11:42:40.583: INFO: The phase of Pod pod-projected-secrets-a081c278-77f5-48b7-ae57-dc33cb83cfb8 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 11:42:42.602: INFO: Pod "pod-projected-secrets-a081c278-77f5-48b7-ae57-dc33cb83cfb8": Phase="Running", Reason="", readiness=true. Elapsed: 2.038620403s
    Jan  3 11:42:42.602: INFO: The phase of Pod pod-projected-secrets-a081c278-77f5-48b7-ae57-dc33cb83cfb8 is Running (Ready = true)
    Jan  3 11:42:42.602: INFO: Pod "pod-projected-secrets-a081c278-77f5-48b7-ae57-dc33cb83cfb8" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-68154daa-1be0-4369-ad62-ff72e507d17b 01/03/24 11:42:42.751
    STEP: Updating secret s-test-opt-upd-598bb807-b2c3-4386-963e-2029cc5631c5 01/03/24 11:42:42.773
    STEP: Creating secret with name s-test-opt-create-e0e57cbf-c410-4dea-aa1e-86705523f833 01/03/24 11:42:42.792
    STEP: waiting to observe update in volume 01/03/24 11:42:42.816
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:42:44.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8483" for this suite. 01/03/24 11:42:45.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:42:45.075
Jan  3 11:42:45.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename sched-pred 01/03/24 11:42:45.078
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:42:45.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:42:45.161
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan  3 11:42:45.191: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  3 11:42:45.233: INFO: Waiting for terminating namespaces to be deleted...
Jan  3 11:42:45.271: INFO: 
Logging pods the apiserver thinks is on node jb-1-26-np-64kerjapxk before test
Jan  3 11:42:45.314: INFO: calico-node-r98wj from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.314: INFO: 	Container calico-node ready: true, restart count 2
Jan  3 11:42:45.314: INFO: calico-typha-8555b568f6-4nmck from kube-system started at 2024-01-03 11:33:49 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.314: INFO: 	Container calico-typha ready: true, restart count 0
Jan  3 11:42:45.314: INFO: csi-ionoscloud-t8qkm from kube-system started at 2024-01-03 11:27:28 +0000 UTC (2 container statuses recorded)
Jan  3 11:42:45.314: INFO: 	Container csi-ionoscloud ready: true, restart count 0
Jan  3 11:42:45.314: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  3 11:42:45.314: INFO: konnectivity-agent-gnnsp from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.314: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  3 11:42:45.314: INFO: kube-proxy-z7q4m from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.314: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 11:42:45.314: INFO: nginx-proxy-jb-1-26-np-64kerjapxk from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.314: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan  3 11:42:45.314: INFO: pod-projected-secrets-a081c278-77f5-48b7-ae57-dc33cb83cfb8 from projected-8483 started at 2024-01-03 11:42:40 +0000 UTC (3 container statuses recorded)
Jan  3 11:42:45.314: INFO: 	Container creates-volume-test ready: true, restart count 0
Jan  3 11:42:45.314: INFO: 	Container dels-volume-test ready: true, restart count 0
Jan  3 11:42:45.314: INFO: 	Container upds-volume-test ready: true, restart count 0
Jan  3 11:42:45.314: INFO: sonobuoy from sonobuoy started at 2024-01-03 11:39:58 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.314: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  3 11:42:45.314: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-4hltf from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 11:42:45.314: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 11:42:45.314: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 11:42:45.314: INFO: 
Logging pods the apiserver thinks is on node jb-1-26-np-adtwo5cmi2 before test
Jan  3 11:42:45.341: INFO: ephemeral-containers-target-pod from ephemeral-containers-test-7484 started at 2024-01-03 11:42:02 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.341: INFO: 	Container test-container-1 ready: true, restart count 0
Jan  3 11:42:45.341: INFO: calico-kube-controllers-54564bcfb4-kjb7m from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.341: INFO: 	Container calico-kube-controllers ready: true, restart count 2
Jan  3 11:42:45.341: INFO: calico-node-j4nfd from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.341: INFO: 	Container calico-node ready: true, restart count 1
Jan  3 11:42:45.341: INFO: csi-ionoscloud-4z7q8 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (2 container statuses recorded)
Jan  3 11:42:45.341: INFO: 	Container csi-ionoscloud ready: true, restart count 0
Jan  3 11:42:45.341: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  3 11:42:45.341: INFO: konnectivity-agent-srxbt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.341: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  3 11:42:45.341: INFO: kube-proxy-ml4kt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.341: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 11:42:45.341: INFO: nginx-proxy-jb-1-26-np-adtwo5cmi2 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.341: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan  3 11:42:45.341: INFO: snapshot-validation-webhook-684799cdd5-x6hm4 from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.341: INFO: 	Container snapshot-validation ready: true, restart count 0
Jan  3 11:42:45.341: INFO: sonobuoy-e2e-job-61ed32d11fea4649 from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 11:42:45.341: INFO: 	Container e2e ready: true, restart count 0
Jan  3 11:42:45.341: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 11:42:45.341: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-cb8zx from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 11:42:45.341: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 11:42:45.341: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 11:42:45.341: INFO: 
Logging pods the apiserver thinks is on node jb-1-26-np-nqeu5xtrab before test
Jan  3 11:42:45.367: INFO: calico-node-t5pwv from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.367: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 11:42:45.367: INFO: calico-typha-8555b568f6-bdlz4 from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.367: INFO: 	Container calico-typha ready: true, restart count 0
Jan  3 11:42:45.367: INFO: coredns-89967fcdc-9jbpx from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.367: INFO: 	Container coredns ready: true, restart count 0
Jan  3 11:42:45.367: INFO: coredns-89967fcdc-zjgbq from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.367: INFO: 	Container coredns ready: true, restart count 0
Jan  3 11:42:45.367: INFO: csi-ionoscloud-kv8wz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (2 container statuses recorded)
Jan  3 11:42:45.367: INFO: 	Container csi-ionoscloud ready: true, restart count 0
Jan  3 11:42:45.367: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  3 11:42:45.367: INFO: ionos-policy-validator-64d9954f65-dpd5v from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.367: INFO: 	Container ionos-policy-validator ready: true, restart count 0
Jan  3 11:42:45.367: INFO: konnectivity-agent-v6hrz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.367: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  3 11:42:45.367: INFO: kube-proxy-hqxkb from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.367: INFO: 	Container kube-proxy ready: true, restart count 1
Jan  3 11:42:45.367: INFO: nginx-proxy-jb-1-26-np-nqeu5xtrab from kube-system started at 2024-01-03 11:30:18 +0000 UTC (1 container statuses recorded)
Jan  3 11:42:45.367: INFO: 	Container nginx-proxy ready: true, restart count 1
Jan  3 11:42:45.367: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-j7ffw from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 11:42:45.367: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 11:42:45.367: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/03/24 11:42:45.367
Jan  3 11:42:45.393: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2041" to be "running"
Jan  3 11:42:45.412: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 18.427882ms
Jan  3 11:42:47.432: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038694907s
Jan  3 11:42:49.434: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.040914519s
Jan  3 11:42:49.434: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/03/24 11:42:49.456
STEP: Trying to apply a random label on the found node. 01/03/24 11:42:49.508
STEP: verifying the node has the label kubernetes.io/e2e-3dfc0ce9-9f2e-427c-986d-ce2da24f0419 95 01/03/24 11:42:49.541
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/03/24 11:42:49.56
Jan  3 11:42:49.584: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2041" to be "not pending"
Jan  3 11:42:49.603: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.550074ms
Jan  3 11:42:51.625: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040920299s
Jan  3 11:42:53.624: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039705087s
Jan  3 11:42:55.625: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041034367s
Jan  3 11:42:57.627: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042515614s
Jan  3 11:42:59.627: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042604321s
Jan  3 11:43:01.623: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.038624226s
Jan  3 11:43:03.625: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.040372854s
Jan  3 11:43:05.623: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.03876353s
Jan  3 11:43:07.625: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.040581263s
Jan  3 11:43:09.640: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.055582647s
Jan  3 11:43:11.625: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.040576064s
Jan  3 11:43:13.627: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 24.042522082s
Jan  3 11:43:13.627: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 85.215.162.129 on the node which pod4 resides and expect not scheduled 01/03/24 11:43:13.627
Jan  3 11:43:13.648: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2041" to be "not pending"
Jan  3 11:43:13.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.195942ms
Jan  3 11:43:15.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044921792s
Jan  3 11:43:17.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039328505s
Jan  3 11:43:19.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040365936s
Jan  3 11:43:21.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042225422s
Jan  3 11:43:23.700: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.05139257s
Jan  3 11:43:25.697: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.048455216s
Jan  3 11:43:27.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.041037612s
Jan  3 11:43:29.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.041236552s
Jan  3 11:43:31.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.041405117s
Jan  3 11:43:33.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.042993154s
Jan  3 11:43:35.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.043035113s
Jan  3 11:43:37.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.039754074s
Jan  3 11:43:39.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.040103553s
Jan  3 11:43:41.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.046105845s
Jan  3 11:43:43.701: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.052972547s
Jan  3 11:43:45.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.044402123s
Jan  3 11:43:47.711: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.062892881s
Jan  3 11:43:49.708: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.059533126s
Jan  3 11:43:51.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.041665874s
Jan  3 11:43:53.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.041623066s
Jan  3 11:43:55.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.040643463s
Jan  3 11:43:57.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.041112068s
Jan  3 11:43:59.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.041012208s
Jan  3 11:44:01.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.0413641s
Jan  3 11:44:03.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.041820156s
Jan  3 11:44:05.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.041870569s
Jan  3 11:44:07.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.041481255s
Jan  3 11:44:09.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.041554245s
Jan  3 11:44:11.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.040629468s
Jan  3 11:44:13.697: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.048173238s
Jan  3 11:44:15.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.043186246s
Jan  3 11:44:17.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.040741671s
Jan  3 11:44:19.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.042128252s
Jan  3 11:44:21.696: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.047263149s
Jan  3 11:44:23.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.046622832s
Jan  3 11:44:25.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.042490098s
Jan  3 11:44:27.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.044804252s
Jan  3 11:44:29.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.039218059s
Jan  3 11:44:31.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.042225374s
Jan  3 11:44:33.696: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.047089848s
Jan  3 11:44:35.698: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.049546499s
Jan  3 11:44:37.696: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.047586096s
Jan  3 11:44:39.696: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.047844823s
Jan  3 11:44:41.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.0406814s
Jan  3 11:44:43.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.039983397s
Jan  3 11:44:45.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.039515506s
Jan  3 11:44:47.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.039160551s
Jan  3 11:44:49.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.044815005s
Jan  3 11:44:51.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.040479053s
Jan  3 11:44:53.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.040714745s
Jan  3 11:44:55.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.04069972s
Jan  3 11:44:57.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.041682603s
Jan  3 11:44:59.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.040152255s
Jan  3 11:45:01.700: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.05130412s
Jan  3 11:45:03.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.044312556s
Jan  3 11:45:05.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.040892189s
Jan  3 11:45:07.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.042499164s
Jan  3 11:45:09.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.040835327s
Jan  3 11:45:11.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.041032014s
Jan  3 11:45:13.712: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.063358346s
Jan  3 11:45:15.686: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.037980005s
Jan  3 11:45:17.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.041694429s
Jan  3 11:45:19.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.040189223s
Jan  3 11:45:21.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.040243779s
Jan  3 11:45:23.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.042720909s
Jan  3 11:45:25.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.039844276s
Jan  3 11:45:27.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.0417838s
Jan  3 11:45:29.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.039244301s
Jan  3 11:45:31.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.040555182s
Jan  3 11:45:33.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.041334969s
Jan  3 11:45:35.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.042880566s
Jan  3 11:45:37.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.042641479s
Jan  3 11:45:39.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.040270069s
Jan  3 11:45:41.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.040556601s
Jan  3 11:45:43.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.042059397s
Jan  3 11:45:45.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.043831575s
Jan  3 11:45:47.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.040458437s
Jan  3 11:45:49.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.039461618s
Jan  3 11:45:51.711: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.062752455s
Jan  3 11:45:53.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.043104373s
Jan  3 11:45:55.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.04095s
Jan  3 11:45:57.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.043666394s
Jan  3 11:45:59.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.040073739s
Jan  3 11:46:01.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.043308134s
Jan  3 11:46:03.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.039962593s
Jan  3 11:46:05.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.044842661s
Jan  3 11:46:07.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.043138941s
Jan  3 11:46:09.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.041667834s
Jan  3 11:46:11.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.044356417s
Jan  3 11:46:13.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.040701921s
Jan  3 11:46:15.708: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.059792365s
Jan  3 11:46:17.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.038603584s
Jan  3 11:46:19.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.043789658s
Jan  3 11:46:21.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.04171896s
Jan  3 11:46:23.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.040007098s
Jan  3 11:46:25.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.043797386s
Jan  3 11:46:27.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.039348452s
Jan  3 11:46:29.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.040252473s
Jan  3 11:46:31.697: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.048642338s
Jan  3 11:46:33.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.039984044s
Jan  3 11:46:35.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.044302694s
Jan  3 11:46:37.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.046443511s
Jan  3 11:46:39.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.042155348s
Jan  3 11:46:41.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.041673922s
Jan  3 11:46:43.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.045522562s
Jan  3 11:46:45.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.039433216s
Jan  3 11:46:47.696: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.047872516s
Jan  3 11:46:49.699: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.050327707s
Jan  3 11:46:51.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.045295159s
Jan  3 11:46:53.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.041264107s
Jan  3 11:46:55.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.041688457s
Jan  3 11:46:57.701: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.052099348s
Jan  3 11:46:59.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.040819683s
Jan  3 11:47:01.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.043785625s
Jan  3 11:47:03.699: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.051002688s
Jan  3 11:47:05.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.04100526s
Jan  3 11:47:07.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.040804888s
Jan  3 11:47:09.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.04007824s
Jan  3 11:47:11.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.041609291s
Jan  3 11:47:13.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.039663633s
Jan  3 11:47:15.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.041006624s
Jan  3 11:47:17.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.046770363s
Jan  3 11:47:19.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.0403463s
Jan  3 11:47:21.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.039308553s
Jan  3 11:47:23.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.044881931s
Jan  3 11:47:25.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.039369804s
Jan  3 11:47:27.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.03977544s
Jan  3 11:47:29.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.042287575s
Jan  3 11:47:31.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.042951547s
Jan  3 11:47:33.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.041204772s
Jan  3 11:47:35.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.043612646s
Jan  3 11:47:37.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.042281422s
Jan  3 11:47:39.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.04515808s
Jan  3 11:47:41.698: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.049113598s
Jan  3 11:47:43.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.040279238s
Jan  3 11:47:45.686: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.037835276s
Jan  3 11:47:47.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.040444002s
Jan  3 11:47:49.698: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.049743648s
Jan  3 11:47:51.697: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.048273981s
Jan  3 11:47:53.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.042211315s
Jan  3 11:47:55.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.040404688s
Jan  3 11:47:57.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.041445121s
Jan  3 11:47:59.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.040335511s
Jan  3 11:48:01.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.044658944s
Jan  3 11:48:03.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.045448025s
Jan  3 11:48:05.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.043834396s
Jan  3 11:48:07.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.038591573s
Jan  3 11:48:09.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.040384856s
Jan  3 11:48:11.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.041727517s
Jan  3 11:48:13.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.040074926s
Jan  3 11:48:13.712: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.063382091s
STEP: removing the label kubernetes.io/e2e-3dfc0ce9-9f2e-427c-986d-ce2da24f0419 off the node jb-1-26-np-adtwo5cmi2 01/03/24 11:48:13.712
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3dfc0ce9-9f2e-427c-986d-ce2da24f0419 01/03/24 11:48:13.758
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 11:48:13.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2041" for this suite. 01/03/24 11:48:13.795
------------------------------
• [SLOW TEST] [328.745 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:42:45.075
    Jan  3 11:42:45.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename sched-pred 01/03/24 11:42:45.078
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:42:45.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:42:45.161
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan  3 11:42:45.191: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  3 11:42:45.233: INFO: Waiting for terminating namespaces to be deleted...
    Jan  3 11:42:45.271: INFO: 
    Logging pods the apiserver thinks is on node jb-1-26-np-64kerjapxk before test
    Jan  3 11:42:45.314: INFO: calico-node-r98wj from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.314: INFO: 	Container calico-node ready: true, restart count 2
    Jan  3 11:42:45.314: INFO: calico-typha-8555b568f6-4nmck from kube-system started at 2024-01-03 11:33:49 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.314: INFO: 	Container calico-typha ready: true, restart count 0
    Jan  3 11:42:45.314: INFO: csi-ionoscloud-t8qkm from kube-system started at 2024-01-03 11:27:28 +0000 UTC (2 container statuses recorded)
    Jan  3 11:42:45.314: INFO: 	Container csi-ionoscloud ready: true, restart count 0
    Jan  3 11:42:45.314: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  3 11:42:45.314: INFO: konnectivity-agent-gnnsp from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.314: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  3 11:42:45.314: INFO: kube-proxy-z7q4m from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.314: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 11:42:45.314: INFO: nginx-proxy-jb-1-26-np-64kerjapxk from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.314: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan  3 11:42:45.314: INFO: pod-projected-secrets-a081c278-77f5-48b7-ae57-dc33cb83cfb8 from projected-8483 started at 2024-01-03 11:42:40 +0000 UTC (3 container statuses recorded)
    Jan  3 11:42:45.314: INFO: 	Container creates-volume-test ready: true, restart count 0
    Jan  3 11:42:45.314: INFO: 	Container dels-volume-test ready: true, restart count 0
    Jan  3 11:42:45.314: INFO: 	Container upds-volume-test ready: true, restart count 0
    Jan  3 11:42:45.314: INFO: sonobuoy from sonobuoy started at 2024-01-03 11:39:58 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.314: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  3 11:42:45.314: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-4hltf from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 11:42:45.314: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 11:42:45.314: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 11:42:45.314: INFO: 
    Logging pods the apiserver thinks is on node jb-1-26-np-adtwo5cmi2 before test
    Jan  3 11:42:45.341: INFO: ephemeral-containers-target-pod from ephemeral-containers-test-7484 started at 2024-01-03 11:42:02 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.341: INFO: 	Container test-container-1 ready: true, restart count 0
    Jan  3 11:42:45.341: INFO: calico-kube-controllers-54564bcfb4-kjb7m from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.341: INFO: 	Container calico-kube-controllers ready: true, restart count 2
    Jan  3 11:42:45.341: INFO: calico-node-j4nfd from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.341: INFO: 	Container calico-node ready: true, restart count 1
    Jan  3 11:42:45.341: INFO: csi-ionoscloud-4z7q8 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (2 container statuses recorded)
    Jan  3 11:42:45.341: INFO: 	Container csi-ionoscloud ready: true, restart count 0
    Jan  3 11:42:45.341: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  3 11:42:45.341: INFO: konnectivity-agent-srxbt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.341: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  3 11:42:45.341: INFO: kube-proxy-ml4kt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.341: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 11:42:45.341: INFO: nginx-proxy-jb-1-26-np-adtwo5cmi2 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.341: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan  3 11:42:45.341: INFO: snapshot-validation-webhook-684799cdd5-x6hm4 from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.341: INFO: 	Container snapshot-validation ready: true, restart count 0
    Jan  3 11:42:45.341: INFO: sonobuoy-e2e-job-61ed32d11fea4649 from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 11:42:45.341: INFO: 	Container e2e ready: true, restart count 0
    Jan  3 11:42:45.341: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 11:42:45.341: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-cb8zx from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 11:42:45.341: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 11:42:45.341: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 11:42:45.341: INFO: 
    Logging pods the apiserver thinks is on node jb-1-26-np-nqeu5xtrab before test
    Jan  3 11:42:45.367: INFO: calico-node-t5pwv from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.367: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 11:42:45.367: INFO: calico-typha-8555b568f6-bdlz4 from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.367: INFO: 	Container calico-typha ready: true, restart count 0
    Jan  3 11:42:45.367: INFO: coredns-89967fcdc-9jbpx from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.367: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 11:42:45.367: INFO: coredns-89967fcdc-zjgbq from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.367: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 11:42:45.367: INFO: csi-ionoscloud-kv8wz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (2 container statuses recorded)
    Jan  3 11:42:45.367: INFO: 	Container csi-ionoscloud ready: true, restart count 0
    Jan  3 11:42:45.367: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  3 11:42:45.367: INFO: ionos-policy-validator-64d9954f65-dpd5v from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.367: INFO: 	Container ionos-policy-validator ready: true, restart count 0
    Jan  3 11:42:45.367: INFO: konnectivity-agent-v6hrz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.367: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  3 11:42:45.367: INFO: kube-proxy-hqxkb from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.367: INFO: 	Container kube-proxy ready: true, restart count 1
    Jan  3 11:42:45.367: INFO: nginx-proxy-jb-1-26-np-nqeu5xtrab from kube-system started at 2024-01-03 11:30:18 +0000 UTC (1 container statuses recorded)
    Jan  3 11:42:45.367: INFO: 	Container nginx-proxy ready: true, restart count 1
    Jan  3 11:42:45.367: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-j7ffw from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 11:42:45.367: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 11:42:45.367: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/03/24 11:42:45.367
    Jan  3 11:42:45.393: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2041" to be "running"
    Jan  3 11:42:45.412: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 18.427882ms
    Jan  3 11:42:47.432: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038694907s
    Jan  3 11:42:49.434: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.040914519s
    Jan  3 11:42:49.434: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/03/24 11:42:49.456
    STEP: Trying to apply a random label on the found node. 01/03/24 11:42:49.508
    STEP: verifying the node has the label kubernetes.io/e2e-3dfc0ce9-9f2e-427c-986d-ce2da24f0419 95 01/03/24 11:42:49.541
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/03/24 11:42:49.56
    Jan  3 11:42:49.584: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2041" to be "not pending"
    Jan  3 11:42:49.603: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.550074ms
    Jan  3 11:42:51.625: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040920299s
    Jan  3 11:42:53.624: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039705087s
    Jan  3 11:42:55.625: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041034367s
    Jan  3 11:42:57.627: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042515614s
    Jan  3 11:42:59.627: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042604321s
    Jan  3 11:43:01.623: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.038624226s
    Jan  3 11:43:03.625: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.040372854s
    Jan  3 11:43:05.623: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.03876353s
    Jan  3 11:43:07.625: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.040581263s
    Jan  3 11:43:09.640: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.055582647s
    Jan  3 11:43:11.625: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.040576064s
    Jan  3 11:43:13.627: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 24.042522082s
    Jan  3 11:43:13.627: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 85.215.162.129 on the node which pod4 resides and expect not scheduled 01/03/24 11:43:13.627
    Jan  3 11:43:13.648: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2041" to be "not pending"
    Jan  3 11:43:13.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.195942ms
    Jan  3 11:43:15.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044921792s
    Jan  3 11:43:17.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039328505s
    Jan  3 11:43:19.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040365936s
    Jan  3 11:43:21.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042225422s
    Jan  3 11:43:23.700: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.05139257s
    Jan  3 11:43:25.697: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.048455216s
    Jan  3 11:43:27.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.041037612s
    Jan  3 11:43:29.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.041236552s
    Jan  3 11:43:31.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.041405117s
    Jan  3 11:43:33.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.042993154s
    Jan  3 11:43:35.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.043035113s
    Jan  3 11:43:37.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.039754074s
    Jan  3 11:43:39.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.040103553s
    Jan  3 11:43:41.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.046105845s
    Jan  3 11:43:43.701: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.052972547s
    Jan  3 11:43:45.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.044402123s
    Jan  3 11:43:47.711: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.062892881s
    Jan  3 11:43:49.708: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.059533126s
    Jan  3 11:43:51.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.041665874s
    Jan  3 11:43:53.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.041623066s
    Jan  3 11:43:55.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.040643463s
    Jan  3 11:43:57.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.041112068s
    Jan  3 11:43:59.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.041012208s
    Jan  3 11:44:01.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.0413641s
    Jan  3 11:44:03.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.041820156s
    Jan  3 11:44:05.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.041870569s
    Jan  3 11:44:07.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.041481255s
    Jan  3 11:44:09.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.041554245s
    Jan  3 11:44:11.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.040629468s
    Jan  3 11:44:13.697: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.048173238s
    Jan  3 11:44:15.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.043186246s
    Jan  3 11:44:17.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.040741671s
    Jan  3 11:44:19.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.042128252s
    Jan  3 11:44:21.696: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.047263149s
    Jan  3 11:44:23.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.046622832s
    Jan  3 11:44:25.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.042490098s
    Jan  3 11:44:27.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.044804252s
    Jan  3 11:44:29.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.039218059s
    Jan  3 11:44:31.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.042225374s
    Jan  3 11:44:33.696: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.047089848s
    Jan  3 11:44:35.698: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.049546499s
    Jan  3 11:44:37.696: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.047586096s
    Jan  3 11:44:39.696: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.047844823s
    Jan  3 11:44:41.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.0406814s
    Jan  3 11:44:43.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.039983397s
    Jan  3 11:44:45.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.039515506s
    Jan  3 11:44:47.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.039160551s
    Jan  3 11:44:49.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.044815005s
    Jan  3 11:44:51.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.040479053s
    Jan  3 11:44:53.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.040714745s
    Jan  3 11:44:55.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.04069972s
    Jan  3 11:44:57.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.041682603s
    Jan  3 11:44:59.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.040152255s
    Jan  3 11:45:01.700: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.05130412s
    Jan  3 11:45:03.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.044312556s
    Jan  3 11:45:05.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.040892189s
    Jan  3 11:45:07.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.042499164s
    Jan  3 11:45:09.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.040835327s
    Jan  3 11:45:11.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.041032014s
    Jan  3 11:45:13.712: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.063358346s
    Jan  3 11:45:15.686: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.037980005s
    Jan  3 11:45:17.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.041694429s
    Jan  3 11:45:19.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.040189223s
    Jan  3 11:45:21.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.040243779s
    Jan  3 11:45:23.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.042720909s
    Jan  3 11:45:25.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.039844276s
    Jan  3 11:45:27.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.0417838s
    Jan  3 11:45:29.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.039244301s
    Jan  3 11:45:31.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.040555182s
    Jan  3 11:45:33.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.041334969s
    Jan  3 11:45:35.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.042880566s
    Jan  3 11:45:37.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.042641479s
    Jan  3 11:45:39.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.040270069s
    Jan  3 11:45:41.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.040556601s
    Jan  3 11:45:43.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.042059397s
    Jan  3 11:45:45.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.043831575s
    Jan  3 11:45:47.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.040458437s
    Jan  3 11:45:49.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.039461618s
    Jan  3 11:45:51.711: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.062752455s
    Jan  3 11:45:53.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.043104373s
    Jan  3 11:45:55.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.04095s
    Jan  3 11:45:57.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.043666394s
    Jan  3 11:45:59.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.040073739s
    Jan  3 11:46:01.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.043308134s
    Jan  3 11:46:03.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.039962593s
    Jan  3 11:46:05.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.044842661s
    Jan  3 11:46:07.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.043138941s
    Jan  3 11:46:09.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.041667834s
    Jan  3 11:46:11.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.044356417s
    Jan  3 11:46:13.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.040701921s
    Jan  3 11:46:15.708: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.059792365s
    Jan  3 11:46:17.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.038603584s
    Jan  3 11:46:19.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.043789658s
    Jan  3 11:46:21.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.04171896s
    Jan  3 11:46:23.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.040007098s
    Jan  3 11:46:25.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.043797386s
    Jan  3 11:46:27.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.039348452s
    Jan  3 11:46:29.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.040252473s
    Jan  3 11:46:31.697: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.048642338s
    Jan  3 11:46:33.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.039984044s
    Jan  3 11:46:35.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.044302694s
    Jan  3 11:46:37.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.046443511s
    Jan  3 11:46:39.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.042155348s
    Jan  3 11:46:41.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.041673922s
    Jan  3 11:46:43.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.045522562s
    Jan  3 11:46:45.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.039433216s
    Jan  3 11:46:47.696: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.047872516s
    Jan  3 11:46:49.699: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.050327707s
    Jan  3 11:46:51.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.045295159s
    Jan  3 11:46:53.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.041264107s
    Jan  3 11:46:55.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.041688457s
    Jan  3 11:46:57.701: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.052099348s
    Jan  3 11:46:59.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.040819683s
    Jan  3 11:47:01.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.043785625s
    Jan  3 11:47:03.699: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.051002688s
    Jan  3 11:47:05.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.04100526s
    Jan  3 11:47:07.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.040804888s
    Jan  3 11:47:09.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.04007824s
    Jan  3 11:47:11.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.041609291s
    Jan  3 11:47:13.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.039663633s
    Jan  3 11:47:15.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.041006624s
    Jan  3 11:47:17.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.046770363s
    Jan  3 11:47:19.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.0403463s
    Jan  3 11:47:21.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.039308553s
    Jan  3 11:47:23.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.044881931s
    Jan  3 11:47:25.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.039369804s
    Jan  3 11:47:27.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.03977544s
    Jan  3 11:47:29.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.042287575s
    Jan  3 11:47:31.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.042951547s
    Jan  3 11:47:33.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.041204772s
    Jan  3 11:47:35.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.043612646s
    Jan  3 11:47:37.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.042281422s
    Jan  3 11:47:39.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.04515808s
    Jan  3 11:47:41.698: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.049113598s
    Jan  3 11:47:43.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.040279238s
    Jan  3 11:47:45.686: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.037835276s
    Jan  3 11:47:47.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.040444002s
    Jan  3 11:47:49.698: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.049743648s
    Jan  3 11:47:51.697: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.048273981s
    Jan  3 11:47:53.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.042211315s
    Jan  3 11:47:55.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.040404688s
    Jan  3 11:47:57.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.041445121s
    Jan  3 11:47:59.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.040335511s
    Jan  3 11:48:01.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.044658944s
    Jan  3 11:48:03.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.045448025s
    Jan  3 11:48:05.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.043834396s
    Jan  3 11:48:07.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.038591573s
    Jan  3 11:48:09.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.040384856s
    Jan  3 11:48:11.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.041727517s
    Jan  3 11:48:13.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.040074926s
    Jan  3 11:48:13.712: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.063382091s
    STEP: removing the label kubernetes.io/e2e-3dfc0ce9-9f2e-427c-986d-ce2da24f0419 off the node jb-1-26-np-adtwo5cmi2 01/03/24 11:48:13.712
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-3dfc0ce9-9f2e-427c-986d-ce2da24f0419 01/03/24 11:48:13.758
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:48:13.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2041" for this suite. 01/03/24 11:48:13.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:48:13.821
Jan  3 11:48:13.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename tables 01/03/24 11:48:13.822
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:48:13.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:48:13.907
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jan  3 11:48:13.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-2594" for this suite. 01/03/24 11:48:13.987
------------------------------
• [0.194 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:48:13.821
    Jan  3 11:48:13.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename tables 01/03/24 11:48:13.822
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:48:13.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:48:13.907
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:48:13.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-2594" for this suite. 01/03/24 11:48:13.987
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:48:14.017
Jan  3 11:48:14.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 11:48:14.02
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:48:14.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:48:14.107
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 11:48:14.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-713" for this suite. 01/03/24 11:48:14.332
------------------------------
• [0.340 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:48:14.017
    Jan  3 11:48:14.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 11:48:14.02
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:48:14.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:48:14.107
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:48:14.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-713" for this suite. 01/03/24 11:48:14.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:48:14.358
Jan  3 11:48:14.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 11:48:14.36
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:48:14.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:48:14.46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-76ce3126-69bb-4f42-af78-83c401ecdd8d 01/03/24 11:48:14.488
STEP: Creating a pod to test consume configMaps 01/03/24 11:48:14.511
Jan  3 11:48:14.543: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8" in namespace "configmap-2402" to be "Succeeded or Failed"
Jan  3 11:48:14.562: INFO: Pod "pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.797342ms
Jan  3 11:48:16.585: INFO: Pod "pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.042035242s
Jan  3 11:48:18.583: INFO: Pod "pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8": Phase="Running", Reason="", readiness=false. Elapsed: 4.039962277s
Jan  3 11:48:20.582: INFO: Pod "pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039877838s
STEP: Saw pod success 01/03/24 11:48:20.583
Jan  3 11:48:20.583: INFO: Pod "pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8" satisfied condition "Succeeded or Failed"
Jan  3 11:48:20.609: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8 container agnhost-container: <nil>
STEP: delete the pod 01/03/24 11:48:20.789
Jan  3 11:48:20.832: INFO: Waiting for pod pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8 to disappear
Jan  3 11:48:20.856: INFO: Pod pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 11:48:20.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2402" for this suite. 01/03/24 11:48:20.886
------------------------------
• [SLOW TEST] [6.553 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:48:14.358
    Jan  3 11:48:14.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 11:48:14.36
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:48:14.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:48:14.46
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-76ce3126-69bb-4f42-af78-83c401ecdd8d 01/03/24 11:48:14.488
    STEP: Creating a pod to test consume configMaps 01/03/24 11:48:14.511
    Jan  3 11:48:14.543: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8" in namespace "configmap-2402" to be "Succeeded or Failed"
    Jan  3 11:48:14.562: INFO: Pod "pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.797342ms
    Jan  3 11:48:16.585: INFO: Pod "pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.042035242s
    Jan  3 11:48:18.583: INFO: Pod "pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8": Phase="Running", Reason="", readiness=false. Elapsed: 4.039962277s
    Jan  3 11:48:20.582: INFO: Pod "pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039877838s
    STEP: Saw pod success 01/03/24 11:48:20.583
    Jan  3 11:48:20.583: INFO: Pod "pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8" satisfied condition "Succeeded or Failed"
    Jan  3 11:48:20.609: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8 container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 11:48:20.789
    Jan  3 11:48:20.832: INFO: Waiting for pod pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8 to disappear
    Jan  3 11:48:20.856: INFO: Pod pod-configmaps-e1ae6812-2f2b-4275-9d9d-db6ae9e9d7e8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:48:20.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2402" for this suite. 01/03/24 11:48:20.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:48:20.915
Jan  3 11:48:20.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename deployment 01/03/24 11:48:20.917
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:48:20.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:48:21
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/03/24 11:48:21.06
Jan  3 11:48:21.060: INFO: Creating simple deployment test-deployment-2gbvz
Jan  3 11:48:21.116: INFO: deployment "test-deployment-2gbvz" doesn't have the required revision set
Jan  3 11:48:23.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-2gbvz-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 11:48:25.195: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-2gbvz-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 11:48:27.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-2gbvz-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 11:48:29.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-2gbvz-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 01/03/24 11:48:31.221
Jan  3 11:48:31.241: INFO: Deployment test-deployment-2gbvz has Conditions: [{Available True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2gbvz-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 01/03/24 11:48:31.241
Jan  3 11:48:31.283: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 30, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 30, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-2gbvz-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/03/24 11:48:31.283
Jan  3 11:48:31.298: INFO: Observed &Deployment event: ADDED
Jan  3 11:48:31.298: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2gbvz-54bc444df"}
Jan  3 11:48:31.299: INFO: Observed &Deployment event: MODIFIED
Jan  3 11:48:31.299: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2gbvz-54bc444df"}
Jan  3 11:48:31.299: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  3 11:48:31.299: INFO: Observed &Deployment event: MODIFIED
Jan  3 11:48:31.299: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  3 11:48:31.299: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2gbvz-54bc444df" is progressing.}
Jan  3 11:48:31.300: INFO: Observed &Deployment event: MODIFIED
Jan  3 11:48:31.300: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  3 11:48:31.300: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2gbvz-54bc444df" has successfully progressed.}
Jan  3 11:48:31.300: INFO: Observed &Deployment event: MODIFIED
Jan  3 11:48:31.300: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  3 11:48:31.300: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2gbvz-54bc444df" has successfully progressed.}
Jan  3 11:48:31.300: INFO: Found Deployment test-deployment-2gbvz in namespace deployment-4373 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  3 11:48:31.300: INFO: Deployment test-deployment-2gbvz has an updated status
STEP: patching the Statefulset Status 01/03/24 11:48:31.3
Jan  3 11:48:31.301: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  3 11:48:31.325: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/03/24 11:48:31.325
Jan  3 11:48:31.339: INFO: Observed &Deployment event: ADDED
Jan  3 11:48:31.339: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2gbvz-54bc444df"}
Jan  3 11:48:31.340: INFO: Observed &Deployment event: MODIFIED
Jan  3 11:48:31.340: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2gbvz-54bc444df"}
Jan  3 11:48:31.340: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  3 11:48:31.340: INFO: Observed &Deployment event: MODIFIED
Jan  3 11:48:31.340: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  3 11:48:31.340: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2gbvz-54bc444df" is progressing.}
Jan  3 11:48:31.341: INFO: Observed &Deployment event: MODIFIED
Jan  3 11:48:31.341: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  3 11:48:31.341: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2gbvz-54bc444df" has successfully progressed.}
Jan  3 11:48:31.342: INFO: Observed &Deployment event: MODIFIED
Jan  3 11:48:31.342: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  3 11:48:31.342: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2gbvz-54bc444df" has successfully progressed.}
Jan  3 11:48:31.342: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  3 11:48:31.342: INFO: Observed &Deployment event: MODIFIED
Jan  3 11:48:31.342: INFO: Found deployment test-deployment-2gbvz in namespace deployment-4373 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan  3 11:48:31.342: INFO: Deployment test-deployment-2gbvz has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 11:48:31.360: INFO: Deployment "test-deployment-2gbvz":
&Deployment{ObjectMeta:{test-deployment-2gbvz  deployment-4373  cd673a70-e08d-4679-9bff-41ace43bcd63 33980766725 1 2024-01-03 11:48:21 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2024-01-03 11:48:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2024-01-03 11:48:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2024-01-03 11:48:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00301fa08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-03 11:48:31 +0000 UTC,LastTransitionTime:2024-01-03 11:48:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-deployment-2gbvz-54bc444df" has successfully progressed.,LastUpdateTime:2024-01-03 11:48:31 +0000 UTC,LastTransitionTime:2024-01-03 11:48:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  3 11:48:31.377: INFO: New ReplicaSet "test-deployment-2gbvz-54bc444df" of Deployment "test-deployment-2gbvz":
&ReplicaSet{ObjectMeta:{test-deployment-2gbvz-54bc444df  deployment-4373  5ecfbd65-2089-4c12-9cc5-c8684d200bb1 33980766669 1 2024-01-03 11:48:21 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-2gbvz cd673a70-e08d-4679-9bff-41ace43bcd63 0xc00301fe47 0xc00301fe48}] [] [{kube-controller-manager Update apps/v1 2024-01-03 11:48:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd673a70-e08d-4679-9bff-41ace43bcd63\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 11:48:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00301fef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  3 11:48:31.399: INFO: Pod "test-deployment-2gbvz-54bc444df-blbjm" is available:
&Pod{ObjectMeta:{test-deployment-2gbvz-54bc444df-blbjm test-deployment-2gbvz-54bc444df- deployment-4373  8096bfb8-b386-4978-813f-6523f068377c 33980766668 0 2024-01-03 11:48:21 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:590107da593b98e666eb3adf9056376e7061780eef806e020ed696e5392699f6 cni.projectcalico.org/podIP:10.221.146.75/32 cni.projectcalico.org/podIPs:10.221.146.75/32] [{apps/v1 ReplicaSet test-deployment-2gbvz-54bc444df 5ecfbd65-2089-4c12-9cc5-c8684d200bb1 0xc002bc42d7 0xc002bc42d8}] [] [{kube-controller-manager Update v1 2024-01-03 11:48:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ecfbd65-2089-4c12-9cc5-c8684d200bb1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 11:48:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 11:48:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwp5k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwp5k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:48:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:48:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:48:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:48:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.75,StartTime:2024-01-03 11:48:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 11:48:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b3387bef426f8d65880d24885b203580620bc887c332e9f9c78f8d8601a6a6ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  3 11:48:31.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4373" for this suite. 01/03/24 11:48:31.431
------------------------------
• [SLOW TEST] [10.540 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:48:20.915
    Jan  3 11:48:20.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename deployment 01/03/24 11:48:20.917
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:48:20.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:48:21
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/03/24 11:48:21.06
    Jan  3 11:48:21.060: INFO: Creating simple deployment test-deployment-2gbvz
    Jan  3 11:48:21.116: INFO: deployment "test-deployment-2gbvz" doesn't have the required revision set
    Jan  3 11:48:23.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-2gbvz-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 11:48:25.195: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-2gbvz-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 11:48:27.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-2gbvz-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 11:48:29.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-2gbvz-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 01/03/24 11:48:31.221
    Jan  3 11:48:31.241: INFO: Deployment test-deployment-2gbvz has Conditions: [{Available True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2gbvz-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 01/03/24 11:48:31.241
    Jan  3 11:48:31.283: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 30, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 11, 48, 30, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 11, 48, 21, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-2gbvz-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/03/24 11:48:31.283
    Jan  3 11:48:31.298: INFO: Observed &Deployment event: ADDED
    Jan  3 11:48:31.298: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2gbvz-54bc444df"}
    Jan  3 11:48:31.299: INFO: Observed &Deployment event: MODIFIED
    Jan  3 11:48:31.299: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2gbvz-54bc444df"}
    Jan  3 11:48:31.299: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  3 11:48:31.299: INFO: Observed &Deployment event: MODIFIED
    Jan  3 11:48:31.299: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  3 11:48:31.299: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2gbvz-54bc444df" is progressing.}
    Jan  3 11:48:31.300: INFO: Observed &Deployment event: MODIFIED
    Jan  3 11:48:31.300: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  3 11:48:31.300: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2gbvz-54bc444df" has successfully progressed.}
    Jan  3 11:48:31.300: INFO: Observed &Deployment event: MODIFIED
    Jan  3 11:48:31.300: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  3 11:48:31.300: INFO: Observed Deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2gbvz-54bc444df" has successfully progressed.}
    Jan  3 11:48:31.300: INFO: Found Deployment test-deployment-2gbvz in namespace deployment-4373 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  3 11:48:31.300: INFO: Deployment test-deployment-2gbvz has an updated status
    STEP: patching the Statefulset Status 01/03/24 11:48:31.3
    Jan  3 11:48:31.301: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  3 11:48:31.325: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/03/24 11:48:31.325
    Jan  3 11:48:31.339: INFO: Observed &Deployment event: ADDED
    Jan  3 11:48:31.339: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2gbvz-54bc444df"}
    Jan  3 11:48:31.340: INFO: Observed &Deployment event: MODIFIED
    Jan  3 11:48:31.340: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2gbvz-54bc444df"}
    Jan  3 11:48:31.340: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  3 11:48:31.340: INFO: Observed &Deployment event: MODIFIED
    Jan  3 11:48:31.340: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  3 11:48:31.340: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:21 +0000 UTC 2024-01-03 11:48:21 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2gbvz-54bc444df" is progressing.}
    Jan  3 11:48:31.341: INFO: Observed &Deployment event: MODIFIED
    Jan  3 11:48:31.341: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  3 11:48:31.341: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2gbvz-54bc444df" has successfully progressed.}
    Jan  3 11:48:31.342: INFO: Observed &Deployment event: MODIFIED
    Jan  3 11:48:31.342: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  3 11:48:31.342: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-03 11:48:30 +0000 UTC 2024-01-03 11:48:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2gbvz-54bc444df" has successfully progressed.}
    Jan  3 11:48:31.342: INFO: Observed deployment test-deployment-2gbvz in namespace deployment-4373 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  3 11:48:31.342: INFO: Observed &Deployment event: MODIFIED
    Jan  3 11:48:31.342: INFO: Found deployment test-deployment-2gbvz in namespace deployment-4373 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan  3 11:48:31.342: INFO: Deployment test-deployment-2gbvz has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 11:48:31.360: INFO: Deployment "test-deployment-2gbvz":
    &Deployment{ObjectMeta:{test-deployment-2gbvz  deployment-4373  cd673a70-e08d-4679-9bff-41ace43bcd63 33980766725 1 2024-01-03 11:48:21 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2024-01-03 11:48:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2024-01-03 11:48:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2024-01-03 11:48:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00301fa08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-03 11:48:31 +0000 UTC,LastTransitionTime:2024-01-03 11:48:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-deployment-2gbvz-54bc444df" has successfully progressed.,LastUpdateTime:2024-01-03 11:48:31 +0000 UTC,LastTransitionTime:2024-01-03 11:48:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  3 11:48:31.377: INFO: New ReplicaSet "test-deployment-2gbvz-54bc444df" of Deployment "test-deployment-2gbvz":
    &ReplicaSet{ObjectMeta:{test-deployment-2gbvz-54bc444df  deployment-4373  5ecfbd65-2089-4c12-9cc5-c8684d200bb1 33980766669 1 2024-01-03 11:48:21 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-2gbvz cd673a70-e08d-4679-9bff-41ace43bcd63 0xc00301fe47 0xc00301fe48}] [] [{kube-controller-manager Update apps/v1 2024-01-03 11:48:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd673a70-e08d-4679-9bff-41ace43bcd63\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 11:48:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00301fef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 11:48:31.399: INFO: Pod "test-deployment-2gbvz-54bc444df-blbjm" is available:
    &Pod{ObjectMeta:{test-deployment-2gbvz-54bc444df-blbjm test-deployment-2gbvz-54bc444df- deployment-4373  8096bfb8-b386-4978-813f-6523f068377c 33980766668 0 2024-01-03 11:48:21 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:590107da593b98e666eb3adf9056376e7061780eef806e020ed696e5392699f6 cni.projectcalico.org/podIP:10.221.146.75/32 cni.projectcalico.org/podIPs:10.221.146.75/32] [{apps/v1 ReplicaSet test-deployment-2gbvz-54bc444df 5ecfbd65-2089-4c12-9cc5-c8684d200bb1 0xc002bc42d7 0xc002bc42d8}] [] [{kube-controller-manager Update v1 2024-01-03 11:48:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ecfbd65-2089-4c12-9cc5-c8684d200bb1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 11:48:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 11:48:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwp5k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwp5k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:48:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:48:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:48:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:48:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.75,StartTime:2024-01-03 11:48:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 11:48:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b3387bef426f8d65880d24885b203580620bc887c332e9f9c78f8d8601a6a6ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:48:31.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4373" for this suite. 01/03/24 11:48:31.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:48:31.458
Jan  3 11:48:31.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 11:48:31.461
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:48:31.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:48:31.543
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 01/03/24 11:48:31.572
Jan  3 11:48:31.599: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768" in namespace "downward-api-7467" to be "Succeeded or Failed"
Jan  3 11:48:31.618: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Pending", Reason="", readiness=false. Elapsed: 19.602864ms
Jan  3 11:48:33.639: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Running", Reason="", readiness=true. Elapsed: 2.039896362s
Jan  3 11:48:35.643: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Running", Reason="", readiness=true. Elapsed: 4.044604874s
Jan  3 11:48:37.642: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Running", Reason="", readiness=true. Elapsed: 6.042851299s
Jan  3 11:48:39.644: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Running", Reason="", readiness=true. Elapsed: 8.045456456s
Jan  3 11:48:41.640: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Running", Reason="", readiness=true. Elapsed: 10.041175572s
Jan  3 11:48:43.639: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Running", Reason="", readiness=true. Elapsed: 12.040495685s
Jan  3 11:48:45.640: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Running", Reason="", readiness=false. Elapsed: 14.041582493s
Jan  3 11:48:47.644: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.045205246s
STEP: Saw pod success 01/03/24 11:48:47.644
Jan  3 11:48:47.644: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768" satisfied condition "Succeeded or Failed"
Jan  3 11:48:47.663: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768 container client-container: <nil>
STEP: delete the pod 01/03/24 11:48:47.702
Jan  3 11:48:47.741: INFO: Waiting for pod downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768 to disappear
Jan  3 11:48:47.759: INFO: Pod downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  3 11:48:47.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7467" for this suite. 01/03/24 11:48:47.791
------------------------------
• [SLOW TEST] [16.358 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:48:31.458
    Jan  3 11:48:31.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 11:48:31.461
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:48:31.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:48:31.543
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 01/03/24 11:48:31.572
    Jan  3 11:48:31.599: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768" in namespace "downward-api-7467" to be "Succeeded or Failed"
    Jan  3 11:48:31.618: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Pending", Reason="", readiness=false. Elapsed: 19.602864ms
    Jan  3 11:48:33.639: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Running", Reason="", readiness=true. Elapsed: 2.039896362s
    Jan  3 11:48:35.643: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Running", Reason="", readiness=true. Elapsed: 4.044604874s
    Jan  3 11:48:37.642: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Running", Reason="", readiness=true. Elapsed: 6.042851299s
    Jan  3 11:48:39.644: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Running", Reason="", readiness=true. Elapsed: 8.045456456s
    Jan  3 11:48:41.640: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Running", Reason="", readiness=true. Elapsed: 10.041175572s
    Jan  3 11:48:43.639: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Running", Reason="", readiness=true. Elapsed: 12.040495685s
    Jan  3 11:48:45.640: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Running", Reason="", readiness=false. Elapsed: 14.041582493s
    Jan  3 11:48:47.644: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.045205246s
    STEP: Saw pod success 01/03/24 11:48:47.644
    Jan  3 11:48:47.644: INFO: Pod "downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768" satisfied condition "Succeeded or Failed"
    Jan  3 11:48:47.663: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768 container client-container: <nil>
    STEP: delete the pod 01/03/24 11:48:47.702
    Jan  3 11:48:47.741: INFO: Waiting for pod downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768 to disappear
    Jan  3 11:48:47.759: INFO: Pod downwardapi-volume-0e7a7be6-0c45-4028-ae47-2c6fb7437768 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:48:47.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7467" for this suite. 01/03/24 11:48:47.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:48:47.823
Jan  3 11:48:47.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename resourcequota 01/03/24 11:48:47.825
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:48:47.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:48:47.914
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 01/03/24 11:48:47.945
STEP: Creating a ResourceQuota 01/03/24 11:48:52.965
STEP: Ensuring resource quota status is calculated 01/03/24 11:48:52.988
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  3 11:48:55.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5750" for this suite. 01/03/24 11:48:55.042
------------------------------
• [SLOW TEST] [7.246 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:48:47.823
    Jan  3 11:48:47.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename resourcequota 01/03/24 11:48:47.825
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:48:47.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:48:47.914
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 01/03/24 11:48:47.945
    STEP: Creating a ResourceQuota 01/03/24 11:48:52.965
    STEP: Ensuring resource quota status is calculated 01/03/24 11:48:52.988
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:48:55.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5750" for this suite. 01/03/24 11:48:55.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:48:55.072
Jan  3 11:48:55.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-probe 01/03/24 11:48:55.075
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:48:55.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:48:55.161
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  3 11:49:55.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9121" for this suite. 01/03/24 11:49:55.273
------------------------------
• [SLOW TEST] [60.229 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:48:55.072
    Jan  3 11:48:55.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-probe 01/03/24 11:48:55.075
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:48:55.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:48:55.161
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:49:55.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9121" for this suite. 01/03/24 11:49:55.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:49:55.304
Jan  3 11:49:55.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pod-network-test 01/03/24 11:49:55.305
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:49:55.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:49:55.39
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-9956 01/03/24 11:49:55.418
STEP: creating a selector 01/03/24 11:49:55.419
STEP: Creating the service pods in kubernetes 01/03/24 11:49:55.419
Jan  3 11:49:55.419: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  3 11:49:55.535: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9956" to be "running and ready"
Jan  3 11:49:55.558: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.17956ms
Jan  3 11:49:55.558: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 11:49:57.579: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.0440074s
Jan  3 11:49:57.579: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:49:59.580: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.044824093s
Jan  3 11:49:59.580: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:50:01.582: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.047556371s
Jan  3 11:50:01.583: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:50:03.580: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.045041178s
Jan  3 11:50:03.580: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:50:05.578: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.043052952s
Jan  3 11:50:05.578: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:50:07.580: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.044787885s
Jan  3 11:50:07.580: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:50:09.582: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.047575166s
Jan  3 11:50:09.583: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:50:11.579: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.043803044s
Jan  3 11:50:11.579: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:50:13.580: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.044872152s
Jan  3 11:50:13.580: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:50:15.604: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.068622166s
Jan  3 11:50:15.604: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:50:17.577: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.042577232s
Jan  3 11:50:17.578: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  3 11:50:17.578: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  3 11:50:17.602: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9956" to be "running and ready"
Jan  3 11:50:17.621: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 18.838689ms
Jan  3 11:50:17.621: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  3 11:50:17.621: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  3 11:50:17.641: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9956" to be "running and ready"
Jan  3 11:50:17.681: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 40.501003ms
Jan  3 11:50:17.681: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  3 11:50:17.681: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/03/24 11:50:17.705
Jan  3 11:50:17.725: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9956" to be "running"
Jan  3 11:50:17.745: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.447455ms
Jan  3 11:50:19.765: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.040281035s
Jan  3 11:50:19.765: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  3 11:50:19.788: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan  3 11:50:19.788: INFO: Breadth first check of 10.221.146.78 on host 185.132.46.116...
Jan  3 11:50:19.809: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.221.146.79:9080/dial?request=hostname&protocol=http&host=10.221.146.78&port=8083&tries=1'] Namespace:pod-network-test-9956 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 11:50:19.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 11:50:19.811: INFO: ExecWithOptions: Clientset creation
Jan  3 11:50:19.811: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9956/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.221.146.79%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.221.146.78%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  3 11:50:20.157: INFO: Waiting for responses: map[]
Jan  3 11:50:20.157: INFO: reached 10.221.146.78 after 0/1 tries
Jan  3 11:50:20.157: INFO: Breadth first check of 10.222.238.202 on host 85.215.162.129...
Jan  3 11:50:20.175: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.221.146.79:9080/dial?request=hostname&protocol=http&host=10.222.238.202&port=8083&tries=1'] Namespace:pod-network-test-9956 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 11:50:20.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 11:50:20.176: INFO: ExecWithOptions: Clientset creation
Jan  3 11:50:20.176: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9956/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.221.146.79%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.222.238.202%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  3 11:50:20.494: INFO: Waiting for responses: map[]
Jan  3 11:50:20.495: INFO: reached 10.222.238.202 after 0/1 tries
Jan  3 11:50:20.495: INFO: Breadth first check of 10.221.146.135 on host 85.215.218.90...
Jan  3 11:50:20.520: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.221.146.79:9080/dial?request=hostname&protocol=http&host=10.221.146.135&port=8083&tries=1'] Namespace:pod-network-test-9956 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 11:50:20.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 11:50:20.522: INFO: ExecWithOptions: Clientset creation
Jan  3 11:50:20.522: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9956/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.221.146.79%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.221.146.135%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  3 11:50:21.018: INFO: Waiting for responses: map[]
Jan  3 11:50:21.018: INFO: reached 10.221.146.135 after 0/1 tries
Jan  3 11:50:21.018: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan  3 11:50:21.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9956" for this suite. 01/03/24 11:50:21.053
------------------------------
• [SLOW TEST] [25.775 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:49:55.304
    Jan  3 11:49:55.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pod-network-test 01/03/24 11:49:55.305
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:49:55.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:49:55.39
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-9956 01/03/24 11:49:55.418
    STEP: creating a selector 01/03/24 11:49:55.419
    STEP: Creating the service pods in kubernetes 01/03/24 11:49:55.419
    Jan  3 11:49:55.419: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  3 11:49:55.535: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9956" to be "running and ready"
    Jan  3 11:49:55.558: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.17956ms
    Jan  3 11:49:55.558: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 11:49:57.579: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.0440074s
    Jan  3 11:49:57.579: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:49:59.580: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.044824093s
    Jan  3 11:49:59.580: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:50:01.582: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.047556371s
    Jan  3 11:50:01.583: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:50:03.580: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.045041178s
    Jan  3 11:50:03.580: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:50:05.578: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.043052952s
    Jan  3 11:50:05.578: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:50:07.580: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.044787885s
    Jan  3 11:50:07.580: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:50:09.582: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.047575166s
    Jan  3 11:50:09.583: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:50:11.579: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.043803044s
    Jan  3 11:50:11.579: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:50:13.580: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.044872152s
    Jan  3 11:50:13.580: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:50:15.604: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.068622166s
    Jan  3 11:50:15.604: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:50:17.577: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.042577232s
    Jan  3 11:50:17.578: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  3 11:50:17.578: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  3 11:50:17.602: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9956" to be "running and ready"
    Jan  3 11:50:17.621: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 18.838689ms
    Jan  3 11:50:17.621: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  3 11:50:17.621: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  3 11:50:17.641: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9956" to be "running and ready"
    Jan  3 11:50:17.681: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 40.501003ms
    Jan  3 11:50:17.681: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  3 11:50:17.681: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/03/24 11:50:17.705
    Jan  3 11:50:17.725: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9956" to be "running"
    Jan  3 11:50:17.745: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.447455ms
    Jan  3 11:50:19.765: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.040281035s
    Jan  3 11:50:19.765: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  3 11:50:19.788: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan  3 11:50:19.788: INFO: Breadth first check of 10.221.146.78 on host 185.132.46.116...
    Jan  3 11:50:19.809: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.221.146.79:9080/dial?request=hostname&protocol=http&host=10.221.146.78&port=8083&tries=1'] Namespace:pod-network-test-9956 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 11:50:19.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 11:50:19.811: INFO: ExecWithOptions: Clientset creation
    Jan  3 11:50:19.811: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9956/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.221.146.79%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.221.146.78%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  3 11:50:20.157: INFO: Waiting for responses: map[]
    Jan  3 11:50:20.157: INFO: reached 10.221.146.78 after 0/1 tries
    Jan  3 11:50:20.157: INFO: Breadth first check of 10.222.238.202 on host 85.215.162.129...
    Jan  3 11:50:20.175: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.221.146.79:9080/dial?request=hostname&protocol=http&host=10.222.238.202&port=8083&tries=1'] Namespace:pod-network-test-9956 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 11:50:20.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 11:50:20.176: INFO: ExecWithOptions: Clientset creation
    Jan  3 11:50:20.176: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9956/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.221.146.79%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.222.238.202%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  3 11:50:20.494: INFO: Waiting for responses: map[]
    Jan  3 11:50:20.495: INFO: reached 10.222.238.202 after 0/1 tries
    Jan  3 11:50:20.495: INFO: Breadth first check of 10.221.146.135 on host 85.215.218.90...
    Jan  3 11:50:20.520: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.221.146.79:9080/dial?request=hostname&protocol=http&host=10.221.146.135&port=8083&tries=1'] Namespace:pod-network-test-9956 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 11:50:20.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 11:50:20.522: INFO: ExecWithOptions: Clientset creation
    Jan  3 11:50:20.522: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9956/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.221.146.79%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.221.146.135%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  3 11:50:21.018: INFO: Waiting for responses: map[]
    Jan  3 11:50:21.018: INFO: reached 10.221.146.135 after 0/1 tries
    Jan  3 11:50:21.018: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:50:21.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9956" for this suite. 01/03/24 11:50:21.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:50:21.083
Jan  3 11:50:21.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename deployment 01/03/24 11:50:21.085
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:50:21.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:50:21.173
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/03/24 11:50:21.221
STEP: waiting for Deployment to be created 01/03/24 11:50:21.242
STEP: waiting for all Replicas to be Ready 01/03/24 11:50:21.257
Jan  3 11:50:21.271: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 11:50:21.271: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 11:50:21.271: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 11:50:21.271: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 11:50:21.281: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 11:50:21.281: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 11:50:21.317: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 11:50:21.318: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 11:50:23.252: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan  3 11:50:23.252: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan  3 11:50:23.411: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/03/24 11:50:23.411
W0103 11:50:23.445665      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  3 11:50:23.460: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/03/24 11:50:23.46
Jan  3 11:50:23.478: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
Jan  3 11:50:23.478: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
Jan  3 11:50:23.479: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
Jan  3 11:50:23.479: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
Jan  3 11:50:23.479: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
Jan  3 11:50:23.479: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:23.505: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:23.505: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:23.517: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
Jan  3 11:50:23.517: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
Jan  3 11:50:23.533: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
Jan  3 11:50:23.533: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
Jan  3 11:50:25.282: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:25.282: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:25.328: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
STEP: listing Deployments 01/03/24 11:50:25.328
Jan  3 11:50:25.347: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/03/24 11:50:25.347
Jan  3 11:50:25.387: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/03/24 11:50:25.387
Jan  3 11:50:25.421: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 11:50:25.421: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 11:50:25.421: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 11:50:25.431: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 11:50:25.448: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 11:50:25.463: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 11:50:27.281: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 11:50:27.299: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 11:50:27.309: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 11:50:27.323: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 11:50:27.334: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 11:50:48.539: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/03/24 11:50:48.587
STEP: fetching the DeploymentStatus 01/03/24 11:50:48.624
Jan  3 11:50:48.659: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
Jan  3 11:50:48.659: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
Jan  3 11:50:48.659: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
Jan  3 11:50:48.660: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
Jan  3 11:50:48.660: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
Jan  3 11:50:48.660: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
Jan  3 11:50:48.660: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:48.661: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:48.661: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:48.661: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:48.661: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
Jan  3 11:50:48.661: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 3
STEP: deleting the Deployment 01/03/24 11:50:48.661
Jan  3 11:50:48.710: INFO: observed event type MODIFIED
Jan  3 11:50:48.710: INFO: observed event type MODIFIED
Jan  3 11:50:48.711: INFO: observed event type MODIFIED
Jan  3 11:50:48.711: INFO: observed event type MODIFIED
Jan  3 11:50:48.711: INFO: observed event type MODIFIED
Jan  3 11:50:48.711: INFO: observed event type MODIFIED
Jan  3 11:50:48.712: INFO: observed event type MODIFIED
Jan  3 11:50:48.712: INFO: observed event type MODIFIED
Jan  3 11:50:48.712: INFO: observed event type MODIFIED
Jan  3 11:50:48.712: INFO: observed event type MODIFIED
Jan  3 11:50:48.712: INFO: observed event type MODIFIED
Jan  3 11:50:48.726: INFO: observed event type MODIFIED
Jan  3 11:50:48.726: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 11:50:48.744: INFO: Log out all the ReplicaSets if there is no deployment created
Jan  3 11:50:48.770: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-9606  f63ea4a2-20af-4944-82cc-3a3b7852d03a 33980783139 2 2024-01-03 11:50:25 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 45ea5f79-7aec-4fcc-bc65-477bda9b08f4 0xc0037462d7 0xc0037462d8}] [] [{kube-controller-manager Update apps/v1 2024-01-03 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45ea5f79-7aec-4fcc-bc65-477bda9b08f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 11:50:48 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003746360 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan  3 11:50:48.795: INFO: pod: "test-deployment-7b7876f9d6-6mctr":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-6mctr test-deployment-7b7876f9d6- deployment-9606  b270114d-6cce-48c3-9273-90478df2583d 33980783187 0 2024-01-03 11:50:25 +0000 UTC 2024-01-03 11:50:49 +0000 UTC 0xc004271278 map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:112e58ae5c2cf555936662e87fc393e82fe8c6c86ff7d44bf433f950a789285e cni.projectcalico.org/podIP:10.221.146.82/32 cni.projectcalico.org/podIPs:10.221.146.82/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f63ea4a2-20af-4944-82cc-3a3b7852d03a 0xc0042712f7 0xc0042712f8}] [] [{kube-controller-manager Update v1 2024-01-03 11:50:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f63ea4a2-20af-4944-82cc-3a3b7852d03a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 11:50:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 11:50:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cv47r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cv47r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.82,StartTime:2024-01-03 11:50:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 11:50:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://dd9e4be7ce4762e5bb0e02c06cd8312333ed3782e37dc2fe1e41556cdd57320b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan  3 11:50:48.796: INFO: pod: "test-deployment-7b7876f9d6-xdcth":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-xdcth test-deployment-7b7876f9d6- deployment-9606  eace5482-a765-43f7-b33a-8a22c415c06d 33980783186 0 2024-01-03 11:50:27 +0000 UTC 2024-01-03 11:50:49 +0000 UTC 0xc0042714e0 map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:fdb26e7e96c52e7c2a82220b823090c490dc41bf6d52c85cb50866148fe933c6 cni.projectcalico.org/podIP:10.222.238.205/32 cni.projectcalico.org/podIPs:10.222.238.205/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f63ea4a2-20af-4944-82cc-3a3b7852d03a 0xc004271517 0xc004271518}] [] [{kube-controller-manager Update v1 2024-01-03 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f63ea4a2-20af-4944-82cc-3a3b7852d03a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 11:50:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 11:50:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.222.238.205\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85qwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85qwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:10.222.238.205,StartTime:2024-01-03 11:50:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 11:50:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f35fa03e7561fa54f6f8bc7cd016878c0a65bcea0f53beb83eb61074c1032213,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.222.238.205,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan  3 11:50:48.796: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-9606  ddad6fab-f5f5-4a82-8984-d953831c26ac 33980783153 4 2024-01-03 11:50:23 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 45ea5f79-7aec-4fcc-bc65-477bda9b08f4 0xc0037463d7 0xc0037463d8}] [] [{kube-controller-manager Update apps/v1 2024-01-03 11:50:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45ea5f79-7aec-4fcc-bc65-477bda9b08f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 11:50:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003746460 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan  3 11:50:48.816: INFO: pod: "test-deployment-7df74c55ff-q7vzt":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-q7vzt test-deployment-7df74c55ff- deployment-9606  f3b147ba-0f36-48d6-a87a-a19d279de2ea 33980783146 0 2024-01-03 11:50:23 +0000 UTC 2024-01-03 11:50:49 +0000 UTC 0xc00033ecc8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:93c566a6a00308938bc2ffeade880c627157740088943ebae274553ca3e1a546 cni.projectcalico.org/podIP:10.221.146.81/32 cni.projectcalico.org/podIPs:10.221.146.81/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff ddad6fab-f5f5-4a82-8984-d953831c26ac 0xc00033ed67 0xc00033ed68}] [] [{kube-controller-manager Update v1 2024-01-03 11:50:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddad6fab-f5f5-4a82-8984-d953831c26ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 11:50:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 11:50:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87ltj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87ltj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.81,StartTime:2024-01-03 11:50:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 11:50:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.infra.cluster.ionos.com/registry.k8s.io/pause:3.9,ImageID:registry.infra.cluster.ionos.com/registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://0ee450dc7a07efbe2ae4f32820a980516c2856984535425737185e0a5caa018a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan  3 11:50:48.817: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-9606  95f5b6c3-e1bf-4a79-8a37-47e97db00cdd 33980780335 3 2024-01-03 11:50:21 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 45ea5f79-7aec-4fcc-bc65-477bda9b08f4 0xc0037464d7 0xc0037464d8}] [] [{kube-controller-manager Update apps/v1 2024-01-03 11:50:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45ea5f79-7aec-4fcc-bc65-477bda9b08f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 11:50:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003746560 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  3 11:50:48.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9606" for this suite. 01/03/24 11:50:48.859
------------------------------
• [SLOW TEST] [27.804 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:50:21.083
    Jan  3 11:50:21.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename deployment 01/03/24 11:50:21.085
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:50:21.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:50:21.173
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/03/24 11:50:21.221
    STEP: waiting for Deployment to be created 01/03/24 11:50:21.242
    STEP: waiting for all Replicas to be Ready 01/03/24 11:50:21.257
    Jan  3 11:50:21.271: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 11:50:21.271: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 11:50:21.271: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 11:50:21.271: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 11:50:21.281: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 11:50:21.281: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 11:50:21.317: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 11:50:21.318: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 11:50:23.252: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan  3 11:50:23.252: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan  3 11:50:23.411: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/03/24 11:50:23.411
    W0103 11:50:23.445665      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  3 11:50:23.460: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/03/24 11:50:23.46
    Jan  3 11:50:23.478: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
    Jan  3 11:50:23.478: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
    Jan  3 11:50:23.479: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
    Jan  3 11:50:23.479: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
    Jan  3 11:50:23.479: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
    Jan  3 11:50:23.479: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
    Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
    Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 0
    Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
    Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
    Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:23.480: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:23.505: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:23.505: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:23.517: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
    Jan  3 11:50:23.517: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
    Jan  3 11:50:23.533: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
    Jan  3 11:50:23.533: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
    Jan  3 11:50:25.282: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:25.282: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:25.328: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
    STEP: listing Deployments 01/03/24 11:50:25.328
    Jan  3 11:50:25.347: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/03/24 11:50:25.347
    Jan  3 11:50:25.387: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/03/24 11:50:25.387
    Jan  3 11:50:25.421: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 11:50:25.421: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 11:50:25.421: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 11:50:25.431: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 11:50:25.448: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 11:50:25.463: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 11:50:27.281: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 11:50:27.299: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 11:50:27.309: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 11:50:27.323: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 11:50:27.334: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 11:50:48.539: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/03/24 11:50:48.587
    STEP: fetching the DeploymentStatus 01/03/24 11:50:48.624
    Jan  3 11:50:48.659: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
    Jan  3 11:50:48.659: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
    Jan  3 11:50:48.659: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
    Jan  3 11:50:48.660: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
    Jan  3 11:50:48.660: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
    Jan  3 11:50:48.660: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 1
    Jan  3 11:50:48.660: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:48.661: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:48.661: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:48.661: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:48.661: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 2
    Jan  3 11:50:48.661: INFO: observed Deployment test-deployment in namespace deployment-9606 with ReadyReplicas 3
    STEP: deleting the Deployment 01/03/24 11:50:48.661
    Jan  3 11:50:48.710: INFO: observed event type MODIFIED
    Jan  3 11:50:48.710: INFO: observed event type MODIFIED
    Jan  3 11:50:48.711: INFO: observed event type MODIFIED
    Jan  3 11:50:48.711: INFO: observed event type MODIFIED
    Jan  3 11:50:48.711: INFO: observed event type MODIFIED
    Jan  3 11:50:48.711: INFO: observed event type MODIFIED
    Jan  3 11:50:48.712: INFO: observed event type MODIFIED
    Jan  3 11:50:48.712: INFO: observed event type MODIFIED
    Jan  3 11:50:48.712: INFO: observed event type MODIFIED
    Jan  3 11:50:48.712: INFO: observed event type MODIFIED
    Jan  3 11:50:48.712: INFO: observed event type MODIFIED
    Jan  3 11:50:48.726: INFO: observed event type MODIFIED
    Jan  3 11:50:48.726: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 11:50:48.744: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan  3 11:50:48.770: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-9606  f63ea4a2-20af-4944-82cc-3a3b7852d03a 33980783139 2 2024-01-03 11:50:25 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 45ea5f79-7aec-4fcc-bc65-477bda9b08f4 0xc0037462d7 0xc0037462d8}] [] [{kube-controller-manager Update apps/v1 2024-01-03 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45ea5f79-7aec-4fcc-bc65-477bda9b08f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 11:50:48 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003746360 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan  3 11:50:48.795: INFO: pod: "test-deployment-7b7876f9d6-6mctr":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-6mctr test-deployment-7b7876f9d6- deployment-9606  b270114d-6cce-48c3-9273-90478df2583d 33980783187 0 2024-01-03 11:50:25 +0000 UTC 2024-01-03 11:50:49 +0000 UTC 0xc004271278 map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:112e58ae5c2cf555936662e87fc393e82fe8c6c86ff7d44bf433f950a789285e cni.projectcalico.org/podIP:10.221.146.82/32 cni.projectcalico.org/podIPs:10.221.146.82/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f63ea4a2-20af-4944-82cc-3a3b7852d03a 0xc0042712f7 0xc0042712f8}] [] [{kube-controller-manager Update v1 2024-01-03 11:50:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f63ea4a2-20af-4944-82cc-3a3b7852d03a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 11:50:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 11:50:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cv47r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cv47r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.82,StartTime:2024-01-03 11:50:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 11:50:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://dd9e4be7ce4762e5bb0e02c06cd8312333ed3782e37dc2fe1e41556cdd57320b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan  3 11:50:48.796: INFO: pod: "test-deployment-7b7876f9d6-xdcth":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-xdcth test-deployment-7b7876f9d6- deployment-9606  eace5482-a765-43f7-b33a-8a22c415c06d 33980783186 0 2024-01-03 11:50:27 +0000 UTC 2024-01-03 11:50:49 +0000 UTC 0xc0042714e0 map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:fdb26e7e96c52e7c2a82220b823090c490dc41bf6d52c85cb50866148fe933c6 cni.projectcalico.org/podIP:10.222.238.205/32 cni.projectcalico.org/podIPs:10.222.238.205/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f63ea4a2-20af-4944-82cc-3a3b7852d03a 0xc004271517 0xc004271518}] [] [{kube-controller-manager Update v1 2024-01-03 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f63ea4a2-20af-4944-82cc-3a3b7852d03a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 11:50:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 11:50:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.222.238.205\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85qwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85qwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:10.222.238.205,StartTime:2024-01-03 11:50:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 11:50:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f35fa03e7561fa54f6f8bc7cd016878c0a65bcea0f53beb83eb61074c1032213,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.222.238.205,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan  3 11:50:48.796: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-9606  ddad6fab-f5f5-4a82-8984-d953831c26ac 33980783153 4 2024-01-03 11:50:23 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 45ea5f79-7aec-4fcc-bc65-477bda9b08f4 0xc0037463d7 0xc0037463d8}] [] [{kube-controller-manager Update apps/v1 2024-01-03 11:50:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45ea5f79-7aec-4fcc-bc65-477bda9b08f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 11:50:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003746460 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan  3 11:50:48.816: INFO: pod: "test-deployment-7df74c55ff-q7vzt":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-q7vzt test-deployment-7df74c55ff- deployment-9606  f3b147ba-0f36-48d6-a87a-a19d279de2ea 33980783146 0 2024-01-03 11:50:23 +0000 UTC 2024-01-03 11:50:49 +0000 UTC 0xc00033ecc8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:93c566a6a00308938bc2ffeade880c627157740088943ebae274553ca3e1a546 cni.projectcalico.org/podIP:10.221.146.81/32 cni.projectcalico.org/podIPs:10.221.146.81/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff ddad6fab-f5f5-4a82-8984-d953831c26ac 0xc00033ed67 0xc00033ed68}] [] [{kube-controller-manager Update v1 2024-01-03 11:50:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddad6fab-f5f5-4a82-8984-d953831c26ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 11:50:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 11:50:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87ltj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87ltj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 11:50:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.81,StartTime:2024-01-03 11:50:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 11:50:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.infra.cluster.ionos.com/registry.k8s.io/pause:3.9,ImageID:registry.infra.cluster.ionos.com/registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://0ee450dc7a07efbe2ae4f32820a980516c2856984535425737185e0a5caa018a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan  3 11:50:48.817: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-9606  95f5b6c3-e1bf-4a79-8a37-47e97db00cdd 33980780335 3 2024-01-03 11:50:21 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 45ea5f79-7aec-4fcc-bc65-477bda9b08f4 0xc0037464d7 0xc0037464d8}] [] [{kube-controller-manager Update apps/v1 2024-01-03 11:50:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45ea5f79-7aec-4fcc-bc65-477bda9b08f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 11:50:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003746560 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:50:48.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9606" for this suite. 01/03/24 11:50:48.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:50:48.889
Jan  3 11:50:48.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 11:50:48.892
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:50:48.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:50:48.978
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 01/03/24 11:50:49.006
Jan  3 11:50:49.037: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31" in namespace "downward-api-1515" to be "Succeeded or Failed"
Jan  3 11:50:49.055: INFO: Pod "downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31": Phase="Pending", Reason="", readiness=false. Elapsed: 17.961693ms
Jan  3 11:50:51.075: INFO: Pod "downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038352076s
Jan  3 11:50:53.075: INFO: Pod "downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038207028s
Jan  3 11:50:55.079: INFO: Pod "downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042436514s
STEP: Saw pod success 01/03/24 11:50:55.079
Jan  3 11:50:55.080: INFO: Pod "downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31" satisfied condition "Succeeded or Failed"
Jan  3 11:50:55.100: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31 container client-container: <nil>
STEP: delete the pod 01/03/24 11:50:55.251
Jan  3 11:50:55.294: INFO: Waiting for pod downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31 to disappear
Jan  3 11:50:55.313: INFO: Pod downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  3 11:50:55.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1515" for this suite. 01/03/24 11:50:55.347
------------------------------
• [SLOW TEST] [6.482 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:50:48.889
    Jan  3 11:50:48.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 11:50:48.892
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:50:48.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:50:48.978
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 01/03/24 11:50:49.006
    Jan  3 11:50:49.037: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31" in namespace "downward-api-1515" to be "Succeeded or Failed"
    Jan  3 11:50:49.055: INFO: Pod "downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31": Phase="Pending", Reason="", readiness=false. Elapsed: 17.961693ms
    Jan  3 11:50:51.075: INFO: Pod "downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038352076s
    Jan  3 11:50:53.075: INFO: Pod "downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038207028s
    Jan  3 11:50:55.079: INFO: Pod "downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042436514s
    STEP: Saw pod success 01/03/24 11:50:55.079
    Jan  3 11:50:55.080: INFO: Pod "downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31" satisfied condition "Succeeded or Failed"
    Jan  3 11:50:55.100: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31 container client-container: <nil>
    STEP: delete the pod 01/03/24 11:50:55.251
    Jan  3 11:50:55.294: INFO: Waiting for pod downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31 to disappear
    Jan  3 11:50:55.313: INFO: Pod downwardapi-volume-9fa1d389-fa42-4e9c-bb76-51012a321f31 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:50:55.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1515" for this suite. 01/03/24 11:50:55.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:50:55.377
Jan  3 11:50:55.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 11:50:55.379
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:50:55.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:50:55.473
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/03/24 11:50:55.502
Jan  3 11:50:55.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-7223 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Jan  3 11:50:55.741: INFO: stderr: ""
Jan  3 11:50:55.741: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/03/24 11:50:55.741
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jan  3 11:50:55.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-7223 delete pods e2e-test-httpd-pod'
Jan  3 11:50:59.405: INFO: stderr: ""
Jan  3 11:50:59.405: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 11:50:59.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7223" for this suite. 01/03/24 11:50:59.437
------------------------------
• [4.083 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:50:55.377
    Jan  3 11:50:55.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 11:50:55.379
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:50:55.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:50:55.473
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/03/24 11:50:55.502
    Jan  3 11:50:55.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-7223 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Jan  3 11:50:55.741: INFO: stderr: ""
    Jan  3 11:50:55.741: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/03/24 11:50:55.741
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jan  3 11:50:55.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-7223 delete pods e2e-test-httpd-pod'
    Jan  3 11:50:59.405: INFO: stderr: ""
    Jan  3 11:50:59.405: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:50:59.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7223" for this suite. 01/03/24 11:50:59.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:50:59.461
Jan  3 11:50:59.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename disruption 01/03/24 11:50:59.464
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:50:59.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:50:59.548
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 01/03/24 11:50:59.576
STEP: Waiting for the pdb to be processed 01/03/24 11:50:59.603
STEP: First trying to evict a pod which shouldn't be evictable 01/03/24 11:50:59.64
STEP: Waiting for all pods to be running 01/03/24 11:50:59.64
Jan  3 11:50:59.661: INFO: pods: 1 < 3
Jan  3 11:51:01.684: INFO: running pods: 2 < 3
STEP: locating a running pod 01/03/24 11:51:03.683
STEP: Updating the pdb to allow a pod to be evicted 01/03/24 11:51:03.73
STEP: Waiting for the pdb to be processed 01/03/24 11:51:03.773
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/03/24 11:51:03.793
STEP: Waiting for all pods to be running 01/03/24 11:51:03.794
STEP: Waiting for the pdb to observed all healthy pods 01/03/24 11:51:03.817
STEP: Patching the pdb to disallow a pod to be evicted 01/03/24 11:51:03.888
STEP: Waiting for the pdb to be processed 01/03/24 11:51:03.927
STEP: Waiting for all pods to be running 01/03/24 11:51:03.943
Jan  3 11:51:03.966: INFO: running pods: 2 < 3
Jan  3 11:51:05.989: INFO: running pods: 2 < 3
STEP: locating a running pod 01/03/24 11:51:07.988
STEP: Deleting the pdb to allow a pod to be evicted 01/03/24 11:51:08.036
STEP: Waiting for the pdb to be deleted 01/03/24 11:51:08.061
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/03/24 11:51:08.079
STEP: Waiting for all pods to be running 01/03/24 11:51:08.079
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  3 11:51:08.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8647" for this suite. 01/03/24 11:51:08.178
------------------------------
• [SLOW TEST] [8.740 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:50:59.461
    Jan  3 11:50:59.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename disruption 01/03/24 11:50:59.464
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:50:59.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:50:59.548
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 01/03/24 11:50:59.576
    STEP: Waiting for the pdb to be processed 01/03/24 11:50:59.603
    STEP: First trying to evict a pod which shouldn't be evictable 01/03/24 11:50:59.64
    STEP: Waiting for all pods to be running 01/03/24 11:50:59.64
    Jan  3 11:50:59.661: INFO: pods: 1 < 3
    Jan  3 11:51:01.684: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/03/24 11:51:03.683
    STEP: Updating the pdb to allow a pod to be evicted 01/03/24 11:51:03.73
    STEP: Waiting for the pdb to be processed 01/03/24 11:51:03.773
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/03/24 11:51:03.793
    STEP: Waiting for all pods to be running 01/03/24 11:51:03.794
    STEP: Waiting for the pdb to observed all healthy pods 01/03/24 11:51:03.817
    STEP: Patching the pdb to disallow a pod to be evicted 01/03/24 11:51:03.888
    STEP: Waiting for the pdb to be processed 01/03/24 11:51:03.927
    STEP: Waiting for all pods to be running 01/03/24 11:51:03.943
    Jan  3 11:51:03.966: INFO: running pods: 2 < 3
    Jan  3 11:51:05.989: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/03/24 11:51:07.988
    STEP: Deleting the pdb to allow a pod to be evicted 01/03/24 11:51:08.036
    STEP: Waiting for the pdb to be deleted 01/03/24 11:51:08.061
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/03/24 11:51:08.079
    STEP: Waiting for all pods to be running 01/03/24 11:51:08.079
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:51:08.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8647" for this suite. 01/03/24 11:51:08.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:51:08.205
Jan  3 11:51:08.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 11:51:08.206
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:51:08.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:51:08.285
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 01/03/24 11:51:08.313
Jan  3 11:51:08.314: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4013 proxy --unix-socket=/tmp/kubectl-proxy-unix3789120803/test'
STEP: retrieving proxy /api/ output 01/03/24 11:51:08.39
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 11:51:08.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4013" for this suite. 01/03/24 11:51:08.414
------------------------------
• [0.236 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:51:08.205
    Jan  3 11:51:08.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 11:51:08.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:51:08.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:51:08.285
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 01/03/24 11:51:08.313
    Jan  3 11:51:08.314: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4013 proxy --unix-socket=/tmp/kubectl-proxy-unix3789120803/test'
    STEP: retrieving proxy /api/ output 01/03/24 11:51:08.39
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:51:08.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4013" for this suite. 01/03/24 11:51:08.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:51:08.443
Jan  3 11:51:08.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 11:51:08.444
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:51:08.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:51:08.523
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-865 01/03/24 11:51:08.556
STEP: creating service affinity-nodeport in namespace services-865 01/03/24 11:51:08.556
STEP: creating replication controller affinity-nodeport in namespace services-865 01/03/24 11:51:08.597
I0103 11:51:08.623787      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-865, replica count: 3
I0103 11:51:11.674678      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 11:51:11.741: INFO: Creating new exec pod
Jan  3 11:51:11.770: INFO: Waiting up to 5m0s for pod "execpod-affinityxmshf" in namespace "services-865" to be "running"
Jan  3 11:51:11.790: INFO: Pod "execpod-affinityxmshf": Phase="Pending", Reason="", readiness=false. Elapsed: 19.480089ms
Jan  3 11:51:13.810: INFO: Pod "execpod-affinityxmshf": Phase="Running", Reason="", readiness=true. Elapsed: 2.040304859s
Jan  3 11:51:13.810: INFO: Pod "execpod-affinityxmshf" satisfied condition "running"
Jan  3 11:51:14.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-865 exec execpod-affinityxmshf -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jan  3 11:51:15.376: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan  3 11:51:15.377: INFO: stdout: ""
Jan  3 11:51:15.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-865 exec execpod-affinityxmshf -- /bin/sh -x -c nc -v -z -w 2 10.233.28.9 80'
Jan  3 11:51:15.814: INFO: stderr: "+ nc -v -z -w 2 10.233.28.9 80\nConnection to 10.233.28.9 80 port [tcp/http] succeeded!\n"
Jan  3 11:51:15.814: INFO: stdout: ""
Jan  3 11:51:15.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-865 exec execpod-affinityxmshf -- /bin/sh -x -c nc -v -z -w 2 85.215.218.90 31548'
Jan  3 11:51:16.246: INFO: stderr: "+ nc -v -z -w 2 85.215.218.90 31548\nConnection to 85.215.218.90 31548 port [tcp/*] succeeded!\n"
Jan  3 11:51:16.246: INFO: stdout: ""
Jan  3 11:51:16.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-865 exec execpod-affinityxmshf -- /bin/sh -x -c nc -v -z -w 2 185.132.46.116 31548'
Jan  3 11:51:16.694: INFO: stderr: "+ nc -v -z -w 2 185.132.46.116 31548\nConnection to 185.132.46.116 31548 port [tcp/*] succeeded!\n"
Jan  3 11:51:16.694: INFO: stdout: ""
Jan  3 11:51:16.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-865 exec execpod-affinityxmshf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://185.132.46.116:31548/ ; done'
Jan  3 11:51:17.204: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n"
Jan  3 11:51:17.204: INFO: stdout: "\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d"
Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.205: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.205: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.205: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.205: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.205: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.205: INFO: Received response from host: affinity-nodeport-k969d
Jan  3 11:51:17.205: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-865, will wait for the garbage collector to delete the pods 01/03/24 11:51:17.265
Jan  3 11:51:17.359: INFO: Deleting ReplicationController affinity-nodeport took: 25.534637ms
Jan  3 11:51:17.460: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.665166ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 11:51:20.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-865" for this suite. 01/03/24 11:51:20.843
------------------------------
• [SLOW TEST] [12.429 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:51:08.443
    Jan  3 11:51:08.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 11:51:08.444
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:51:08.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:51:08.523
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-865 01/03/24 11:51:08.556
    STEP: creating service affinity-nodeport in namespace services-865 01/03/24 11:51:08.556
    STEP: creating replication controller affinity-nodeport in namespace services-865 01/03/24 11:51:08.597
    I0103 11:51:08.623787      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-865, replica count: 3
    I0103 11:51:11.674678      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 11:51:11.741: INFO: Creating new exec pod
    Jan  3 11:51:11.770: INFO: Waiting up to 5m0s for pod "execpod-affinityxmshf" in namespace "services-865" to be "running"
    Jan  3 11:51:11.790: INFO: Pod "execpod-affinityxmshf": Phase="Pending", Reason="", readiness=false. Elapsed: 19.480089ms
    Jan  3 11:51:13.810: INFO: Pod "execpod-affinityxmshf": Phase="Running", Reason="", readiness=true. Elapsed: 2.040304859s
    Jan  3 11:51:13.810: INFO: Pod "execpod-affinityxmshf" satisfied condition "running"
    Jan  3 11:51:14.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-865 exec execpod-affinityxmshf -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jan  3 11:51:15.376: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan  3 11:51:15.377: INFO: stdout: ""
    Jan  3 11:51:15.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-865 exec execpod-affinityxmshf -- /bin/sh -x -c nc -v -z -w 2 10.233.28.9 80'
    Jan  3 11:51:15.814: INFO: stderr: "+ nc -v -z -w 2 10.233.28.9 80\nConnection to 10.233.28.9 80 port [tcp/http] succeeded!\n"
    Jan  3 11:51:15.814: INFO: stdout: ""
    Jan  3 11:51:15.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-865 exec execpod-affinityxmshf -- /bin/sh -x -c nc -v -z -w 2 85.215.218.90 31548'
    Jan  3 11:51:16.246: INFO: stderr: "+ nc -v -z -w 2 85.215.218.90 31548\nConnection to 85.215.218.90 31548 port [tcp/*] succeeded!\n"
    Jan  3 11:51:16.246: INFO: stdout: ""
    Jan  3 11:51:16.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-865 exec execpod-affinityxmshf -- /bin/sh -x -c nc -v -z -w 2 185.132.46.116 31548'
    Jan  3 11:51:16.694: INFO: stderr: "+ nc -v -z -w 2 185.132.46.116 31548\nConnection to 185.132.46.116 31548 port [tcp/*] succeeded!\n"
    Jan  3 11:51:16.694: INFO: stdout: ""
    Jan  3 11:51:16.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-865 exec execpod-affinityxmshf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://185.132.46.116:31548/ ; done'
    Jan  3 11:51:17.204: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:31548/\n"
    Jan  3 11:51:17.204: INFO: stdout: "\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d\naffinity-nodeport-k969d"
    Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.204: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.205: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.205: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.205: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.205: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.205: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.205: INFO: Received response from host: affinity-nodeport-k969d
    Jan  3 11:51:17.205: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-865, will wait for the garbage collector to delete the pods 01/03/24 11:51:17.265
    Jan  3 11:51:17.359: INFO: Deleting ReplicationController affinity-nodeport took: 25.534637ms
    Jan  3 11:51:17.460: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.665166ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:51:20.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-865" for this suite. 01/03/24 11:51:20.843
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:51:20.877
Jan  3 11:51:20.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 11:51:20.88
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:51:20.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:51:20.967
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 01/03/24 11:51:20.996
Jan  3 11:51:21.025: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-6831d571-1062-4c2f-9ceb-4c2189dbfa60" in namespace "emptydir-7303" to be "running"
Jan  3 11:51:21.044: INFO: Pod "pod-sharedvolume-6831d571-1062-4c2f-9ceb-4c2189dbfa60": Phase="Pending", Reason="", readiness=false. Elapsed: 19.615964ms
Jan  3 11:51:23.066: INFO: Pod "pod-sharedvolume-6831d571-1062-4c2f-9ceb-4c2189dbfa60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04107953s
Jan  3 11:51:25.069: INFO: Pod "pod-sharedvolume-6831d571-1062-4c2f-9ceb-4c2189dbfa60": Phase="Running", Reason="", readiness=false. Elapsed: 4.043839153s
Jan  3 11:51:25.069: INFO: Pod "pod-sharedvolume-6831d571-1062-4c2f-9ceb-4c2189dbfa60" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/03/24 11:51:25.069
Jan  3 11:51:25.070: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7303 PodName:pod-sharedvolume-6831d571-1062-4c2f-9ceb-4c2189dbfa60 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 11:51:25.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 11:51:25.072: INFO: ExecWithOptions: Clientset creation
Jan  3 11:51:25.072: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-7303/pods/pod-sharedvolume-6831d571-1062-4c2f-9ceb-4c2189dbfa60/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan  3 11:51:25.371: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 11:51:25.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7303" for this suite. 01/03/24 11:51:25.408
------------------------------
• [4.558 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:51:20.877
    Jan  3 11:51:20.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 11:51:20.88
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:51:20.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:51:20.967
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 01/03/24 11:51:20.996
    Jan  3 11:51:21.025: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-6831d571-1062-4c2f-9ceb-4c2189dbfa60" in namespace "emptydir-7303" to be "running"
    Jan  3 11:51:21.044: INFO: Pod "pod-sharedvolume-6831d571-1062-4c2f-9ceb-4c2189dbfa60": Phase="Pending", Reason="", readiness=false. Elapsed: 19.615964ms
    Jan  3 11:51:23.066: INFO: Pod "pod-sharedvolume-6831d571-1062-4c2f-9ceb-4c2189dbfa60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04107953s
    Jan  3 11:51:25.069: INFO: Pod "pod-sharedvolume-6831d571-1062-4c2f-9ceb-4c2189dbfa60": Phase="Running", Reason="", readiness=false. Elapsed: 4.043839153s
    Jan  3 11:51:25.069: INFO: Pod "pod-sharedvolume-6831d571-1062-4c2f-9ceb-4c2189dbfa60" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/03/24 11:51:25.069
    Jan  3 11:51:25.070: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7303 PodName:pod-sharedvolume-6831d571-1062-4c2f-9ceb-4c2189dbfa60 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 11:51:25.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 11:51:25.072: INFO: ExecWithOptions: Clientset creation
    Jan  3 11:51:25.072: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-7303/pods/pod-sharedvolume-6831d571-1062-4c2f-9ceb-4c2189dbfa60/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan  3 11:51:25.371: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:51:25.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7303" for this suite. 01/03/24 11:51:25.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:51:25.438
Jan  3 11:51:25.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-probe 01/03/24 11:51:25.44
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:51:25.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:51:25.534
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0 in namespace container-probe-201 01/03/24 11:51:25.563
Jan  3 11:51:25.591: INFO: Waiting up to 5m0s for pod "liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0" in namespace "container-probe-201" to be "not pending"
Jan  3 11:51:25.633: INFO: Pod "liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0": Phase="Pending", Reason="", readiness=false. Elapsed: 42.209454ms
Jan  3 11:51:27.655: INFO: Pod "liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0": Phase="Running", Reason="", readiness=true. Elapsed: 2.064001765s
Jan  3 11:51:27.655: INFO: Pod "liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0" satisfied condition "not pending"
Jan  3 11:51:27.655: INFO: Started pod liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0 in namespace container-probe-201
STEP: checking the pod's current state and verifying that restartCount is present 01/03/24 11:51:27.655
Jan  3 11:51:27.678: INFO: Initial restart count of pod liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0 is 0
Jan  3 11:51:47.947: INFO: Restart count of pod container-probe-201/liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0 is now 1 (20.269124553s elapsed)
STEP: deleting the pod 01/03/24 11:51:47.947
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  3 11:51:47.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-201" for this suite. 01/03/24 11:51:48.014
------------------------------
• [SLOW TEST] [22.604 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:51:25.438
    Jan  3 11:51:25.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-probe 01/03/24 11:51:25.44
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:51:25.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:51:25.534
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0 in namespace container-probe-201 01/03/24 11:51:25.563
    Jan  3 11:51:25.591: INFO: Waiting up to 5m0s for pod "liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0" in namespace "container-probe-201" to be "not pending"
    Jan  3 11:51:25.633: INFO: Pod "liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0": Phase="Pending", Reason="", readiness=false. Elapsed: 42.209454ms
    Jan  3 11:51:27.655: INFO: Pod "liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0": Phase="Running", Reason="", readiness=true. Elapsed: 2.064001765s
    Jan  3 11:51:27.655: INFO: Pod "liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0" satisfied condition "not pending"
    Jan  3 11:51:27.655: INFO: Started pod liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0 in namespace container-probe-201
    STEP: checking the pod's current state and verifying that restartCount is present 01/03/24 11:51:27.655
    Jan  3 11:51:27.678: INFO: Initial restart count of pod liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0 is 0
    Jan  3 11:51:47.947: INFO: Restart count of pod container-probe-201/liveness-c7a89dd7-5d01-4fb2-bfb9-3ef5e8e46de0 is now 1 (20.269124553s elapsed)
    STEP: deleting the pod 01/03/24 11:51:47.947
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:51:47.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-201" for this suite. 01/03/24 11:51:48.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:51:48.051
Jan  3 11:51:48.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename job 01/03/24 11:51:48.053
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:51:48.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:51:48.137
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 01/03/24 11:51:48.165
STEP: Ensuring job reaches completions 01/03/24 11:51:48.187
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  3 11:52:02.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6540" for this suite. 01/03/24 11:52:02.241
------------------------------
• [SLOW TEST] [14.219 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:51:48.051
    Jan  3 11:51:48.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename job 01/03/24 11:51:48.053
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:51:48.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:51:48.137
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 01/03/24 11:51:48.165
    STEP: Ensuring job reaches completions 01/03/24 11:51:48.187
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:52:02.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6540" for this suite. 01/03/24 11:52:02.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:52:02.272
Jan  3 11:52:02.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename svcaccounts 01/03/24 11:52:02.275
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:02.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:02.356
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-qg5sz"  01/03/24 11:52:02.386
Jan  3 11:52:02.411: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-qg5sz"  01/03/24 11:52:02.411
Jan  3 11:52:02.452: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  3 11:52:02.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4043" for this suite. 01/03/24 11:52:02.473
------------------------------
• [0.230 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:52:02.272
    Jan  3 11:52:02.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename svcaccounts 01/03/24 11:52:02.275
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:02.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:02.356
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-qg5sz"  01/03/24 11:52:02.386
    Jan  3 11:52:02.411: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-qg5sz"  01/03/24 11:52:02.411
    Jan  3 11:52:02.452: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:52:02.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4043" for this suite. 01/03/24 11:52:02.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:52:02.515
Jan  3 11:52:02.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename sched-pred 01/03/24 11:52:02.516
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:02.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:02.645
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan  3 11:52:02.674: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  3 11:52:02.713: INFO: Waiting for terminating namespaces to be deleted...
Jan  3 11:52:02.732: INFO: 
Logging pods the apiserver thinks is on node jb-1-26-np-64kerjapxk before test
Jan  3 11:52:02.772: INFO: fail-once-local-2wqzf from job-6540 started at 2024-01-03 11:51:55 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.772: INFO: 	Container c ready: false, restart count 1
Jan  3 11:52:02.772: INFO: fail-once-local-qqscg from job-6540 started at 2024-01-03 11:51:55 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.772: INFO: 	Container c ready: false, restart count 1
Jan  3 11:52:02.772: INFO: fail-once-local-z2g2j from job-6540 started at 2024-01-03 11:51:48 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.772: INFO: 	Container c ready: false, restart count 1
Jan  3 11:52:02.772: INFO: fail-once-local-z7lxp from job-6540 started at 2024-01-03 11:51:48 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.772: INFO: 	Container c ready: false, restart count 1
Jan  3 11:52:02.772: INFO: calico-node-r98wj from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.772: INFO: 	Container calico-node ready: true, restart count 2
Jan  3 11:52:02.772: INFO: calico-typha-8555b568f6-4nmck from kube-system started at 2024-01-03 11:33:49 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.772: INFO: 	Container calico-typha ready: true, restart count 0
Jan  3 11:52:02.772: INFO: csi-ionoscloud-t8qkm from kube-system started at 2024-01-03 11:27:28 +0000 UTC (2 container statuses recorded)
Jan  3 11:52:02.772: INFO: 	Container csi-ionoscloud ready: true, restart count 0
Jan  3 11:52:02.772: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  3 11:52:02.772: INFO: konnectivity-agent-gnnsp from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.772: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  3 11:52:02.772: INFO: kube-proxy-z7q4m from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.772: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 11:52:02.772: INFO: nginx-proxy-jb-1-26-np-64kerjapxk from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.772: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan  3 11:52:02.772: INFO: sonobuoy from sonobuoy started at 2024-01-03 11:39:58 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.772: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  3 11:52:02.772: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-4hltf from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 11:52:02.772: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 11:52:02.772: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 11:52:02.772: INFO: 
Logging pods the apiserver thinks is on node jb-1-26-np-adtwo5cmi2 before test
Jan  3 11:52:02.803: INFO: rs-v5qs6 from disruption-8647 started at 2024-01-03 11:51:08 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.803: INFO: 	Container donothing ready: false, restart count 0
Jan  3 11:52:02.804: INFO: calico-kube-controllers-54564bcfb4-kjb7m from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.804: INFO: 	Container calico-kube-controllers ready: true, restart count 2
Jan  3 11:52:02.804: INFO: calico-node-j4nfd from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.804: INFO: 	Container calico-node ready: true, restart count 1
Jan  3 11:52:02.804: INFO: csi-ionoscloud-4z7q8 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (2 container statuses recorded)
Jan  3 11:52:02.804: INFO: 	Container csi-ionoscloud ready: true, restart count 0
Jan  3 11:52:02.804: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  3 11:52:02.804: INFO: konnectivity-agent-srxbt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.804: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  3 11:52:02.804: INFO: kube-proxy-ml4kt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.804: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 11:52:02.804: INFO: nginx-proxy-jb-1-26-np-adtwo5cmi2 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.804: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan  3 11:52:02.804: INFO: snapshot-validation-webhook-684799cdd5-x6hm4 from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.804: INFO: 	Container snapshot-validation ready: true, restart count 0
Jan  3 11:52:02.804: INFO: sonobuoy-e2e-job-61ed32d11fea4649 from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 11:52:02.804: INFO: 	Container e2e ready: true, restart count 0
Jan  3 11:52:02.804: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 11:52:02.804: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-cb8zx from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 11:52:02.804: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 11:52:02.804: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 11:52:02.804: INFO: 
Logging pods the apiserver thinks is on node jb-1-26-np-nqeu5xtrab before test
Jan  3 11:52:02.833: INFO: calico-node-t5pwv from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.833: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 11:52:02.833: INFO: calico-typha-8555b568f6-bdlz4 from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.833: INFO: 	Container calico-typha ready: true, restart count 0
Jan  3 11:52:02.833: INFO: coredns-89967fcdc-9jbpx from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.833: INFO: 	Container coredns ready: true, restart count 0
Jan  3 11:52:02.833: INFO: coredns-89967fcdc-zjgbq from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.833: INFO: 	Container coredns ready: true, restart count 0
Jan  3 11:52:02.833: INFO: csi-ionoscloud-kv8wz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (2 container statuses recorded)
Jan  3 11:52:02.833: INFO: 	Container csi-ionoscloud ready: true, restart count 0
Jan  3 11:52:02.833: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  3 11:52:02.833: INFO: ionos-policy-validator-64d9954f65-dpd5v from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.833: INFO: 	Container ionos-policy-validator ready: true, restart count 0
Jan  3 11:52:02.833: INFO: konnectivity-agent-v6hrz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.833: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  3 11:52:02.833: INFO: kube-proxy-hqxkb from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.833: INFO: 	Container kube-proxy ready: true, restart count 1
Jan  3 11:52:02.833: INFO: nginx-proxy-jb-1-26-np-nqeu5xtrab from kube-system started at 2024-01-03 11:30:18 +0000 UTC (1 container statuses recorded)
Jan  3 11:52:02.833: INFO: 	Container nginx-proxy ready: true, restart count 1
Jan  3 11:52:02.833: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-j7ffw from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 11:52:02.833: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 11:52:02.833: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/03/24 11:52:02.834
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17a6d41b595bed6c], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 01/03/24 11:52:03.017
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 11:52:03.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-768" for this suite. 01/03/24 11:52:04.004
------------------------------
• [1.519 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:52:02.515
    Jan  3 11:52:02.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename sched-pred 01/03/24 11:52:02.516
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:02.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:02.645
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan  3 11:52:02.674: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  3 11:52:02.713: INFO: Waiting for terminating namespaces to be deleted...
    Jan  3 11:52:02.732: INFO: 
    Logging pods the apiserver thinks is on node jb-1-26-np-64kerjapxk before test
    Jan  3 11:52:02.772: INFO: fail-once-local-2wqzf from job-6540 started at 2024-01-03 11:51:55 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.772: INFO: 	Container c ready: false, restart count 1
    Jan  3 11:52:02.772: INFO: fail-once-local-qqscg from job-6540 started at 2024-01-03 11:51:55 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.772: INFO: 	Container c ready: false, restart count 1
    Jan  3 11:52:02.772: INFO: fail-once-local-z2g2j from job-6540 started at 2024-01-03 11:51:48 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.772: INFO: 	Container c ready: false, restart count 1
    Jan  3 11:52:02.772: INFO: fail-once-local-z7lxp from job-6540 started at 2024-01-03 11:51:48 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.772: INFO: 	Container c ready: false, restart count 1
    Jan  3 11:52:02.772: INFO: calico-node-r98wj from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.772: INFO: 	Container calico-node ready: true, restart count 2
    Jan  3 11:52:02.772: INFO: calico-typha-8555b568f6-4nmck from kube-system started at 2024-01-03 11:33:49 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.772: INFO: 	Container calico-typha ready: true, restart count 0
    Jan  3 11:52:02.772: INFO: csi-ionoscloud-t8qkm from kube-system started at 2024-01-03 11:27:28 +0000 UTC (2 container statuses recorded)
    Jan  3 11:52:02.772: INFO: 	Container csi-ionoscloud ready: true, restart count 0
    Jan  3 11:52:02.772: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  3 11:52:02.772: INFO: konnectivity-agent-gnnsp from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.772: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  3 11:52:02.772: INFO: kube-proxy-z7q4m from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.772: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 11:52:02.772: INFO: nginx-proxy-jb-1-26-np-64kerjapxk from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.772: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan  3 11:52:02.772: INFO: sonobuoy from sonobuoy started at 2024-01-03 11:39:58 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.772: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  3 11:52:02.772: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-4hltf from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 11:52:02.772: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 11:52:02.772: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 11:52:02.772: INFO: 
    Logging pods the apiserver thinks is on node jb-1-26-np-adtwo5cmi2 before test
    Jan  3 11:52:02.803: INFO: rs-v5qs6 from disruption-8647 started at 2024-01-03 11:51:08 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.803: INFO: 	Container donothing ready: false, restart count 0
    Jan  3 11:52:02.804: INFO: calico-kube-controllers-54564bcfb4-kjb7m from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.804: INFO: 	Container calico-kube-controllers ready: true, restart count 2
    Jan  3 11:52:02.804: INFO: calico-node-j4nfd from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.804: INFO: 	Container calico-node ready: true, restart count 1
    Jan  3 11:52:02.804: INFO: csi-ionoscloud-4z7q8 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (2 container statuses recorded)
    Jan  3 11:52:02.804: INFO: 	Container csi-ionoscloud ready: true, restart count 0
    Jan  3 11:52:02.804: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  3 11:52:02.804: INFO: konnectivity-agent-srxbt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.804: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  3 11:52:02.804: INFO: kube-proxy-ml4kt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.804: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 11:52:02.804: INFO: nginx-proxy-jb-1-26-np-adtwo5cmi2 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.804: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan  3 11:52:02.804: INFO: snapshot-validation-webhook-684799cdd5-x6hm4 from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.804: INFO: 	Container snapshot-validation ready: true, restart count 0
    Jan  3 11:52:02.804: INFO: sonobuoy-e2e-job-61ed32d11fea4649 from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 11:52:02.804: INFO: 	Container e2e ready: true, restart count 0
    Jan  3 11:52:02.804: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 11:52:02.804: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-cb8zx from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 11:52:02.804: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 11:52:02.804: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 11:52:02.804: INFO: 
    Logging pods the apiserver thinks is on node jb-1-26-np-nqeu5xtrab before test
    Jan  3 11:52:02.833: INFO: calico-node-t5pwv from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.833: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 11:52:02.833: INFO: calico-typha-8555b568f6-bdlz4 from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.833: INFO: 	Container calico-typha ready: true, restart count 0
    Jan  3 11:52:02.833: INFO: coredns-89967fcdc-9jbpx from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.833: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 11:52:02.833: INFO: coredns-89967fcdc-zjgbq from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.833: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 11:52:02.833: INFO: csi-ionoscloud-kv8wz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (2 container statuses recorded)
    Jan  3 11:52:02.833: INFO: 	Container csi-ionoscloud ready: true, restart count 0
    Jan  3 11:52:02.833: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  3 11:52:02.833: INFO: ionos-policy-validator-64d9954f65-dpd5v from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.833: INFO: 	Container ionos-policy-validator ready: true, restart count 0
    Jan  3 11:52:02.833: INFO: konnectivity-agent-v6hrz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.833: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  3 11:52:02.833: INFO: kube-proxy-hqxkb from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.833: INFO: 	Container kube-proxy ready: true, restart count 1
    Jan  3 11:52:02.833: INFO: nginx-proxy-jb-1-26-np-nqeu5xtrab from kube-system started at 2024-01-03 11:30:18 +0000 UTC (1 container statuses recorded)
    Jan  3 11:52:02.833: INFO: 	Container nginx-proxy ready: true, restart count 1
    Jan  3 11:52:02.833: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-j7ffw from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 11:52:02.833: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 11:52:02.833: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/03/24 11:52:02.834
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17a6d41b595bed6c], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 01/03/24 11:52:03.017
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:52:03.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-768" for this suite. 01/03/24 11:52:04.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:52:04.038
Jan  3 11:52:04.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename sysctl 01/03/24 11:52:04.04
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:04.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:04.129
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/03/24 11:52:04.158
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  3 11:52:04.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-3198" for this suite. 01/03/24 11:52:04.202
------------------------------
• [0.189 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:52:04.038
    Jan  3 11:52:04.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename sysctl 01/03/24 11:52:04.04
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:04.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:04.129
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/03/24 11:52:04.158
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:52:04.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-3198" for this suite. 01/03/24 11:52:04.202
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:52:04.227
Jan  3 11:52:04.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename custom-resource-definition 01/03/24 11:52:04.229
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:04.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:04.314
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/03/24 11:52:04.342
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/03/24 11:52:04.356
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/03/24 11:52:04.356
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/03/24 11:52:04.357
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/03/24 11:52:04.37
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/03/24 11:52:04.37
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/03/24 11:52:04.384
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 11:52:04.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-331" for this suite. 01/03/24 11:52:04.404
------------------------------
• [0.212 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:52:04.227
    Jan  3 11:52:04.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename custom-resource-definition 01/03/24 11:52:04.229
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:04.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:04.314
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/03/24 11:52:04.342
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/03/24 11:52:04.356
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/03/24 11:52:04.356
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/03/24 11:52:04.357
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/03/24 11:52:04.37
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/03/24 11:52:04.37
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/03/24 11:52:04.384
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:52:04.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-331" for this suite. 01/03/24 11:52:04.404
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:52:04.44
Jan  3 11:52:04.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 11:52:04.443
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:04.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:04.54
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-32e3618c-3c0a-439f-8410-04e2ca2ece3a 01/03/24 11:52:04.57
STEP: Creating a pod to test consume configMaps 01/03/24 11:52:04.59
Jan  3 11:52:04.626: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068" in namespace "projected-5210" to be "Succeeded or Failed"
Jan  3 11:52:04.646: INFO: Pod "pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068": Phase="Pending", Reason="", readiness=false. Elapsed: 20.120068ms
Jan  3 11:52:06.669: INFO: Pod "pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043085226s
Jan  3 11:52:08.667: INFO: Pod "pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040840086s
STEP: Saw pod success 01/03/24 11:52:08.667
Jan  3 11:52:08.667: INFO: Pod "pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068" satisfied condition "Succeeded or Failed"
Jan  3 11:52:08.704: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068 container agnhost-container: <nil>
STEP: delete the pod 01/03/24 11:52:08.745
Jan  3 11:52:08.796: INFO: Waiting for pod pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068 to disappear
Jan  3 11:52:08.814: INFO: Pod pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  3 11:52:08.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5210" for this suite. 01/03/24 11:52:08.846
------------------------------
• [4.439 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:52:04.44
    Jan  3 11:52:04.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 11:52:04.443
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:04.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:04.54
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-32e3618c-3c0a-439f-8410-04e2ca2ece3a 01/03/24 11:52:04.57
    STEP: Creating a pod to test consume configMaps 01/03/24 11:52:04.59
    Jan  3 11:52:04.626: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068" in namespace "projected-5210" to be "Succeeded or Failed"
    Jan  3 11:52:04.646: INFO: Pod "pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068": Phase="Pending", Reason="", readiness=false. Elapsed: 20.120068ms
    Jan  3 11:52:06.669: INFO: Pod "pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043085226s
    Jan  3 11:52:08.667: INFO: Pod "pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040840086s
    STEP: Saw pod success 01/03/24 11:52:08.667
    Jan  3 11:52:08.667: INFO: Pod "pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068" satisfied condition "Succeeded or Failed"
    Jan  3 11:52:08.704: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068 container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 11:52:08.745
    Jan  3 11:52:08.796: INFO: Waiting for pod pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068 to disappear
    Jan  3 11:52:08.814: INFO: Pod pod-projected-configmaps-7e2777f8-bfe3-4608-a664-89cae026c068 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:52:08.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5210" for this suite. 01/03/24 11:52:08.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:52:08.885
Jan  3 11:52:08.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename gc 01/03/24 11:52:08.887
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:08.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:08.98
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/03/24 11:52:09.009
STEP: Wait for the Deployment to create new ReplicaSet 01/03/24 11:52:09.032
STEP: delete the deployment 01/03/24 11:52:09.048
STEP: wait for all rs to be garbage collected 01/03/24 11:52:09.077
STEP: expected 0 pods, got 2 pods 01/03/24 11:52:09.097
STEP: Gathering metrics 01/03/24 11:52:09.69
W0103 11:52:09.762212      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  3 11:52:09.762: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  3 11:52:09.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7002" for this suite. 01/03/24 11:52:09.787
------------------------------
• [0.926 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:52:08.885
    Jan  3 11:52:08.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename gc 01/03/24 11:52:08.887
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:08.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:08.98
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/03/24 11:52:09.009
    STEP: Wait for the Deployment to create new ReplicaSet 01/03/24 11:52:09.032
    STEP: delete the deployment 01/03/24 11:52:09.048
    STEP: wait for all rs to be garbage collected 01/03/24 11:52:09.077
    STEP: expected 0 pods, got 2 pods 01/03/24 11:52:09.097
    STEP: Gathering metrics 01/03/24 11:52:09.69
    W0103 11:52:09.762212      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  3 11:52:09.762: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:52:09.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7002" for this suite. 01/03/24 11:52:09.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:52:09.817
Jan  3 11:52:09.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename var-expansion 01/03/24 11:52:09.82
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:09.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:09.904
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Jan  3 11:52:09.962: INFO: Waiting up to 2m0s for pod "var-expansion-a69d8275-6756-4d39-bd3b-7b59d96011ff" in namespace "var-expansion-8754" to be "container 0 failed with reason CreateContainerConfigError"
Jan  3 11:52:09.988: INFO: Pod "var-expansion-a69d8275-6756-4d39-bd3b-7b59d96011ff": Phase="Pending", Reason="", readiness=false. Elapsed: 25.33182ms
Jan  3 11:52:12.010: INFO: Pod "var-expansion-a69d8275-6756-4d39-bd3b-7b59d96011ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047405577s
Jan  3 11:52:12.010: INFO: Pod "var-expansion-a69d8275-6756-4d39-bd3b-7b59d96011ff" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan  3 11:52:12.010: INFO: Deleting pod "var-expansion-a69d8275-6756-4d39-bd3b-7b59d96011ff" in namespace "var-expansion-8754"
Jan  3 11:52:12.045: INFO: Wait up to 5m0s for pod "var-expansion-a69d8275-6756-4d39-bd3b-7b59d96011ff" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  3 11:52:16.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8754" for this suite. 01/03/24 11:52:16.124
------------------------------
• [SLOW TEST] [6.329 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:52:09.817
    Jan  3 11:52:09.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename var-expansion 01/03/24 11:52:09.82
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:09.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:09.904
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Jan  3 11:52:09.962: INFO: Waiting up to 2m0s for pod "var-expansion-a69d8275-6756-4d39-bd3b-7b59d96011ff" in namespace "var-expansion-8754" to be "container 0 failed with reason CreateContainerConfigError"
    Jan  3 11:52:09.988: INFO: Pod "var-expansion-a69d8275-6756-4d39-bd3b-7b59d96011ff": Phase="Pending", Reason="", readiness=false. Elapsed: 25.33182ms
    Jan  3 11:52:12.010: INFO: Pod "var-expansion-a69d8275-6756-4d39-bd3b-7b59d96011ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047405577s
    Jan  3 11:52:12.010: INFO: Pod "var-expansion-a69d8275-6756-4d39-bd3b-7b59d96011ff" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan  3 11:52:12.010: INFO: Deleting pod "var-expansion-a69d8275-6756-4d39-bd3b-7b59d96011ff" in namespace "var-expansion-8754"
    Jan  3 11:52:12.045: INFO: Wait up to 5m0s for pod "var-expansion-a69d8275-6756-4d39-bd3b-7b59d96011ff" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:52:16.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8754" for this suite. 01/03/24 11:52:16.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:52:16.15
Jan  3 11:52:16.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 11:52:16.153
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:16.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:16.243
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 01/03/24 11:52:16.278
Jan  3 11:52:16.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-7271 create -f -'
Jan  3 11:52:16.662: INFO: stderr: ""
Jan  3 11:52:16.662: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/03/24 11:52:16.662
Jan  3 11:52:16.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-7271 diff -f -'
Jan  3 11:52:17.303: INFO: rc: 1
Jan  3 11:52:17.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-7271 delete -f -'
Jan  3 11:52:17.455: INFO: stderr: ""
Jan  3 11:52:17.455: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 11:52:17.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7271" for this suite. 01/03/24 11:52:17.486
------------------------------
• [1.363 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:52:16.15
    Jan  3 11:52:16.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 11:52:16.153
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:16.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:16.243
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 01/03/24 11:52:16.278
    Jan  3 11:52:16.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-7271 create -f -'
    Jan  3 11:52:16.662: INFO: stderr: ""
    Jan  3 11:52:16.662: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/03/24 11:52:16.662
    Jan  3 11:52:16.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-7271 diff -f -'
    Jan  3 11:52:17.303: INFO: rc: 1
    Jan  3 11:52:17.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-7271 delete -f -'
    Jan  3 11:52:17.455: INFO: stderr: ""
    Jan  3 11:52:17.455: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:52:17.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7271" for this suite. 01/03/24 11:52:17.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:52:17.517
Jan  3 11:52:17.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 11:52:17.52
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:17.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:17.606
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 01/03/24 11:52:17.636
Jan  3 11:52:17.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan  3 11:52:17.783: INFO: stderr: ""
Jan  3 11:52:17.783: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 01/03/24 11:52:17.784
Jan  3 11:52:17.784: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan  3 11:52:17.784: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4709" to be "running and ready, or succeeded"
Jan  3 11:52:17.806: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 22.096422ms
Jan  3 11:52:17.806: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'jb-1-26-np-64kerjapxk' to be 'Running' but was 'Pending'
Jan  3 11:52:19.827: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042674763s
Jan  3 11:52:19.827: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'jb-1-26-np-64kerjapxk' to be 'Running' but was 'Pending'
Jan  3 11:52:21.827: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.042955926s
Jan  3 11:52:21.827: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan  3 11:52:21.827: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/03/24 11:52:21.827
Jan  3 11:52:21.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 logs logs-generator logs-generator'
Jan  3 11:52:22.127: INFO: stderr: ""
Jan  3 11:52:22.127: INFO: stdout: "I0103 11:52:19.624737       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/5vdd 300\nI0103 11:52:19.825211       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/n2h 359\nI0103 11:52:20.025586       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/pcx 290\nI0103 11:52:20.224900       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/8j5g 330\nI0103 11:52:20.425295       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/stmw 535\nI0103 11:52:20.625608       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/f54 438\nI0103 11:52:20.824903       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/dsj 445\nI0103 11:52:21.025295       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/hqfx 453\nI0103 11:52:21.225714       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/w8q 524\nI0103 11:52:21.424905       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/qd4 315\nI0103 11:52:21.625328       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/fdxb 457\nI0103 11:52:21.825716       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/6pj 413\nI0103 11:52:22.025573       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/d6r 540\n"
STEP: limiting log lines 01/03/24 11:52:22.127
Jan  3 11:52:22.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 logs logs-generator logs-generator --tail=1'
Jan  3 11:52:22.312: INFO: stderr: ""
Jan  3 11:52:22.312: INFO: stdout: "I0103 11:52:22.224909       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/wzl 338\n"
Jan  3 11:52:22.312: INFO: got output "I0103 11:52:22.224909       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/wzl 338\n"
STEP: limiting log bytes 01/03/24 11:52:22.312
Jan  3 11:52:22.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 logs logs-generator logs-generator --limit-bytes=1'
Jan  3 11:52:22.491: INFO: stderr: ""
Jan  3 11:52:22.491: INFO: stdout: "I"
Jan  3 11:52:22.491: INFO: got output "I"
STEP: exposing timestamps 01/03/24 11:52:22.491
Jan  3 11:52:22.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 logs logs-generator logs-generator --tail=1 --timestamps'
Jan  3 11:52:22.674: INFO: stderr: ""
Jan  3 11:52:22.674: INFO: stdout: "2024-01-03T11:52:22.625920330Z I0103 11:52:22.625715       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/sgh 483\n"
Jan  3 11:52:22.674: INFO: got output "2024-01-03T11:52:22.625920330Z I0103 11:52:22.625715       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/sgh 483\n"
STEP: restricting to a time range 01/03/24 11:52:22.674
Jan  3 11:52:25.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 logs logs-generator logs-generator --since=1s'
Jan  3 11:52:25.372: INFO: stderr: ""
Jan  3 11:52:25.372: INFO: stdout: "I0103 11:52:24.425215       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/mmf 290\nI0103 11:52:24.625530       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/lbkg 567\nI0103 11:52:24.824875       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/kndz 359\nI0103 11:52:25.025289       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/qkzz 411\nI0103 11:52:25.225629       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/wjc 561\n"
Jan  3 11:52:25.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 logs logs-generator logs-generator --since=24h'
Jan  3 11:52:25.546: INFO: stderr: ""
Jan  3 11:52:25.546: INFO: stdout: "I0103 11:52:19.624737       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/5vdd 300\nI0103 11:52:19.825211       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/n2h 359\nI0103 11:52:20.025586       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/pcx 290\nI0103 11:52:20.224900       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/8j5g 330\nI0103 11:52:20.425295       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/stmw 535\nI0103 11:52:20.625608       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/f54 438\nI0103 11:52:20.824903       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/dsj 445\nI0103 11:52:21.025295       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/hqfx 453\nI0103 11:52:21.225714       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/w8q 524\nI0103 11:52:21.424905       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/qd4 315\nI0103 11:52:21.625328       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/fdxb 457\nI0103 11:52:21.825716       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/6pj 413\nI0103 11:52:22.025573       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/d6r 540\nI0103 11:52:22.224909       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/wzl 338\nI0103 11:52:22.425331       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/7cs2 532\nI0103 11:52:22.625715       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/sgh 483\nI0103 11:52:22.825106       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/mt8 252\nI0103 11:52:23.025518       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/t56f 408\nI0103 11:52:23.224868       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/jlb 578\nI0103 11:52:23.425375       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/c86 210\nI0103 11:52:23.625698       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/znt 563\nI0103 11:52:23.824945       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/hz7q 496\nI0103 11:52:24.025460       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/qwj 492\nI0103 11:52:24.224780       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/9rb 243\nI0103 11:52:24.425215       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/mmf 290\nI0103 11:52:24.625530       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/lbkg 567\nI0103 11:52:24.824875       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/kndz 359\nI0103 11:52:25.025289       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/qkzz 411\nI0103 11:52:25.225629       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/wjc 561\nI0103 11:52:25.424905       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/p5z 342\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jan  3 11:52:25.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 delete pod logs-generator'
Jan  3 11:52:27.679: INFO: stderr: ""
Jan  3 11:52:27.679: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 11:52:27.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4709" for this suite. 01/03/24 11:52:27.711
------------------------------
• [SLOW TEST] [10.218 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:52:17.517
    Jan  3 11:52:17.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 11:52:17.52
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:17.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:17.606
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 01/03/24 11:52:17.636
    Jan  3 11:52:17.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan  3 11:52:17.783: INFO: stderr: ""
    Jan  3 11:52:17.783: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 01/03/24 11:52:17.784
    Jan  3 11:52:17.784: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan  3 11:52:17.784: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4709" to be "running and ready, or succeeded"
    Jan  3 11:52:17.806: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 22.096422ms
    Jan  3 11:52:17.806: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'jb-1-26-np-64kerjapxk' to be 'Running' but was 'Pending'
    Jan  3 11:52:19.827: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042674763s
    Jan  3 11:52:19.827: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'jb-1-26-np-64kerjapxk' to be 'Running' but was 'Pending'
    Jan  3 11:52:21.827: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.042955926s
    Jan  3 11:52:21.827: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan  3 11:52:21.827: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/03/24 11:52:21.827
    Jan  3 11:52:21.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 logs logs-generator logs-generator'
    Jan  3 11:52:22.127: INFO: stderr: ""
    Jan  3 11:52:22.127: INFO: stdout: "I0103 11:52:19.624737       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/5vdd 300\nI0103 11:52:19.825211       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/n2h 359\nI0103 11:52:20.025586       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/pcx 290\nI0103 11:52:20.224900       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/8j5g 330\nI0103 11:52:20.425295       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/stmw 535\nI0103 11:52:20.625608       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/f54 438\nI0103 11:52:20.824903       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/dsj 445\nI0103 11:52:21.025295       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/hqfx 453\nI0103 11:52:21.225714       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/w8q 524\nI0103 11:52:21.424905       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/qd4 315\nI0103 11:52:21.625328       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/fdxb 457\nI0103 11:52:21.825716       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/6pj 413\nI0103 11:52:22.025573       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/d6r 540\n"
    STEP: limiting log lines 01/03/24 11:52:22.127
    Jan  3 11:52:22.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 logs logs-generator logs-generator --tail=1'
    Jan  3 11:52:22.312: INFO: stderr: ""
    Jan  3 11:52:22.312: INFO: stdout: "I0103 11:52:22.224909       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/wzl 338\n"
    Jan  3 11:52:22.312: INFO: got output "I0103 11:52:22.224909       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/wzl 338\n"
    STEP: limiting log bytes 01/03/24 11:52:22.312
    Jan  3 11:52:22.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 logs logs-generator logs-generator --limit-bytes=1'
    Jan  3 11:52:22.491: INFO: stderr: ""
    Jan  3 11:52:22.491: INFO: stdout: "I"
    Jan  3 11:52:22.491: INFO: got output "I"
    STEP: exposing timestamps 01/03/24 11:52:22.491
    Jan  3 11:52:22.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan  3 11:52:22.674: INFO: stderr: ""
    Jan  3 11:52:22.674: INFO: stdout: "2024-01-03T11:52:22.625920330Z I0103 11:52:22.625715       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/sgh 483\n"
    Jan  3 11:52:22.674: INFO: got output "2024-01-03T11:52:22.625920330Z I0103 11:52:22.625715       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/sgh 483\n"
    STEP: restricting to a time range 01/03/24 11:52:22.674
    Jan  3 11:52:25.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 logs logs-generator logs-generator --since=1s'
    Jan  3 11:52:25.372: INFO: stderr: ""
    Jan  3 11:52:25.372: INFO: stdout: "I0103 11:52:24.425215       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/mmf 290\nI0103 11:52:24.625530       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/lbkg 567\nI0103 11:52:24.824875       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/kndz 359\nI0103 11:52:25.025289       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/qkzz 411\nI0103 11:52:25.225629       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/wjc 561\n"
    Jan  3 11:52:25.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 logs logs-generator logs-generator --since=24h'
    Jan  3 11:52:25.546: INFO: stderr: ""
    Jan  3 11:52:25.546: INFO: stdout: "I0103 11:52:19.624737       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/5vdd 300\nI0103 11:52:19.825211       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/n2h 359\nI0103 11:52:20.025586       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/pcx 290\nI0103 11:52:20.224900       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/8j5g 330\nI0103 11:52:20.425295       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/stmw 535\nI0103 11:52:20.625608       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/f54 438\nI0103 11:52:20.824903       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/dsj 445\nI0103 11:52:21.025295       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/hqfx 453\nI0103 11:52:21.225714       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/w8q 524\nI0103 11:52:21.424905       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/qd4 315\nI0103 11:52:21.625328       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/fdxb 457\nI0103 11:52:21.825716       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/6pj 413\nI0103 11:52:22.025573       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/d6r 540\nI0103 11:52:22.224909       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/wzl 338\nI0103 11:52:22.425331       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/7cs2 532\nI0103 11:52:22.625715       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/sgh 483\nI0103 11:52:22.825106       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/mt8 252\nI0103 11:52:23.025518       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/t56f 408\nI0103 11:52:23.224868       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/jlb 578\nI0103 11:52:23.425375       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/c86 210\nI0103 11:52:23.625698       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/znt 563\nI0103 11:52:23.824945       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/hz7q 496\nI0103 11:52:24.025460       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/qwj 492\nI0103 11:52:24.224780       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/9rb 243\nI0103 11:52:24.425215       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/mmf 290\nI0103 11:52:24.625530       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/lbkg 567\nI0103 11:52:24.824875       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/kndz 359\nI0103 11:52:25.025289       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/qkzz 411\nI0103 11:52:25.225629       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/wjc 561\nI0103 11:52:25.424905       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/p5z 342\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jan  3 11:52:25.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4709 delete pod logs-generator'
    Jan  3 11:52:27.679: INFO: stderr: ""
    Jan  3 11:52:27.679: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:52:27.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4709" for this suite. 01/03/24 11:52:27.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:52:27.738
Jan  3 11:52:27.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 11:52:27.741
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:27.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:27.827
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 11:52:27.925
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 11:52:28.126
STEP: Deploying the webhook pod 01/03/24 11:52:28.171
STEP: Wait for the deployment to be ready 01/03/24 11:52:28.212
Jan  3 11:52:28.247: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/24 11:52:30.303
STEP: Verifying the service has paired with the endpoint 01/03/24 11:52:30.331
Jan  3 11:52:31.332: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/03/24 11:52:31.356
STEP: create a namespace for the webhook 01/03/24 11:52:31.506
STEP: create a configmap should be unconditionally rejected by the webhook 01/03/24 11:52:31.534
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 11:52:31.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4928" for this suite. 01/03/24 11:52:31.858
STEP: Destroying namespace "webhook-4928-markers" for this suite. 01/03/24 11:52:31.885
------------------------------
• [4.179 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:52:27.738
    Jan  3 11:52:27.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 11:52:27.741
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:27.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:27.827
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 11:52:27.925
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 11:52:28.126
    STEP: Deploying the webhook pod 01/03/24 11:52:28.171
    STEP: Wait for the deployment to be ready 01/03/24 11:52:28.212
    Jan  3 11:52:28.247: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/24 11:52:30.303
    STEP: Verifying the service has paired with the endpoint 01/03/24 11:52:30.331
    Jan  3 11:52:31.332: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/03/24 11:52:31.356
    STEP: create a namespace for the webhook 01/03/24 11:52:31.506
    STEP: create a configmap should be unconditionally rejected by the webhook 01/03/24 11:52:31.534
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:52:31.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4928" for this suite. 01/03/24 11:52:31.858
    STEP: Destroying namespace "webhook-4928-markers" for this suite. 01/03/24 11:52:31.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:52:31.92
Jan  3 11:52:31.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename gc 01/03/24 11:52:31.922
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:31.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:32.009
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/03/24 11:52:32.057
STEP: delete the rc 01/03/24 11:52:37.102
STEP: wait for the rc to be deleted 01/03/24 11:52:37.124
Jan  3 11:52:38.193: INFO: 80 pods remaining
Jan  3 11:52:38.193: INFO: 80 pods has nil DeletionTimestamp
Jan  3 11:52:38.193: INFO: 
Jan  3 11:52:39.235: INFO: 70 pods remaining
Jan  3 11:52:39.235: INFO: 70 pods has nil DeletionTimestamp
Jan  3 11:52:39.241: INFO: 
Jan  3 11:52:40.205: INFO: 60 pods remaining
Jan  3 11:52:40.205: INFO: 60 pods has nil DeletionTimestamp
Jan  3 11:52:40.205: INFO: 
Jan  3 11:52:41.192: INFO: 40 pods remaining
Jan  3 11:52:41.192: INFO: 40 pods has nil DeletionTimestamp
Jan  3 11:52:41.192: INFO: 
Jan  3 11:52:42.196: INFO: 30 pods remaining
Jan  3 11:52:42.196: INFO: 29 pods has nil DeletionTimestamp
Jan  3 11:52:42.196: INFO: 
Jan  3 11:52:43.202: INFO: 20 pods remaining
Jan  3 11:52:43.203: INFO: 20 pods has nil DeletionTimestamp
Jan  3 11:52:43.203: INFO: 
STEP: Gathering metrics 01/03/24 11:52:44.167
W0103 11:52:44.217016      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  3 11:52:44.217: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  3 11:52:44.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3165" for this suite. 01/03/24 11:52:44.236
------------------------------
• [SLOW TEST] [12.342 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:52:31.92
    Jan  3 11:52:31.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename gc 01/03/24 11:52:31.922
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:31.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:32.009
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/03/24 11:52:32.057
    STEP: delete the rc 01/03/24 11:52:37.102
    STEP: wait for the rc to be deleted 01/03/24 11:52:37.124
    Jan  3 11:52:38.193: INFO: 80 pods remaining
    Jan  3 11:52:38.193: INFO: 80 pods has nil DeletionTimestamp
    Jan  3 11:52:38.193: INFO: 
    Jan  3 11:52:39.235: INFO: 70 pods remaining
    Jan  3 11:52:39.235: INFO: 70 pods has nil DeletionTimestamp
    Jan  3 11:52:39.241: INFO: 
    Jan  3 11:52:40.205: INFO: 60 pods remaining
    Jan  3 11:52:40.205: INFO: 60 pods has nil DeletionTimestamp
    Jan  3 11:52:40.205: INFO: 
    Jan  3 11:52:41.192: INFO: 40 pods remaining
    Jan  3 11:52:41.192: INFO: 40 pods has nil DeletionTimestamp
    Jan  3 11:52:41.192: INFO: 
    Jan  3 11:52:42.196: INFO: 30 pods remaining
    Jan  3 11:52:42.196: INFO: 29 pods has nil DeletionTimestamp
    Jan  3 11:52:42.196: INFO: 
    Jan  3 11:52:43.202: INFO: 20 pods remaining
    Jan  3 11:52:43.203: INFO: 20 pods has nil DeletionTimestamp
    Jan  3 11:52:43.203: INFO: 
    STEP: Gathering metrics 01/03/24 11:52:44.167
    W0103 11:52:44.217016      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  3 11:52:44.217: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:52:44.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3165" for this suite. 01/03/24 11:52:44.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:52:44.264
Jan  3 11:52:44.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename sched-preemption 01/03/24 11:52:44.267
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:44.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:44.36
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan  3 11:52:44.459: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  3 11:53:44.615: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:53:44.633
Jan  3 11:53:44.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename sched-preemption-path 01/03/24 11:53:44.637
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:53:44.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:53:44.724
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Jan  3 11:53:44.830: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan  3 11:53:44.851: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jan  3 11:53:44.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 11:53:45.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-738" for this suite. 01/03/24 11:53:45.193
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3801" for this suite. 01/03/24 11:53:45.22
------------------------------
• [SLOW TEST] [60.980 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:52:44.264
    Jan  3 11:52:44.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename sched-preemption 01/03/24 11:52:44.267
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:52:44.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:52:44.36
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan  3 11:52:44.459: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  3 11:53:44.615: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:53:44.633
    Jan  3 11:53:44.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename sched-preemption-path 01/03/24 11:53:44.637
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:53:44.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:53:44.724
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Jan  3 11:53:44.830: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan  3 11:53:44.851: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:53:44.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:53:45.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-738" for this suite. 01/03/24 11:53:45.193
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3801" for this suite. 01/03/24 11:53:45.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:53:45.246
Jan  3 11:53:45.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 11:53:45.248
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:53:45.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:53:45.34
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 11:53:45.419
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 11:53:45.854
STEP: Deploying the webhook pod 01/03/24 11:53:45.882
STEP: Wait for the deployment to be ready 01/03/24 11:53:45.923
Jan  3 11:53:45.963: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/24 11:53:48.028
STEP: Verifying the service has paired with the endpoint 01/03/24 11:53:48.059
Jan  3 11:53:49.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 01/03/24 11:53:49.079
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/03/24 11:53:49.235
STEP: Creating a configMap that should not be mutated 01/03/24 11:53:49.258
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/03/24 11:53:49.301
STEP: Creating a configMap that should be mutated 01/03/24 11:53:49.326
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 11:53:49.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1998" for this suite. 01/03/24 11:53:49.631
STEP: Destroying namespace "webhook-1998-markers" for this suite. 01/03/24 11:53:49.653
------------------------------
• [4.432 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:53:45.246
    Jan  3 11:53:45.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 11:53:45.248
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:53:45.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:53:45.34
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 11:53:45.419
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 11:53:45.854
    STEP: Deploying the webhook pod 01/03/24 11:53:45.882
    STEP: Wait for the deployment to be ready 01/03/24 11:53:45.923
    Jan  3 11:53:45.963: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/24 11:53:48.028
    STEP: Verifying the service has paired with the endpoint 01/03/24 11:53:48.059
    Jan  3 11:53:49.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 01/03/24 11:53:49.079
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/03/24 11:53:49.235
    STEP: Creating a configMap that should not be mutated 01/03/24 11:53:49.258
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/03/24 11:53:49.301
    STEP: Creating a configMap that should be mutated 01/03/24 11:53:49.326
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:53:49.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1998" for this suite. 01/03/24 11:53:49.631
    STEP: Destroying namespace "webhook-1998-markers" for this suite. 01/03/24 11:53:49.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:53:49.681
Jan  3 11:53:49.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename podtemplate 01/03/24 11:53:49.684
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:53:49.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:53:49.775
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan  3 11:53:49.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8635" for this suite. 01/03/24 11:53:49.975
------------------------------
• [0.320 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:53:49.681
    Jan  3 11:53:49.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename podtemplate 01/03/24 11:53:49.684
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:53:49.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:53:49.775
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:53:49.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8635" for this suite. 01/03/24 11:53:49.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:53:50.01
Jan  3 11:53:50.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 11:53:50.012
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:53:50.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:53:50.139
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-4db76034-8a6b-4e67-b54d-2edd77a3e6fd 01/03/24 11:53:50.167
STEP: Creating a pod to test consume configMaps 01/03/24 11:53:50.187
Jan  3 11:53:50.214: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18" in namespace "projected-928" to be "Succeeded or Failed"
Jan  3 11:53:50.245: INFO: Pod "pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18": Phase="Pending", Reason="", readiness=false. Elapsed: 30.561574ms
Jan  3 11:53:52.279: INFO: Pod "pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063894071s
Jan  3 11:53:54.266: INFO: Pod "pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051681883s
STEP: Saw pod success 01/03/24 11:53:54.266
Jan  3 11:53:54.267: INFO: Pod "pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18" satisfied condition "Succeeded or Failed"
Jan  3 11:53:54.291: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18 container agnhost-container: <nil>
STEP: delete the pod 01/03/24 11:53:54.457
Jan  3 11:53:54.492: INFO: Waiting for pod pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18 to disappear
Jan  3 11:53:54.511: INFO: Pod pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  3 11:53:54.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-928" for this suite. 01/03/24 11:53:54.545
------------------------------
• [4.562 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:53:50.01
    Jan  3 11:53:50.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 11:53:50.012
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:53:50.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:53:50.139
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-4db76034-8a6b-4e67-b54d-2edd77a3e6fd 01/03/24 11:53:50.167
    STEP: Creating a pod to test consume configMaps 01/03/24 11:53:50.187
    Jan  3 11:53:50.214: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18" in namespace "projected-928" to be "Succeeded or Failed"
    Jan  3 11:53:50.245: INFO: Pod "pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18": Phase="Pending", Reason="", readiness=false. Elapsed: 30.561574ms
    Jan  3 11:53:52.279: INFO: Pod "pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063894071s
    Jan  3 11:53:54.266: INFO: Pod "pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051681883s
    STEP: Saw pod success 01/03/24 11:53:54.266
    Jan  3 11:53:54.267: INFO: Pod "pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18" satisfied condition "Succeeded or Failed"
    Jan  3 11:53:54.291: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18 container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 11:53:54.457
    Jan  3 11:53:54.492: INFO: Waiting for pod pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18 to disappear
    Jan  3 11:53:54.511: INFO: Pod pod-projected-configmaps-a820d21f-1290-4e63-86bb-8eec90b1bd18 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:53:54.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-928" for this suite. 01/03/24 11:53:54.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:53:54.576
Jan  3 11:53:54.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 11:53:54.579
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:53:54.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:53:54.663
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 01/03/24 11:53:54.692
Jan  3 11:53:54.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6769 create -f -'
Jan  3 11:53:54.989: INFO: stderr: ""
Jan  3 11:53:54.989: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/03/24 11:53:54.989
Jan  3 11:53:56.014: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 11:53:56.015: INFO: Found 0 / 1
Jan  3 11:53:57.015: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 11:53:57.015: INFO: Found 1 / 1
Jan  3 11:53:57.015: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/03/24 11:53:57.015
Jan  3 11:53:57.050: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 11:53:57.050: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  3 11:53:57.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6769 patch pod agnhost-primary-5z4mn -p {"metadata":{"annotations":{"x":"y"}}}'
Jan  3 11:53:57.260: INFO: stderr: ""
Jan  3 11:53:57.261: INFO: stdout: "pod/agnhost-primary-5z4mn patched\n"
STEP: checking annotations 01/03/24 11:53:57.261
Jan  3 11:53:57.280: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 11:53:57.280: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 11:53:57.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6769" for this suite. 01/03/24 11:53:57.311
------------------------------
• [2.763 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:53:54.576
    Jan  3 11:53:54.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 11:53:54.579
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:53:54.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:53:54.663
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 01/03/24 11:53:54.692
    Jan  3 11:53:54.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6769 create -f -'
    Jan  3 11:53:54.989: INFO: stderr: ""
    Jan  3 11:53:54.989: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/03/24 11:53:54.989
    Jan  3 11:53:56.014: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 11:53:56.015: INFO: Found 0 / 1
    Jan  3 11:53:57.015: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 11:53:57.015: INFO: Found 1 / 1
    Jan  3 11:53:57.015: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/03/24 11:53:57.015
    Jan  3 11:53:57.050: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 11:53:57.050: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  3 11:53:57.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6769 patch pod agnhost-primary-5z4mn -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan  3 11:53:57.260: INFO: stderr: ""
    Jan  3 11:53:57.261: INFO: stdout: "pod/agnhost-primary-5z4mn patched\n"
    STEP: checking annotations 01/03/24 11:53:57.261
    Jan  3 11:53:57.280: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 11:53:57.280: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:53:57.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6769" for this suite. 01/03/24 11:53:57.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:53:57.344
Jan  3 11:53:57.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 11:53:57.347
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:53:57.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:53:57.43
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8287 01/03/24 11:53:57.458
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/03/24 11:53:57.495
STEP: creating service externalsvc in namespace services-8287 01/03/24 11:53:57.496
STEP: creating replication controller externalsvc in namespace services-8287 01/03/24 11:53:57.521
I0103 11:53:57.543303      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8287, replica count: 2
I0103 11:54:00.593855      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/03/24 11:54:00.61
Jan  3 11:54:00.680: INFO: Creating new exec pod
Jan  3 11:54:00.709: INFO: Waiting up to 5m0s for pod "execpodgxwq6" in namespace "services-8287" to be "running"
Jan  3 11:54:00.727: INFO: Pod "execpodgxwq6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.242783ms
Jan  3 11:54:02.749: INFO: Pod "execpodgxwq6": Phase="Running", Reason="", readiness=true. Elapsed: 2.040266928s
Jan  3 11:54:02.749: INFO: Pod "execpodgxwq6" satisfied condition "running"
Jan  3 11:54:02.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-8287 exec execpodgxwq6 -- /bin/sh -x -c nslookup nodeport-service.services-8287.svc.cluster.local'
Jan  3 11:54:03.255: INFO: stderr: "+ nslookup nodeport-service.services-8287.svc.cluster.local\n"
Jan  3 11:54:03.256: INFO: stdout: "Server:\t\t10.233.26.106\nAddress:\t10.233.26.106#53\n\nnodeport-service.services-8287.svc.cluster.local\tcanonical name = externalsvc.services-8287.svc.cluster.local.\nName:\texternalsvc.services-8287.svc.cluster.local\nAddress: 10.233.31.192\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8287, will wait for the garbage collector to delete the pods 01/03/24 11:54:03.256
Jan  3 11:54:03.356: INFO: Deleting ReplicationController externalsvc took: 28.483352ms
Jan  3 11:54:03.457: INFO: Terminating ReplicationController externalsvc pods took: 100.918933ms
Jan  3 11:54:06.594: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 11:54:06.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8287" for this suite. 01/03/24 11:54:06.655
------------------------------
• [SLOW TEST] [9.339 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:53:57.344
    Jan  3 11:53:57.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 11:53:57.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:53:57.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:53:57.43
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-8287 01/03/24 11:53:57.458
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/03/24 11:53:57.495
    STEP: creating service externalsvc in namespace services-8287 01/03/24 11:53:57.496
    STEP: creating replication controller externalsvc in namespace services-8287 01/03/24 11:53:57.521
    I0103 11:53:57.543303      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8287, replica count: 2
    I0103 11:54:00.593855      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/03/24 11:54:00.61
    Jan  3 11:54:00.680: INFO: Creating new exec pod
    Jan  3 11:54:00.709: INFO: Waiting up to 5m0s for pod "execpodgxwq6" in namespace "services-8287" to be "running"
    Jan  3 11:54:00.727: INFO: Pod "execpodgxwq6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.242783ms
    Jan  3 11:54:02.749: INFO: Pod "execpodgxwq6": Phase="Running", Reason="", readiness=true. Elapsed: 2.040266928s
    Jan  3 11:54:02.749: INFO: Pod "execpodgxwq6" satisfied condition "running"
    Jan  3 11:54:02.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-8287 exec execpodgxwq6 -- /bin/sh -x -c nslookup nodeport-service.services-8287.svc.cluster.local'
    Jan  3 11:54:03.255: INFO: stderr: "+ nslookup nodeport-service.services-8287.svc.cluster.local\n"
    Jan  3 11:54:03.256: INFO: stdout: "Server:\t\t10.233.26.106\nAddress:\t10.233.26.106#53\n\nnodeport-service.services-8287.svc.cluster.local\tcanonical name = externalsvc.services-8287.svc.cluster.local.\nName:\texternalsvc.services-8287.svc.cluster.local\nAddress: 10.233.31.192\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8287, will wait for the garbage collector to delete the pods 01/03/24 11:54:03.256
    Jan  3 11:54:03.356: INFO: Deleting ReplicationController externalsvc took: 28.483352ms
    Jan  3 11:54:03.457: INFO: Terminating ReplicationController externalsvc pods took: 100.918933ms
    Jan  3 11:54:06.594: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:54:06.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8287" for this suite. 01/03/24 11:54:06.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:54:06.683
Jan  3 11:54:06.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename cronjob 01/03/24 11:54:06.684
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:54:06.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:54:06.788
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/03/24 11:54:06.816
STEP: Ensuring more than one job is running at a time 01/03/24 11:54:06.844
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/03/24 11:56:00.874
STEP: Removing cronjob 01/03/24 11:56:00.892
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  3 11:56:00.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2971" for this suite. 01/03/24 11:56:00.95
------------------------------
• [SLOW TEST] [114.292 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:54:06.683
    Jan  3 11:54:06.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename cronjob 01/03/24 11:54:06.684
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:54:06.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:54:06.788
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/03/24 11:54:06.816
    STEP: Ensuring more than one job is running at a time 01/03/24 11:54:06.844
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/03/24 11:56:00.874
    STEP: Removing cronjob 01/03/24 11:56:00.892
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:56:00.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2971" for this suite. 01/03/24 11:56:00.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:56:00.98
Jan  3 11:56:00.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename secrets 01/03/24 11:56:00.982
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:56:01.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:56:01.072
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-5379107f-6479-4df2-ac04-e10cb3f5b22e 01/03/24 11:56:01.1
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  3 11:56:01.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7247" for this suite. 01/03/24 11:56:01.136
------------------------------
• [0.193 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:56:00.98
    Jan  3 11:56:00.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename secrets 01/03/24 11:56:00.982
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:56:01.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:56:01.072
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-5379107f-6479-4df2-ac04-e10cb3f5b22e 01/03/24 11:56:01.1
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:56:01.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7247" for this suite. 01/03/24 11:56:01.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:56:01.175
Jan  3 11:56:01.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 11:56:01.177
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:56:01.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:56:01.259
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jan  3 11:56:01.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/03/24 11:56:03.92
Jan  3 11:56:03.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-1643 --namespace=crd-publish-openapi-1643 create -f -'
Jan  3 11:56:05.399: INFO: stderr: ""
Jan  3 11:56:05.399: INFO: stdout: "e2e-test-crd-publish-openapi-1349-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan  3 11:56:05.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-1643 --namespace=crd-publish-openapi-1643 delete e2e-test-crd-publish-openapi-1349-crds test-cr'
Jan  3 11:56:05.629: INFO: stderr: ""
Jan  3 11:56:05.629: INFO: stdout: "e2e-test-crd-publish-openapi-1349-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan  3 11:56:05.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-1643 --namespace=crd-publish-openapi-1643 apply -f -'
Jan  3 11:56:05.976: INFO: stderr: ""
Jan  3 11:56:05.976: INFO: stdout: "e2e-test-crd-publish-openapi-1349-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan  3 11:56:05.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-1643 --namespace=crd-publish-openapi-1643 delete e2e-test-crd-publish-openapi-1349-crds test-cr'
Jan  3 11:56:06.139: INFO: stderr: ""
Jan  3 11:56:06.139: INFO: stdout: "e2e-test-crd-publish-openapi-1349-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/03/24 11:56:06.139
Jan  3 11:56:06.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-1643 explain e2e-test-crd-publish-openapi-1349-crds'
Jan  3 11:56:07.088: INFO: stderr: ""
Jan  3 11:56:07.088: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1349-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 11:56:09.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1643" for this suite. 01/03/24 11:56:09.92
------------------------------
• [SLOW TEST] [8.769 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:56:01.175
    Jan  3 11:56:01.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 11:56:01.177
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:56:01.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:56:01.259
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jan  3 11:56:01.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/03/24 11:56:03.92
    Jan  3 11:56:03.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-1643 --namespace=crd-publish-openapi-1643 create -f -'
    Jan  3 11:56:05.399: INFO: stderr: ""
    Jan  3 11:56:05.399: INFO: stdout: "e2e-test-crd-publish-openapi-1349-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan  3 11:56:05.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-1643 --namespace=crd-publish-openapi-1643 delete e2e-test-crd-publish-openapi-1349-crds test-cr'
    Jan  3 11:56:05.629: INFO: stderr: ""
    Jan  3 11:56:05.629: INFO: stdout: "e2e-test-crd-publish-openapi-1349-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan  3 11:56:05.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-1643 --namespace=crd-publish-openapi-1643 apply -f -'
    Jan  3 11:56:05.976: INFO: stderr: ""
    Jan  3 11:56:05.976: INFO: stdout: "e2e-test-crd-publish-openapi-1349-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan  3 11:56:05.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-1643 --namespace=crd-publish-openapi-1643 delete e2e-test-crd-publish-openapi-1349-crds test-cr'
    Jan  3 11:56:06.139: INFO: stderr: ""
    Jan  3 11:56:06.139: INFO: stdout: "e2e-test-crd-publish-openapi-1349-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/03/24 11:56:06.139
    Jan  3 11:56:06.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-1643 explain e2e-test-crd-publish-openapi-1349-crds'
    Jan  3 11:56:07.088: INFO: stderr: ""
    Jan  3 11:56:07.088: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1349-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:56:09.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1643" for this suite. 01/03/24 11:56:09.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:56:09.947
Jan  3 11:56:09.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 11:56:09.949
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:56:09.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:56:10.025
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 01/03/24 11:56:10.053
Jan  3 11:56:10.108: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671" in namespace "downward-api-9296" to be "Succeeded or Failed"
Jan  3 11:56:10.133: INFO: Pod "downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671": Phase="Pending", Reason="", readiness=false. Elapsed: 25.245361ms
Jan  3 11:56:12.152: INFO: Pod "downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671": Phase="Running", Reason="", readiness=true. Elapsed: 2.044562639s
Jan  3 11:56:14.155: INFO: Pod "downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671": Phase="Running", Reason="", readiness=false. Elapsed: 4.046743926s
Jan  3 11:56:16.154: INFO: Pod "downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046307223s
STEP: Saw pod success 01/03/24 11:56:16.154
Jan  3 11:56:16.155: INFO: Pod "downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671" satisfied condition "Succeeded or Failed"
Jan  3 11:56:16.181: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671 container client-container: <nil>
STEP: delete the pod 01/03/24 11:56:16.321
Jan  3 11:56:16.356: INFO: Waiting for pod downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671 to disappear
Jan  3 11:56:16.376: INFO: Pod downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  3 11:56:16.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9296" for this suite. 01/03/24 11:56:16.406
------------------------------
• [SLOW TEST] [6.485 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:56:09.947
    Jan  3 11:56:09.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 11:56:09.949
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:56:09.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:56:10.025
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 01/03/24 11:56:10.053
    Jan  3 11:56:10.108: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671" in namespace "downward-api-9296" to be "Succeeded or Failed"
    Jan  3 11:56:10.133: INFO: Pod "downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671": Phase="Pending", Reason="", readiness=false. Elapsed: 25.245361ms
    Jan  3 11:56:12.152: INFO: Pod "downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671": Phase="Running", Reason="", readiness=true. Elapsed: 2.044562639s
    Jan  3 11:56:14.155: INFO: Pod "downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671": Phase="Running", Reason="", readiness=false. Elapsed: 4.046743926s
    Jan  3 11:56:16.154: INFO: Pod "downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046307223s
    STEP: Saw pod success 01/03/24 11:56:16.154
    Jan  3 11:56:16.155: INFO: Pod "downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671" satisfied condition "Succeeded or Failed"
    Jan  3 11:56:16.181: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671 container client-container: <nil>
    STEP: delete the pod 01/03/24 11:56:16.321
    Jan  3 11:56:16.356: INFO: Waiting for pod downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671 to disappear
    Jan  3 11:56:16.376: INFO: Pod downwardapi-volume-f5adcc86-95b4-4cb0-8bc7-1536f726f671 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:56:16.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9296" for this suite. 01/03/24 11:56:16.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:56:16.435
Jan  3 11:56:16.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pods 01/03/24 11:56:16.437
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:56:16.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:56:16.521
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/03/24 11:56:16.548
STEP: submitting the pod to kubernetes 01/03/24 11:56:16.548
STEP: verifying QOS class is set on the pod 01/03/24 11:56:16.574
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jan  3 11:56:16.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2076" for this suite. 01/03/24 11:56:16.612
------------------------------
• [0.202 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:56:16.435
    Jan  3 11:56:16.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pods 01/03/24 11:56:16.437
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:56:16.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:56:16.521
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/03/24 11:56:16.548
    STEP: submitting the pod to kubernetes 01/03/24 11:56:16.548
    STEP: verifying QOS class is set on the pod 01/03/24 11:56:16.574
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:56:16.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2076" for this suite. 01/03/24 11:56:16.612
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:56:16.642
Jan  3 11:56:16.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename var-expansion 01/03/24 11:56:16.644
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:56:16.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:56:16.724
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 01/03/24 11:56:16.753
Jan  3 11:56:16.782: INFO: Waiting up to 2m0s for pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03" in namespace "var-expansion-1968" to be "running"
Jan  3 11:56:16.805: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 23.487892ms
Jan  3 11:56:18.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043361761s
Jan  3 11:56:20.824: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042416533s
Jan  3 11:56:22.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04424736s
Jan  3 11:56:24.828: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 8.046229231s
Jan  3 11:56:26.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042995777s
Jan  3 11:56:28.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 12.043230184s
Jan  3 11:56:30.827: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 14.045464702s
Jan  3 11:56:32.824: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 16.042234154s
Jan  3 11:56:34.824: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 18.042280021s
Jan  3 11:56:36.827: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 20.045183873s
Jan  3 11:56:38.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 22.042991146s
Jan  3 11:56:40.828: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 24.046684372s
Jan  3 11:56:42.824: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 26.042687669s
Jan  3 11:56:44.830: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 28.048303065s
Jan  3 11:56:46.827: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 30.044757859s
Jan  3 11:56:48.827: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 32.044921694s
Jan  3 11:56:50.829: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 34.046742822s
Jan  3 11:56:52.829: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 36.047592738s
Jan  3 11:56:54.830: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 38.048410791s
Jan  3 11:56:56.831: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 40.049234129s
Jan  3 11:56:58.827: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 42.044778293s
Jan  3 11:57:00.827: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 44.045639192s
Jan  3 11:57:02.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 46.043877681s
Jan  3 11:57:04.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 48.043429961s
Jan  3 11:57:06.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 50.044469547s
Jan  3 11:57:08.824: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 52.042535612s
Jan  3 11:57:10.829: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 54.04728518s
Jan  3 11:57:12.833: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 56.050976583s
Jan  3 11:57:14.828: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 58.045924529s
Jan  3 11:57:16.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.042981547s
Jan  3 11:57:18.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.044088514s
Jan  3 11:57:20.828: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.046061988s
Jan  3 11:57:22.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.044088078s
Jan  3 11:57:24.831: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.049102477s
Jan  3 11:57:26.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.042882317s
Jan  3 11:57:28.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.044203031s
Jan  3 11:57:30.833: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.051077059s
Jan  3 11:57:32.833: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.05165192s
Jan  3 11:57:34.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.043742977s
Jan  3 11:57:36.829: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.047449057s
Jan  3 11:57:38.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.043758189s
Jan  3 11:57:40.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.044631966s
Jan  3 11:57:42.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.042702869s
Jan  3 11:57:44.828: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.045797931s
Jan  3 11:57:46.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.042964629s
Jan  3 11:57:48.830: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.047668949s
Jan  3 11:57:50.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.043305105s
Jan  3 11:57:52.828: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.046688162s
Jan  3 11:57:54.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.044432828s
Jan  3 11:57:56.827: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.044769061s
Jan  3 11:57:58.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.04388363s
Jan  3 11:58:00.851: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.068804065s
Jan  3 11:58:02.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.043778238s
Jan  3 11:58:04.829: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.04689143s
Jan  3 11:58:06.829: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.047332007s
Jan  3 11:58:08.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.044661267s
Jan  3 11:58:10.828: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.045989296s
Jan  3 11:58:12.848: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.066264787s
Jan  3 11:58:14.833: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.051679549s
Jan  3 11:58:16.824: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.041760624s
Jan  3 11:58:16.846: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.063715671s
STEP: updating the pod 01/03/24 11:58:16.846
Jan  3 11:58:17.423: INFO: Successfully updated pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03"
STEP: waiting for pod running 01/03/24 11:58:17.423
Jan  3 11:58:17.423: INFO: Waiting up to 2m0s for pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03" in namespace "var-expansion-1968" to be "running"
Jan  3 11:58:17.441: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 17.559611ms
Jan  3 11:58:19.461: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Running", Reason="", readiness=true. Elapsed: 2.037303928s
Jan  3 11:58:19.461: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03" satisfied condition "running"
STEP: deleting the pod gracefully 01/03/24 11:58:19.461
Jan  3 11:58:19.461: INFO: Deleting pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03" in namespace "var-expansion-1968"
Jan  3 11:58:19.488: INFO: Wait up to 5m0s for pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  3 11:58:51.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1968" for this suite. 01/03/24 11:58:51.557
------------------------------
• [SLOW TEST] [154.941 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:56:16.642
    Jan  3 11:56:16.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename var-expansion 01/03/24 11:56:16.644
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:56:16.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:56:16.724
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 01/03/24 11:56:16.753
    Jan  3 11:56:16.782: INFO: Waiting up to 2m0s for pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03" in namespace "var-expansion-1968" to be "running"
    Jan  3 11:56:16.805: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 23.487892ms
    Jan  3 11:56:18.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043361761s
    Jan  3 11:56:20.824: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042416533s
    Jan  3 11:56:22.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04424736s
    Jan  3 11:56:24.828: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 8.046229231s
    Jan  3 11:56:26.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042995777s
    Jan  3 11:56:28.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 12.043230184s
    Jan  3 11:56:30.827: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 14.045464702s
    Jan  3 11:56:32.824: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 16.042234154s
    Jan  3 11:56:34.824: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 18.042280021s
    Jan  3 11:56:36.827: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 20.045183873s
    Jan  3 11:56:38.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 22.042991146s
    Jan  3 11:56:40.828: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 24.046684372s
    Jan  3 11:56:42.824: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 26.042687669s
    Jan  3 11:56:44.830: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 28.048303065s
    Jan  3 11:56:46.827: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 30.044757859s
    Jan  3 11:56:48.827: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 32.044921694s
    Jan  3 11:56:50.829: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 34.046742822s
    Jan  3 11:56:52.829: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 36.047592738s
    Jan  3 11:56:54.830: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 38.048410791s
    Jan  3 11:56:56.831: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 40.049234129s
    Jan  3 11:56:58.827: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 42.044778293s
    Jan  3 11:57:00.827: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 44.045639192s
    Jan  3 11:57:02.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 46.043877681s
    Jan  3 11:57:04.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 48.043429961s
    Jan  3 11:57:06.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 50.044469547s
    Jan  3 11:57:08.824: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 52.042535612s
    Jan  3 11:57:10.829: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 54.04728518s
    Jan  3 11:57:12.833: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 56.050976583s
    Jan  3 11:57:14.828: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 58.045924529s
    Jan  3 11:57:16.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.042981547s
    Jan  3 11:57:18.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.044088514s
    Jan  3 11:57:20.828: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.046061988s
    Jan  3 11:57:22.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.044088078s
    Jan  3 11:57:24.831: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.049102477s
    Jan  3 11:57:26.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.042882317s
    Jan  3 11:57:28.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.044203031s
    Jan  3 11:57:30.833: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.051077059s
    Jan  3 11:57:32.833: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.05165192s
    Jan  3 11:57:34.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.043742977s
    Jan  3 11:57:36.829: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.047449057s
    Jan  3 11:57:38.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.043758189s
    Jan  3 11:57:40.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.044631966s
    Jan  3 11:57:42.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.042702869s
    Jan  3 11:57:44.828: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.045797931s
    Jan  3 11:57:46.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.042964629s
    Jan  3 11:57:48.830: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.047668949s
    Jan  3 11:57:50.825: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.043305105s
    Jan  3 11:57:52.828: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.046688162s
    Jan  3 11:57:54.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.044432828s
    Jan  3 11:57:56.827: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.044769061s
    Jan  3 11:57:58.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.04388363s
    Jan  3 11:58:00.851: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.068804065s
    Jan  3 11:58:02.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.043778238s
    Jan  3 11:58:04.829: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.04689143s
    Jan  3 11:58:06.829: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.047332007s
    Jan  3 11:58:08.826: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.044661267s
    Jan  3 11:58:10.828: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.045989296s
    Jan  3 11:58:12.848: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.066264787s
    Jan  3 11:58:14.833: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.051679549s
    Jan  3 11:58:16.824: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.041760624s
    Jan  3 11:58:16.846: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.063715671s
    STEP: updating the pod 01/03/24 11:58:16.846
    Jan  3 11:58:17.423: INFO: Successfully updated pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03"
    STEP: waiting for pod running 01/03/24 11:58:17.423
    Jan  3 11:58:17.423: INFO: Waiting up to 2m0s for pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03" in namespace "var-expansion-1968" to be "running"
    Jan  3 11:58:17.441: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Pending", Reason="", readiness=false. Elapsed: 17.559611ms
    Jan  3 11:58:19.461: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03": Phase="Running", Reason="", readiness=true. Elapsed: 2.037303928s
    Jan  3 11:58:19.461: INFO: Pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03" satisfied condition "running"
    STEP: deleting the pod gracefully 01/03/24 11:58:19.461
    Jan  3 11:58:19.461: INFO: Deleting pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03" in namespace "var-expansion-1968"
    Jan  3 11:58:19.488: INFO: Wait up to 5m0s for pod "var-expansion-193394d8-e63c-448e-b7d3-2ea681657f03" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:58:51.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1968" for this suite. 01/03/24 11:58:51.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:58:51.589
Jan  3 11:58:51.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 11:58:51.593
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:58:51.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:58:51.672
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-45641191-c0cb-498f-92ae-c1799b21a60c 01/03/24 11:58:51.698
STEP: Creating a pod to test consume secrets 01/03/24 11:58:51.717
Jan  3 11:58:51.742: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4" in namespace "projected-3993" to be "Succeeded or Failed"
Jan  3 11:58:51.765: INFO: Pod "pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.55712ms
Jan  3 11:58:53.788: INFO: Pod "pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04547213s
Jan  3 11:58:55.785: INFO: Pod "pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042538032s
STEP: Saw pod success 01/03/24 11:58:55.785
Jan  3 11:58:55.786: INFO: Pod "pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4" satisfied condition "Succeeded or Failed"
Jan  3 11:58:55.804: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/03/24 11:58:55.955
Jan  3 11:58:55.993: INFO: Waiting for pod pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4 to disappear
Jan  3 11:58:56.010: INFO: Pod pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  3 11:58:56.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3993" for this suite. 01/03/24 11:58:56.041
------------------------------
• [4.478 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:58:51.589
    Jan  3 11:58:51.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 11:58:51.593
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:58:51.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:58:51.672
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-45641191-c0cb-498f-92ae-c1799b21a60c 01/03/24 11:58:51.698
    STEP: Creating a pod to test consume secrets 01/03/24 11:58:51.717
    Jan  3 11:58:51.742: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4" in namespace "projected-3993" to be "Succeeded or Failed"
    Jan  3 11:58:51.765: INFO: Pod "pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.55712ms
    Jan  3 11:58:53.788: INFO: Pod "pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04547213s
    Jan  3 11:58:55.785: INFO: Pod "pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042538032s
    STEP: Saw pod success 01/03/24 11:58:55.785
    Jan  3 11:58:55.786: INFO: Pod "pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4" satisfied condition "Succeeded or Failed"
    Jan  3 11:58:55.804: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/03/24 11:58:55.955
    Jan  3 11:58:55.993: INFO: Waiting for pod pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4 to disappear
    Jan  3 11:58:56.010: INFO: Pod pod-projected-secrets-bb29d6ce-20f7-4e99-8a3b-65224c2ae2c4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:58:56.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3993" for this suite. 01/03/24 11:58:56.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:58:56.071
Jan  3 11:58:56.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename containers 01/03/24 11:58:56.073
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:58:56.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:58:56.161
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Jan  3 11:58:56.214: INFO: Waiting up to 5m0s for pod "client-containers-bbde03b3-34ec-4891-84dd-7f5065c90875" in namespace "containers-3266" to be "running"
Jan  3 11:58:56.232: INFO: Pod "client-containers-bbde03b3-34ec-4891-84dd-7f5065c90875": Phase="Pending", Reason="", readiness=false. Elapsed: 18.207135ms
Jan  3 11:58:58.258: INFO: Pod "client-containers-bbde03b3-34ec-4891-84dd-7f5065c90875": Phase="Running", Reason="", readiness=true. Elapsed: 2.043929595s
Jan  3 11:58:58.258: INFO: Pod "client-containers-bbde03b3-34ec-4891-84dd-7f5065c90875" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan  3 11:58:58.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3266" for this suite. 01/03/24 11:58:58.329
------------------------------
• [2.307 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:58:56.071
    Jan  3 11:58:56.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename containers 01/03/24 11:58:56.073
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:58:56.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:58:56.161
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Jan  3 11:58:56.214: INFO: Waiting up to 5m0s for pod "client-containers-bbde03b3-34ec-4891-84dd-7f5065c90875" in namespace "containers-3266" to be "running"
    Jan  3 11:58:56.232: INFO: Pod "client-containers-bbde03b3-34ec-4891-84dd-7f5065c90875": Phase="Pending", Reason="", readiness=false. Elapsed: 18.207135ms
    Jan  3 11:58:58.258: INFO: Pod "client-containers-bbde03b3-34ec-4891-84dd-7f5065c90875": Phase="Running", Reason="", readiness=true. Elapsed: 2.043929595s
    Jan  3 11:58:58.258: INFO: Pod "client-containers-bbde03b3-34ec-4891-84dd-7f5065c90875" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:58:58.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3266" for this suite. 01/03/24 11:58:58.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:58:58.382
Jan  3 11:58:58.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pod-network-test 01/03/24 11:58:58.385
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:58:58.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:58:58.468
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-1865 01/03/24 11:58:58.495
STEP: creating a selector 01/03/24 11:58:58.495
STEP: Creating the service pods in kubernetes 01/03/24 11:58:58.496
Jan  3 11:58:58.496: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  3 11:58:58.601: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1865" to be "running and ready"
Jan  3 11:58:58.619: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.90973ms
Jan  3 11:58:58.619: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 11:59:00.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.037273591s
Jan  3 11:59:00.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:59:02.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.041199913s
Jan  3 11:59:02.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:59:04.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.038394428s
Jan  3 11:59:04.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:59:06.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.041038304s
Jan  3 11:59:06.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:59:08.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.039065205s
Jan  3 11:59:08.641: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:59:10.641: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.03998056s
Jan  3 11:59:10.641: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:59:12.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.03906119s
Jan  3 11:59:12.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:59:14.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.041664851s
Jan  3 11:59:14.643: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:59:16.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.040460996s
Jan  3 11:59:16.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:59:18.641: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.039609195s
Jan  3 11:59:18.641: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 11:59:20.648: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.046401556s
Jan  3 11:59:20.648: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  3 11:59:20.648: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  3 11:59:20.667: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1865" to be "running and ready"
Jan  3 11:59:20.686: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 18.851786ms
Jan  3 11:59:20.686: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  3 11:59:20.686: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  3 11:59:20.705: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1865" to be "running and ready"
Jan  3 11:59:20.722: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 17.525392ms
Jan  3 11:59:20.723: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  3 11:59:20.723: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/03/24 11:59:20.741
Jan  3 11:59:20.761: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1865" to be "running"
Jan  3 11:59:20.781: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.320502ms
Jan  3 11:59:22.804: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.04312246s
Jan  3 11:59:22.804: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  3 11:59:22.822: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan  3 11:59:22.822: INFO: Breadth first check of 10.221.146.75 on host 185.132.46.116...
Jan  3 11:59:22.841: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.221.146.76:9080/dial?request=hostname&protocol=udp&host=10.221.146.75&port=8081&tries=1'] Namespace:pod-network-test-1865 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 11:59:22.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 11:59:22.842: INFO: ExecWithOptions: Clientset creation
Jan  3 11:59:22.843: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1865/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.221.146.76%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.221.146.75%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  3 11:59:23.168: INFO: Waiting for responses: map[]
Jan  3 11:59:23.168: INFO: reached 10.221.146.75 after 0/1 tries
Jan  3 11:59:23.168: INFO: Breadth first check of 10.222.238.234 on host 85.215.162.129...
Jan  3 11:59:23.186: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.221.146.76:9080/dial?request=hostname&protocol=udp&host=10.222.238.234&port=8081&tries=1'] Namespace:pod-network-test-1865 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 11:59:23.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 11:59:23.188: INFO: ExecWithOptions: Clientset creation
Jan  3 11:59:23.188: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1865/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.221.146.76%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.222.238.234%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  3 11:59:23.492: INFO: Waiting for responses: map[]
Jan  3 11:59:23.492: INFO: reached 10.222.238.234 after 0/1 tries
Jan  3 11:59:23.492: INFO: Breadth first check of 10.221.146.162 on host 85.215.218.90...
Jan  3 11:59:23.511: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.221.146.76:9080/dial?request=hostname&protocol=udp&host=10.221.146.162&port=8081&tries=1'] Namespace:pod-network-test-1865 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 11:59:23.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 11:59:23.512: INFO: ExecWithOptions: Clientset creation
Jan  3 11:59:23.512: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1865/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.221.146.76%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.221.146.162%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  3 11:59:23.830: INFO: Waiting for responses: map[]
Jan  3 11:59:23.830: INFO: reached 10.221.146.162 after 0/1 tries
Jan  3 11:59:23.831: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan  3 11:59:23.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1865" for this suite. 01/03/24 11:59:23.862
------------------------------
• [SLOW TEST] [25.505 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:58:58.382
    Jan  3 11:58:58.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pod-network-test 01/03/24 11:58:58.385
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:58:58.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:58:58.468
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-1865 01/03/24 11:58:58.495
    STEP: creating a selector 01/03/24 11:58:58.495
    STEP: Creating the service pods in kubernetes 01/03/24 11:58:58.496
    Jan  3 11:58:58.496: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  3 11:58:58.601: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1865" to be "running and ready"
    Jan  3 11:58:58.619: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.90973ms
    Jan  3 11:58:58.619: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 11:59:00.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.037273591s
    Jan  3 11:59:00.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:59:02.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.041199913s
    Jan  3 11:59:02.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:59:04.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.038394428s
    Jan  3 11:59:04.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:59:06.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.041038304s
    Jan  3 11:59:06.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:59:08.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.039065205s
    Jan  3 11:59:08.641: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:59:10.641: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.03998056s
    Jan  3 11:59:10.641: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:59:12.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.03906119s
    Jan  3 11:59:12.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:59:14.643: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.041664851s
    Jan  3 11:59:14.643: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:59:16.642: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.040460996s
    Jan  3 11:59:16.642: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:59:18.641: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.039609195s
    Jan  3 11:59:18.641: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 11:59:20.648: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.046401556s
    Jan  3 11:59:20.648: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  3 11:59:20.648: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  3 11:59:20.667: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1865" to be "running and ready"
    Jan  3 11:59:20.686: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 18.851786ms
    Jan  3 11:59:20.686: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  3 11:59:20.686: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  3 11:59:20.705: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1865" to be "running and ready"
    Jan  3 11:59:20.722: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 17.525392ms
    Jan  3 11:59:20.723: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  3 11:59:20.723: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/03/24 11:59:20.741
    Jan  3 11:59:20.761: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1865" to be "running"
    Jan  3 11:59:20.781: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.320502ms
    Jan  3 11:59:22.804: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.04312246s
    Jan  3 11:59:22.804: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  3 11:59:22.822: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan  3 11:59:22.822: INFO: Breadth first check of 10.221.146.75 on host 185.132.46.116...
    Jan  3 11:59:22.841: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.221.146.76:9080/dial?request=hostname&protocol=udp&host=10.221.146.75&port=8081&tries=1'] Namespace:pod-network-test-1865 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 11:59:22.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 11:59:22.842: INFO: ExecWithOptions: Clientset creation
    Jan  3 11:59:22.843: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1865/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.221.146.76%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.221.146.75%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  3 11:59:23.168: INFO: Waiting for responses: map[]
    Jan  3 11:59:23.168: INFO: reached 10.221.146.75 after 0/1 tries
    Jan  3 11:59:23.168: INFO: Breadth first check of 10.222.238.234 on host 85.215.162.129...
    Jan  3 11:59:23.186: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.221.146.76:9080/dial?request=hostname&protocol=udp&host=10.222.238.234&port=8081&tries=1'] Namespace:pod-network-test-1865 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 11:59:23.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 11:59:23.188: INFO: ExecWithOptions: Clientset creation
    Jan  3 11:59:23.188: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1865/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.221.146.76%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.222.238.234%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  3 11:59:23.492: INFO: Waiting for responses: map[]
    Jan  3 11:59:23.492: INFO: reached 10.222.238.234 after 0/1 tries
    Jan  3 11:59:23.492: INFO: Breadth first check of 10.221.146.162 on host 85.215.218.90...
    Jan  3 11:59:23.511: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.221.146.76:9080/dial?request=hostname&protocol=udp&host=10.221.146.162&port=8081&tries=1'] Namespace:pod-network-test-1865 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 11:59:23.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 11:59:23.512: INFO: ExecWithOptions: Clientset creation
    Jan  3 11:59:23.512: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1865/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.221.146.76%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.221.146.162%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  3 11:59:23.830: INFO: Waiting for responses: map[]
    Jan  3 11:59:23.830: INFO: reached 10.221.146.162 after 0/1 tries
    Jan  3 11:59:23.831: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:59:23.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1865" for this suite. 01/03/24 11:59:23.862
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:59:23.888
Jan  3 11:59:23.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename controllerrevisions 01/03/24 11:59:23.89
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:59:23.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:59:23.971
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-z2nrf-daemon-set" 01/03/24 11:59:24.144
STEP: Check that daemon pods launch on every node of the cluster. 01/03/24 11:59:24.164
Jan  3 11:59:24.201: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 0
Jan  3 11:59:24.201: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 11:59:25.255: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 0
Jan  3 11:59:25.255: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 11:59:26.254: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
Jan  3 11:59:26.254: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
Jan  3 11:59:27.256: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
Jan  3 11:59:27.256: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
Jan  3 11:59:28.268: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
Jan  3 11:59:28.269: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
Jan  3 11:59:29.251: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
Jan  3 11:59:29.251: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
Jan  3 11:59:30.251: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
Jan  3 11:59:30.251: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
Jan  3 11:59:31.255: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
Jan  3 11:59:31.255: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
Jan  3 11:59:32.263: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
Jan  3 11:59:32.264: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
Jan  3 11:59:33.253: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
Jan  3 11:59:33.253: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
Jan  3 11:59:34.258: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
Jan  3 11:59:34.258: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
Jan  3 11:59:35.256: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
Jan  3 11:59:35.256: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
Jan  3 11:59:36.254: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 3
Jan  3 11:59:36.254: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-z2nrf-daemon-set
STEP: Confirm DaemonSet "e2e-z2nrf-daemon-set" successfully created with "daemonset-name=e2e-z2nrf-daemon-set" label 01/03/24 11:59:36.271
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-z2nrf-daemon-set" 01/03/24 11:59:36.314
Jan  3 11:59:36.334: INFO: Located ControllerRevision: "e2e-z2nrf-daemon-set-8f5465575"
STEP: Patching ControllerRevision "e2e-z2nrf-daemon-set-8f5465575" 01/03/24 11:59:36.352
Jan  3 11:59:36.373: INFO: e2e-z2nrf-daemon-set-8f5465575 has been patched
STEP: Create a new ControllerRevision 01/03/24 11:59:36.373
Jan  3 11:59:36.393: INFO: Created ControllerRevision: e2e-z2nrf-daemon-set-5cdbd889f
STEP: Confirm that there are two ControllerRevisions 01/03/24 11:59:36.393
Jan  3 11:59:36.393: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  3 11:59:36.411: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-z2nrf-daemon-set-8f5465575" 01/03/24 11:59:36.411
STEP: Confirm that there is only one ControllerRevision 01/03/24 11:59:36.435
Jan  3 11:59:36.436: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  3 11:59:36.454: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-z2nrf-daemon-set-5cdbd889f" 01/03/24 11:59:36.472
Jan  3 11:59:36.526: INFO: e2e-z2nrf-daemon-set-5cdbd889f has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/03/24 11:59:36.526
W0103 11:59:36.553423      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/03/24 11:59:36.553
Jan  3 11:59:36.554: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  3 11:59:37.581: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  3 11:59:37.616: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-z2nrf-daemon-set-5cdbd889f=updated" 01/03/24 11:59:37.616
STEP: Confirm that there is only one ControllerRevision 01/03/24 11:59:37.648
Jan  3 11:59:37.649: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  3 11:59:37.668: INFO: Found 1 ControllerRevisions
Jan  3 11:59:37.685: INFO: ControllerRevision "e2e-z2nrf-daemon-set-75d6765446" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-z2nrf-daemon-set" 01/03/24 11:59:37.702
STEP: deleting DaemonSet.extensions e2e-z2nrf-daemon-set in namespace controllerrevisions-3912, will wait for the garbage collector to delete the pods 01/03/24 11:59:37.702
Jan  3 11:59:37.796: INFO: Deleting DaemonSet.extensions e2e-z2nrf-daemon-set took: 23.047142ms
Jan  3 11:59:37.897: INFO: Terminating DaemonSet.extensions e2e-z2nrf-daemon-set pods took: 100.970947ms
Jan  3 11:59:50.518: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 0
Jan  3 11:59:50.519: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-z2nrf-daemon-set
Jan  3 11:59:50.537: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33980849994"},"items":null}

Jan  3 11:59:50.557: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33980849994"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 11:59:50.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-3912" for this suite. 01/03/24 11:59:50.663
------------------------------
• [SLOW TEST] [26.802 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:59:23.888
    Jan  3 11:59:23.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename controllerrevisions 01/03/24 11:59:23.89
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:59:23.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:59:23.971
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-z2nrf-daemon-set" 01/03/24 11:59:24.144
    STEP: Check that daemon pods launch on every node of the cluster. 01/03/24 11:59:24.164
    Jan  3 11:59:24.201: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 0
    Jan  3 11:59:24.201: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 11:59:25.255: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 0
    Jan  3 11:59:25.255: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 11:59:26.254: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
    Jan  3 11:59:26.254: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
    Jan  3 11:59:27.256: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
    Jan  3 11:59:27.256: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
    Jan  3 11:59:28.268: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
    Jan  3 11:59:28.269: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
    Jan  3 11:59:29.251: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
    Jan  3 11:59:29.251: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
    Jan  3 11:59:30.251: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
    Jan  3 11:59:30.251: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
    Jan  3 11:59:31.255: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
    Jan  3 11:59:31.255: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
    Jan  3 11:59:32.263: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
    Jan  3 11:59:32.264: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
    Jan  3 11:59:33.253: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
    Jan  3 11:59:33.253: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
    Jan  3 11:59:34.258: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
    Jan  3 11:59:34.258: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
    Jan  3 11:59:35.256: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 2
    Jan  3 11:59:35.256: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
    Jan  3 11:59:36.254: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 3
    Jan  3 11:59:36.254: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-z2nrf-daemon-set
    STEP: Confirm DaemonSet "e2e-z2nrf-daemon-set" successfully created with "daemonset-name=e2e-z2nrf-daemon-set" label 01/03/24 11:59:36.271
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-z2nrf-daemon-set" 01/03/24 11:59:36.314
    Jan  3 11:59:36.334: INFO: Located ControllerRevision: "e2e-z2nrf-daemon-set-8f5465575"
    STEP: Patching ControllerRevision "e2e-z2nrf-daemon-set-8f5465575" 01/03/24 11:59:36.352
    Jan  3 11:59:36.373: INFO: e2e-z2nrf-daemon-set-8f5465575 has been patched
    STEP: Create a new ControllerRevision 01/03/24 11:59:36.373
    Jan  3 11:59:36.393: INFO: Created ControllerRevision: e2e-z2nrf-daemon-set-5cdbd889f
    STEP: Confirm that there are two ControllerRevisions 01/03/24 11:59:36.393
    Jan  3 11:59:36.393: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  3 11:59:36.411: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-z2nrf-daemon-set-8f5465575" 01/03/24 11:59:36.411
    STEP: Confirm that there is only one ControllerRevision 01/03/24 11:59:36.435
    Jan  3 11:59:36.436: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  3 11:59:36.454: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-z2nrf-daemon-set-5cdbd889f" 01/03/24 11:59:36.472
    Jan  3 11:59:36.526: INFO: e2e-z2nrf-daemon-set-5cdbd889f has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/03/24 11:59:36.526
    W0103 11:59:36.553423      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/03/24 11:59:36.553
    Jan  3 11:59:36.554: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  3 11:59:37.581: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  3 11:59:37.616: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-z2nrf-daemon-set-5cdbd889f=updated" 01/03/24 11:59:37.616
    STEP: Confirm that there is only one ControllerRevision 01/03/24 11:59:37.648
    Jan  3 11:59:37.649: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  3 11:59:37.668: INFO: Found 1 ControllerRevisions
    Jan  3 11:59:37.685: INFO: ControllerRevision "e2e-z2nrf-daemon-set-75d6765446" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-z2nrf-daemon-set" 01/03/24 11:59:37.702
    STEP: deleting DaemonSet.extensions e2e-z2nrf-daemon-set in namespace controllerrevisions-3912, will wait for the garbage collector to delete the pods 01/03/24 11:59:37.702
    Jan  3 11:59:37.796: INFO: Deleting DaemonSet.extensions e2e-z2nrf-daemon-set took: 23.047142ms
    Jan  3 11:59:37.897: INFO: Terminating DaemonSet.extensions e2e-z2nrf-daemon-set pods took: 100.970947ms
    Jan  3 11:59:50.518: INFO: Number of nodes with available pods controlled by daemonset e2e-z2nrf-daemon-set: 0
    Jan  3 11:59:50.519: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-z2nrf-daemon-set
    Jan  3 11:59:50.537: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33980849994"},"items":null}

    Jan  3 11:59:50.557: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33980849994"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:59:50.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-3912" for this suite. 01/03/24 11:59:50.663
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:59:50.691
Jan  3 11:59:50.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 11:59:50.694
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:59:50.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:59:50.77
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 01/03/24 11:59:50.797
Jan  3 11:59:50.825: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0" in namespace "projected-71" to be "Succeeded or Failed"
Jan  3 11:59:50.855: INFO: Pod "downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0": Phase="Pending", Reason="", readiness=false. Elapsed: 29.96861ms
Jan  3 11:59:52.884: INFO: Pod "downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058332635s
Jan  3 11:59:54.887: INFO: Pod "downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061423462s
STEP: Saw pod success 01/03/24 11:59:54.887
Jan  3 11:59:54.887: INFO: Pod "downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0" satisfied condition "Succeeded or Failed"
Jan  3 11:59:54.906: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0 container client-container: <nil>
STEP: delete the pod 01/03/24 11:59:54.949
Jan  3 11:59:54.998: INFO: Waiting for pod downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0 to disappear
Jan  3 11:59:55.025: INFO: Pod downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  3 11:59:55.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-71" for this suite. 01/03/24 11:59:55.056
------------------------------
• [4.390 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:59:50.691
    Jan  3 11:59:50.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 11:59:50.694
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:59:50.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:59:50.77
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 01/03/24 11:59:50.797
    Jan  3 11:59:50.825: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0" in namespace "projected-71" to be "Succeeded or Failed"
    Jan  3 11:59:50.855: INFO: Pod "downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0": Phase="Pending", Reason="", readiness=false. Elapsed: 29.96861ms
    Jan  3 11:59:52.884: INFO: Pod "downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058332635s
    Jan  3 11:59:54.887: INFO: Pod "downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061423462s
    STEP: Saw pod success 01/03/24 11:59:54.887
    Jan  3 11:59:54.887: INFO: Pod "downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0" satisfied condition "Succeeded or Failed"
    Jan  3 11:59:54.906: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0 container client-container: <nil>
    STEP: delete the pod 01/03/24 11:59:54.949
    Jan  3 11:59:54.998: INFO: Waiting for pod downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0 to disappear
    Jan  3 11:59:55.025: INFO: Pod downwardapi-volume-7f5aff45-4fac-4973-bf7e-af8836adbad0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  3 11:59:55.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-71" for this suite. 01/03/24 11:59:55.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 11:59:55.083
Jan  3 11:59:55.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename security-context-test 01/03/24 11:59:55.085
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:59:55.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:59:55.164
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Jan  3 11:59:55.224: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-fc34399d-77ed-4477-9c13-b9d171366b02" in namespace "security-context-test-303" to be "Succeeded or Failed"
Jan  3 11:59:55.243: INFO: Pod "alpine-nnp-false-fc34399d-77ed-4477-9c13-b9d171366b02": Phase="Pending", Reason="", readiness=false. Elapsed: 18.991854ms
Jan  3 11:59:57.265: INFO: Pod "alpine-nnp-false-fc34399d-77ed-4477-9c13-b9d171366b02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040299458s
Jan  3 11:59:59.262: INFO: Pod "alpine-nnp-false-fc34399d-77ed-4477-9c13-b9d171366b02": Phase="Running", Reason="", readiness=true. Elapsed: 4.038019169s
Jan  3 12:00:01.270: INFO: Pod "alpine-nnp-false-fc34399d-77ed-4477-9c13-b9d171366b02": Phase="Running", Reason="", readiness=false. Elapsed: 6.045375816s
Jan  3 12:00:03.266: INFO: Pod "alpine-nnp-false-fc34399d-77ed-4477-9c13-b9d171366b02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.04193247s
Jan  3 12:00:03.266: INFO: Pod "alpine-nnp-false-fc34399d-77ed-4477-9c13-b9d171366b02" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  3 12:00:03.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-303" for this suite. 01/03/24 12:00:03.338
------------------------------
• [SLOW TEST] [8.281 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 11:59:55.083
    Jan  3 11:59:55.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename security-context-test 01/03/24 11:59:55.085
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 11:59:55.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 11:59:55.164
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Jan  3 11:59:55.224: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-fc34399d-77ed-4477-9c13-b9d171366b02" in namespace "security-context-test-303" to be "Succeeded or Failed"
    Jan  3 11:59:55.243: INFO: Pod "alpine-nnp-false-fc34399d-77ed-4477-9c13-b9d171366b02": Phase="Pending", Reason="", readiness=false. Elapsed: 18.991854ms
    Jan  3 11:59:57.265: INFO: Pod "alpine-nnp-false-fc34399d-77ed-4477-9c13-b9d171366b02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040299458s
    Jan  3 11:59:59.262: INFO: Pod "alpine-nnp-false-fc34399d-77ed-4477-9c13-b9d171366b02": Phase="Running", Reason="", readiness=true. Elapsed: 4.038019169s
    Jan  3 12:00:01.270: INFO: Pod "alpine-nnp-false-fc34399d-77ed-4477-9c13-b9d171366b02": Phase="Running", Reason="", readiness=false. Elapsed: 6.045375816s
    Jan  3 12:00:03.266: INFO: Pod "alpine-nnp-false-fc34399d-77ed-4477-9c13-b9d171366b02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.04193247s
    Jan  3 12:00:03.266: INFO: Pod "alpine-nnp-false-fc34399d-77ed-4477-9c13-b9d171366b02" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:00:03.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-303" for this suite. 01/03/24 12:00:03.338
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:00:03.365
Jan  3 12:00:03.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 12:00:03.367
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:03.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:03.451
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 01/03/24 12:00:03.478
Jan  3 12:00:03.518: INFO: Waiting up to 5m0s for pod "downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a" in namespace "downward-api-2658" to be "Succeeded or Failed"
Jan  3 12:00:03.536: INFO: Pod "downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.835654ms
Jan  3 12:00:05.558: INFO: Pod "downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040148432s
Jan  3 12:00:07.556: INFO: Pod "downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037958514s
Jan  3 12:00:09.556: INFO: Pod "downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037608911s
STEP: Saw pod success 01/03/24 12:00:09.556
Jan  3 12:00:09.556: INFO: Pod "downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a" satisfied condition "Succeeded or Failed"
Jan  3 12:00:09.574: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a container client-container: <nil>
STEP: delete the pod 01/03/24 12:00:09.614
Jan  3 12:00:09.668: INFO: Waiting for pod downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a to disappear
Jan  3 12:00:09.687: INFO: Pod downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  3 12:00:09.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2658" for this suite. 01/03/24 12:00:09.723
------------------------------
• [SLOW TEST] [6.384 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:00:03.365
    Jan  3 12:00:03.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 12:00:03.367
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:03.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:03.451
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 01/03/24 12:00:03.478
    Jan  3 12:00:03.518: INFO: Waiting up to 5m0s for pod "downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a" in namespace "downward-api-2658" to be "Succeeded or Failed"
    Jan  3 12:00:03.536: INFO: Pod "downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.835654ms
    Jan  3 12:00:05.558: INFO: Pod "downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040148432s
    Jan  3 12:00:07.556: INFO: Pod "downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037958514s
    Jan  3 12:00:09.556: INFO: Pod "downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037608911s
    STEP: Saw pod success 01/03/24 12:00:09.556
    Jan  3 12:00:09.556: INFO: Pod "downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a" satisfied condition "Succeeded or Failed"
    Jan  3 12:00:09.574: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a container client-container: <nil>
    STEP: delete the pod 01/03/24 12:00:09.614
    Jan  3 12:00:09.668: INFO: Waiting for pod downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a to disappear
    Jan  3 12:00:09.687: INFO: Pod downwardapi-volume-791ce120-4490-4ac4-b90d-057592c8567a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:00:09.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2658" for this suite. 01/03/24 12:00:09.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:00:09.751
Jan  3 12:00:09.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename deployment 01/03/24 12:00:09.753
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:09.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:09.866
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan  3 12:00:09.949: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/03/24 12:00:09.949
Jan  3 12:00:09.949: INFO: Waiting up to 5m0s for pod "test-cleanup-controller-xd7dq" in namespace "deployment-9805" to be "running"
Jan  3 12:00:09.966: INFO: Pod "test-cleanup-controller-xd7dq": Phase="Pending", Reason="", readiness=false. Elapsed: 17.267728ms
Jan  3 12:00:11.992: INFO: Pod "test-cleanup-controller-xd7dq": Phase="Running", Reason="", readiness=true. Elapsed: 2.042739173s
Jan  3 12:00:11.992: INFO: Pod "test-cleanup-controller-xd7dq" satisfied condition "running"
Jan  3 12:00:11.992: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/03/24 12:00:12.04
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 12:00:14.134: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9805  9db19d62-caa3-4ef0-bfae-9719f7bd2407 33980853327 1 2024-01-03 12:00:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2024-01-03 12:00:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:00:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033b0a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-03 12:00:12 +0000 UTC,LastTransitionTime:2024-01-03 12:00:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2024-01-03 12:00:13 +0000 UTC,LastTransitionTime:2024-01-03 12:00:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  3 12:00:14.152: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-9805  414bc7e4-9ced-4c74-8adc-551de608427b 33980853303 1 2024-01-03 12:00:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 9db19d62-caa3-4ef0-bfae-9719f7bd2407 0xc0033b0de7 0xc0033b0de8}] [] [{kube-controller-manager Update apps/v1 2024-01-03 12:00:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9db19d62-caa3-4ef0-bfae-9719f7bd2407\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:00:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033b0e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  3 12:00:14.174: INFO: Pod "test-cleanup-deployment-7698ff6f6b-fqpnq" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-fqpnq test-cleanup-deployment-7698ff6f6b- deployment-9805  5869a346-c04b-4459-b387-369c62a34871 33980853301 0 2024-01-03 12:00:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[cni.projectcalico.org/containerID:6a7a1cc27591c51c48f68c85273c19b3f826cc428aade60cf1f9c059304b0091 cni.projectcalico.org/podIP:10.221.146.81/32 cni.projectcalico.org/podIPs:10.221.146.81/32] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 414bc7e4-9ced-4c74-8adc-551de608427b 0xc0033b1557 0xc0033b1558}] [] [{calico Update v1 2024-01-03 12:00:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-03 12:00:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"414bc7e4-9ced-4c74-8adc-551de608427b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:00:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6f5zh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6f5zh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:00:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:00:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:00:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:00:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.81,StartTime:2024-01-03 12:00:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:00:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://7e283981fbec1290360ae133080a9ad2ac523fb120c3e70dad560afd0d23e6e9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  3 12:00:14.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9805" for this suite. 01/03/24 12:00:14.205
------------------------------
• [4.479 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:00:09.751
    Jan  3 12:00:09.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename deployment 01/03/24 12:00:09.753
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:09.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:09.866
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan  3 12:00:09.949: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/03/24 12:00:09.949
    Jan  3 12:00:09.949: INFO: Waiting up to 5m0s for pod "test-cleanup-controller-xd7dq" in namespace "deployment-9805" to be "running"
    Jan  3 12:00:09.966: INFO: Pod "test-cleanup-controller-xd7dq": Phase="Pending", Reason="", readiness=false. Elapsed: 17.267728ms
    Jan  3 12:00:11.992: INFO: Pod "test-cleanup-controller-xd7dq": Phase="Running", Reason="", readiness=true. Elapsed: 2.042739173s
    Jan  3 12:00:11.992: INFO: Pod "test-cleanup-controller-xd7dq" satisfied condition "running"
    Jan  3 12:00:11.992: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/03/24 12:00:12.04
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 12:00:14.134: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9805  9db19d62-caa3-4ef0-bfae-9719f7bd2407 33980853327 1 2024-01-03 12:00:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2024-01-03 12:00:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:00:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033b0a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-03 12:00:12 +0000 UTC,LastTransitionTime:2024-01-03 12:00:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2024-01-03 12:00:13 +0000 UTC,LastTransitionTime:2024-01-03 12:00:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  3 12:00:14.152: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-9805  414bc7e4-9ced-4c74-8adc-551de608427b 33980853303 1 2024-01-03 12:00:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 9db19d62-caa3-4ef0-bfae-9719f7bd2407 0xc0033b0de7 0xc0033b0de8}] [] [{kube-controller-manager Update apps/v1 2024-01-03 12:00:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9db19d62-caa3-4ef0-bfae-9719f7bd2407\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:00:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033b0e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 12:00:14.174: INFO: Pod "test-cleanup-deployment-7698ff6f6b-fqpnq" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-fqpnq test-cleanup-deployment-7698ff6f6b- deployment-9805  5869a346-c04b-4459-b387-369c62a34871 33980853301 0 2024-01-03 12:00:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[cni.projectcalico.org/containerID:6a7a1cc27591c51c48f68c85273c19b3f826cc428aade60cf1f9c059304b0091 cni.projectcalico.org/podIP:10.221.146.81/32 cni.projectcalico.org/podIPs:10.221.146.81/32] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 414bc7e4-9ced-4c74-8adc-551de608427b 0xc0033b1557 0xc0033b1558}] [] [{calico Update v1 2024-01-03 12:00:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2024-01-03 12:00:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"414bc7e4-9ced-4c74-8adc-551de608427b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:00:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6f5zh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6f5zh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:00:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:00:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:00:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:00:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.81,StartTime:2024-01-03 12:00:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:00:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://7e283981fbec1290360ae133080a9ad2ac523fb120c3e70dad560afd0d23e6e9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:00:14.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9805" for this suite. 01/03/24 12:00:14.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:00:14.232
Jan  3 12:00:14.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 12:00:14.235
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:14.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:14.314
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-2cb3040c-5f8a-4ff2-8fc9-2d96c493020a 01/03/24 12:00:14.341
STEP: Creating a pod to test consume configMaps 01/03/24 12:00:14.359
Jan  3 12:00:14.384: INFO: Waiting up to 5m0s for pod "pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155" in namespace "configmap-1438" to be "Succeeded or Failed"
Jan  3 12:00:14.414: INFO: Pod "pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155": Phase="Pending", Reason="", readiness=false. Elapsed: 29.86227ms
Jan  3 12:00:16.436: INFO: Pod "pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052597199s
Jan  3 12:00:18.434: INFO: Pod "pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049738348s
STEP: Saw pod success 01/03/24 12:00:18.434
Jan  3 12:00:18.434: INFO: Pod "pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155" satisfied condition "Succeeded or Failed"
Jan  3 12:00:18.456: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155 container configmap-volume-test: <nil>
STEP: delete the pod 01/03/24 12:00:18.494
Jan  3 12:00:18.525: INFO: Waiting for pod pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155 to disappear
Jan  3 12:00:18.541: INFO: Pod pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 12:00:18.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1438" for this suite. 01/03/24 12:00:18.572
------------------------------
• [4.363 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:00:14.232
    Jan  3 12:00:14.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 12:00:14.235
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:14.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:14.314
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-2cb3040c-5f8a-4ff2-8fc9-2d96c493020a 01/03/24 12:00:14.341
    STEP: Creating a pod to test consume configMaps 01/03/24 12:00:14.359
    Jan  3 12:00:14.384: INFO: Waiting up to 5m0s for pod "pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155" in namespace "configmap-1438" to be "Succeeded or Failed"
    Jan  3 12:00:14.414: INFO: Pod "pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155": Phase="Pending", Reason="", readiness=false. Elapsed: 29.86227ms
    Jan  3 12:00:16.436: INFO: Pod "pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052597199s
    Jan  3 12:00:18.434: INFO: Pod "pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049738348s
    STEP: Saw pod success 01/03/24 12:00:18.434
    Jan  3 12:00:18.434: INFO: Pod "pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155" satisfied condition "Succeeded or Failed"
    Jan  3 12:00:18.456: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155 container configmap-volume-test: <nil>
    STEP: delete the pod 01/03/24 12:00:18.494
    Jan  3 12:00:18.525: INFO: Waiting for pod pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155 to disappear
    Jan  3 12:00:18.541: INFO: Pod pod-configmaps-08268ab5-e0df-439c-9eb4-b32cd44c1155 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:00:18.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1438" for this suite. 01/03/24 12:00:18.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:00:18.599
Jan  3 12:00:18.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 12:00:18.602
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:18.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:18.679
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/03/24 12:00:18.707
Jan  3 12:00:18.749: INFO: Waiting up to 5m0s for pod "pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb" in namespace "emptydir-3228" to be "Succeeded or Failed"
Jan  3 12:00:18.776: INFO: Pod "pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb": Phase="Pending", Reason="", readiness=false. Elapsed: 26.651885ms
Jan  3 12:00:20.795: INFO: Pod "pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046269562s
Jan  3 12:00:22.797: INFO: Pod "pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04856974s
Jan  3 12:00:24.795: INFO: Pod "pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045681966s
STEP: Saw pod success 01/03/24 12:00:24.795
Jan  3 12:00:24.795: INFO: Pod "pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb" satisfied condition "Succeeded or Failed"
Jan  3 12:00:24.813: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb container test-container: <nil>
STEP: delete the pod 01/03/24 12:00:24.85
Jan  3 12:00:24.894: INFO: Waiting for pod pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb to disappear
Jan  3 12:00:24.911: INFO: Pod pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:00:24.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3228" for this suite. 01/03/24 12:00:24.942
------------------------------
• [SLOW TEST] [6.367 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:00:18.599
    Jan  3 12:00:18.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 12:00:18.602
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:18.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:18.679
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/03/24 12:00:18.707
    Jan  3 12:00:18.749: INFO: Waiting up to 5m0s for pod "pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb" in namespace "emptydir-3228" to be "Succeeded or Failed"
    Jan  3 12:00:18.776: INFO: Pod "pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb": Phase="Pending", Reason="", readiness=false. Elapsed: 26.651885ms
    Jan  3 12:00:20.795: INFO: Pod "pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046269562s
    Jan  3 12:00:22.797: INFO: Pod "pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04856974s
    Jan  3 12:00:24.795: INFO: Pod "pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045681966s
    STEP: Saw pod success 01/03/24 12:00:24.795
    Jan  3 12:00:24.795: INFO: Pod "pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb" satisfied condition "Succeeded or Failed"
    Jan  3 12:00:24.813: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb container test-container: <nil>
    STEP: delete the pod 01/03/24 12:00:24.85
    Jan  3 12:00:24.894: INFO: Waiting for pod pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb to disappear
    Jan  3 12:00:24.911: INFO: Pod pod-42ea7eee-ec8e-4b91-9c3f-824cecd01ebb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:00:24.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3228" for this suite. 01/03/24 12:00:24.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:00:24.967
Jan  3 12:00:24.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename events 01/03/24 12:00:24.97
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:25.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:25.048
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/03/24 12:00:25.075
Jan  3 12:00:25.121: INFO: created test-event-1
Jan  3 12:00:25.138: INFO: created test-event-2
Jan  3 12:00:25.155: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/03/24 12:00:25.155
STEP: delete collection of events 01/03/24 12:00:25.174
Jan  3 12:00:25.174: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/03/24 12:00:25.228
Jan  3 12:00:25.228: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan  3 12:00:25.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8024" for this suite. 01/03/24 12:00:25.263
------------------------------
• [0.319 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:00:24.967
    Jan  3 12:00:24.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename events 01/03/24 12:00:24.97
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:25.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:25.048
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/03/24 12:00:25.075
    Jan  3 12:00:25.121: INFO: created test-event-1
    Jan  3 12:00:25.138: INFO: created test-event-2
    Jan  3 12:00:25.155: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/03/24 12:00:25.155
    STEP: delete collection of events 01/03/24 12:00:25.174
    Jan  3 12:00:25.174: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/03/24 12:00:25.228
    Jan  3 12:00:25.228: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:00:25.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8024" for this suite. 01/03/24 12:00:25.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:00:25.289
Jan  3 12:00:25.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename containers 01/03/24 12:00:25.291
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:25.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:25.369
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 01/03/24 12:00:25.395
Jan  3 12:00:25.423: INFO: Waiting up to 5m0s for pod "client-containers-8b237e90-0242-4118-8605-66ed2946c6ae" in namespace "containers-5443" to be "Succeeded or Failed"
Jan  3 12:00:25.442: INFO: Pod "client-containers-8b237e90-0242-4118-8605-66ed2946c6ae": Phase="Pending", Reason="", readiness=false. Elapsed: 19.009509ms
Jan  3 12:00:27.463: INFO: Pod "client-containers-8b237e90-0242-4118-8605-66ed2946c6ae": Phase="Running", Reason="", readiness=true. Elapsed: 2.039995061s
Jan  3 12:00:29.461: INFO: Pod "client-containers-8b237e90-0242-4118-8605-66ed2946c6ae": Phase="Running", Reason="", readiness=false. Elapsed: 4.037694378s
Jan  3 12:00:31.460: INFO: Pod "client-containers-8b237e90-0242-4118-8605-66ed2946c6ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036747327s
STEP: Saw pod success 01/03/24 12:00:31.46
Jan  3 12:00:31.460: INFO: Pod "client-containers-8b237e90-0242-4118-8605-66ed2946c6ae" satisfied condition "Succeeded or Failed"
Jan  3 12:00:31.478: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod client-containers-8b237e90-0242-4118-8605-66ed2946c6ae container agnhost-container: <nil>
STEP: delete the pod 01/03/24 12:00:31.516
Jan  3 12:00:31.551: INFO: Waiting for pod client-containers-8b237e90-0242-4118-8605-66ed2946c6ae to disappear
Jan  3 12:00:31.572: INFO: Pod client-containers-8b237e90-0242-4118-8605-66ed2946c6ae no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan  3 12:00:31.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5443" for this suite. 01/03/24 12:00:31.605
------------------------------
• [SLOW TEST] [6.343 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:00:25.289
    Jan  3 12:00:25.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename containers 01/03/24 12:00:25.291
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:25.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:25.369
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 01/03/24 12:00:25.395
    Jan  3 12:00:25.423: INFO: Waiting up to 5m0s for pod "client-containers-8b237e90-0242-4118-8605-66ed2946c6ae" in namespace "containers-5443" to be "Succeeded or Failed"
    Jan  3 12:00:25.442: INFO: Pod "client-containers-8b237e90-0242-4118-8605-66ed2946c6ae": Phase="Pending", Reason="", readiness=false. Elapsed: 19.009509ms
    Jan  3 12:00:27.463: INFO: Pod "client-containers-8b237e90-0242-4118-8605-66ed2946c6ae": Phase="Running", Reason="", readiness=true. Elapsed: 2.039995061s
    Jan  3 12:00:29.461: INFO: Pod "client-containers-8b237e90-0242-4118-8605-66ed2946c6ae": Phase="Running", Reason="", readiness=false. Elapsed: 4.037694378s
    Jan  3 12:00:31.460: INFO: Pod "client-containers-8b237e90-0242-4118-8605-66ed2946c6ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036747327s
    STEP: Saw pod success 01/03/24 12:00:31.46
    Jan  3 12:00:31.460: INFO: Pod "client-containers-8b237e90-0242-4118-8605-66ed2946c6ae" satisfied condition "Succeeded or Failed"
    Jan  3 12:00:31.478: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod client-containers-8b237e90-0242-4118-8605-66ed2946c6ae container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 12:00:31.516
    Jan  3 12:00:31.551: INFO: Waiting for pod client-containers-8b237e90-0242-4118-8605-66ed2946c6ae to disappear
    Jan  3 12:00:31.572: INFO: Pod client-containers-8b237e90-0242-4118-8605-66ed2946c6ae no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:00:31.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5443" for this suite. 01/03/24 12:00:31.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:00:31.637
Jan  3 12:00:31.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename daemonsets 01/03/24 12:00:31.639
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:31.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:31.724
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
Jan  3 12:00:31.846: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/03/24 12:00:31.866
Jan  3 12:00:31.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:00:31.884: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/03/24 12:00:31.884
Jan  3 12:00:31.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:00:31.970: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:00:32.993: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:00:32.993: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:00:34.000: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  3 12:00:34.000: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/03/24 12:00:34.021
Jan  3 12:00:34.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:00:34.086: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/03/24 12:00:34.086
Jan  3 12:00:34.125: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:00:34.125: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:00:35.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:00:35.146: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:00:36.145: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:00:36.145: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:00:37.144: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:00:37.144: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:00:38.147: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:00:38.147: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:00:39.156: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:00:39.156: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:00:40.156: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  3 12:00:40.156: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 01/03/24 12:00:40.192
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4409, will wait for the garbage collector to delete the pods 01/03/24 12:00:40.193
Jan  3 12:00:40.285: INFO: Deleting DaemonSet.extensions daemon-set took: 24.641478ms
Jan  3 12:00:40.386: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.11432ms
Jan  3 12:00:44.106: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:00:44.106: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  3 12:00:44.123: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33980857148"},"items":null}

Jan  3 12:00:44.140: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33980857152"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:00:44.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4409" for this suite. 01/03/24 12:00:44.273
------------------------------
• [SLOW TEST] [12.662 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:00:31.637
    Jan  3 12:00:31.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename daemonsets 01/03/24 12:00:31.639
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:31.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:31.724
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:205
    Jan  3 12:00:31.846: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/03/24 12:00:31.866
    Jan  3 12:00:31.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:00:31.884: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/03/24 12:00:31.884
    Jan  3 12:00:31.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:00:31.970: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:00:32.993: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:00:32.993: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:00:34.000: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  3 12:00:34.000: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/03/24 12:00:34.021
    Jan  3 12:00:34.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:00:34.086: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/03/24 12:00:34.086
    Jan  3 12:00:34.125: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:00:34.125: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:00:35.146: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:00:35.146: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:00:36.145: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:00:36.145: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:00:37.144: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:00:37.144: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:00:38.147: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:00:38.147: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:00:39.156: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:00:39.156: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:00:40.156: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  3 12:00:40.156: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 01/03/24 12:00:40.192
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4409, will wait for the garbage collector to delete the pods 01/03/24 12:00:40.193
    Jan  3 12:00:40.285: INFO: Deleting DaemonSet.extensions daemon-set took: 24.641478ms
    Jan  3 12:00:40.386: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.11432ms
    Jan  3 12:00:44.106: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:00:44.106: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  3 12:00:44.123: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33980857148"},"items":null}

    Jan  3 12:00:44.140: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33980857152"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:00:44.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4409" for this suite. 01/03/24 12:00:44.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:00:44.304
Jan  3 12:00:44.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:00:44.307
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:44.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:44.394
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 01/03/24 12:00:44.424
Jan  3 12:00:44.465: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40" in namespace "projected-5117" to be "Succeeded or Failed"
Jan  3 12:00:44.485: INFO: Pod "downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40": Phase="Pending", Reason="", readiness=false. Elapsed: 19.841952ms
Jan  3 12:00:46.521: INFO: Pod "downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40": Phase="Running", Reason="", readiness=true. Elapsed: 2.056629924s
Jan  3 12:00:48.505: INFO: Pod "downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40": Phase="Running", Reason="", readiness=false. Elapsed: 4.040194343s
Jan  3 12:00:50.503: INFO: Pod "downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038227537s
STEP: Saw pod success 01/03/24 12:00:50.503
Jan  3 12:00:50.504: INFO: Pod "downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40" satisfied condition "Succeeded or Failed"
Jan  3 12:00:50.522: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40 container client-container: <nil>
STEP: delete the pod 01/03/24 12:00:50.561
Jan  3 12:00:50.614: INFO: Waiting for pod downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40 to disappear
Jan  3 12:00:50.632: INFO: Pod downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  3 12:00:50.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5117" for this suite. 01/03/24 12:00:50.664
------------------------------
• [SLOW TEST] [6.386 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:00:44.304
    Jan  3 12:00:44.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:00:44.307
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:44.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:44.394
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 01/03/24 12:00:44.424
    Jan  3 12:00:44.465: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40" in namespace "projected-5117" to be "Succeeded or Failed"
    Jan  3 12:00:44.485: INFO: Pod "downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40": Phase="Pending", Reason="", readiness=false. Elapsed: 19.841952ms
    Jan  3 12:00:46.521: INFO: Pod "downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40": Phase="Running", Reason="", readiness=true. Elapsed: 2.056629924s
    Jan  3 12:00:48.505: INFO: Pod "downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40": Phase="Running", Reason="", readiness=false. Elapsed: 4.040194343s
    Jan  3 12:00:50.503: INFO: Pod "downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038227537s
    STEP: Saw pod success 01/03/24 12:00:50.503
    Jan  3 12:00:50.504: INFO: Pod "downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40" satisfied condition "Succeeded or Failed"
    Jan  3 12:00:50.522: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40 container client-container: <nil>
    STEP: delete the pod 01/03/24 12:00:50.561
    Jan  3 12:00:50.614: INFO: Waiting for pod downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40 to disappear
    Jan  3 12:00:50.632: INFO: Pod downwardapi-volume-78e362e5-418a-4a67-8690-b9874de9ca40 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:00:50.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5117" for this suite. 01/03/24 12:00:50.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:00:50.692
Jan  3 12:00:50.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename dns 01/03/24 12:00:50.694
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:50.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:50.773
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1580.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1580.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/03/24 12:00:50.8
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1580.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1580.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/03/24 12:00:50.8
STEP: creating a pod to probe /etc/hosts 01/03/24 12:00:50.8
STEP: submitting the pod to kubernetes 01/03/24 12:00:50.801
Jan  3 12:00:50.835: INFO: Waiting up to 15m0s for pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107" in namespace "dns-1580" to be "running"
Jan  3 12:00:50.854: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 18.73102ms
Jan  3 12:00:52.875: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039782983s
Jan  3 12:00:54.877: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041268697s
Jan  3 12:00:56.903: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06753974s
Jan  3 12:00:58.874: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038938291s
Jan  3 12:01:00.873: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 10.037635263s
Jan  3 12:01:02.875: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039553907s
Jan  3 12:01:04.874: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 14.03848145s
Jan  3 12:01:06.879: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 16.043863675s
Jan  3 12:01:08.875: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 18.039181393s
Jan  3 12:01:10.872: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 20.036653392s
Jan  3 12:01:12.906: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 22.070517321s
Jan  3 12:01:14.874: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 24.03849112s
Jan  3 12:01:16.879: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 26.043271403s
Jan  3 12:01:18.876: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 28.040323458s
Jan  3 12:01:20.874: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 30.038303169s
Jan  3 12:01:22.884: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 32.048497076s
Jan  3 12:01:24.875: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Running", Reason="", readiness=true. Elapsed: 34.039946301s
Jan  3 12:01:24.876: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107" satisfied condition "running"
STEP: retrieving the pod 01/03/24 12:01:24.876
STEP: looking for the results for each expected name from probers 01/03/24 12:01:24.893
Jan  3 12:01:25.097: INFO: DNS probes using dns-1580/dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107 succeeded

STEP: deleting the pod 01/03/24 12:01:25.097
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  3 12:01:25.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1580" for this suite. 01/03/24 12:01:25.166
------------------------------
• [SLOW TEST] [34.501 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:00:50.692
    Jan  3 12:00:50.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename dns 01/03/24 12:00:50.694
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:00:50.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:00:50.773
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1580.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1580.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/03/24 12:00:50.8
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1580.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1580.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/03/24 12:00:50.8
    STEP: creating a pod to probe /etc/hosts 01/03/24 12:00:50.8
    STEP: submitting the pod to kubernetes 01/03/24 12:00:50.801
    Jan  3 12:00:50.835: INFO: Waiting up to 15m0s for pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107" in namespace "dns-1580" to be "running"
    Jan  3 12:00:50.854: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 18.73102ms
    Jan  3 12:00:52.875: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039782983s
    Jan  3 12:00:54.877: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041268697s
    Jan  3 12:00:56.903: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06753974s
    Jan  3 12:00:58.874: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038938291s
    Jan  3 12:01:00.873: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 10.037635263s
    Jan  3 12:01:02.875: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039553907s
    Jan  3 12:01:04.874: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 14.03848145s
    Jan  3 12:01:06.879: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 16.043863675s
    Jan  3 12:01:08.875: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 18.039181393s
    Jan  3 12:01:10.872: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 20.036653392s
    Jan  3 12:01:12.906: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 22.070517321s
    Jan  3 12:01:14.874: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 24.03849112s
    Jan  3 12:01:16.879: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 26.043271403s
    Jan  3 12:01:18.876: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 28.040323458s
    Jan  3 12:01:20.874: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 30.038303169s
    Jan  3 12:01:22.884: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Pending", Reason="", readiness=false. Elapsed: 32.048497076s
    Jan  3 12:01:24.875: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107": Phase="Running", Reason="", readiness=true. Elapsed: 34.039946301s
    Jan  3 12:01:24.876: INFO: Pod "dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107" satisfied condition "running"
    STEP: retrieving the pod 01/03/24 12:01:24.876
    STEP: looking for the results for each expected name from probers 01/03/24 12:01:24.893
    Jan  3 12:01:25.097: INFO: DNS probes using dns-1580/dns-test-0b5509df-fbc1-46ea-aff9-e7b4873c1107 succeeded

    STEP: deleting the pod 01/03/24 12:01:25.097
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:01:25.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1580" for this suite. 01/03/24 12:01:25.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:01:25.198
Jan  3 12:01:25.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:01:25.2
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:01:25.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:01:25.28
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 01/03/24 12:01:25.307
Jan  3 12:01:25.337: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447" in namespace "projected-7756" to be "Succeeded or Failed"
Jan  3 12:01:25.360: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 23.022157ms
Jan  3 12:01:27.380: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043529981s
Jan  3 12:01:29.382: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045552495s
Jan  3 12:01:31.381: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044181641s
Jan  3 12:01:33.382: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044967792s
Jan  3 12:01:35.379: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 10.04251674s
Jan  3 12:01:37.381: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 12.044306062s
Jan  3 12:01:39.380: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 14.04323794s
Jan  3 12:01:41.383: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.046136187s
STEP: Saw pod success 01/03/24 12:01:41.383
Jan  3 12:01:41.384: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447" satisfied condition "Succeeded or Failed"
Jan  3 12:01:41.401: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447 container client-container: <nil>
STEP: delete the pod 01/03/24 12:01:41.442
Jan  3 12:01:41.481: INFO: Waiting for pod downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447 to disappear
Jan  3 12:01:41.499: INFO: Pod downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  3 12:01:41.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7756" for this suite. 01/03/24 12:01:41.53
------------------------------
• [SLOW TEST] [16.367 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:01:25.198
    Jan  3 12:01:25.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:01:25.2
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:01:25.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:01:25.28
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 01/03/24 12:01:25.307
    Jan  3 12:01:25.337: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447" in namespace "projected-7756" to be "Succeeded or Failed"
    Jan  3 12:01:25.360: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 23.022157ms
    Jan  3 12:01:27.380: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043529981s
    Jan  3 12:01:29.382: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045552495s
    Jan  3 12:01:31.381: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044181641s
    Jan  3 12:01:33.382: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044967792s
    Jan  3 12:01:35.379: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 10.04251674s
    Jan  3 12:01:37.381: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 12.044306062s
    Jan  3 12:01:39.380: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Pending", Reason="", readiness=false. Elapsed: 14.04323794s
    Jan  3 12:01:41.383: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.046136187s
    STEP: Saw pod success 01/03/24 12:01:41.383
    Jan  3 12:01:41.384: INFO: Pod "downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447" satisfied condition "Succeeded or Failed"
    Jan  3 12:01:41.401: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447 container client-container: <nil>
    STEP: delete the pod 01/03/24 12:01:41.442
    Jan  3 12:01:41.481: INFO: Waiting for pod downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447 to disappear
    Jan  3 12:01:41.499: INFO: Pod downwardapi-volume-a7d89e90-ad29-4926-a4a9-0007c1f18447 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:01:41.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7756" for this suite. 01/03/24 12:01:41.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:01:41.571
Jan  3 12:01:41.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename daemonsets 01/03/24 12:01:41.572
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:01:41.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:01:41.663
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
STEP: Creating simple DaemonSet "daemon-set" 01/03/24 12:01:41.786
STEP: Check that daemon pods launch on every node of the cluster. 01/03/24 12:01:41.811
Jan  3 12:01:41.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:01:41.848: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:01:42.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:01:42.901: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:01:43.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  3 12:01:43.901: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:01:44.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 12:01:44.901: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/03/24 12:01:44.918
Jan  3 12:01:45.025: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  3 12:01:45.025: INFO: Node jb-1-26-np-adtwo5cmi2 is running 0 daemon pod, expected 1
Jan  3 12:01:46.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  3 12:01:46.075: INFO: Node jb-1-26-np-adtwo5cmi2 is running 0 daemon pod, expected 1
Jan  3 12:01:47.083: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  3 12:01:47.083: INFO: Node jb-1-26-np-adtwo5cmi2 is running 0 daemon pod, expected 1
Jan  3 12:01:48.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  3 12:01:48.074: INFO: Node jb-1-26-np-adtwo5cmi2 is running 0 daemon pod, expected 1
Jan  3 12:01:49.073: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  3 12:01:49.073: INFO: Node jb-1-26-np-adtwo5cmi2 is running 0 daemon pod, expected 1
Jan  3 12:01:50.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  3 12:01:50.103: INFO: Node jb-1-26-np-adtwo5cmi2 is running 0 daemon pod, expected 1
Jan  3 12:01:51.093: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 12:01:51.093: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 01/03/24 12:01:51.112
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1171, will wait for the garbage collector to delete the pods 01/03/24 12:01:51.112
Jan  3 12:01:51.207: INFO: Deleting DaemonSet.extensions daemon-set took: 26.636969ms
Jan  3 12:01:51.308: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.294718ms
Jan  3 12:01:54.230: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:01:54.230: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  3 12:01:54.246: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33980865487"},"items":null}

Jan  3 12:01:54.264: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33980865490"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:01:54.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1171" for this suite. 01/03/24 12:01:54.367
------------------------------
• [SLOW TEST] [12.820 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:01:41.571
    Jan  3 12:01:41.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename daemonsets 01/03/24 12:01:41.572
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:01:41.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:01:41.663
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:177
    STEP: Creating simple DaemonSet "daemon-set" 01/03/24 12:01:41.786
    STEP: Check that daemon pods launch on every node of the cluster. 01/03/24 12:01:41.811
    Jan  3 12:01:41.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:01:41.848: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:01:42.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:01:42.901: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:01:43.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  3 12:01:43.901: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:01:44.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 12:01:44.901: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/03/24 12:01:44.918
    Jan  3 12:01:45.025: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  3 12:01:45.025: INFO: Node jb-1-26-np-adtwo5cmi2 is running 0 daemon pod, expected 1
    Jan  3 12:01:46.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  3 12:01:46.075: INFO: Node jb-1-26-np-adtwo5cmi2 is running 0 daemon pod, expected 1
    Jan  3 12:01:47.083: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  3 12:01:47.083: INFO: Node jb-1-26-np-adtwo5cmi2 is running 0 daemon pod, expected 1
    Jan  3 12:01:48.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  3 12:01:48.074: INFO: Node jb-1-26-np-adtwo5cmi2 is running 0 daemon pod, expected 1
    Jan  3 12:01:49.073: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  3 12:01:49.073: INFO: Node jb-1-26-np-adtwo5cmi2 is running 0 daemon pod, expected 1
    Jan  3 12:01:50.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  3 12:01:50.103: INFO: Node jb-1-26-np-adtwo5cmi2 is running 0 daemon pod, expected 1
    Jan  3 12:01:51.093: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 12:01:51.093: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 01/03/24 12:01:51.112
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1171, will wait for the garbage collector to delete the pods 01/03/24 12:01:51.112
    Jan  3 12:01:51.207: INFO: Deleting DaemonSet.extensions daemon-set took: 26.636969ms
    Jan  3 12:01:51.308: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.294718ms
    Jan  3 12:01:54.230: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:01:54.230: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  3 12:01:54.246: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33980865487"},"items":null}

    Jan  3 12:01:54.264: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33980865490"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:01:54.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1171" for this suite. 01/03/24 12:01:54.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:01:54.393
Jan  3 12:01:54.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename var-expansion 01/03/24 12:01:54.396
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:01:54.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:01:54.482
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Jan  3 12:01:54.537: INFO: Waiting up to 2m0s for pod "var-expansion-d01d3cb8-afac-474b-a2a6-347987b713fa" in namespace "var-expansion-9146" to be "container 0 failed with reason CreateContainerConfigError"
Jan  3 12:01:54.559: INFO: Pod "var-expansion-d01d3cb8-afac-474b-a2a6-347987b713fa": Phase="Pending", Reason="", readiness=false. Elapsed: 22.250419ms
Jan  3 12:01:56.578: INFO: Pod "var-expansion-d01d3cb8-afac-474b-a2a6-347987b713fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041348611s
Jan  3 12:01:56.578: INFO: Pod "var-expansion-d01d3cb8-afac-474b-a2a6-347987b713fa" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan  3 12:01:56.578: INFO: Deleting pod "var-expansion-d01d3cb8-afac-474b-a2a6-347987b713fa" in namespace "var-expansion-9146"
Jan  3 12:01:56.604: INFO: Wait up to 5m0s for pod "var-expansion-d01d3cb8-afac-474b-a2a6-347987b713fa" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  3 12:02:00.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9146" for this suite. 01/03/24 12:02:00.684
------------------------------
• [SLOW TEST] [6.314 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:01:54.393
    Jan  3 12:01:54.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename var-expansion 01/03/24 12:01:54.396
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:01:54.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:01:54.482
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Jan  3 12:01:54.537: INFO: Waiting up to 2m0s for pod "var-expansion-d01d3cb8-afac-474b-a2a6-347987b713fa" in namespace "var-expansion-9146" to be "container 0 failed with reason CreateContainerConfigError"
    Jan  3 12:01:54.559: INFO: Pod "var-expansion-d01d3cb8-afac-474b-a2a6-347987b713fa": Phase="Pending", Reason="", readiness=false. Elapsed: 22.250419ms
    Jan  3 12:01:56.578: INFO: Pod "var-expansion-d01d3cb8-afac-474b-a2a6-347987b713fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041348611s
    Jan  3 12:01:56.578: INFO: Pod "var-expansion-d01d3cb8-afac-474b-a2a6-347987b713fa" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan  3 12:01:56.578: INFO: Deleting pod "var-expansion-d01d3cb8-afac-474b-a2a6-347987b713fa" in namespace "var-expansion-9146"
    Jan  3 12:01:56.604: INFO: Wait up to 5m0s for pod "var-expansion-d01d3cb8-afac-474b-a2a6-347987b713fa" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:02:00.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9146" for this suite. 01/03/24 12:02:00.684
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:02:00.708
Jan  3 12:02:00.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:02:00.711
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:02:00.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:02:00.789
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-f84d5b8f-2706-444f-b954-c558fd67e38d 01/03/24 12:02:00.817
STEP: Creating a pod to test consume secrets 01/03/24 12:02:00.834
Jan  3 12:02:00.864: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1" in namespace "projected-6788" to be "Succeeded or Failed"
Jan  3 12:02:00.883: INFO: Pod "pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.118871ms
Jan  3 12:02:02.903: INFO: Pod "pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.038651679s
Jan  3 12:02:04.904: INFO: Pod "pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1": Phase="Running", Reason="", readiness=false. Elapsed: 4.039069721s
Jan  3 12:02:06.902: INFO: Pod "pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037452493s
STEP: Saw pod success 01/03/24 12:02:06.902
Jan  3 12:02:06.902: INFO: Pod "pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1" satisfied condition "Succeeded or Failed"
Jan  3 12:02:06.923: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1 container secret-volume-test: <nil>
STEP: delete the pod 01/03/24 12:02:06.961
Jan  3 12:02:06.992: INFO: Waiting for pod pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1 to disappear
Jan  3 12:02:07.010: INFO: Pod pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  3 12:02:07.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6788" for this suite. 01/03/24 12:02:07.047
------------------------------
• [SLOW TEST] [6.368 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:02:00.708
    Jan  3 12:02:00.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:02:00.711
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:02:00.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:02:00.789
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-f84d5b8f-2706-444f-b954-c558fd67e38d 01/03/24 12:02:00.817
    STEP: Creating a pod to test consume secrets 01/03/24 12:02:00.834
    Jan  3 12:02:00.864: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1" in namespace "projected-6788" to be "Succeeded or Failed"
    Jan  3 12:02:00.883: INFO: Pod "pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.118871ms
    Jan  3 12:02:02.903: INFO: Pod "pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.038651679s
    Jan  3 12:02:04.904: INFO: Pod "pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1": Phase="Running", Reason="", readiness=false. Elapsed: 4.039069721s
    Jan  3 12:02:06.902: INFO: Pod "pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037452493s
    STEP: Saw pod success 01/03/24 12:02:06.902
    Jan  3 12:02:06.902: INFO: Pod "pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1" satisfied condition "Succeeded or Failed"
    Jan  3 12:02:06.923: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1 container secret-volume-test: <nil>
    STEP: delete the pod 01/03/24 12:02:06.961
    Jan  3 12:02:06.992: INFO: Waiting for pod pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1 to disappear
    Jan  3 12:02:07.010: INFO: Pod pod-projected-secrets-29df7a2a-8160-44c9-ae9b-a104e12453d1 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:02:07.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6788" for this suite. 01/03/24 12:02:07.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:02:07.079
Jan  3 12:02:07.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename daemonsets 01/03/24 12:02:07.082
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:02:07.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:02:07.158
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
Jan  3 12:02:07.278: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/03/24 12:02:07.297
Jan  3 12:02:07.335: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:02:07.336: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:02:08.392: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:02:08.392: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:02:09.392: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 12:02:09.392: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 01/03/24 12:02:09.47
STEP: Check that daemon pods images are updated. 01/03/24 12:02:09.511
Jan  3 12:02:09.532: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:09.532: INFO: Wrong image for pod: daemon-set-vrqm4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:10.572: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:10.572: INFO: Wrong image for pod: daemon-set-vrqm4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:11.570: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:11.570: INFO: Wrong image for pod: daemon-set-vrqm4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:12.575: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:12.575: INFO: Wrong image for pod: daemon-set-vrqm4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:13.569: INFO: Pod daemon-set-6lg4k is not available
Jan  3 12:02:13.569: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:13.569: INFO: Wrong image for pod: daemon-set-vrqm4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:14.579: INFO: Pod daemon-set-6lg4k is not available
Jan  3 12:02:14.579: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:14.579: INFO: Wrong image for pod: daemon-set-vrqm4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:15.574: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:16.581: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:17.570: INFO: Pod daemon-set-8qpt4 is not available
Jan  3 12:02:17.570: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:18.569: INFO: Pod daemon-set-8qpt4 is not available
Jan  3 12:02:18.569: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  3 12:02:21.569: INFO: Pod daemon-set-wqflt is not available
STEP: Check that daemon pods are still running on every node of the cluster. 01/03/24 12:02:21.609
Jan  3 12:02:21.651: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  3 12:02:21.651: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
Jan  3 12:02:22.705: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  3 12:02:22.705: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
Jan  3 12:02:23.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 12:02:23.702: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 01/03/24 12:02:23.8
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6501, will wait for the garbage collector to delete the pods 01/03/24 12:02:23.8
Jan  3 12:02:23.897: INFO: Deleting DaemonSet.extensions daemon-set took: 25.3511ms
Jan  3 12:02:23.998: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.110154ms
Jan  3 12:02:27.122: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:02:27.123: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  3 12:02:27.141: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33980869603"},"items":null}

Jan  3 12:02:27.159: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33980869605"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:02:27.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6501" for this suite. 01/03/24 12:02:27.273
------------------------------
• [SLOW TEST] [20.217 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:02:07.079
    Jan  3 12:02:07.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename daemonsets 01/03/24 12:02:07.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:02:07.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:02:07.158
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:385
    Jan  3 12:02:07.278: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/03/24 12:02:07.297
    Jan  3 12:02:07.335: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:02:07.336: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:02:08.392: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:02:08.392: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:02:09.392: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 12:02:09.392: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 01/03/24 12:02:09.47
    STEP: Check that daemon pods images are updated. 01/03/24 12:02:09.511
    Jan  3 12:02:09.532: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:09.532: INFO: Wrong image for pod: daemon-set-vrqm4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:10.572: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:10.572: INFO: Wrong image for pod: daemon-set-vrqm4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:11.570: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:11.570: INFO: Wrong image for pod: daemon-set-vrqm4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:12.575: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:12.575: INFO: Wrong image for pod: daemon-set-vrqm4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:13.569: INFO: Pod daemon-set-6lg4k is not available
    Jan  3 12:02:13.569: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:13.569: INFO: Wrong image for pod: daemon-set-vrqm4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:14.579: INFO: Pod daemon-set-6lg4k is not available
    Jan  3 12:02:14.579: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:14.579: INFO: Wrong image for pod: daemon-set-vrqm4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:15.574: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:16.581: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:17.570: INFO: Pod daemon-set-8qpt4 is not available
    Jan  3 12:02:17.570: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:18.569: INFO: Pod daemon-set-8qpt4 is not available
    Jan  3 12:02:18.569: INFO: Wrong image for pod: daemon-set-q2vhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  3 12:02:21.569: INFO: Pod daemon-set-wqflt is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 01/03/24 12:02:21.609
    Jan  3 12:02:21.651: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  3 12:02:21.651: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
    Jan  3 12:02:22.705: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  3 12:02:22.705: INFO: Node jb-1-26-np-nqeu5xtrab is running 0 daemon pod, expected 1
    Jan  3 12:02:23.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 12:02:23.702: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 01/03/24 12:02:23.8
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6501, will wait for the garbage collector to delete the pods 01/03/24 12:02:23.8
    Jan  3 12:02:23.897: INFO: Deleting DaemonSet.extensions daemon-set took: 25.3511ms
    Jan  3 12:02:23.998: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.110154ms
    Jan  3 12:02:27.122: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:02:27.123: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  3 12:02:27.141: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33980869603"},"items":null}

    Jan  3 12:02:27.159: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33980869605"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:02:27.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6501" for this suite. 01/03/24 12:02:27.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:02:27.305
Jan  3 12:02:27.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename discovery 01/03/24 12:02:27.308
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:02:27.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:02:27.387
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/03/24 12:02:27.426
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan  3 12:02:27.631: INFO: Checking APIGroup: apiregistration.k8s.io
Jan  3 12:02:27.643: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan  3 12:02:27.643: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan  3 12:02:27.643: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan  3 12:02:27.643: INFO: Checking APIGroup: apps
Jan  3 12:02:27.656: INFO: PreferredVersion.GroupVersion: apps/v1
Jan  3 12:02:27.656: INFO: Versions found [{apps/v1 v1}]
Jan  3 12:02:27.656: INFO: apps/v1 matches apps/v1
Jan  3 12:02:27.656: INFO: Checking APIGroup: events.k8s.io
Jan  3 12:02:27.668: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan  3 12:02:27.668: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan  3 12:02:27.668: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan  3 12:02:27.668: INFO: Checking APIGroup: authentication.k8s.io
Jan  3 12:02:27.681: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan  3 12:02:27.681: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan  3 12:02:27.681: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan  3 12:02:27.681: INFO: Checking APIGroup: authorization.k8s.io
Jan  3 12:02:27.693: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan  3 12:02:27.693: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan  3 12:02:27.693: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan  3 12:02:27.693: INFO: Checking APIGroup: autoscaling
Jan  3 12:02:27.706: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan  3 12:02:27.706: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jan  3 12:02:27.706: INFO: autoscaling/v2 matches autoscaling/v2
Jan  3 12:02:27.706: INFO: Checking APIGroup: batch
Jan  3 12:02:27.718: INFO: PreferredVersion.GroupVersion: batch/v1
Jan  3 12:02:27.718: INFO: Versions found [{batch/v1 v1}]
Jan  3 12:02:27.718: INFO: batch/v1 matches batch/v1
Jan  3 12:02:27.718: INFO: Checking APIGroup: certificates.k8s.io
Jan  3 12:02:27.730: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan  3 12:02:27.730: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan  3 12:02:27.730: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan  3 12:02:27.730: INFO: Checking APIGroup: networking.k8s.io
Jan  3 12:02:27.743: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan  3 12:02:27.743: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan  3 12:02:27.743: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan  3 12:02:27.743: INFO: Checking APIGroup: policy
Jan  3 12:02:27.755: INFO: PreferredVersion.GroupVersion: policy/v1
Jan  3 12:02:27.755: INFO: Versions found [{policy/v1 v1}]
Jan  3 12:02:27.755: INFO: policy/v1 matches policy/v1
Jan  3 12:02:27.755: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan  3 12:02:27.768: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan  3 12:02:27.768: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan  3 12:02:27.768: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan  3 12:02:27.768: INFO: Checking APIGroup: storage.k8s.io
Jan  3 12:02:27.780: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan  3 12:02:27.780: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan  3 12:02:27.780: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan  3 12:02:27.780: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan  3 12:02:27.793: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan  3 12:02:27.793: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan  3 12:02:27.793: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan  3 12:02:27.793: INFO: Checking APIGroup: apiextensions.k8s.io
Jan  3 12:02:27.818: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan  3 12:02:27.818: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan  3 12:02:27.818: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan  3 12:02:27.818: INFO: Checking APIGroup: scheduling.k8s.io
Jan  3 12:02:27.831: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan  3 12:02:27.831: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan  3 12:02:27.831: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan  3 12:02:27.831: INFO: Checking APIGroup: coordination.k8s.io
Jan  3 12:02:27.843: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan  3 12:02:27.843: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan  3 12:02:27.843: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan  3 12:02:27.843: INFO: Checking APIGroup: node.k8s.io
Jan  3 12:02:27.856: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan  3 12:02:27.856: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan  3 12:02:27.856: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan  3 12:02:27.856: INFO: Checking APIGroup: discovery.k8s.io
Jan  3 12:02:27.869: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan  3 12:02:27.869: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan  3 12:02:27.869: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan  3 12:02:27.869: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan  3 12:02:27.882: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jan  3 12:02:27.882: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jan  3 12:02:27.882: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Jan  3 12:02:27.882: INFO: Checking APIGroup: crd.projectcalico.org
Jan  3 12:02:27.895: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan  3 12:02:27.895: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan  3 12:02:27.895: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan  3 12:02:27.895: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan  3 12:02:27.907: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan  3 12:02:27.907: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jan  3 12:02:27.907: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jan  3 12:02:27.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-3981" for this suite. 01/03/24 12:02:27.929
------------------------------
• [0.648 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:02:27.305
    Jan  3 12:02:27.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename discovery 01/03/24 12:02:27.308
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:02:27.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:02:27.387
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/03/24 12:02:27.426
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan  3 12:02:27.631: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan  3 12:02:27.643: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan  3 12:02:27.643: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan  3 12:02:27.643: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan  3 12:02:27.643: INFO: Checking APIGroup: apps
    Jan  3 12:02:27.656: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan  3 12:02:27.656: INFO: Versions found [{apps/v1 v1}]
    Jan  3 12:02:27.656: INFO: apps/v1 matches apps/v1
    Jan  3 12:02:27.656: INFO: Checking APIGroup: events.k8s.io
    Jan  3 12:02:27.668: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan  3 12:02:27.668: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan  3 12:02:27.668: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan  3 12:02:27.668: INFO: Checking APIGroup: authentication.k8s.io
    Jan  3 12:02:27.681: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan  3 12:02:27.681: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan  3 12:02:27.681: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan  3 12:02:27.681: INFO: Checking APIGroup: authorization.k8s.io
    Jan  3 12:02:27.693: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan  3 12:02:27.693: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan  3 12:02:27.693: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan  3 12:02:27.693: INFO: Checking APIGroup: autoscaling
    Jan  3 12:02:27.706: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan  3 12:02:27.706: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jan  3 12:02:27.706: INFO: autoscaling/v2 matches autoscaling/v2
    Jan  3 12:02:27.706: INFO: Checking APIGroup: batch
    Jan  3 12:02:27.718: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan  3 12:02:27.718: INFO: Versions found [{batch/v1 v1}]
    Jan  3 12:02:27.718: INFO: batch/v1 matches batch/v1
    Jan  3 12:02:27.718: INFO: Checking APIGroup: certificates.k8s.io
    Jan  3 12:02:27.730: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan  3 12:02:27.730: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan  3 12:02:27.730: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan  3 12:02:27.730: INFO: Checking APIGroup: networking.k8s.io
    Jan  3 12:02:27.743: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan  3 12:02:27.743: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan  3 12:02:27.743: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan  3 12:02:27.743: INFO: Checking APIGroup: policy
    Jan  3 12:02:27.755: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan  3 12:02:27.755: INFO: Versions found [{policy/v1 v1}]
    Jan  3 12:02:27.755: INFO: policy/v1 matches policy/v1
    Jan  3 12:02:27.755: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan  3 12:02:27.768: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan  3 12:02:27.768: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan  3 12:02:27.768: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan  3 12:02:27.768: INFO: Checking APIGroup: storage.k8s.io
    Jan  3 12:02:27.780: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan  3 12:02:27.780: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan  3 12:02:27.780: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan  3 12:02:27.780: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan  3 12:02:27.793: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan  3 12:02:27.793: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan  3 12:02:27.793: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan  3 12:02:27.793: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan  3 12:02:27.818: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan  3 12:02:27.818: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan  3 12:02:27.818: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan  3 12:02:27.818: INFO: Checking APIGroup: scheduling.k8s.io
    Jan  3 12:02:27.831: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan  3 12:02:27.831: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan  3 12:02:27.831: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan  3 12:02:27.831: INFO: Checking APIGroup: coordination.k8s.io
    Jan  3 12:02:27.843: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan  3 12:02:27.843: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan  3 12:02:27.843: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan  3 12:02:27.843: INFO: Checking APIGroup: node.k8s.io
    Jan  3 12:02:27.856: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan  3 12:02:27.856: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan  3 12:02:27.856: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan  3 12:02:27.856: INFO: Checking APIGroup: discovery.k8s.io
    Jan  3 12:02:27.869: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan  3 12:02:27.869: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan  3 12:02:27.869: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan  3 12:02:27.869: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan  3 12:02:27.882: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jan  3 12:02:27.882: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jan  3 12:02:27.882: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Jan  3 12:02:27.882: INFO: Checking APIGroup: crd.projectcalico.org
    Jan  3 12:02:27.895: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan  3 12:02:27.895: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan  3 12:02:27.895: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jan  3 12:02:27.895: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jan  3 12:02:27.907: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jan  3 12:02:27.907: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    Jan  3 12:02:27.907: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:02:27.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-3981" for this suite. 01/03/24 12:02:27.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:02:27.962
Jan  3 12:02:27.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-runtime 01/03/24 12:02:27.963
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:02:28.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:02:28.046
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 01/03/24 12:02:28.073
STEP: wait for the container to reach Failed 01/03/24 12:02:28.101
STEP: get the container status 01/03/24 12:02:33.227
STEP: the container should be terminated 01/03/24 12:02:33.246
STEP: the termination message should be set 01/03/24 12:02:33.246
Jan  3 12:02:33.246: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/03/24 12:02:33.246
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  3 12:02:33.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9328" for this suite. 01/03/24 12:02:33.326
------------------------------
• [SLOW TEST] [5.390 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:02:27.962
    Jan  3 12:02:27.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-runtime 01/03/24 12:02:27.963
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:02:28.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:02:28.046
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 01/03/24 12:02:28.073
    STEP: wait for the container to reach Failed 01/03/24 12:02:28.101
    STEP: get the container status 01/03/24 12:02:33.227
    STEP: the container should be terminated 01/03/24 12:02:33.246
    STEP: the termination message should be set 01/03/24 12:02:33.246
    Jan  3 12:02:33.246: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/03/24 12:02:33.246
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:02:33.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9328" for this suite. 01/03/24 12:02:33.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:02:33.354
Jan  3 12:02:33.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:02:33.356
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:02:33.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:02:33.436
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-63590522-aee6-476a-bd37-d6be7cd33d02 01/03/24 12:02:33.462
STEP: Creating a pod to test consume configMaps 01/03/24 12:02:33.481
Jan  3 12:02:33.511: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f" in namespace "projected-809" to be "Succeeded or Failed"
Jan  3 12:02:33.540: INFO: Pod "pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.590032ms
Jan  3 12:02:35.558: INFO: Pod "pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.047305983s
Jan  3 12:02:37.562: INFO: Pod "pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f": Phase="Running", Reason="", readiness=false. Elapsed: 4.050877039s
Jan  3 12:02:39.559: INFO: Pod "pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048355773s
STEP: Saw pod success 01/03/24 12:02:39.56
Jan  3 12:02:39.560: INFO: Pod "pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f" satisfied condition "Succeeded or Failed"
Jan  3 12:02:39.579: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f container agnhost-container: <nil>
STEP: delete the pod 01/03/24 12:02:39.617
Jan  3 12:02:39.650: INFO: Waiting for pod pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f to disappear
Jan  3 12:02:39.671: INFO: Pod pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  3 12:02:39.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-809" for this suite. 01/03/24 12:02:39.705
------------------------------
• [SLOW TEST] [6.377 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:02:33.354
    Jan  3 12:02:33.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:02:33.356
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:02:33.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:02:33.436
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-63590522-aee6-476a-bd37-d6be7cd33d02 01/03/24 12:02:33.462
    STEP: Creating a pod to test consume configMaps 01/03/24 12:02:33.481
    Jan  3 12:02:33.511: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f" in namespace "projected-809" to be "Succeeded or Failed"
    Jan  3 12:02:33.540: INFO: Pod "pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.590032ms
    Jan  3 12:02:35.558: INFO: Pod "pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.047305983s
    Jan  3 12:02:37.562: INFO: Pod "pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f": Phase="Running", Reason="", readiness=false. Elapsed: 4.050877039s
    Jan  3 12:02:39.559: INFO: Pod "pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048355773s
    STEP: Saw pod success 01/03/24 12:02:39.56
    Jan  3 12:02:39.560: INFO: Pod "pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f" satisfied condition "Succeeded or Failed"
    Jan  3 12:02:39.579: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 12:02:39.617
    Jan  3 12:02:39.650: INFO: Waiting for pod pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f to disappear
    Jan  3 12:02:39.671: INFO: Pod pod-projected-configmaps-6ecbc60b-1afd-43dc-bec1-b5623a212b2f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:02:39.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-809" for this suite. 01/03/24 12:02:39.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:02:39.735
Jan  3 12:02:39.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 12:02:39.738
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:02:39.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:02:39.816
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 01/03/24 12:02:39.842
Jan  3 12:02:39.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3529 cluster-info'
Jan  3 12:02:39.981: INFO: stderr: ""
Jan  3 12:02:39.982: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 12:02:39.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3529" for this suite. 01/03/24 12:02:40.001
------------------------------
• [0.292 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:02:39.735
    Jan  3 12:02:39.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 12:02:39.738
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:02:39.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:02:39.816
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 01/03/24 12:02:39.842
    Jan  3 12:02:39.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3529 cluster-info'
    Jan  3 12:02:39.981: INFO: stderr: ""
    Jan  3 12:02:39.982: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:02:39.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3529" for this suite. 01/03/24 12:02:40.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:02:40.033
Jan  3 12:02:40.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename statefulset 01/03/24 12:02:40.035
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:02:40.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:02:40.167
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8015 01/03/24 12:02:40.195
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-8015 01/03/24 12:02:40.232
Jan  3 12:02:40.271: INFO: Found 0 stateful pods, waiting for 1
Jan  3 12:02:50.293: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/03/24 12:02:50.331
STEP: Getting /status 01/03/24 12:02:50.365
Jan  3 12:02:50.389: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/03/24 12:02:50.389
Jan  3 12:02:50.431: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/03/24 12:02:50.431
Jan  3 12:02:50.446: INFO: Observed &StatefulSet event: ADDED
Jan  3 12:02:50.446: INFO: Found Statefulset ss in namespace statefulset-8015 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  3 12:02:50.446: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/03/24 12:02:50.446
Jan  3 12:02:50.447: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  3 12:02:50.472: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/03/24 12:02:50.472
Jan  3 12:02:50.487: INFO: Observed &StatefulSet event: ADDED
Jan  3 12:02:50.487: INFO: Observed Statefulset ss in namespace statefulset-8015 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  3 12:02:50.487: INFO: Observed &StatefulSet event: MODIFIED
Jan  3 12:02:50.487: INFO: Found Statefulset ss in namespace statefulset-8015 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  3 12:02:50.488: INFO: Deleting all statefulset in ns statefulset-8015
Jan  3 12:02:50.506: INFO: Scaling statefulset ss to 0
Jan  3 12:03:00.598: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 12:03:00.631: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  3 12:03:00.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8015" for this suite. 01/03/24 12:03:00.732
------------------------------
• [SLOW TEST] [20.724 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:02:40.033
    Jan  3 12:02:40.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename statefulset 01/03/24 12:02:40.035
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:02:40.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:02:40.167
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8015 01/03/24 12:02:40.195
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-8015 01/03/24 12:02:40.232
    Jan  3 12:02:40.271: INFO: Found 0 stateful pods, waiting for 1
    Jan  3 12:02:50.293: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/03/24 12:02:50.331
    STEP: Getting /status 01/03/24 12:02:50.365
    Jan  3 12:02:50.389: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/03/24 12:02:50.389
    Jan  3 12:02:50.431: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/03/24 12:02:50.431
    Jan  3 12:02:50.446: INFO: Observed &StatefulSet event: ADDED
    Jan  3 12:02:50.446: INFO: Found Statefulset ss in namespace statefulset-8015 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  3 12:02:50.446: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/03/24 12:02:50.446
    Jan  3 12:02:50.447: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  3 12:02:50.472: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/03/24 12:02:50.472
    Jan  3 12:02:50.487: INFO: Observed &StatefulSet event: ADDED
    Jan  3 12:02:50.487: INFO: Observed Statefulset ss in namespace statefulset-8015 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  3 12:02:50.487: INFO: Observed &StatefulSet event: MODIFIED
    Jan  3 12:02:50.487: INFO: Found Statefulset ss in namespace statefulset-8015 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  3 12:02:50.488: INFO: Deleting all statefulset in ns statefulset-8015
    Jan  3 12:02:50.506: INFO: Scaling statefulset ss to 0
    Jan  3 12:03:00.598: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 12:03:00.631: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:03:00.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8015" for this suite. 01/03/24 12:03:00.732
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:03:00.759
Jan  3 12:03:00.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:03:00.761
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:03:00.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:03:00.84
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-699b759b-f8f6-4606-a305-f5da4d67a9cd 01/03/24 12:03:00.866
STEP: Creating secret with name secret-projected-all-test-volume-43459dfb-b313-4f4c-b6fb-cfb369d30304 01/03/24 12:03:00.885
STEP: Creating a pod to test Check all projections for projected volume plugin 01/03/24 12:03:00.904
Jan  3 12:03:00.932: INFO: Waiting up to 5m0s for pod "projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a" in namespace "projected-6372" to be "Succeeded or Failed"
Jan  3 12:03:00.952: INFO: Pod "projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.432802ms
Jan  3 12:03:02.974: INFO: Pod "projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.042250162s
Jan  3 12:03:04.971: INFO: Pod "projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a": Phase="Running", Reason="", readiness=false. Elapsed: 4.038633836s
Jan  3 12:03:06.974: INFO: Pod "projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042053399s
STEP: Saw pod success 01/03/24 12:03:06.974
Jan  3 12:03:06.974: INFO: Pod "projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a" satisfied condition "Succeeded or Failed"
Jan  3 12:03:06.998: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a container projected-all-volume-test: <nil>
STEP: delete the pod 01/03/24 12:03:07.046
Jan  3 12:03:07.084: INFO: Waiting for pod projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a to disappear
Jan  3 12:03:07.102: INFO: Pod projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jan  3 12:03:07.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6372" for this suite. 01/03/24 12:03:07.133
------------------------------
• [SLOW TEST] [6.397 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:03:00.759
    Jan  3 12:03:00.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:03:00.761
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:03:00.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:03:00.84
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-699b759b-f8f6-4606-a305-f5da4d67a9cd 01/03/24 12:03:00.866
    STEP: Creating secret with name secret-projected-all-test-volume-43459dfb-b313-4f4c-b6fb-cfb369d30304 01/03/24 12:03:00.885
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/03/24 12:03:00.904
    Jan  3 12:03:00.932: INFO: Waiting up to 5m0s for pod "projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a" in namespace "projected-6372" to be "Succeeded or Failed"
    Jan  3 12:03:00.952: INFO: Pod "projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.432802ms
    Jan  3 12:03:02.974: INFO: Pod "projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.042250162s
    Jan  3 12:03:04.971: INFO: Pod "projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a": Phase="Running", Reason="", readiness=false. Elapsed: 4.038633836s
    Jan  3 12:03:06.974: INFO: Pod "projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042053399s
    STEP: Saw pod success 01/03/24 12:03:06.974
    Jan  3 12:03:06.974: INFO: Pod "projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a" satisfied condition "Succeeded or Failed"
    Jan  3 12:03:06.998: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a container projected-all-volume-test: <nil>
    STEP: delete the pod 01/03/24 12:03:07.046
    Jan  3 12:03:07.084: INFO: Waiting for pod projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a to disappear
    Jan  3 12:03:07.102: INFO: Pod projected-volume-bcab41be-b95a-4f09-ac9d-ecd55e138a8a no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:03:07.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6372" for this suite. 01/03/24 12:03:07.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:03:07.162
Jan  3 12:03:07.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubelet-test 01/03/24 12:03:07.164
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:03:07.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:03:07.253
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/03/24 12:03:07.306
Jan  3 12:03:07.306: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases4c36ff29-f44b-4bb7-aecb-8a6bba28add2" in namespace "kubelet-test-8925" to be "completed"
Jan  3 12:03:07.324: INFO: Pod "agnhost-host-aliases4c36ff29-f44b-4bb7-aecb-8a6bba28add2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.040371ms
Jan  3 12:03:09.344: INFO: Pod "agnhost-host-aliases4c36ff29-f44b-4bb7-aecb-8a6bba28add2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037506451s
Jan  3 12:03:11.345: INFO: Pod "agnhost-host-aliases4c36ff29-f44b-4bb7-aecb-8a6bba28add2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039149133s
Jan  3 12:03:13.348: INFO: Pod "agnhost-host-aliases4c36ff29-f44b-4bb7-aecb-8a6bba28add2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041327564s
Jan  3 12:03:13.348: INFO: Pod "agnhost-host-aliases4c36ff29-f44b-4bb7-aecb-8a6bba28add2" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  3 12:03:13.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8925" for this suite. 01/03/24 12:03:13.422
------------------------------
• [SLOW TEST] [6.284 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:03:07.162
    Jan  3 12:03:07.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubelet-test 01/03/24 12:03:07.164
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:03:07.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:03:07.253
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/03/24 12:03:07.306
    Jan  3 12:03:07.306: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases4c36ff29-f44b-4bb7-aecb-8a6bba28add2" in namespace "kubelet-test-8925" to be "completed"
    Jan  3 12:03:07.324: INFO: Pod "agnhost-host-aliases4c36ff29-f44b-4bb7-aecb-8a6bba28add2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.040371ms
    Jan  3 12:03:09.344: INFO: Pod "agnhost-host-aliases4c36ff29-f44b-4bb7-aecb-8a6bba28add2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037506451s
    Jan  3 12:03:11.345: INFO: Pod "agnhost-host-aliases4c36ff29-f44b-4bb7-aecb-8a6bba28add2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039149133s
    Jan  3 12:03:13.348: INFO: Pod "agnhost-host-aliases4c36ff29-f44b-4bb7-aecb-8a6bba28add2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041327564s
    Jan  3 12:03:13.348: INFO: Pod "agnhost-host-aliases4c36ff29-f44b-4bb7-aecb-8a6bba28add2" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:03:13.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8925" for this suite. 01/03/24 12:03:13.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:03:13.448
Jan  3 12:03:13.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename containers 01/03/24 12:03:13.451
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:03:13.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:03:13.533
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 01/03/24 12:03:13.56
Jan  3 12:03:13.583: INFO: Waiting up to 5m0s for pod "client-containers-8365e945-0dc3-4856-820a-f1001909ca59" in namespace "containers-1449" to be "Succeeded or Failed"
Jan  3 12:03:13.601: INFO: Pod "client-containers-8365e945-0dc3-4856-820a-f1001909ca59": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016447ms
Jan  3 12:03:15.625: INFO: Pod "client-containers-8365e945-0dc3-4856-820a-f1001909ca59": Phase="Running", Reason="", readiness=true. Elapsed: 2.042395155s
Jan  3 12:03:17.621: INFO: Pod "client-containers-8365e945-0dc3-4856-820a-f1001909ca59": Phase="Running", Reason="", readiness=false. Elapsed: 4.038403315s
Jan  3 12:03:19.622: INFO: Pod "client-containers-8365e945-0dc3-4856-820a-f1001909ca59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038708116s
STEP: Saw pod success 01/03/24 12:03:19.622
Jan  3 12:03:19.622: INFO: Pod "client-containers-8365e945-0dc3-4856-820a-f1001909ca59" satisfied condition "Succeeded or Failed"
Jan  3 12:03:19.640: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod client-containers-8365e945-0dc3-4856-820a-f1001909ca59 container agnhost-container: <nil>
STEP: delete the pod 01/03/24 12:03:19.677
Jan  3 12:03:19.715: INFO: Waiting for pod client-containers-8365e945-0dc3-4856-820a-f1001909ca59 to disappear
Jan  3 12:03:19.733: INFO: Pod client-containers-8365e945-0dc3-4856-820a-f1001909ca59 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan  3 12:03:19.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1449" for this suite. 01/03/24 12:03:19.78
------------------------------
• [SLOW TEST] [6.360 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:03:13.448
    Jan  3 12:03:13.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename containers 01/03/24 12:03:13.451
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:03:13.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:03:13.533
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 01/03/24 12:03:13.56
    Jan  3 12:03:13.583: INFO: Waiting up to 5m0s for pod "client-containers-8365e945-0dc3-4856-820a-f1001909ca59" in namespace "containers-1449" to be "Succeeded or Failed"
    Jan  3 12:03:13.601: INFO: Pod "client-containers-8365e945-0dc3-4856-820a-f1001909ca59": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016447ms
    Jan  3 12:03:15.625: INFO: Pod "client-containers-8365e945-0dc3-4856-820a-f1001909ca59": Phase="Running", Reason="", readiness=true. Elapsed: 2.042395155s
    Jan  3 12:03:17.621: INFO: Pod "client-containers-8365e945-0dc3-4856-820a-f1001909ca59": Phase="Running", Reason="", readiness=false. Elapsed: 4.038403315s
    Jan  3 12:03:19.622: INFO: Pod "client-containers-8365e945-0dc3-4856-820a-f1001909ca59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038708116s
    STEP: Saw pod success 01/03/24 12:03:19.622
    Jan  3 12:03:19.622: INFO: Pod "client-containers-8365e945-0dc3-4856-820a-f1001909ca59" satisfied condition "Succeeded or Failed"
    Jan  3 12:03:19.640: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod client-containers-8365e945-0dc3-4856-820a-f1001909ca59 container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 12:03:19.677
    Jan  3 12:03:19.715: INFO: Waiting for pod client-containers-8365e945-0dc3-4856-820a-f1001909ca59 to disappear
    Jan  3 12:03:19.733: INFO: Pod client-containers-8365e945-0dc3-4856-820a-f1001909ca59 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:03:19.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1449" for this suite. 01/03/24 12:03:19.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:03:19.81
Jan  3 12:03:19.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename init-container 01/03/24 12:03:19.812
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:03:19.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:03:19.896
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 01/03/24 12:03:19.922
Jan  3 12:03:19.922: INFO: PodSpec: initContainers in spec.initContainers
Jan  3 12:04:06.525: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-4ec903c6-28b7-43ab-8b31-3d0fb5ea8ed0", GenerateName:"", Namespace:"init-container-748", SelfLink:"", UID:"e369bc69-4c2f-44d6-b796-efcc81b49faf", ResourceVersion:"33980881597", Generation:0, CreationTimestamp:time.Date(2024, time.January, 3, 12, 3, 19, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"922730677"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"0aefb700d596e2aa6fb5c0baac8cb228a7a379dcbc03897b1356e3d2712861d8", "cni.projectcalico.org/podIP":"10.221.146.100/32", "cni.projectcalico.org/podIPs":"10.221.146.100/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 3, 12, 3, 19, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005395cf8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 3, 12, 3, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005395dd0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 3, 12, 4, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005395e00), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-hf8r7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00131c720), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hf8r7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hf8r7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hf8r7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004141c48), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"jb-1-26-np-64kerjapxk", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000359b20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004141cc0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004141ce0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004141ce8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004141cec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000fb2bd0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 3, 12, 3, 19, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 3, 12, 3, 19, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 3, 12, 3, 19, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 3, 12, 3, 19, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"185.132.46.116", PodIP:"10.221.146.100", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.221.146.100"}}, StartTime:time.Date(2024, time.January, 3, 12, 3, 19, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000359c70)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000359dc0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://b6719d3b2ccbd30442979e24a158cf401dd7868c79014273b86771facb5ef449", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00131c7a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00131c780), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004141d64)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:04:06.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-748" for this suite. 01/03/24 12:04:06.563
------------------------------
• [SLOW TEST] [46.787 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:03:19.81
    Jan  3 12:03:19.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename init-container 01/03/24 12:03:19.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:03:19.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:03:19.896
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 01/03/24 12:03:19.922
    Jan  3 12:03:19.922: INFO: PodSpec: initContainers in spec.initContainers
    Jan  3 12:04:06.525: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-4ec903c6-28b7-43ab-8b31-3d0fb5ea8ed0", GenerateName:"", Namespace:"init-container-748", SelfLink:"", UID:"e369bc69-4c2f-44d6-b796-efcc81b49faf", ResourceVersion:"33980881597", Generation:0, CreationTimestamp:time.Date(2024, time.January, 3, 12, 3, 19, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"922730677"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"0aefb700d596e2aa6fb5c0baac8cb228a7a379dcbc03897b1356e3d2712861d8", "cni.projectcalico.org/podIP":"10.221.146.100/32", "cni.projectcalico.org/podIPs":"10.221.146.100/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 3, 12, 3, 19, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005395cf8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 3, 12, 3, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005395dd0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 3, 12, 4, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005395e00), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-hf8r7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00131c720), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hf8r7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hf8r7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hf8r7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004141c48), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"jb-1-26-np-64kerjapxk", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000359b20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004141cc0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004141ce0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004141ce8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004141cec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000fb2bd0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 3, 12, 3, 19, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 3, 12, 3, 19, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 3, 12, 3, 19, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 3, 12, 3, 19, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"185.132.46.116", PodIP:"10.221.146.100", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.221.146.100"}}, StartTime:time.Date(2024, time.January, 3, 12, 3, 19, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000359c70)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000359dc0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://b6719d3b2ccbd30442979e24a158cf401dd7868c79014273b86771facb5ef449", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00131c7a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00131c780), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004141d64)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:04:06.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-748" for this suite. 01/03/24 12:04:06.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:04:06.607
Jan  3 12:04:06.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubelet-test 01/03/24 12:04:06.609
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:04:06.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:04:06.687
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  3 12:04:06.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5175" for this suite. 01/03/24 12:04:06.79
------------------------------
• [0.210 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:04:06.607
    Jan  3 12:04:06.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubelet-test 01/03/24 12:04:06.609
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:04:06.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:04:06.687
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:04:06.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5175" for this suite. 01/03/24 12:04:06.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:04:06.82
Jan  3 12:04:06.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename resourcequota 01/03/24 12:04:06.822
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:04:06.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:04:06.897
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 01/03/24 12:04:23.951
STEP: Creating a ResourceQuota 01/03/24 12:04:28.97
STEP: Ensuring resource quota status is calculated 01/03/24 12:04:28.992
STEP: Creating a ConfigMap 01/03/24 12:04:31.016
STEP: Ensuring resource quota status captures configMap creation 01/03/24 12:04:31.045
STEP: Deleting a ConfigMap 01/03/24 12:04:33.071
STEP: Ensuring resource quota status released usage 01/03/24 12:04:33.093
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  3 12:04:35.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9398" for this suite. 01/03/24 12:04:35.145
------------------------------
• [SLOW TEST] [28.357 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:04:06.82
    Jan  3 12:04:06.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename resourcequota 01/03/24 12:04:06.822
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:04:06.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:04:06.897
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 01/03/24 12:04:23.951
    STEP: Creating a ResourceQuota 01/03/24 12:04:28.97
    STEP: Ensuring resource quota status is calculated 01/03/24 12:04:28.992
    STEP: Creating a ConfigMap 01/03/24 12:04:31.016
    STEP: Ensuring resource quota status captures configMap creation 01/03/24 12:04:31.045
    STEP: Deleting a ConfigMap 01/03/24 12:04:33.071
    STEP: Ensuring resource quota status released usage 01/03/24 12:04:33.093
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:04:35.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9398" for this suite. 01/03/24 12:04:35.145
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:04:35.179
Jan  3 12:04:35.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 12:04:35.181
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:04:35.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:04:35.264
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 01/03/24 12:04:35.293
Jan  3 12:04:35.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 create -f -'
Jan  3 12:04:36.346: INFO: stderr: ""
Jan  3 12:04:36.346: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/03/24 12:04:36.346
Jan  3 12:04:36.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 12:04:36.501: INFO: stderr: ""
Jan  3 12:04:36.501: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-lgpz8 "
Jan  3 12:04:36.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 12:04:36.621: INFO: stderr: ""
Jan  3 12:04:36.621: INFO: stdout: ""
Jan  3 12:04:36.621: INFO: update-demo-nautilus-b5dfb is created but not running
Jan  3 12:04:41.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 12:04:41.772: INFO: stderr: ""
Jan  3 12:04:41.772: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-lgpz8 "
Jan  3 12:04:41.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 12:04:41.896: INFO: stderr: ""
Jan  3 12:04:41.896: INFO: stdout: ""
Jan  3 12:04:41.896: INFO: update-demo-nautilus-b5dfb is created but not running
Jan  3 12:04:46.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 12:04:47.055: INFO: stderr: ""
Jan  3 12:04:47.055: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-lgpz8 "
Jan  3 12:04:47.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 12:04:47.181: INFO: stderr: ""
Jan  3 12:04:47.182: INFO: stdout: ""
Jan  3 12:04:47.182: INFO: update-demo-nautilus-b5dfb is created but not running
Jan  3 12:04:52.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 12:04:52.337: INFO: stderr: ""
Jan  3 12:04:52.337: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-lgpz8 "
Jan  3 12:04:52.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 12:04:52.501: INFO: stderr: ""
Jan  3 12:04:52.501: INFO: stdout: "true"
Jan  3 12:04:52.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 12:04:52.651: INFO: stderr: ""
Jan  3 12:04:52.651: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  3 12:04:52.651: INFO: validating pod update-demo-nautilus-b5dfb
Jan  3 12:04:52.783: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 12:04:52.783: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 12:04:52.783: INFO: update-demo-nautilus-b5dfb is verified up and running
Jan  3 12:04:52.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-lgpz8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 12:04:52.931: INFO: stderr: ""
Jan  3 12:04:52.931: INFO: stdout: "true"
Jan  3 12:04:52.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-lgpz8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 12:04:53.070: INFO: stderr: ""
Jan  3 12:04:53.070: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  3 12:04:53.070: INFO: validating pod update-demo-nautilus-lgpz8
Jan  3 12:04:53.164: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 12:04:53.164: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 12:04:53.164: INFO: update-demo-nautilus-lgpz8 is verified up and running
STEP: scaling down the replication controller 01/03/24 12:04:53.164
Jan  3 12:04:53.169: INFO: scanned /root for discovery docs: <nil>
Jan  3 12:04:53.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan  3 12:04:54.385: INFO: stderr: ""
Jan  3 12:04:54.385: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/03/24 12:04:54.385
Jan  3 12:04:54.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 12:04:54.531: INFO: stderr: ""
Jan  3 12:04:54.531: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-lgpz8 "
STEP: Replicas for name=update-demo: expected=1 actual=2 01/03/24 12:04:54.531
Jan  3 12:04:59.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 12:04:59.689: INFO: stderr: ""
Jan  3 12:04:59.689: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-lgpz8 "
STEP: Replicas for name=update-demo: expected=1 actual=2 01/03/24 12:04:59.689
Jan  3 12:05:04.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 12:05:04.835: INFO: stderr: ""
Jan  3 12:05:04.835: INFO: stdout: "update-demo-nautilus-b5dfb "
Jan  3 12:05:04.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 12:05:04.968: INFO: stderr: ""
Jan  3 12:05:04.968: INFO: stdout: "true"
Jan  3 12:05:04.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 12:05:05.093: INFO: stderr: ""
Jan  3 12:05:05.093: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  3 12:05:05.093: INFO: validating pod update-demo-nautilus-b5dfb
Jan  3 12:05:05.126: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 12:05:05.127: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 12:05:05.127: INFO: update-demo-nautilus-b5dfb is verified up and running
STEP: scaling up the replication controller 01/03/24 12:05:05.127
Jan  3 12:05:05.132: INFO: scanned /root for discovery docs: <nil>
Jan  3 12:05:05.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan  3 12:05:06.342: INFO: stderr: ""
Jan  3 12:05:06.342: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/03/24 12:05:06.342
Jan  3 12:05:06.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 12:05:06.484: INFO: stderr: ""
Jan  3 12:05:06.484: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-fhzbn "
Jan  3 12:05:06.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 12:05:06.617: INFO: stderr: ""
Jan  3 12:05:06.617: INFO: stdout: "true"
Jan  3 12:05:06.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 12:05:06.757: INFO: stderr: ""
Jan  3 12:05:06.758: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  3 12:05:06.758: INFO: validating pod update-demo-nautilus-b5dfb
Jan  3 12:05:06.793: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 12:05:06.793: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 12:05:06.793: INFO: update-demo-nautilus-b5dfb is verified up and running
Jan  3 12:05:06.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-fhzbn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 12:05:06.928: INFO: stderr: ""
Jan  3 12:05:06.928: INFO: stdout: ""
Jan  3 12:05:06.928: INFO: update-demo-nautilus-fhzbn is created but not running
Jan  3 12:05:11.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 12:05:12.088: INFO: stderr: ""
Jan  3 12:05:12.088: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-fhzbn "
Jan  3 12:05:12.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 12:05:12.223: INFO: stderr: ""
Jan  3 12:05:12.223: INFO: stdout: "true"
Jan  3 12:05:12.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 12:05:12.363: INFO: stderr: ""
Jan  3 12:05:12.363: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  3 12:05:12.363: INFO: validating pod update-demo-nautilus-b5dfb
Jan  3 12:05:12.396: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 12:05:12.396: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 12:05:12.396: INFO: update-demo-nautilus-b5dfb is verified up and running
Jan  3 12:05:12.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-fhzbn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 12:05:12.539: INFO: stderr: ""
Jan  3 12:05:12.539: INFO: stdout: "true"
Jan  3 12:05:12.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-fhzbn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 12:05:12.682: INFO: stderr: ""
Jan  3 12:05:12.682: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  3 12:05:12.682: INFO: validating pod update-demo-nautilus-fhzbn
Jan  3 12:05:12.795: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 12:05:12.796: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 12:05:12.796: INFO: update-demo-nautilus-fhzbn is verified up and running
STEP: using delete to clean up resources 01/03/24 12:05:12.796
Jan  3 12:05:12.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 delete --grace-period=0 --force -f -'
Jan  3 12:05:12.957: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 12:05:12.957: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan  3 12:05:12.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get rc,svc -l name=update-demo --no-headers'
Jan  3 12:05:13.126: INFO: stderr: "No resources found in kubectl-9545 namespace.\n"
Jan  3 12:05:13.127: INFO: stdout: ""
Jan  3 12:05:13.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  3 12:05:13.287: INFO: stderr: ""
Jan  3 12:05:13.287: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 12:05:13.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9545" for this suite. 01/03/24 12:05:13.317
------------------------------
• [SLOW TEST] [38.162 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:04:35.179
    Jan  3 12:04:35.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 12:04:35.181
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:04:35.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:04:35.264
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 01/03/24 12:04:35.293
    Jan  3 12:04:35.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 create -f -'
    Jan  3 12:04:36.346: INFO: stderr: ""
    Jan  3 12:04:36.346: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/03/24 12:04:36.346
    Jan  3 12:04:36.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 12:04:36.501: INFO: stderr: ""
    Jan  3 12:04:36.501: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-lgpz8 "
    Jan  3 12:04:36.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 12:04:36.621: INFO: stderr: ""
    Jan  3 12:04:36.621: INFO: stdout: ""
    Jan  3 12:04:36.621: INFO: update-demo-nautilus-b5dfb is created but not running
    Jan  3 12:04:41.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 12:04:41.772: INFO: stderr: ""
    Jan  3 12:04:41.772: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-lgpz8 "
    Jan  3 12:04:41.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 12:04:41.896: INFO: stderr: ""
    Jan  3 12:04:41.896: INFO: stdout: ""
    Jan  3 12:04:41.896: INFO: update-demo-nautilus-b5dfb is created but not running
    Jan  3 12:04:46.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 12:04:47.055: INFO: stderr: ""
    Jan  3 12:04:47.055: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-lgpz8 "
    Jan  3 12:04:47.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 12:04:47.181: INFO: stderr: ""
    Jan  3 12:04:47.182: INFO: stdout: ""
    Jan  3 12:04:47.182: INFO: update-demo-nautilus-b5dfb is created but not running
    Jan  3 12:04:52.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 12:04:52.337: INFO: stderr: ""
    Jan  3 12:04:52.337: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-lgpz8 "
    Jan  3 12:04:52.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 12:04:52.501: INFO: stderr: ""
    Jan  3 12:04:52.501: INFO: stdout: "true"
    Jan  3 12:04:52.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 12:04:52.651: INFO: stderr: ""
    Jan  3 12:04:52.651: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  3 12:04:52.651: INFO: validating pod update-demo-nautilus-b5dfb
    Jan  3 12:04:52.783: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 12:04:52.783: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 12:04:52.783: INFO: update-demo-nautilus-b5dfb is verified up and running
    Jan  3 12:04:52.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-lgpz8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 12:04:52.931: INFO: stderr: ""
    Jan  3 12:04:52.931: INFO: stdout: "true"
    Jan  3 12:04:52.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-lgpz8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 12:04:53.070: INFO: stderr: ""
    Jan  3 12:04:53.070: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  3 12:04:53.070: INFO: validating pod update-demo-nautilus-lgpz8
    Jan  3 12:04:53.164: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 12:04:53.164: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 12:04:53.164: INFO: update-demo-nautilus-lgpz8 is verified up and running
    STEP: scaling down the replication controller 01/03/24 12:04:53.164
    Jan  3 12:04:53.169: INFO: scanned /root for discovery docs: <nil>
    Jan  3 12:04:53.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan  3 12:04:54.385: INFO: stderr: ""
    Jan  3 12:04:54.385: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/03/24 12:04:54.385
    Jan  3 12:04:54.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 12:04:54.531: INFO: stderr: ""
    Jan  3 12:04:54.531: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-lgpz8 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 01/03/24 12:04:54.531
    Jan  3 12:04:59.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 12:04:59.689: INFO: stderr: ""
    Jan  3 12:04:59.689: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-lgpz8 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 01/03/24 12:04:59.689
    Jan  3 12:05:04.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 12:05:04.835: INFO: stderr: ""
    Jan  3 12:05:04.835: INFO: stdout: "update-demo-nautilus-b5dfb "
    Jan  3 12:05:04.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 12:05:04.968: INFO: stderr: ""
    Jan  3 12:05:04.968: INFO: stdout: "true"
    Jan  3 12:05:04.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 12:05:05.093: INFO: stderr: ""
    Jan  3 12:05:05.093: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  3 12:05:05.093: INFO: validating pod update-demo-nautilus-b5dfb
    Jan  3 12:05:05.126: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 12:05:05.127: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 12:05:05.127: INFO: update-demo-nautilus-b5dfb is verified up and running
    STEP: scaling up the replication controller 01/03/24 12:05:05.127
    Jan  3 12:05:05.132: INFO: scanned /root for discovery docs: <nil>
    Jan  3 12:05:05.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan  3 12:05:06.342: INFO: stderr: ""
    Jan  3 12:05:06.342: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/03/24 12:05:06.342
    Jan  3 12:05:06.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 12:05:06.484: INFO: stderr: ""
    Jan  3 12:05:06.484: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-fhzbn "
    Jan  3 12:05:06.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 12:05:06.617: INFO: stderr: ""
    Jan  3 12:05:06.617: INFO: stdout: "true"
    Jan  3 12:05:06.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 12:05:06.757: INFO: stderr: ""
    Jan  3 12:05:06.758: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  3 12:05:06.758: INFO: validating pod update-demo-nautilus-b5dfb
    Jan  3 12:05:06.793: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 12:05:06.793: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 12:05:06.793: INFO: update-demo-nautilus-b5dfb is verified up and running
    Jan  3 12:05:06.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-fhzbn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 12:05:06.928: INFO: stderr: ""
    Jan  3 12:05:06.928: INFO: stdout: ""
    Jan  3 12:05:06.928: INFO: update-demo-nautilus-fhzbn is created but not running
    Jan  3 12:05:11.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 12:05:12.088: INFO: stderr: ""
    Jan  3 12:05:12.088: INFO: stdout: "update-demo-nautilus-b5dfb update-demo-nautilus-fhzbn "
    Jan  3 12:05:12.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 12:05:12.223: INFO: stderr: ""
    Jan  3 12:05:12.223: INFO: stdout: "true"
    Jan  3 12:05:12.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-b5dfb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 12:05:12.363: INFO: stderr: ""
    Jan  3 12:05:12.363: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  3 12:05:12.363: INFO: validating pod update-demo-nautilus-b5dfb
    Jan  3 12:05:12.396: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 12:05:12.396: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 12:05:12.396: INFO: update-demo-nautilus-b5dfb is verified up and running
    Jan  3 12:05:12.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-fhzbn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 12:05:12.539: INFO: stderr: ""
    Jan  3 12:05:12.539: INFO: stdout: "true"
    Jan  3 12:05:12.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods update-demo-nautilus-fhzbn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 12:05:12.682: INFO: stderr: ""
    Jan  3 12:05:12.682: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  3 12:05:12.682: INFO: validating pod update-demo-nautilus-fhzbn
    Jan  3 12:05:12.795: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 12:05:12.796: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 12:05:12.796: INFO: update-demo-nautilus-fhzbn is verified up and running
    STEP: using delete to clean up resources 01/03/24 12:05:12.796
    Jan  3 12:05:12.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 delete --grace-period=0 --force -f -'
    Jan  3 12:05:12.957: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 12:05:12.957: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan  3 12:05:12.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get rc,svc -l name=update-demo --no-headers'
    Jan  3 12:05:13.126: INFO: stderr: "No resources found in kubectl-9545 namespace.\n"
    Jan  3 12:05:13.127: INFO: stdout: ""
    Jan  3 12:05:13.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-9545 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  3 12:05:13.287: INFO: stderr: ""
    Jan  3 12:05:13.287: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:05:13.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9545" for this suite. 01/03/24 12:05:13.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:05:13.343
Jan  3 12:05:13.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename endpointslice 01/03/24 12:05:13.347
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:05:13.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:05:13.425
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan  3 12:05:13.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2387" for this suite. 01/03/24 12:05:13.611
------------------------------
• [0.296 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:05:13.343
    Jan  3 12:05:13.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename endpointslice 01/03/24 12:05:13.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:05:13.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:05:13.425
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:05:13.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2387" for this suite. 01/03/24 12:05:13.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:05:13.641
Jan  3 12:05:13.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename crd-webhook 01/03/24 12:05:13.643
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:05:13.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:05:13.738
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/03/24 12:05:13.768
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/03/24 12:05:14.028
STEP: Deploying the custom resource conversion webhook pod 01/03/24 12:05:14.054
STEP: Wait for the deployment to be ready 01/03/24 12:05:14.094
Jan  3 12:05:14.163: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 5, 14, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-crd-conversion-webhook-deployment-74ff66dd47\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/03/24 12:05:16.183
STEP: Verifying the service has paired with the endpoint 01/03/24 12:05:16.209
Jan  3 12:05:17.210: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan  3 12:05:17.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Creating a v1 custom resource 01/03/24 12:05:20.222
STEP: Create a v2 custom resource 01/03/24 12:05:20.288
STEP: List CRs in v1 01/03/24 12:05:20.489
STEP: List CRs in v2 01/03/24 12:05:20.541
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:05:21.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-9932" for this suite. 01/03/24 12:05:21.336
------------------------------
• [SLOW TEST] [7.725 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:05:13.641
    Jan  3 12:05:13.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename crd-webhook 01/03/24 12:05:13.643
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:05:13.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:05:13.738
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/03/24 12:05:13.768
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/03/24 12:05:14.028
    STEP: Deploying the custom resource conversion webhook pod 01/03/24 12:05:14.054
    STEP: Wait for the deployment to be ready 01/03/24 12:05:14.094
    Jan  3 12:05:14.163: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 5, 14, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-crd-conversion-webhook-deployment-74ff66dd47\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 5, 14, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 5, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/03/24 12:05:16.183
    STEP: Verifying the service has paired with the endpoint 01/03/24 12:05:16.209
    Jan  3 12:05:17.210: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan  3 12:05:17.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Creating a v1 custom resource 01/03/24 12:05:20.222
    STEP: Create a v2 custom resource 01/03/24 12:05:20.288
    STEP: List CRs in v1 01/03/24 12:05:20.489
    STEP: List CRs in v2 01/03/24 12:05:20.541
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:05:21.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-9932" for this suite. 01/03/24 12:05:21.336
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:05:21.367
Jan  3 12:05:21.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 12:05:21.369
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:05:21.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:05:21.445
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-9607 01/03/24 12:05:21.473
STEP: creating service affinity-nodeport-transition in namespace services-9607 01/03/24 12:05:21.473
STEP: creating replication controller affinity-nodeport-transition in namespace services-9607 01/03/24 12:05:21.51
I0103 12:05:21.530182      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9607, replica count: 3
I0103 12:05:24.584429      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 12:05:24.649: INFO: Creating new exec pod
Jan  3 12:05:24.671: INFO: Waiting up to 5m0s for pod "execpod-affinitybf5xx" in namespace "services-9607" to be "running"
Jan  3 12:05:24.712: INFO: Pod "execpod-affinitybf5xx": Phase="Pending", Reason="", readiness=false. Elapsed: 41.095099ms
Jan  3 12:05:26.735: INFO: Pod "execpod-affinitybf5xx": Phase="Running", Reason="", readiness=true. Elapsed: 2.063902736s
Jan  3 12:05:26.735: INFO: Pod "execpod-affinitybf5xx" satisfied condition "running"
Jan  3 12:05:27.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9607 exec execpod-affinitybf5xx -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan  3 12:05:28.234: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan  3 12:05:28.234: INFO: stdout: ""
Jan  3 12:05:28.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9607 exec execpod-affinitybf5xx -- /bin/sh -x -c nc -v -z -w 2 10.233.19.38 80'
Jan  3 12:05:28.683: INFO: stderr: "+ nc -v -z -w 2 10.233.19.38 80\nConnection to 10.233.19.38 80 port [tcp/http] succeeded!\n"
Jan  3 12:05:28.683: INFO: stdout: ""
Jan  3 12:05:28.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9607 exec execpod-affinitybf5xx -- /bin/sh -x -c nc -v -z -w 2 85.215.162.129 32314'
Jan  3 12:05:29.144: INFO: stderr: "+ nc -v -z -w 2 85.215.162.129 32314\nConnection to 85.215.162.129 32314 port [tcp/*] succeeded!\n"
Jan  3 12:05:29.144: INFO: stdout: ""
Jan  3 12:05:29.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9607 exec execpod-affinitybf5xx -- /bin/sh -x -c nc -v -z -w 2 185.132.46.116 32314'
Jan  3 12:05:29.575: INFO: stderr: "+ nc -v -z -w 2 185.132.46.116 32314\nConnection to 185.132.46.116 32314 port [tcp/*] succeeded!\n"
Jan  3 12:05:29.575: INFO: stdout: ""
Jan  3 12:05:29.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9607 exec execpod-affinitybf5xx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://185.132.46.116:32314/ ; done'
Jan  3 12:05:30.194: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n"
Jan  3 12:05:30.194: INFO: stdout: "\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-mvccf\naffinity-nodeport-transition-kjzkj\naffinity-nodeport-transition-kjzkj\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-kjzkj\naffinity-nodeport-transition-kjzkj\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-mvccf\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-kjzkj\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-kjzkj\naffinity-nodeport-transition-mvccf\naffinity-nodeport-transition-mvccf"
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-mvccf
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-kjzkj
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-kjzkj
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-kjzkj
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-kjzkj
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-mvccf
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-kjzkj
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-kjzkj
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-mvccf
Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-mvccf
Jan  3 12:05:30.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9607 exec execpod-affinitybf5xx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://185.132.46.116:32314/ ; done'
Jan  3 12:05:30.766: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n"
Jan  3 12:05:30.766: INFO: stdout: "\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v"
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
Jan  3 12:05:30.766: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9607, will wait for the garbage collector to delete the pods 01/03/24 12:05:30.805
Jan  3 12:05:30.899: INFO: Deleting ReplicationController affinity-nodeport-transition took: 23.717971ms
Jan  3 12:05:30.999: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.477865ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 12:05:34.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9607" for this suite. 01/03/24 12:05:34.084
------------------------------
• [SLOW TEST] [12.743 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:05:21.367
    Jan  3 12:05:21.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 12:05:21.369
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:05:21.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:05:21.445
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-9607 01/03/24 12:05:21.473
    STEP: creating service affinity-nodeport-transition in namespace services-9607 01/03/24 12:05:21.473
    STEP: creating replication controller affinity-nodeport-transition in namespace services-9607 01/03/24 12:05:21.51
    I0103 12:05:21.530182      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9607, replica count: 3
    I0103 12:05:24.584429      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 12:05:24.649: INFO: Creating new exec pod
    Jan  3 12:05:24.671: INFO: Waiting up to 5m0s for pod "execpod-affinitybf5xx" in namespace "services-9607" to be "running"
    Jan  3 12:05:24.712: INFO: Pod "execpod-affinitybf5xx": Phase="Pending", Reason="", readiness=false. Elapsed: 41.095099ms
    Jan  3 12:05:26.735: INFO: Pod "execpod-affinitybf5xx": Phase="Running", Reason="", readiness=true. Elapsed: 2.063902736s
    Jan  3 12:05:26.735: INFO: Pod "execpod-affinitybf5xx" satisfied condition "running"
    Jan  3 12:05:27.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9607 exec execpod-affinitybf5xx -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan  3 12:05:28.234: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan  3 12:05:28.234: INFO: stdout: ""
    Jan  3 12:05:28.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9607 exec execpod-affinitybf5xx -- /bin/sh -x -c nc -v -z -w 2 10.233.19.38 80'
    Jan  3 12:05:28.683: INFO: stderr: "+ nc -v -z -w 2 10.233.19.38 80\nConnection to 10.233.19.38 80 port [tcp/http] succeeded!\n"
    Jan  3 12:05:28.683: INFO: stdout: ""
    Jan  3 12:05:28.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9607 exec execpod-affinitybf5xx -- /bin/sh -x -c nc -v -z -w 2 85.215.162.129 32314'
    Jan  3 12:05:29.144: INFO: stderr: "+ nc -v -z -w 2 85.215.162.129 32314\nConnection to 85.215.162.129 32314 port [tcp/*] succeeded!\n"
    Jan  3 12:05:29.144: INFO: stdout: ""
    Jan  3 12:05:29.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9607 exec execpod-affinitybf5xx -- /bin/sh -x -c nc -v -z -w 2 185.132.46.116 32314'
    Jan  3 12:05:29.575: INFO: stderr: "+ nc -v -z -w 2 185.132.46.116 32314\nConnection to 185.132.46.116 32314 port [tcp/*] succeeded!\n"
    Jan  3 12:05:29.575: INFO: stdout: ""
    Jan  3 12:05:29.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9607 exec execpod-affinitybf5xx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://185.132.46.116:32314/ ; done'
    Jan  3 12:05:30.194: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n"
    Jan  3 12:05:30.194: INFO: stdout: "\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-mvccf\naffinity-nodeport-transition-kjzkj\naffinity-nodeport-transition-kjzkj\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-kjzkj\naffinity-nodeport-transition-kjzkj\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-mvccf\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-kjzkj\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-kjzkj\naffinity-nodeport-transition-mvccf\naffinity-nodeport-transition-mvccf"
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-mvccf
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-kjzkj
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-kjzkj
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-kjzkj
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-kjzkj
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-mvccf
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-kjzkj
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-kjzkj
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-mvccf
    Jan  3 12:05:30.194: INFO: Received response from host: affinity-nodeport-transition-mvccf
    Jan  3 12:05:30.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9607 exec execpod-affinitybf5xx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://185.132.46.116:32314/ ; done'
    Jan  3 12:05:30.766: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://185.132.46.116:32314/\n"
    Jan  3 12:05:30.766: INFO: stdout: "\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v\naffinity-nodeport-transition-hll2v"
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Received response from host: affinity-nodeport-transition-hll2v
    Jan  3 12:05:30.766: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9607, will wait for the garbage collector to delete the pods 01/03/24 12:05:30.805
    Jan  3 12:05:30.899: INFO: Deleting ReplicationController affinity-nodeport-transition took: 23.717971ms
    Jan  3 12:05:30.999: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.477865ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:05:34.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9607" for this suite. 01/03/24 12:05:34.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:05:34.118
Jan  3 12:05:34.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename deployment 01/03/24 12:05:34.12
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:05:34.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:05:34.201
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan  3 12:05:34.229: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan  3 12:05:34.270: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/03/24 12:05:34.27
Jan  3 12:05:34.271: INFO: Waiting up to 5m0s for pod "test-rolling-update-controller-lbj2q" in namespace "deployment-1650" to be "running"
Jan  3 12:05:34.290: INFO: Pod "test-rolling-update-controller-lbj2q": Phase="Pending", Reason="", readiness=false. Elapsed: 19.02129ms
Jan  3 12:05:36.310: INFO: Pod "test-rolling-update-controller-lbj2q": Phase="Running", Reason="", readiness=true. Elapsed: 2.038808091s
Jan  3 12:05:36.310: INFO: Pod "test-rolling-update-controller-lbj2q" satisfied condition "running"
Jan  3 12:05:36.310: INFO: Creating deployment "test-rolling-update-deployment"
Jan  3 12:05:36.328: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan  3 12:05:36.372: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Jan  3 12:05:38.410: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan  3 12:05:38.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 5, 36, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 5, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 5, 36, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 5, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:05:40.445: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 12:05:40.497: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1650  d73f9734-e0fd-4366-a7c3-3380380d79ec 33980892923 1 2024-01-03 12:05:36 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2024-01-03 12:05:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:05:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f7b7e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-03 12:05:36 +0000 UTC,LastTransitionTime:2024-01-03 12:05:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2024-01-03 12:05:38 +0000 UTC,LastTransitionTime:2024-01-03 12:05:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  3 12:05:40.517: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1650  08c2c912-86cf-4a8d-8fbe-9e62e8bbd05c 33980892905 1 2024-01-03 12:05:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d73f9734-e0fd-4366-a7c3-3380380d79ec 0xc0041ab327 0xc0041ab328}] [] [{kube-controller-manager Update apps/v1 2024-01-03 12:05:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d73f9734-e0fd-4366-a7c3-3380380d79ec\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:05:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041ab3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  3 12:05:40.517: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan  3 12:05:40.518: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1650  c6e4ff7f-56ef-411e-bdc0-99d69dc2daa6 33980892921 2 2024-01-03 12:05:34 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d73f9734-e0fd-4366-a7c3-3380380d79ec 0xc0041ab1f7 0xc0041ab1f8}] [] [{e2e.test Update apps/v1 2024-01-03 12:05:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:05:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d73f9734-e0fd-4366-a7c3-3380380d79ec\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:05:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0041ab2b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 12:05:40.538: INFO: Pod "test-rolling-update-deployment-7549d9f46d-rkczg" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-rkczg test-rolling-update-deployment-7549d9f46d- deployment-1650  727ca1da-0d4a-4806-a7f7-3033cd042902 33980892902 0 2024-01-03 12:05:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:ea74d1776a77a565e41e55a67fc2cdf5bc8281690299d962e51e830bb7c7d319 cni.projectcalico.org/podIP:10.221.146.107/32 cni.projectcalico.org/podIPs:10.221.146.107/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 08c2c912-86cf-4a8d-8fbe-9e62e8bbd05c 0xc003f7bb87 0xc003f7bb88}] [] [{kube-controller-manager Update v1 2024-01-03 12:05:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08c2c912-86cf-4a8d-8fbe-9e62e8bbd05c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:05:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zvbhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zvbhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:05:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:05:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:05:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:05:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.107,StartTime:2024-01-03 12:05:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:05:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://5d89fc13888fb4ad008368fe4a94ccda11cf7f04a92a7a2714fa51e9665d347a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  3 12:05:40.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1650" for this suite. 01/03/24 12:05:40.568
------------------------------
• [SLOW TEST] [6.476 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:05:34.118
    Jan  3 12:05:34.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename deployment 01/03/24 12:05:34.12
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:05:34.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:05:34.201
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan  3 12:05:34.229: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan  3 12:05:34.270: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/03/24 12:05:34.27
    Jan  3 12:05:34.271: INFO: Waiting up to 5m0s for pod "test-rolling-update-controller-lbj2q" in namespace "deployment-1650" to be "running"
    Jan  3 12:05:34.290: INFO: Pod "test-rolling-update-controller-lbj2q": Phase="Pending", Reason="", readiness=false. Elapsed: 19.02129ms
    Jan  3 12:05:36.310: INFO: Pod "test-rolling-update-controller-lbj2q": Phase="Running", Reason="", readiness=true. Elapsed: 2.038808091s
    Jan  3 12:05:36.310: INFO: Pod "test-rolling-update-controller-lbj2q" satisfied condition "running"
    Jan  3 12:05:36.310: INFO: Creating deployment "test-rolling-update-deployment"
    Jan  3 12:05:36.328: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan  3 12:05:36.372: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
    Jan  3 12:05:38.410: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan  3 12:05:38.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 5, 36, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 5, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 5, 36, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 5, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:05:40.445: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 12:05:40.497: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1650  d73f9734-e0fd-4366-a7c3-3380380d79ec 33980892923 1 2024-01-03 12:05:36 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2024-01-03 12:05:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:05:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f7b7e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-03 12:05:36 +0000 UTC,LastTransitionTime:2024-01-03 12:05:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2024-01-03 12:05:38 +0000 UTC,LastTransitionTime:2024-01-03 12:05:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  3 12:05:40.517: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1650  08c2c912-86cf-4a8d-8fbe-9e62e8bbd05c 33980892905 1 2024-01-03 12:05:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d73f9734-e0fd-4366-a7c3-3380380d79ec 0xc0041ab327 0xc0041ab328}] [] [{kube-controller-manager Update apps/v1 2024-01-03 12:05:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d73f9734-e0fd-4366-a7c3-3380380d79ec\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:05:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041ab3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 12:05:40.517: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan  3 12:05:40.518: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1650  c6e4ff7f-56ef-411e-bdc0-99d69dc2daa6 33980892921 2 2024-01-03 12:05:34 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d73f9734-e0fd-4366-a7c3-3380380d79ec 0xc0041ab1f7 0xc0041ab1f8}] [] [{e2e.test Update apps/v1 2024-01-03 12:05:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:05:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d73f9734-e0fd-4366-a7c3-3380380d79ec\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:05:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0041ab2b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 12:05:40.538: INFO: Pod "test-rolling-update-deployment-7549d9f46d-rkczg" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-rkczg test-rolling-update-deployment-7549d9f46d- deployment-1650  727ca1da-0d4a-4806-a7f7-3033cd042902 33980892902 0 2024-01-03 12:05:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:ea74d1776a77a565e41e55a67fc2cdf5bc8281690299d962e51e830bb7c7d319 cni.projectcalico.org/podIP:10.221.146.107/32 cni.projectcalico.org/podIPs:10.221.146.107/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 08c2c912-86cf-4a8d-8fbe-9e62e8bbd05c 0xc003f7bb87 0xc003f7bb88}] [] [{kube-controller-manager Update v1 2024-01-03 12:05:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08c2c912-86cf-4a8d-8fbe-9e62e8bbd05c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:05:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zvbhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zvbhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:05:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:05:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:05:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:05:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.107,StartTime:2024-01-03 12:05:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:05:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://5d89fc13888fb4ad008368fe4a94ccda11cf7f04a92a7a2714fa51e9665d347a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:05:40.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1650" for this suite. 01/03/24 12:05:40.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:05:40.597
Jan  3 12:05:40.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename secrets 01/03/24 12:05:40.599
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:05:40.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:05:40.715
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-8fa94b17-07a4-4c83-bee1-806869f0d2fa 01/03/24 12:05:40.763
STEP: Creating secret with name s-test-opt-upd-e2a5e34d-8af0-4482-a2d8-80f559101a86 01/03/24 12:05:40.782
STEP: Creating the pod 01/03/24 12:05:40.801
Jan  3 12:05:40.830: INFO: Waiting up to 5m0s for pod "pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2" in namespace "secrets-4277" to be "running and ready"
Jan  3 12:05:40.849: INFO: Pod "pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.725992ms
Jan  3 12:05:40.849: INFO: The phase of Pod pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:05:42.868: INFO: Pod "pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038319715s
Jan  3 12:05:42.868: INFO: The phase of Pod pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:05:44.869: INFO: Pod "pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2": Phase="Running", Reason="", readiness=true. Elapsed: 4.039607408s
Jan  3 12:05:44.869: INFO: The phase of Pod pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2 is Running (Ready = true)
Jan  3 12:05:44.869: INFO: Pod "pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-8fa94b17-07a4-4c83-bee1-806869f0d2fa 01/03/24 12:05:45.173
STEP: Updating secret s-test-opt-upd-e2a5e34d-8af0-4482-a2d8-80f559101a86 01/03/24 12:05:45.194
STEP: Creating secret with name s-test-opt-create-4be300b5-bc96-46ab-8e83-22e99423d6dc 01/03/24 12:05:45.212
STEP: waiting to observe update in volume 01/03/24 12:05:45.233
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  3 12:06:48.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4277" for this suite. 01/03/24 12:06:48.986
------------------------------
• [SLOW TEST] [68.415 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:05:40.597
    Jan  3 12:05:40.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename secrets 01/03/24 12:05:40.599
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:05:40.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:05:40.715
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-8fa94b17-07a4-4c83-bee1-806869f0d2fa 01/03/24 12:05:40.763
    STEP: Creating secret with name s-test-opt-upd-e2a5e34d-8af0-4482-a2d8-80f559101a86 01/03/24 12:05:40.782
    STEP: Creating the pod 01/03/24 12:05:40.801
    Jan  3 12:05:40.830: INFO: Waiting up to 5m0s for pod "pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2" in namespace "secrets-4277" to be "running and ready"
    Jan  3 12:05:40.849: INFO: Pod "pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.725992ms
    Jan  3 12:05:40.849: INFO: The phase of Pod pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:05:42.868: INFO: Pod "pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038319715s
    Jan  3 12:05:42.868: INFO: The phase of Pod pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:05:44.869: INFO: Pod "pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2": Phase="Running", Reason="", readiness=true. Elapsed: 4.039607408s
    Jan  3 12:05:44.869: INFO: The phase of Pod pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2 is Running (Ready = true)
    Jan  3 12:05:44.869: INFO: Pod "pod-secrets-02ac3275-2f72-41f8-b37d-17b204dea7e2" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-8fa94b17-07a4-4c83-bee1-806869f0d2fa 01/03/24 12:05:45.173
    STEP: Updating secret s-test-opt-upd-e2a5e34d-8af0-4482-a2d8-80f559101a86 01/03/24 12:05:45.194
    STEP: Creating secret with name s-test-opt-create-4be300b5-bc96-46ab-8e83-22e99423d6dc 01/03/24 12:05:45.212
    STEP: waiting to observe update in volume 01/03/24 12:05:45.233
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:06:48.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4277" for this suite. 01/03/24 12:06:48.986
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:06:49.012
Jan  3 12:06:49.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename sched-preemption 01/03/24 12:06:49.015
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:06:49.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:06:49.087
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan  3 12:06:49.169: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  3 12:07:49.318: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:07:49.337
Jan  3 12:07:49.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename sched-preemption-path 01/03/24 12:07:49.34
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:07:49.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:07:49.449
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 01/03/24 12:07:49.476
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/03/24 12:07:49.476
Jan  3 12:07:49.507: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2723" to be "running"
Jan  3 12:07:49.525: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 18.388551ms
Jan  3 12:07:51.546: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.039362942s
Jan  3 12:07:51.546: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/03/24 12:07:51.565
Jan  3 12:07:51.601: INFO: found a healthy node: jb-1-26-np-64kerjapxk
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Jan  3 12:08:01.910: INFO: pods created so far: [1 1 1]
Jan  3 12:08:01.910: INFO: length of pods created so far: 3
Jan  3 12:08:05.951: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jan  3 12:08:12.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:08:13.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-2723" for this suite. 01/03/24 12:08:13.284
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8099" for this suite. 01/03/24 12:08:13.313
------------------------------
• [SLOW TEST] [84.323 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:06:49.012
    Jan  3 12:06:49.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename sched-preemption 01/03/24 12:06:49.015
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:06:49.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:06:49.087
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan  3 12:06:49.169: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  3 12:07:49.318: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:07:49.337
    Jan  3 12:07:49.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename sched-preemption-path 01/03/24 12:07:49.34
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:07:49.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:07:49.449
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 01/03/24 12:07:49.476
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/03/24 12:07:49.476
    Jan  3 12:07:49.507: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2723" to be "running"
    Jan  3 12:07:49.525: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 18.388551ms
    Jan  3 12:07:51.546: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.039362942s
    Jan  3 12:07:51.546: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/03/24 12:07:51.565
    Jan  3 12:07:51.601: INFO: found a healthy node: jb-1-26-np-64kerjapxk
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Jan  3 12:08:01.910: INFO: pods created so far: [1 1 1]
    Jan  3 12:08:01.910: INFO: length of pods created so far: 3
    Jan  3 12:08:05.951: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:08:12.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:08:13.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-2723" for this suite. 01/03/24 12:08:13.284
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8099" for this suite. 01/03/24 12:08:13.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:08:13.338
Jan  3 12:08:13.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 12:08:13.34
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:08:13.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:08:13.421
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jan  3 12:08:13.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-5761 version'
Jan  3 12:08:13.581: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan  3 12:08:13.581: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.9\", GitCommit:\"d1483fdf7a0578c83523bc1e2212a606a44fd71d\", GitTreeState:\"clean\", BuildDate:\"2023-09-13T11:32:41Z\", GoVersion:\"go1.20.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.9\", GitCommit:\"d1483fdf7a0578c83523bc1e2212a606a44fd71d\", GitTreeState:\"clean\", BuildDate:\"2023-09-13T11:25:26Z\", GoVersion:\"go1.20.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 12:08:13.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5761" for this suite. 01/03/24 12:08:13.601
------------------------------
• [0.288 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:08:13.338
    Jan  3 12:08:13.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 12:08:13.34
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:08:13.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:08:13.421
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jan  3 12:08:13.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-5761 version'
    Jan  3 12:08:13.581: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan  3 12:08:13.581: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.9\", GitCommit:\"d1483fdf7a0578c83523bc1e2212a606a44fd71d\", GitTreeState:\"clean\", BuildDate:\"2023-09-13T11:32:41Z\", GoVersion:\"go1.20.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.9\", GitCommit:\"d1483fdf7a0578c83523bc1e2212a606a44fd71d\", GitTreeState:\"clean\", BuildDate:\"2023-09-13T11:25:26Z\", GoVersion:\"go1.20.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:08:13.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5761" for this suite. 01/03/24 12:08:13.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:08:13.627
Jan  3 12:08:13.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:08:13.629
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:08:13.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:08:13.706
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-6b6a4f18-85a9-4210-8c77-a8696a360105 01/03/24 12:08:13.733
STEP: Creating a pod to test consume secrets 01/03/24 12:08:13.752
Jan  3 12:08:13.781: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a" in namespace "projected-1080" to be "Succeeded or Failed"
Jan  3 12:08:13.799: INFO: Pod "pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.924907ms
Jan  3 12:08:15.819: INFO: Pod "pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037419313s
Jan  3 12:08:17.821: INFO: Pod "pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039210868s
Jan  3 12:08:19.823: INFO: Pod "pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041802996s
STEP: Saw pod success 01/03/24 12:08:19.823
Jan  3 12:08:19.824: INFO: Pod "pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a" satisfied condition "Succeeded or Failed"
Jan  3 12:08:19.842: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a container projected-secret-volume-test: <nil>
STEP: delete the pod 01/03/24 12:08:20.015
Jan  3 12:08:20.127: INFO: Waiting for pod pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a to disappear
Jan  3 12:08:20.164: INFO: Pod pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  3 12:08:20.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1080" for this suite. 01/03/24 12:08:20.215
------------------------------
• [SLOW TEST] [6.615 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:08:13.627
    Jan  3 12:08:13.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:08:13.629
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:08:13.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:08:13.706
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-6b6a4f18-85a9-4210-8c77-a8696a360105 01/03/24 12:08:13.733
    STEP: Creating a pod to test consume secrets 01/03/24 12:08:13.752
    Jan  3 12:08:13.781: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a" in namespace "projected-1080" to be "Succeeded or Failed"
    Jan  3 12:08:13.799: INFO: Pod "pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.924907ms
    Jan  3 12:08:15.819: INFO: Pod "pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037419313s
    Jan  3 12:08:17.821: INFO: Pod "pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039210868s
    Jan  3 12:08:19.823: INFO: Pod "pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041802996s
    STEP: Saw pod success 01/03/24 12:08:19.823
    Jan  3 12:08:19.824: INFO: Pod "pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a" satisfied condition "Succeeded or Failed"
    Jan  3 12:08:19.842: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/03/24 12:08:20.015
    Jan  3 12:08:20.127: INFO: Waiting for pod pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a to disappear
    Jan  3 12:08:20.164: INFO: Pod pod-projected-secrets-10c3a469-3cfb-4296-b0c6-e3ffcff5ed4a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:08:20.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1080" for this suite. 01/03/24 12:08:20.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:08:20.249
Jan  3 12:08:20.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pods 01/03/24 12:08:20.251
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:08:20.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:08:20.334
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 01/03/24 12:08:20.361
STEP: submitting the pod to kubernetes 01/03/24 12:08:20.362
Jan  3 12:08:20.430: INFO: Waiting up to 5m0s for pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447" in namespace "pods-6000" to be "running and ready"
Jan  3 12:08:20.451: INFO: Pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447": Phase="Pending", Reason="", readiness=false. Elapsed: 20.564411ms
Jan  3 12:08:20.451: INFO: The phase of Pod pod-update-0d5f059b-546b-48e8-907c-2821b1957447 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:08:22.470: INFO: Pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447": Phase="Running", Reason="", readiness=true. Elapsed: 2.040005995s
Jan  3 12:08:22.470: INFO: The phase of Pod pod-update-0d5f059b-546b-48e8-907c-2821b1957447 is Running (Ready = true)
Jan  3 12:08:22.470: INFO: Pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/03/24 12:08:22.489
STEP: updating the pod 01/03/24 12:08:22.508
Jan  3 12:08:23.059: INFO: Successfully updated pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447"
Jan  3 12:08:23.059: INFO: Waiting up to 5m0s for pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447" in namespace "pods-6000" to be "running"
Jan  3 12:08:23.077: INFO: Pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447": Phase="Running", Reason="", readiness=true. Elapsed: 17.180244ms
Jan  3 12:08:23.077: INFO: Pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/03/24 12:08:23.077
Jan  3 12:08:23.097: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  3 12:08:23.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6000" for this suite. 01/03/24 12:08:23.128
------------------------------
• [2.905 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:08:20.249
    Jan  3 12:08:20.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pods 01/03/24 12:08:20.251
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:08:20.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:08:20.334
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 01/03/24 12:08:20.361
    STEP: submitting the pod to kubernetes 01/03/24 12:08:20.362
    Jan  3 12:08:20.430: INFO: Waiting up to 5m0s for pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447" in namespace "pods-6000" to be "running and ready"
    Jan  3 12:08:20.451: INFO: Pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447": Phase="Pending", Reason="", readiness=false. Elapsed: 20.564411ms
    Jan  3 12:08:20.451: INFO: The phase of Pod pod-update-0d5f059b-546b-48e8-907c-2821b1957447 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:08:22.470: INFO: Pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447": Phase="Running", Reason="", readiness=true. Elapsed: 2.040005995s
    Jan  3 12:08:22.470: INFO: The phase of Pod pod-update-0d5f059b-546b-48e8-907c-2821b1957447 is Running (Ready = true)
    Jan  3 12:08:22.470: INFO: Pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/03/24 12:08:22.489
    STEP: updating the pod 01/03/24 12:08:22.508
    Jan  3 12:08:23.059: INFO: Successfully updated pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447"
    Jan  3 12:08:23.059: INFO: Waiting up to 5m0s for pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447" in namespace "pods-6000" to be "running"
    Jan  3 12:08:23.077: INFO: Pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447": Phase="Running", Reason="", readiness=true. Elapsed: 17.180244ms
    Jan  3 12:08:23.077: INFO: Pod "pod-update-0d5f059b-546b-48e8-907c-2821b1957447" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/03/24 12:08:23.077
    Jan  3 12:08:23.097: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:08:23.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6000" for this suite. 01/03/24 12:08:23.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:08:23.161
Jan  3 12:08:23.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename job 01/03/24 12:08:23.163
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:08:23.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:08:23.241
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 01/03/24 12:08:23.268
STEP: Ensuring job reaches completions 01/03/24 12:08:23.288
STEP: Ensuring pods with index for job exist 01/03/24 12:08:35.31
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  3 12:08:35.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7398" for this suite. 01/03/24 12:08:35.367
------------------------------
• [SLOW TEST] [12.232 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:08:23.161
    Jan  3 12:08:23.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename job 01/03/24 12:08:23.163
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:08:23.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:08:23.241
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 01/03/24 12:08:23.268
    STEP: Ensuring job reaches completions 01/03/24 12:08:23.288
    STEP: Ensuring pods with index for job exist 01/03/24 12:08:35.31
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:08:35.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7398" for this suite. 01/03/24 12:08:35.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:08:35.397
Jan  3 12:08:35.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename resourcequota 01/03/24 12:08:35.4
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:08:35.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:08:35.477
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 01/03/24 12:08:35.505
STEP: Counting existing ResourceQuota 01/03/24 12:08:40.527
STEP: Creating a ResourceQuota 01/03/24 12:08:45.545
STEP: Ensuring resource quota status is calculated 01/03/24 12:08:45.565
STEP: Creating a Secret 01/03/24 12:08:47.586
STEP: Ensuring resource quota status captures secret creation 01/03/24 12:08:47.618
STEP: Deleting a secret 01/03/24 12:08:49.639
STEP: Ensuring resource quota status released usage 01/03/24 12:08:49.661
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  3 12:08:51.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2718" for this suite. 01/03/24 12:08:51.714
------------------------------
• [SLOW TEST] [16.341 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:08:35.397
    Jan  3 12:08:35.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename resourcequota 01/03/24 12:08:35.4
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:08:35.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:08:35.477
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 01/03/24 12:08:35.505
    STEP: Counting existing ResourceQuota 01/03/24 12:08:40.527
    STEP: Creating a ResourceQuota 01/03/24 12:08:45.545
    STEP: Ensuring resource quota status is calculated 01/03/24 12:08:45.565
    STEP: Creating a Secret 01/03/24 12:08:47.586
    STEP: Ensuring resource quota status captures secret creation 01/03/24 12:08:47.618
    STEP: Deleting a secret 01/03/24 12:08:49.639
    STEP: Ensuring resource quota status released usage 01/03/24 12:08:49.661
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:08:51.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2718" for this suite. 01/03/24 12:08:51.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:08:51.743
Jan  3 12:08:51.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-probe 01/03/24 12:08:51.745
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:08:51.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:08:51.826
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Jan  3 12:08:51.929: INFO: Waiting up to 5m0s for pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c" in namespace "container-probe-3097" to be "running and ready"
Jan  3 12:08:51.946: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.366794ms
Jan  3 12:08:51.946: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:08:53.969: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 2.039999715s
Jan  3 12:08:53.969: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
Jan  3 12:08:55.965: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 4.035718662s
Jan  3 12:08:55.965: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
Jan  3 12:08:57.975: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 6.046645919s
Jan  3 12:08:57.976: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
Jan  3 12:08:59.971: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 8.042323577s
Jan  3 12:08:59.971: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
Jan  3 12:09:01.966: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 10.03741151s
Jan  3 12:09:01.966: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
Jan  3 12:09:03.969: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 12.040241371s
Jan  3 12:09:03.969: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
Jan  3 12:09:05.966: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 14.036774582s
Jan  3 12:09:05.966: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
Jan  3 12:09:07.968: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 16.039519237s
Jan  3 12:09:07.968: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
Jan  3 12:09:09.965: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 18.035995633s
Jan  3 12:09:09.965: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
Jan  3 12:09:11.968: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 20.039383875s
Jan  3 12:09:11.968: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
Jan  3 12:09:13.965: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=true. Elapsed: 22.035994798s
Jan  3 12:09:13.965: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = true)
Jan  3 12:09:13.965: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c" satisfied condition "running and ready"
Jan  3 12:09:13.982: INFO: Container started at 2024-01-03 12:08:52 +0000 UTC, pod became ready at 2024-01-03 12:09:12 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  3 12:09:13.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3097" for this suite. 01/03/24 12:09:14.014
------------------------------
• [SLOW TEST] [22.297 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:08:51.743
    Jan  3 12:08:51.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-probe 01/03/24 12:08:51.745
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:08:51.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:08:51.826
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Jan  3 12:08:51.929: INFO: Waiting up to 5m0s for pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c" in namespace "container-probe-3097" to be "running and ready"
    Jan  3 12:08:51.946: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.366794ms
    Jan  3 12:08:51.946: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:08:53.969: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 2.039999715s
    Jan  3 12:08:53.969: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
    Jan  3 12:08:55.965: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 4.035718662s
    Jan  3 12:08:55.965: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
    Jan  3 12:08:57.975: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 6.046645919s
    Jan  3 12:08:57.976: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
    Jan  3 12:08:59.971: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 8.042323577s
    Jan  3 12:08:59.971: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
    Jan  3 12:09:01.966: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 10.03741151s
    Jan  3 12:09:01.966: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
    Jan  3 12:09:03.969: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 12.040241371s
    Jan  3 12:09:03.969: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
    Jan  3 12:09:05.966: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 14.036774582s
    Jan  3 12:09:05.966: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
    Jan  3 12:09:07.968: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 16.039519237s
    Jan  3 12:09:07.968: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
    Jan  3 12:09:09.965: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 18.035995633s
    Jan  3 12:09:09.965: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
    Jan  3 12:09:11.968: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=false. Elapsed: 20.039383875s
    Jan  3 12:09:11.968: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = false)
    Jan  3 12:09:13.965: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c": Phase="Running", Reason="", readiness=true. Elapsed: 22.035994798s
    Jan  3 12:09:13.965: INFO: The phase of Pod test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c is Running (Ready = true)
    Jan  3 12:09:13.965: INFO: Pod "test-webserver-f080f01b-cc80-4ee6-a816-d836de46453c" satisfied condition "running and ready"
    Jan  3 12:09:13.982: INFO: Container started at 2024-01-03 12:08:52 +0000 UTC, pod became ready at 2024-01-03 12:09:12 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:09:13.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3097" for this suite. 01/03/24 12:09:14.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:09:14.043
Jan  3 12:09:14.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename csiinlinevolumes 01/03/24 12:09:14.045
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:09:14.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:09:14.13
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 01/03/24 12:09:14.16
STEP: getting 01/03/24 12:09:14.218
STEP: listing in namespace 01/03/24 12:09:14.235
STEP: patching 01/03/24 12:09:14.254
STEP: deleting 01/03/24 12:09:14.278
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:09:14.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-6926" for this suite. 01/03/24 12:09:14.352
------------------------------
• [0.349 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:09:14.043
    Jan  3 12:09:14.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename csiinlinevolumes 01/03/24 12:09:14.045
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:09:14.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:09:14.13
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 01/03/24 12:09:14.16
    STEP: getting 01/03/24 12:09:14.218
    STEP: listing in namespace 01/03/24 12:09:14.235
    STEP: patching 01/03/24 12:09:14.254
    STEP: deleting 01/03/24 12:09:14.278
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:09:14.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-6926" for this suite. 01/03/24 12:09:14.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:09:14.397
Jan  3 12:09:14.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 12:09:14.399
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:09:14.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:09:14.481
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 01/03/24 12:09:14.51
Jan  3 12:09:14.510: INFO: Creating e2e-svc-a-2jmg4
Jan  3 12:09:14.538: INFO: Creating e2e-svc-b-vb6zz
Jan  3 12:09:14.567: INFO: Creating e2e-svc-c-sn2vq
STEP: deleting service collection 01/03/24 12:09:14.608
Jan  3 12:09:14.681: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 12:09:14.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1623" for this suite. 01/03/24 12:09:14.703
------------------------------
• [0.330 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:09:14.397
    Jan  3 12:09:14.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 12:09:14.399
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:09:14.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:09:14.481
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 01/03/24 12:09:14.51
    Jan  3 12:09:14.510: INFO: Creating e2e-svc-a-2jmg4
    Jan  3 12:09:14.538: INFO: Creating e2e-svc-b-vb6zz
    Jan  3 12:09:14.567: INFO: Creating e2e-svc-c-sn2vq
    STEP: deleting service collection 01/03/24 12:09:14.608
    Jan  3 12:09:14.681: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:09:14.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1623" for this suite. 01/03/24 12:09:14.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:09:14.728
Jan  3 12:09:14.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename aggregator 01/03/24 12:09:14.732
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:09:14.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:09:14.809
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan  3 12:09:14.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/03/24 12:09:14.836
Jan  3 12:09:15.279: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jan  3 12:09:17.434: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:19.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:21.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:23.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:25.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:27.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:29.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:31.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:33.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:35.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:37.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:39.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:41.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:43.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:45.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:47.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:49.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:51.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:53.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:55.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:57.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:09:59.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:01.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:03.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:05.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:07.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:09.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:11.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:13.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:15.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:17.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:19.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:21.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:23.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:25.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:27.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:29.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 12:10:31.761: INFO: Waited 283.080616ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/03/24 12:10:32.101
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/03/24 12:10:32.118
STEP: List APIServices 01/03/24 12:10:32.14
Jan  3 12:10:32.163: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jan  3 12:10:32.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-2036" for this suite. 01/03/24 12:10:32.63
------------------------------
• [SLOW TEST] [77.926 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:09:14.728
    Jan  3 12:09:14.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename aggregator 01/03/24 12:09:14.732
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:09:14.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:09:14.809
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan  3 12:09:14.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/03/24 12:09:14.836
    Jan  3 12:09:15.279: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Jan  3 12:09:17.434: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:19.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:21.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:23.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:25.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:27.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:29.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:31.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:33.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:35.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:37.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:39.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:41.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:43.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:45.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:47.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:49.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:51.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:53.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:55.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:57.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:09:59.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:01.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:03.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:05.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:07.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:09.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:11.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:13.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:15.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:17.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:19.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:21.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:23.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:25.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:27.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:29.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 9, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 12:10:31.761: INFO: Waited 283.080616ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/03/24 12:10:32.101
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/03/24 12:10:32.118
    STEP: List APIServices 01/03/24 12:10:32.14
    Jan  3 12:10:32.163: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:10:32.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-2036" for this suite. 01/03/24 12:10:32.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:10:32.658
Jan  3 12:10:32.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 12:10:32.66
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:10:32.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:10:32.75
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jan  3 12:10:32.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/03/24 12:10:35.309
Jan  3 12:10:35.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3998 --namespace=crd-publish-openapi-3998 create -f -'
Jan  3 12:10:36.803: INFO: stderr: ""
Jan  3 12:10:36.803: INFO: stdout: "e2e-test-crd-publish-openapi-44-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan  3 12:10:36.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3998 --namespace=crd-publish-openapi-3998 delete e2e-test-crd-publish-openapi-44-crds test-cr'
Jan  3 12:10:36.988: INFO: stderr: ""
Jan  3 12:10:36.988: INFO: stdout: "e2e-test-crd-publish-openapi-44-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan  3 12:10:36.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3998 --namespace=crd-publish-openapi-3998 apply -f -'
Jan  3 12:10:37.341: INFO: stderr: ""
Jan  3 12:10:37.341: INFO: stdout: "e2e-test-crd-publish-openapi-44-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan  3 12:10:37.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3998 --namespace=crd-publish-openapi-3998 delete e2e-test-crd-publish-openapi-44-crds test-cr'
Jan  3 12:10:37.539: INFO: stderr: ""
Jan  3 12:10:37.539: INFO: stdout: "e2e-test-crd-publish-openapi-44-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/03/24 12:10:37.539
Jan  3 12:10:37.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3998 explain e2e-test-crd-publish-openapi-44-crds'
Jan  3 12:10:38.582: INFO: stderr: ""
Jan  3 12:10:38.582: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-44-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:10:41.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3998" for this suite. 01/03/24 12:10:41.203
------------------------------
• [SLOW TEST] [8.573 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:10:32.658
    Jan  3 12:10:32.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 12:10:32.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:10:32.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:10:32.75
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jan  3 12:10:32.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/03/24 12:10:35.309
    Jan  3 12:10:35.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3998 --namespace=crd-publish-openapi-3998 create -f -'
    Jan  3 12:10:36.803: INFO: stderr: ""
    Jan  3 12:10:36.803: INFO: stdout: "e2e-test-crd-publish-openapi-44-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan  3 12:10:36.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3998 --namespace=crd-publish-openapi-3998 delete e2e-test-crd-publish-openapi-44-crds test-cr'
    Jan  3 12:10:36.988: INFO: stderr: ""
    Jan  3 12:10:36.988: INFO: stdout: "e2e-test-crd-publish-openapi-44-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan  3 12:10:36.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3998 --namespace=crd-publish-openapi-3998 apply -f -'
    Jan  3 12:10:37.341: INFO: stderr: ""
    Jan  3 12:10:37.341: INFO: stdout: "e2e-test-crd-publish-openapi-44-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan  3 12:10:37.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3998 --namespace=crd-publish-openapi-3998 delete e2e-test-crd-publish-openapi-44-crds test-cr'
    Jan  3 12:10:37.539: INFO: stderr: ""
    Jan  3 12:10:37.539: INFO: stdout: "e2e-test-crd-publish-openapi-44-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/03/24 12:10:37.539
    Jan  3 12:10:37.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3998 explain e2e-test-crd-publish-openapi-44-crds'
    Jan  3 12:10:38.582: INFO: stderr: ""
    Jan  3 12:10:38.582: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-44-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:10:41.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3998" for this suite. 01/03/24 12:10:41.203
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:10:41.232
Jan  3 12:10:41.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename subpath 01/03/24 12:10:41.235
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:10:41.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:10:41.319
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/03/24 12:10:41.348
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-8p2f 01/03/24 12:10:41.388
STEP: Creating a pod to test atomic-volume-subpath 01/03/24 12:10:41.388
Jan  3 12:10:41.415: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8p2f" in namespace "subpath-7979" to be "Succeeded or Failed"
Jan  3 12:10:41.435: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.904332ms
Jan  3 12:10:43.457: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.042158839s
Jan  3 12:10:45.454: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 4.039709442s
Jan  3 12:10:47.460: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 6.044811059s
Jan  3 12:10:49.455: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 8.039819239s
Jan  3 12:10:51.706: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 10.291306041s
Jan  3 12:10:53.456: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 12.041552446s
Jan  3 12:10:55.459: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 14.043764273s
Jan  3 12:10:57.456: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 16.041206552s
Jan  3 12:10:59.457: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 18.042287548s
Jan  3 12:11:01.455: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 20.040559028s
Jan  3 12:11:03.457: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=false. Elapsed: 22.042449424s
Jan  3 12:11:05.456: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.041461063s
STEP: Saw pod success 01/03/24 12:11:05.456
Jan  3 12:11:05.457: INFO: Pod "pod-subpath-test-secret-8p2f" satisfied condition "Succeeded or Failed"
Jan  3 12:11:05.478: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-subpath-test-secret-8p2f container test-container-subpath-secret-8p2f: <nil>
STEP: delete the pod 01/03/24 12:11:05.62
Jan  3 12:11:05.699: INFO: Waiting for pod pod-subpath-test-secret-8p2f to disappear
Jan  3 12:11:05.721: INFO: Pod pod-subpath-test-secret-8p2f no longer exists
STEP: Deleting pod pod-subpath-test-secret-8p2f 01/03/24 12:11:05.721
Jan  3 12:11:05.721: INFO: Deleting pod "pod-subpath-test-secret-8p2f" in namespace "subpath-7979"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  3 12:11:05.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7979" for this suite. 01/03/24 12:11:05.78
------------------------------
• [SLOW TEST] [24.573 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:10:41.232
    Jan  3 12:10:41.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename subpath 01/03/24 12:10:41.235
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:10:41.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:10:41.319
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/03/24 12:10:41.348
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-8p2f 01/03/24 12:10:41.388
    STEP: Creating a pod to test atomic-volume-subpath 01/03/24 12:10:41.388
    Jan  3 12:10:41.415: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8p2f" in namespace "subpath-7979" to be "Succeeded or Failed"
    Jan  3 12:10:41.435: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.904332ms
    Jan  3 12:10:43.457: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.042158839s
    Jan  3 12:10:45.454: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 4.039709442s
    Jan  3 12:10:47.460: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 6.044811059s
    Jan  3 12:10:49.455: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 8.039819239s
    Jan  3 12:10:51.706: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 10.291306041s
    Jan  3 12:10:53.456: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 12.041552446s
    Jan  3 12:10:55.459: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 14.043764273s
    Jan  3 12:10:57.456: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 16.041206552s
    Jan  3 12:10:59.457: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 18.042287548s
    Jan  3 12:11:01.455: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=true. Elapsed: 20.040559028s
    Jan  3 12:11:03.457: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Running", Reason="", readiness=false. Elapsed: 22.042449424s
    Jan  3 12:11:05.456: INFO: Pod "pod-subpath-test-secret-8p2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.041461063s
    STEP: Saw pod success 01/03/24 12:11:05.456
    Jan  3 12:11:05.457: INFO: Pod "pod-subpath-test-secret-8p2f" satisfied condition "Succeeded or Failed"
    Jan  3 12:11:05.478: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-subpath-test-secret-8p2f container test-container-subpath-secret-8p2f: <nil>
    STEP: delete the pod 01/03/24 12:11:05.62
    Jan  3 12:11:05.699: INFO: Waiting for pod pod-subpath-test-secret-8p2f to disappear
    Jan  3 12:11:05.721: INFO: Pod pod-subpath-test-secret-8p2f no longer exists
    STEP: Deleting pod pod-subpath-test-secret-8p2f 01/03/24 12:11:05.721
    Jan  3 12:11:05.721: INFO: Deleting pod "pod-subpath-test-secret-8p2f" in namespace "subpath-7979"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:11:05.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7979" for this suite. 01/03/24 12:11:05.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:11:05.81
Jan  3 12:11:05.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 12:11:05.812
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:05.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:05.895
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 01/03/24 12:11:05.942
STEP: watching for the Service to be added 01/03/24 12:11:05.976
Jan  3 12:11:05.992: INFO: Found Service test-service-qlzj6 in namespace services-6751 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan  3 12:11:05.992: INFO: Service test-service-qlzj6 created
STEP: Getting /status 01/03/24 12:11:05.992
Jan  3 12:11:06.009: INFO: Service test-service-qlzj6 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/03/24 12:11:06.009
STEP: watching for the Service to be patched 01/03/24 12:11:06.029
Jan  3 12:11:06.043: INFO: observed Service test-service-qlzj6 in namespace services-6751 with annotations: map[] & LoadBalancer: {[]}
Jan  3 12:11:06.043: INFO: Found Service test-service-qlzj6 in namespace services-6751 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan  3 12:11:06.043: INFO: Service test-service-qlzj6 has service status patched
STEP: updating the ServiceStatus 01/03/24 12:11:06.043
Jan  3 12:11:06.079: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/03/24 12:11:06.079
Jan  3 12:11:06.092: INFO: Observed Service test-service-qlzj6 in namespace services-6751 with annotations: map[] & Conditions: {[]}
Jan  3 12:11:06.092: INFO: Observed event: &Service{ObjectMeta:{test-service-qlzj6  services-6751  334e6395-2c9f-4890-9e3d-4b16ea0e2051 33980932099 0 2024-01-03 12:11:05 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2024-01-03 12:11:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2024-01-03 12:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.42.42,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.42.42],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan  3 12:11:06.093: INFO: Found Service test-service-qlzj6 in namespace services-6751 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  3 12:11:06.093: INFO: Service test-service-qlzj6 has service status updated
STEP: patching the service 01/03/24 12:11:06.093
STEP: watching for the Service to be patched 01/03/24 12:11:06.12
Jan  3 12:11:06.133: INFO: observed Service test-service-qlzj6 in namespace services-6751 with labels: map[test-service-static:true]
Jan  3 12:11:06.133: INFO: observed Service test-service-qlzj6 in namespace services-6751 with labels: map[test-service-static:true]
Jan  3 12:11:06.133: INFO: observed Service test-service-qlzj6 in namespace services-6751 with labels: map[test-service-static:true]
Jan  3 12:11:06.133: INFO: Found Service test-service-qlzj6 in namespace services-6751 with labels: map[test-service:patched test-service-static:true]
Jan  3 12:11:06.133: INFO: Service test-service-qlzj6 patched
STEP: deleting the service 01/03/24 12:11:06.133
STEP: watching for the Service to be deleted 01/03/24 12:11:06.171
Jan  3 12:11:06.185: INFO: Observed event: ADDED
Jan  3 12:11:06.185: INFO: Observed event: MODIFIED
Jan  3 12:11:06.185: INFO: Observed event: MODIFIED
Jan  3 12:11:06.185: INFO: Observed event: MODIFIED
Jan  3 12:11:06.186: INFO: Found Service test-service-qlzj6 in namespace services-6751 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan  3 12:11:06.186: INFO: Service test-service-qlzj6 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 12:11:06.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6751" for this suite. 01/03/24 12:11:06.206
------------------------------
• [0.422 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:11:05.81
    Jan  3 12:11:05.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 12:11:05.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:05.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:05.895
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 01/03/24 12:11:05.942
    STEP: watching for the Service to be added 01/03/24 12:11:05.976
    Jan  3 12:11:05.992: INFO: Found Service test-service-qlzj6 in namespace services-6751 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan  3 12:11:05.992: INFO: Service test-service-qlzj6 created
    STEP: Getting /status 01/03/24 12:11:05.992
    Jan  3 12:11:06.009: INFO: Service test-service-qlzj6 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/03/24 12:11:06.009
    STEP: watching for the Service to be patched 01/03/24 12:11:06.029
    Jan  3 12:11:06.043: INFO: observed Service test-service-qlzj6 in namespace services-6751 with annotations: map[] & LoadBalancer: {[]}
    Jan  3 12:11:06.043: INFO: Found Service test-service-qlzj6 in namespace services-6751 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan  3 12:11:06.043: INFO: Service test-service-qlzj6 has service status patched
    STEP: updating the ServiceStatus 01/03/24 12:11:06.043
    Jan  3 12:11:06.079: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/03/24 12:11:06.079
    Jan  3 12:11:06.092: INFO: Observed Service test-service-qlzj6 in namespace services-6751 with annotations: map[] & Conditions: {[]}
    Jan  3 12:11:06.092: INFO: Observed event: &Service{ObjectMeta:{test-service-qlzj6  services-6751  334e6395-2c9f-4890-9e3d-4b16ea0e2051 33980932099 0 2024-01-03 12:11:05 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2024-01-03 12:11:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2024-01-03 12:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.42.42,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.42.42],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan  3 12:11:06.093: INFO: Found Service test-service-qlzj6 in namespace services-6751 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  3 12:11:06.093: INFO: Service test-service-qlzj6 has service status updated
    STEP: patching the service 01/03/24 12:11:06.093
    STEP: watching for the Service to be patched 01/03/24 12:11:06.12
    Jan  3 12:11:06.133: INFO: observed Service test-service-qlzj6 in namespace services-6751 with labels: map[test-service-static:true]
    Jan  3 12:11:06.133: INFO: observed Service test-service-qlzj6 in namespace services-6751 with labels: map[test-service-static:true]
    Jan  3 12:11:06.133: INFO: observed Service test-service-qlzj6 in namespace services-6751 with labels: map[test-service-static:true]
    Jan  3 12:11:06.133: INFO: Found Service test-service-qlzj6 in namespace services-6751 with labels: map[test-service:patched test-service-static:true]
    Jan  3 12:11:06.133: INFO: Service test-service-qlzj6 patched
    STEP: deleting the service 01/03/24 12:11:06.133
    STEP: watching for the Service to be deleted 01/03/24 12:11:06.171
    Jan  3 12:11:06.185: INFO: Observed event: ADDED
    Jan  3 12:11:06.185: INFO: Observed event: MODIFIED
    Jan  3 12:11:06.185: INFO: Observed event: MODIFIED
    Jan  3 12:11:06.185: INFO: Observed event: MODIFIED
    Jan  3 12:11:06.186: INFO: Found Service test-service-qlzj6 in namespace services-6751 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan  3 12:11:06.186: INFO: Service test-service-qlzj6 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:11:06.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6751" for this suite. 01/03/24 12:11:06.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:11:06.235
Jan  3 12:11:06.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 12:11:06.237
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:06.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:06.333
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2776 01/03/24 12:11:06.36
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/03/24 12:11:06.391
STEP: creating service externalsvc in namespace services-2776 01/03/24 12:11:06.391
STEP: creating replication controller externalsvc in namespace services-2776 01/03/24 12:11:06.415
I0103 12:11:06.438283      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2776, replica count: 2
I0103 12:11:09.489826      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/03/24 12:11:09.507
Jan  3 12:11:09.552: INFO: Creating new exec pod
Jan  3 12:11:09.582: INFO: Waiting up to 5m0s for pod "execpodbxtxl" in namespace "services-2776" to be "running"
Jan  3 12:11:09.601: INFO: Pod "execpodbxtxl": Phase="Pending", Reason="", readiness=false. Elapsed: 19.027434ms
Jan  3 12:11:11.627: INFO: Pod "execpodbxtxl": Phase="Running", Reason="", readiness=true. Elapsed: 2.045515718s
Jan  3 12:11:11.628: INFO: Pod "execpodbxtxl" satisfied condition "running"
Jan  3 12:11:11.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-2776 exec execpodbxtxl -- /bin/sh -x -c nslookup clusterip-service.services-2776.svc.cluster.local'
Jan  3 12:11:12.513: INFO: stderr: "+ nslookup clusterip-service.services-2776.svc.cluster.local\n"
Jan  3 12:11:12.513: INFO: stdout: "Server:\t\t10.233.26.106\nAddress:\t10.233.26.106#53\n\nclusterip-service.services-2776.svc.cluster.local\tcanonical name = externalsvc.services-2776.svc.cluster.local.\nName:\texternalsvc.services-2776.svc.cluster.local\nAddress: 10.233.9.63\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2776, will wait for the garbage collector to delete the pods 01/03/24 12:11:12.513
Jan  3 12:11:12.623: INFO: Deleting ReplicationController externalsvc took: 28.836262ms
Jan  3 12:11:12.723: INFO: Terminating ReplicationController externalsvc pods took: 100.150247ms
Jan  3 12:11:15.377: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 12:11:15.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2776" for this suite. 01/03/24 12:11:15.44
------------------------------
• [SLOW TEST] [9.233 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:11:06.235
    Jan  3 12:11:06.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 12:11:06.237
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:06.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:06.333
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2776 01/03/24 12:11:06.36
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/03/24 12:11:06.391
    STEP: creating service externalsvc in namespace services-2776 01/03/24 12:11:06.391
    STEP: creating replication controller externalsvc in namespace services-2776 01/03/24 12:11:06.415
    I0103 12:11:06.438283      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2776, replica count: 2
    I0103 12:11:09.489826      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/03/24 12:11:09.507
    Jan  3 12:11:09.552: INFO: Creating new exec pod
    Jan  3 12:11:09.582: INFO: Waiting up to 5m0s for pod "execpodbxtxl" in namespace "services-2776" to be "running"
    Jan  3 12:11:09.601: INFO: Pod "execpodbxtxl": Phase="Pending", Reason="", readiness=false. Elapsed: 19.027434ms
    Jan  3 12:11:11.627: INFO: Pod "execpodbxtxl": Phase="Running", Reason="", readiness=true. Elapsed: 2.045515718s
    Jan  3 12:11:11.628: INFO: Pod "execpodbxtxl" satisfied condition "running"
    Jan  3 12:11:11.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-2776 exec execpodbxtxl -- /bin/sh -x -c nslookup clusterip-service.services-2776.svc.cluster.local'
    Jan  3 12:11:12.513: INFO: stderr: "+ nslookup clusterip-service.services-2776.svc.cluster.local\n"
    Jan  3 12:11:12.513: INFO: stdout: "Server:\t\t10.233.26.106\nAddress:\t10.233.26.106#53\n\nclusterip-service.services-2776.svc.cluster.local\tcanonical name = externalsvc.services-2776.svc.cluster.local.\nName:\texternalsvc.services-2776.svc.cluster.local\nAddress: 10.233.9.63\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2776, will wait for the garbage collector to delete the pods 01/03/24 12:11:12.513
    Jan  3 12:11:12.623: INFO: Deleting ReplicationController externalsvc took: 28.836262ms
    Jan  3 12:11:12.723: INFO: Terminating ReplicationController externalsvc pods took: 100.150247ms
    Jan  3 12:11:15.377: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:11:15.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2776" for this suite. 01/03/24 12:11:15.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:11:15.476
Jan  3 12:11:15.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename svcaccounts 01/03/24 12:11:15.478
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:15.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:15.563
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  01/03/24 12:11:15.592
Jan  3 12:11:15.621: INFO: Waiting up to 5m0s for pod "test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee" in namespace "svcaccounts-5795" to be "Succeeded or Failed"
Jan  3 12:11:15.639: INFO: Pod "test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 18.23886ms
Jan  3 12:11:17.661: INFO: Pod "test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039958432s
Jan  3 12:11:19.661: INFO: Pod "test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039981541s
STEP: Saw pod success 01/03/24 12:11:19.661
Jan  3 12:11:19.661: INFO: Pod "test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee" satisfied condition "Succeeded or Failed"
Jan  3 12:11:19.682: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee container agnhost-container: <nil>
STEP: delete the pod 01/03/24 12:11:19.725
Jan  3 12:11:19.762: INFO: Waiting for pod test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee to disappear
Jan  3 12:11:19.781: INFO: Pod test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  3 12:11:19.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5795" for this suite. 01/03/24 12:11:19.813
------------------------------
• [4.362 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:11:15.476
    Jan  3 12:11:15.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename svcaccounts 01/03/24 12:11:15.478
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:15.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:15.563
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  01/03/24 12:11:15.592
    Jan  3 12:11:15.621: INFO: Waiting up to 5m0s for pod "test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee" in namespace "svcaccounts-5795" to be "Succeeded or Failed"
    Jan  3 12:11:15.639: INFO: Pod "test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 18.23886ms
    Jan  3 12:11:17.661: INFO: Pod "test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039958432s
    Jan  3 12:11:19.661: INFO: Pod "test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039981541s
    STEP: Saw pod success 01/03/24 12:11:19.661
    Jan  3 12:11:19.661: INFO: Pod "test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee" satisfied condition "Succeeded or Failed"
    Jan  3 12:11:19.682: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 12:11:19.725
    Jan  3 12:11:19.762: INFO: Waiting for pod test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee to disappear
    Jan  3 12:11:19.781: INFO: Pod test-pod-9877bf65-b183-4d09-9c57-18b52090c6ee no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:11:19.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5795" for this suite. 01/03/24 12:11:19.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:11:19.844
Jan  3 12:11:19.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename gc 01/03/24 12:11:19.847
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:19.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:19.933
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan  3 12:11:20.121: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"735015b7-54cf-4d01-b356-4cc9971f2c0d", Controller:(*bool)(0xc0048e4ee6), BlockOwnerDeletion:(*bool)(0xc0048e4ee7)}}
Jan  3 12:11:20.164: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a689dafb-d572-4740-a896-6c21e0a707fa", Controller:(*bool)(0xc004600286), BlockOwnerDeletion:(*bool)(0xc004600287)}}
Jan  3 12:11:20.187: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4cbd9e86-997f-44cf-a8c2-8fe8a24e63b0", Controller:(*bool)(0xc004600512), BlockOwnerDeletion:(*bool)(0xc004600513)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  3 12:11:25.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5888" for this suite. 01/03/24 12:11:25.28
------------------------------
• [SLOW TEST] [5.467 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:11:19.844
    Jan  3 12:11:19.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename gc 01/03/24 12:11:19.847
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:19.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:19.933
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan  3 12:11:20.121: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"735015b7-54cf-4d01-b356-4cc9971f2c0d", Controller:(*bool)(0xc0048e4ee6), BlockOwnerDeletion:(*bool)(0xc0048e4ee7)}}
    Jan  3 12:11:20.164: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a689dafb-d572-4740-a896-6c21e0a707fa", Controller:(*bool)(0xc004600286), BlockOwnerDeletion:(*bool)(0xc004600287)}}
    Jan  3 12:11:20.187: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4cbd9e86-997f-44cf-a8c2-8fe8a24e63b0", Controller:(*bool)(0xc004600512), BlockOwnerDeletion:(*bool)(0xc004600513)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:11:25.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5888" for this suite. 01/03/24 12:11:25.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:11:25.315
Jan  3 12:11:25.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename replicaset 01/03/24 12:11:25.318
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:25.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:25.406
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/03/24 12:11:25.433
STEP: Verify that the required pods have come up 01/03/24 12:11:25.452
Jan  3 12:11:25.470: INFO: Pod name sample-pod: Found 1 pods out of 3
Jan  3 12:11:30.491: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/03/24 12:11:30.491
Jan  3 12:11:30.509: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/03/24 12:11:30.509
STEP: DeleteCollection of the ReplicaSets 01/03/24 12:11:30.535
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/03/24 12:11:30.56
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  3 12:11:30.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5981" for this suite. 01/03/24 12:11:30.62
------------------------------
• [SLOW TEST] [5.333 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:11:25.315
    Jan  3 12:11:25.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename replicaset 01/03/24 12:11:25.318
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:25.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:25.406
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/03/24 12:11:25.433
    STEP: Verify that the required pods have come up 01/03/24 12:11:25.452
    Jan  3 12:11:25.470: INFO: Pod name sample-pod: Found 1 pods out of 3
    Jan  3 12:11:30.491: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/03/24 12:11:30.491
    Jan  3 12:11:30.509: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/03/24 12:11:30.509
    STEP: DeleteCollection of the ReplicaSets 01/03/24 12:11:30.535
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/03/24 12:11:30.56
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:11:30.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5981" for this suite. 01/03/24 12:11:30.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:11:30.652
Jan  3 12:11:30.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 12:11:30.653
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:30.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:30.732
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 01/03/24 12:11:30.761
Jan  3 12:11:30.790: INFO: Waiting up to 5m0s for pod "downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6" in namespace "downward-api-4002" to be "Succeeded or Failed"
Jan  3 12:11:30.809: INFO: Pod "downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01326ms
Jan  3 12:11:32.829: INFO: Pod "downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038266963s
Jan  3 12:11:34.829: INFO: Pod "downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038248044s
Jan  3 12:11:36.829: INFO: Pod "downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038529026s
STEP: Saw pod success 01/03/24 12:11:36.829
Jan  3 12:11:36.829: INFO: Pod "downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6" satisfied condition "Succeeded or Failed"
Jan  3 12:11:36.850: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6 container client-container: <nil>
STEP: delete the pod 01/03/24 12:11:36.895
Jan  3 12:11:36.937: INFO: Waiting for pod downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6 to disappear
Jan  3 12:11:36.956: INFO: Pod downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  3 12:11:36.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4002" for this suite. 01/03/24 12:11:36.987
------------------------------
• [SLOW TEST] [6.361 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:11:30.652
    Jan  3 12:11:30.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 12:11:30.653
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:30.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:30.732
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 01/03/24 12:11:30.761
    Jan  3 12:11:30.790: INFO: Waiting up to 5m0s for pod "downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6" in namespace "downward-api-4002" to be "Succeeded or Failed"
    Jan  3 12:11:30.809: INFO: Pod "downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01326ms
    Jan  3 12:11:32.829: INFO: Pod "downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038266963s
    Jan  3 12:11:34.829: INFO: Pod "downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038248044s
    Jan  3 12:11:36.829: INFO: Pod "downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038529026s
    STEP: Saw pod success 01/03/24 12:11:36.829
    Jan  3 12:11:36.829: INFO: Pod "downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6" satisfied condition "Succeeded or Failed"
    Jan  3 12:11:36.850: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6 container client-container: <nil>
    STEP: delete the pod 01/03/24 12:11:36.895
    Jan  3 12:11:36.937: INFO: Waiting for pod downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6 to disappear
    Jan  3 12:11:36.956: INFO: Pod downwardapi-volume-097efbb1-bb1d-4d7b-802f-a1ebc76cf9f6 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:11:36.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4002" for this suite. 01/03/24 12:11:36.987
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:11:37.013
Jan  3 12:11:37.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 12:11:37.014
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:37.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:37.11
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 12:11:37.18
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 12:11:37.351
STEP: Deploying the webhook pod 01/03/24 12:11:37.379
STEP: Wait for the deployment to be ready 01/03/24 12:11:37.415
Jan  3 12:11:37.448: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/24 12:11:39.5
STEP: Verifying the service has paired with the endpoint 01/03/24 12:11:39.53
Jan  3 12:11:40.531: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 01/03/24 12:11:40.551
STEP: create a pod 01/03/24 12:11:40.689
Jan  3 12:11:40.718: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6340" to be "running"
Jan  3 12:11:40.736: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.929871ms
Jan  3 12:11:42.756: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.03813493s
Jan  3 12:11:42.756: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/03/24 12:11:42.756
Jan  3 12:11:42.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=webhook-6340 attach --namespace=webhook-6340 to-be-attached-pod -i -c=container1'
Jan  3 12:11:43.074: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:11:43.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6340" for this suite. 01/03/24 12:11:43.237
STEP: Destroying namespace "webhook-6340-markers" for this suite. 01/03/24 12:11:43.264
------------------------------
• [SLOW TEST] [6.308 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:11:37.013
    Jan  3 12:11:37.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 12:11:37.014
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:37.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:37.11
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 12:11:37.18
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 12:11:37.351
    STEP: Deploying the webhook pod 01/03/24 12:11:37.379
    STEP: Wait for the deployment to be ready 01/03/24 12:11:37.415
    Jan  3 12:11:37.448: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/24 12:11:39.5
    STEP: Verifying the service has paired with the endpoint 01/03/24 12:11:39.53
    Jan  3 12:11:40.531: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 01/03/24 12:11:40.551
    STEP: create a pod 01/03/24 12:11:40.689
    Jan  3 12:11:40.718: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6340" to be "running"
    Jan  3 12:11:40.736: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.929871ms
    Jan  3 12:11:42.756: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.03813493s
    Jan  3 12:11:42.756: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/03/24 12:11:42.756
    Jan  3 12:11:42.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=webhook-6340 attach --namespace=webhook-6340 to-be-attached-pod -i -c=container1'
    Jan  3 12:11:43.074: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:11:43.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6340" for this suite. 01/03/24 12:11:43.237
    STEP: Destroying namespace "webhook-6340-markers" for this suite. 01/03/24 12:11:43.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:11:43.325
Jan  3 12:11:43.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:11:43.328
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:43.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:43.451
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 01/03/24 12:11:43.48
Jan  3 12:11:43.509: INFO: Waiting up to 5m0s for pod "annotationupdate615237f3-d5ae-4cde-adaf-1de4b99ffc20" in namespace "projected-1583" to be "running and ready"
Jan  3 12:11:43.535: INFO: Pod "annotationupdate615237f3-d5ae-4cde-adaf-1de4b99ffc20": Phase="Pending", Reason="", readiness=false. Elapsed: 26.137261ms
Jan  3 12:11:43.535: INFO: The phase of Pod annotationupdate615237f3-d5ae-4cde-adaf-1de4b99ffc20 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:11:45.568: INFO: Pod "annotationupdate615237f3-d5ae-4cde-adaf-1de4b99ffc20": Phase="Running", Reason="", readiness=true. Elapsed: 2.059204469s
Jan  3 12:11:45.568: INFO: The phase of Pod annotationupdate615237f3-d5ae-4cde-adaf-1de4b99ffc20 is Running (Ready = true)
Jan  3 12:11:45.568: INFO: Pod "annotationupdate615237f3-d5ae-4cde-adaf-1de4b99ffc20" satisfied condition "running and ready"
Jan  3 12:11:46.195: INFO: Successfully updated pod "annotationupdate615237f3-d5ae-4cde-adaf-1de4b99ffc20"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  3 12:11:48.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1583" for this suite. 01/03/24 12:11:48.314
------------------------------
• [SLOW TEST] [5.016 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:11:43.325
    Jan  3 12:11:43.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:11:43.328
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:43.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:43.451
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 01/03/24 12:11:43.48
    Jan  3 12:11:43.509: INFO: Waiting up to 5m0s for pod "annotationupdate615237f3-d5ae-4cde-adaf-1de4b99ffc20" in namespace "projected-1583" to be "running and ready"
    Jan  3 12:11:43.535: INFO: Pod "annotationupdate615237f3-d5ae-4cde-adaf-1de4b99ffc20": Phase="Pending", Reason="", readiness=false. Elapsed: 26.137261ms
    Jan  3 12:11:43.535: INFO: The phase of Pod annotationupdate615237f3-d5ae-4cde-adaf-1de4b99ffc20 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:11:45.568: INFO: Pod "annotationupdate615237f3-d5ae-4cde-adaf-1de4b99ffc20": Phase="Running", Reason="", readiness=true. Elapsed: 2.059204469s
    Jan  3 12:11:45.568: INFO: The phase of Pod annotationupdate615237f3-d5ae-4cde-adaf-1de4b99ffc20 is Running (Ready = true)
    Jan  3 12:11:45.568: INFO: Pod "annotationupdate615237f3-d5ae-4cde-adaf-1de4b99ffc20" satisfied condition "running and ready"
    Jan  3 12:11:46.195: INFO: Successfully updated pod "annotationupdate615237f3-d5ae-4cde-adaf-1de4b99ffc20"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:11:48.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1583" for this suite. 01/03/24 12:11:48.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:11:48.344
Jan  3 12:11:48.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename secrets 01/03/24 12:11:48.347
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:48.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:48.427
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-8d178c64-03f9-406c-b225-ebc4ec9b2f70 01/03/24 12:11:48.533
STEP: Creating a pod to test consume secrets 01/03/24 12:11:48.551
Jan  3 12:11:48.577: INFO: Waiting up to 5m0s for pod "pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35" in namespace "secrets-1563" to be "Succeeded or Failed"
Jan  3 12:11:48.625: INFO: Pod "pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35": Phase="Pending", Reason="", readiness=false. Elapsed: 47.712009ms
Jan  3 12:11:50.654: INFO: Pod "pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35": Phase="Running", Reason="", readiness=true. Elapsed: 2.076988743s
Jan  3 12:11:52.650: INFO: Pod "pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35": Phase="Running", Reason="", readiness=false. Elapsed: 4.072169346s
Jan  3 12:11:54.666: INFO: Pod "pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088770839s
STEP: Saw pod success 01/03/24 12:11:54.666
Jan  3 12:11:54.667: INFO: Pod "pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35" satisfied condition "Succeeded or Failed"
Jan  3 12:11:54.685: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35 container secret-volume-test: <nil>
STEP: delete the pod 01/03/24 12:11:54.726
Jan  3 12:11:54.777: INFO: Waiting for pod pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35 to disappear
Jan  3 12:11:54.795: INFO: Pod pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  3 12:11:54.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1563" for this suite. 01/03/24 12:11:54.851
STEP: Destroying namespace "secret-namespace-2359" for this suite. 01/03/24 12:11:54.88
------------------------------
• [SLOW TEST] [6.568 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:11:48.344
    Jan  3 12:11:48.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename secrets 01/03/24 12:11:48.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:48.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:48.427
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-8d178c64-03f9-406c-b225-ebc4ec9b2f70 01/03/24 12:11:48.533
    STEP: Creating a pod to test consume secrets 01/03/24 12:11:48.551
    Jan  3 12:11:48.577: INFO: Waiting up to 5m0s for pod "pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35" in namespace "secrets-1563" to be "Succeeded or Failed"
    Jan  3 12:11:48.625: INFO: Pod "pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35": Phase="Pending", Reason="", readiness=false. Elapsed: 47.712009ms
    Jan  3 12:11:50.654: INFO: Pod "pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35": Phase="Running", Reason="", readiness=true. Elapsed: 2.076988743s
    Jan  3 12:11:52.650: INFO: Pod "pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35": Phase="Running", Reason="", readiness=false. Elapsed: 4.072169346s
    Jan  3 12:11:54.666: INFO: Pod "pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088770839s
    STEP: Saw pod success 01/03/24 12:11:54.666
    Jan  3 12:11:54.667: INFO: Pod "pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35" satisfied condition "Succeeded or Failed"
    Jan  3 12:11:54.685: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35 container secret-volume-test: <nil>
    STEP: delete the pod 01/03/24 12:11:54.726
    Jan  3 12:11:54.777: INFO: Waiting for pod pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35 to disappear
    Jan  3 12:11:54.795: INFO: Pod pod-secrets-859ee428-ab1a-43b1-9069-4470147b6b35 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:11:54.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1563" for this suite. 01/03/24 12:11:54.851
    STEP: Destroying namespace "secret-namespace-2359" for this suite. 01/03/24 12:11:54.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:11:54.913
Jan  3 12:11:54.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename replication-controller 01/03/24 12:11:54.915
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:54.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:54.996
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-klbl4" 01/03/24 12:11:55.025
Jan  3 12:11:55.050: INFO: Get Replication Controller "e2e-rc-klbl4" to confirm replicas
Jan  3 12:11:56.072: INFO: Get Replication Controller "e2e-rc-klbl4" to confirm replicas
Jan  3 12:11:56.093: INFO: Found 1 replicas for "e2e-rc-klbl4" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-klbl4" 01/03/24 12:11:56.093
STEP: Updating a scale subresource 01/03/24 12:11:56.117
STEP: Verifying replicas where modified for replication controller "e2e-rc-klbl4" 01/03/24 12:11:56.145
Jan  3 12:11:56.146: INFO: Get Replication Controller "e2e-rc-klbl4" to confirm replicas
Jan  3 12:11:57.167: INFO: Get Replication Controller "e2e-rc-klbl4" to confirm replicas
Jan  3 12:11:57.188: INFO: Found 2 replicas for "e2e-rc-klbl4" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  3 12:11:57.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-948" for this suite. 01/03/24 12:11:57.22
------------------------------
• [2.335 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:11:54.913
    Jan  3 12:11:54.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename replication-controller 01/03/24 12:11:54.915
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:54.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:54.996
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-klbl4" 01/03/24 12:11:55.025
    Jan  3 12:11:55.050: INFO: Get Replication Controller "e2e-rc-klbl4" to confirm replicas
    Jan  3 12:11:56.072: INFO: Get Replication Controller "e2e-rc-klbl4" to confirm replicas
    Jan  3 12:11:56.093: INFO: Found 1 replicas for "e2e-rc-klbl4" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-klbl4" 01/03/24 12:11:56.093
    STEP: Updating a scale subresource 01/03/24 12:11:56.117
    STEP: Verifying replicas where modified for replication controller "e2e-rc-klbl4" 01/03/24 12:11:56.145
    Jan  3 12:11:56.146: INFO: Get Replication Controller "e2e-rc-klbl4" to confirm replicas
    Jan  3 12:11:57.167: INFO: Get Replication Controller "e2e-rc-klbl4" to confirm replicas
    Jan  3 12:11:57.188: INFO: Found 2 replicas for "e2e-rc-klbl4" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:11:57.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-948" for this suite. 01/03/24 12:11:57.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:11:57.25
Jan  3 12:11:57.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 12:11:57.251
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:57.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:57.334
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 01/03/24 12:11:57.362
Jan  3 12:11:57.399: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3" in namespace "downward-api-2197" to be "Succeeded or Failed"
Jan  3 12:11:57.417: INFO: Pod "downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.677108ms
Jan  3 12:11:59.437: INFO: Pod "downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038811876s
Jan  3 12:12:01.446: INFO: Pod "downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047117105s
STEP: Saw pod success 01/03/24 12:12:01.446
Jan  3 12:12:01.446: INFO: Pod "downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3" satisfied condition "Succeeded or Failed"
Jan  3 12:12:01.465: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3 container client-container: <nil>
STEP: delete the pod 01/03/24 12:12:01.507
Jan  3 12:12:01.552: INFO: Waiting for pod downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3 to disappear
Jan  3 12:12:01.575: INFO: Pod downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  3 12:12:01.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2197" for this suite. 01/03/24 12:12:01.609
------------------------------
• [4.388 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:11:57.25
    Jan  3 12:11:57.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 12:11:57.251
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:11:57.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:11:57.334
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 01/03/24 12:11:57.362
    Jan  3 12:11:57.399: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3" in namespace "downward-api-2197" to be "Succeeded or Failed"
    Jan  3 12:11:57.417: INFO: Pod "downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.677108ms
    Jan  3 12:11:59.437: INFO: Pod "downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038811876s
    Jan  3 12:12:01.446: INFO: Pod "downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047117105s
    STEP: Saw pod success 01/03/24 12:12:01.446
    Jan  3 12:12:01.446: INFO: Pod "downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3" satisfied condition "Succeeded or Failed"
    Jan  3 12:12:01.465: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3 container client-container: <nil>
    STEP: delete the pod 01/03/24 12:12:01.507
    Jan  3 12:12:01.552: INFO: Waiting for pod downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3 to disappear
    Jan  3 12:12:01.575: INFO: Pod downwardapi-volume-df94752b-ee3c-4c77-84c5-3cf186726cf3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:12:01.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2197" for this suite. 01/03/24 12:12:01.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:12:01.64
Jan  3 12:12:01.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename secrets 01/03/24 12:12:01.643
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:01.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:01.754
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-ca3cbcb4-e0a0-4c59-8a15-f12d6e3f0144 01/03/24 12:12:01.786
STEP: Creating a pod to test consume secrets 01/03/24 12:12:01.806
Jan  3 12:12:01.833: INFO: Waiting up to 5m0s for pod "pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027" in namespace "secrets-4402" to be "Succeeded or Failed"
Jan  3 12:12:01.850: INFO: Pod "pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027": Phase="Pending", Reason="", readiness=false. Elapsed: 17.146069ms
Jan  3 12:12:03.871: INFO: Pod "pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037768584s
Jan  3 12:12:05.870: INFO: Pod "pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036523171s
Jan  3 12:12:07.870: INFO: Pod "pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037292809s
STEP: Saw pod success 01/03/24 12:12:07.87
Jan  3 12:12:07.870: INFO: Pod "pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027" satisfied condition "Succeeded or Failed"
Jan  3 12:12:07.890: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027 container secret-volume-test: <nil>
STEP: delete the pod 01/03/24 12:12:07.927
Jan  3 12:12:07.984: INFO: Waiting for pod pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027 to disappear
Jan  3 12:12:08.002: INFO: Pod pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  3 12:12:08.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4402" for this suite. 01/03/24 12:12:08.033
------------------------------
• [SLOW TEST] [6.438 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:12:01.64
    Jan  3 12:12:01.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename secrets 01/03/24 12:12:01.643
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:01.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:01.754
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-ca3cbcb4-e0a0-4c59-8a15-f12d6e3f0144 01/03/24 12:12:01.786
    STEP: Creating a pod to test consume secrets 01/03/24 12:12:01.806
    Jan  3 12:12:01.833: INFO: Waiting up to 5m0s for pod "pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027" in namespace "secrets-4402" to be "Succeeded or Failed"
    Jan  3 12:12:01.850: INFO: Pod "pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027": Phase="Pending", Reason="", readiness=false. Elapsed: 17.146069ms
    Jan  3 12:12:03.871: INFO: Pod "pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037768584s
    Jan  3 12:12:05.870: INFO: Pod "pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036523171s
    Jan  3 12:12:07.870: INFO: Pod "pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037292809s
    STEP: Saw pod success 01/03/24 12:12:07.87
    Jan  3 12:12:07.870: INFO: Pod "pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027" satisfied condition "Succeeded or Failed"
    Jan  3 12:12:07.890: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027 container secret-volume-test: <nil>
    STEP: delete the pod 01/03/24 12:12:07.927
    Jan  3 12:12:07.984: INFO: Waiting for pod pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027 to disappear
    Jan  3 12:12:08.002: INFO: Pod pod-secrets-c06fac1a-0e41-4378-9283-a05331de1027 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:12:08.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4402" for this suite. 01/03/24 12:12:08.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:12:08.082
Jan  3 12:12:08.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename custom-resource-definition 01/03/24 12:12:08.084
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:08.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:08.164
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan  3 12:12:08.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:12:08.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3956" for this suite. 01/03/24 12:12:08.898
------------------------------
• [0.842 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:12:08.082
    Jan  3 12:12:08.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename custom-resource-definition 01/03/24 12:12:08.084
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:08.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:08.164
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan  3 12:12:08.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:12:08.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3956" for this suite. 01/03/24 12:12:08.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:12:08.93
Jan  3 12:12:08.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 12:12:08.933
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:08.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:09.02
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/03/24 12:12:09.048
Jan  3 12:12:09.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3215 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan  3 12:12:09.266: INFO: stderr: ""
Jan  3 12:12:09.266: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/03/24 12:12:09.266
Jan  3 12:12:09.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3215 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Jan  3 12:12:10.159: INFO: stderr: ""
Jan  3 12:12:10.159: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/03/24 12:12:10.159
Jan  3 12:12:10.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3215 delete pods e2e-test-httpd-pod'
Jan  3 12:12:14.437: INFO: stderr: ""
Jan  3 12:12:14.437: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 12:12:14.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3215" for this suite. 01/03/24 12:12:14.47
------------------------------
• [SLOW TEST] [5.586 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:12:08.93
    Jan  3 12:12:08.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 12:12:08.933
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:08.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:09.02
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/03/24 12:12:09.048
    Jan  3 12:12:09.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3215 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan  3 12:12:09.266: INFO: stderr: ""
    Jan  3 12:12:09.266: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/03/24 12:12:09.266
    Jan  3 12:12:09.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3215 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Jan  3 12:12:10.159: INFO: stderr: ""
    Jan  3 12:12:10.159: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/03/24 12:12:10.159
    Jan  3 12:12:10.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3215 delete pods e2e-test-httpd-pod'
    Jan  3 12:12:14.437: INFO: stderr: ""
    Jan  3 12:12:14.437: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:12:14.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3215" for this suite. 01/03/24 12:12:14.47
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:12:14.517
Jan  3 12:12:14.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename secrets 01/03/24 12:12:14.519
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:14.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:14.61
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 01/03/24 12:12:14.637
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/03/24 12:12:14.655
STEP: patching the secret 01/03/24 12:12:14.678
STEP: deleting the secret using a LabelSelector 01/03/24 12:12:14.723
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/03/24 12:12:14.746
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  3 12:12:14.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3038" for this suite. 01/03/24 12:12:14.787
------------------------------
• [0.297 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:12:14.517
    Jan  3 12:12:14.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename secrets 01/03/24 12:12:14.519
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:14.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:14.61
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 01/03/24 12:12:14.637
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/03/24 12:12:14.655
    STEP: patching the secret 01/03/24 12:12:14.678
    STEP: deleting the secret using a LabelSelector 01/03/24 12:12:14.723
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/03/24 12:12:14.746
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:12:14.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3038" for this suite. 01/03/24 12:12:14.787
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:12:14.836
Jan  3 12:12:14.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 12:12:14.838
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:14.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:14.919
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 01/03/24 12:12:14.949
Jan  3 12:12:14.980: INFO: Waiting up to 5m0s for pod "downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d" in namespace "downward-api-3834" to be "Succeeded or Failed"
Jan  3 12:12:15.000: INFO: Pod "downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.375379ms
Jan  3 12:12:17.022: INFO: Pod "downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d": Phase="Running", Reason="", readiness=true. Elapsed: 2.041866719s
Jan  3 12:12:19.020: INFO: Pod "downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d": Phase="Running", Reason="", readiness=false. Elapsed: 4.040357992s
Jan  3 12:12:21.022: INFO: Pod "downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041587725s
STEP: Saw pod success 01/03/24 12:12:21.022
Jan  3 12:12:21.022: INFO: Pod "downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d" satisfied condition "Succeeded or Failed"
Jan  3 12:12:21.042: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d container client-container: <nil>
STEP: delete the pod 01/03/24 12:12:21.088
Jan  3 12:12:21.133: INFO: Waiting for pod downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d to disappear
Jan  3 12:12:21.152: INFO: Pod downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  3 12:12:21.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3834" for this suite. 01/03/24 12:12:21.185
------------------------------
• [SLOW TEST] [6.378 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:12:14.836
    Jan  3 12:12:14.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 12:12:14.838
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:14.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:14.919
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 01/03/24 12:12:14.949
    Jan  3 12:12:14.980: INFO: Waiting up to 5m0s for pod "downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d" in namespace "downward-api-3834" to be "Succeeded or Failed"
    Jan  3 12:12:15.000: INFO: Pod "downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.375379ms
    Jan  3 12:12:17.022: INFO: Pod "downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d": Phase="Running", Reason="", readiness=true. Elapsed: 2.041866719s
    Jan  3 12:12:19.020: INFO: Pod "downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d": Phase="Running", Reason="", readiness=false. Elapsed: 4.040357992s
    Jan  3 12:12:21.022: INFO: Pod "downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041587725s
    STEP: Saw pod success 01/03/24 12:12:21.022
    Jan  3 12:12:21.022: INFO: Pod "downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d" satisfied condition "Succeeded or Failed"
    Jan  3 12:12:21.042: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d container client-container: <nil>
    STEP: delete the pod 01/03/24 12:12:21.088
    Jan  3 12:12:21.133: INFO: Waiting for pod downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d to disappear
    Jan  3 12:12:21.152: INFO: Pod downwardapi-volume-252e552a-356e-4b26-ad13-e8cbc3bfc34d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:12:21.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3834" for this suite. 01/03/24 12:12:21.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:12:21.218
Jan  3 12:12:21.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 12:12:21.22
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:21.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:21.309
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 01/03/24 12:12:21.337
Jan  3 12:12:21.368: INFO: Waiting up to 5m0s for pod "downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b" in namespace "downward-api-6989" to be "Succeeded or Failed"
Jan  3 12:12:21.403: INFO: Pod "downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b": Phase="Pending", Reason="", readiness=false. Elapsed: 34.918523ms
Jan  3 12:12:23.425: INFO: Pod "downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05694422s
Jan  3 12:12:25.425: INFO: Pod "downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057093417s
Jan  3 12:12:27.425: INFO: Pod "downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057039412s
STEP: Saw pod success 01/03/24 12:12:27.425
Jan  3 12:12:27.425: INFO: Pod "downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b" satisfied condition "Succeeded or Failed"
Jan  3 12:12:27.453: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b container dapi-container: <nil>
STEP: delete the pod 01/03/24 12:12:27.495
Jan  3 12:12:27.553: INFO: Waiting for pod downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b to disappear
Jan  3 12:12:27.574: INFO: Pod downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  3 12:12:27.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6989" for this suite. 01/03/24 12:12:27.609
------------------------------
• [SLOW TEST] [6.419 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:12:21.218
    Jan  3 12:12:21.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 12:12:21.22
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:21.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:21.309
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 01/03/24 12:12:21.337
    Jan  3 12:12:21.368: INFO: Waiting up to 5m0s for pod "downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b" in namespace "downward-api-6989" to be "Succeeded or Failed"
    Jan  3 12:12:21.403: INFO: Pod "downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b": Phase="Pending", Reason="", readiness=false. Elapsed: 34.918523ms
    Jan  3 12:12:23.425: INFO: Pod "downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05694422s
    Jan  3 12:12:25.425: INFO: Pod "downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057093417s
    Jan  3 12:12:27.425: INFO: Pod "downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057039412s
    STEP: Saw pod success 01/03/24 12:12:27.425
    Jan  3 12:12:27.425: INFO: Pod "downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b" satisfied condition "Succeeded or Failed"
    Jan  3 12:12:27.453: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b container dapi-container: <nil>
    STEP: delete the pod 01/03/24 12:12:27.495
    Jan  3 12:12:27.553: INFO: Waiting for pod downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b to disappear
    Jan  3 12:12:27.574: INFO: Pod downward-api-680c8636-9665-4ab1-a3db-4ed7de7f188b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:12:27.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6989" for this suite. 01/03/24 12:12:27.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:12:27.639
Jan  3 12:12:27.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename daemonsets 01/03/24 12:12:27.64
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:27.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:27.725
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
STEP: Creating a simple DaemonSet "daemon-set" 01/03/24 12:12:27.858
STEP: Check that daemon pods launch on every node of the cluster. 01/03/24 12:12:27.882
Jan  3 12:12:27.918: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:12:27.918: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:12:28.975: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:12:28.975: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:12:29.969: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 12:12:29.969: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/03/24 12:12:29.987
Jan  3 12:12:30.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 12:12:30.131: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/03/24 12:12:30.131
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 01/03/24 12:12:30.169
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4846, will wait for the garbage collector to delete the pods 01/03/24 12:12:30.17
Jan  3 12:12:30.269: INFO: Deleting DaemonSet.extensions daemon-set took: 26.797677ms
Jan  3 12:12:30.370: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.234109ms
Jan  3 12:12:33.590: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:12:33.590: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  3 12:12:33.610: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33980943197"},"items":null}

Jan  3 12:12:33.630: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33980943200"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:12:33.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4846" for this suite. 01/03/24 12:12:33.751
------------------------------
• [SLOW TEST] [6.140 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:12:27.639
    Jan  3 12:12:27.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename daemonsets 01/03/24 12:12:27.64
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:27.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:27.725
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:305
    STEP: Creating a simple DaemonSet "daemon-set" 01/03/24 12:12:27.858
    STEP: Check that daemon pods launch on every node of the cluster. 01/03/24 12:12:27.882
    Jan  3 12:12:27.918: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:12:27.918: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:12:28.975: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:12:28.975: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:12:29.969: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 12:12:29.969: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/03/24 12:12:29.987
    Jan  3 12:12:30.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 12:12:30.131: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/03/24 12:12:30.131
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 01/03/24 12:12:30.169
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4846, will wait for the garbage collector to delete the pods 01/03/24 12:12:30.17
    Jan  3 12:12:30.269: INFO: Deleting DaemonSet.extensions daemon-set took: 26.797677ms
    Jan  3 12:12:30.370: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.234109ms
    Jan  3 12:12:33.590: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:12:33.590: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  3 12:12:33.610: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33980943197"},"items":null}

    Jan  3 12:12:33.630: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33980943200"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:12:33.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4846" for this suite. 01/03/24 12:12:33.751
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:12:33.78
Jan  3 12:12:33.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename limitrange 01/03/24 12:12:33.783
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:33.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:33.867
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 01/03/24 12:12:33.895
STEP: Setting up watch 01/03/24 12:12:33.896
STEP: Submitting a LimitRange 01/03/24 12:12:34.015
STEP: Verifying LimitRange creation was observed 01/03/24 12:12:34.037
STEP: Fetching the LimitRange to ensure it has proper values 01/03/24 12:12:34.038
Jan  3 12:12:34.057: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan  3 12:12:34.057: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/03/24 12:12:34.057
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/03/24 12:12:34.076
Jan  3 12:12:34.099: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan  3 12:12:34.099: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/03/24 12:12:34.099
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/03/24 12:12:34.12
Jan  3 12:12:34.138: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan  3 12:12:34.138: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/03/24 12:12:34.138
STEP: Failing to create a Pod with more than max resources 01/03/24 12:12:34.152
STEP: Updating a LimitRange 01/03/24 12:12:34.168
STEP: Verifying LimitRange updating is effective 01/03/24 12:12:34.19
STEP: Creating a Pod with less than former min resources 01/03/24 12:12:36.213
STEP: Failing to create a Pod with more than max resources 01/03/24 12:12:36.237
STEP: Deleting a LimitRange 01/03/24 12:12:36.255
STEP: Verifying the LimitRange was deleted 01/03/24 12:12:36.281
Jan  3 12:12:41.305: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/03/24 12:12:41.305
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan  3 12:12:41.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5718" for this suite. 01/03/24 12:12:41.379
------------------------------
• [SLOW TEST] [7.631 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:12:33.78
    Jan  3 12:12:33.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename limitrange 01/03/24 12:12:33.783
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:33.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:33.867
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 01/03/24 12:12:33.895
    STEP: Setting up watch 01/03/24 12:12:33.896
    STEP: Submitting a LimitRange 01/03/24 12:12:34.015
    STEP: Verifying LimitRange creation was observed 01/03/24 12:12:34.037
    STEP: Fetching the LimitRange to ensure it has proper values 01/03/24 12:12:34.038
    Jan  3 12:12:34.057: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan  3 12:12:34.057: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/03/24 12:12:34.057
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/03/24 12:12:34.076
    Jan  3 12:12:34.099: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan  3 12:12:34.099: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/03/24 12:12:34.099
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/03/24 12:12:34.12
    Jan  3 12:12:34.138: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan  3 12:12:34.138: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/03/24 12:12:34.138
    STEP: Failing to create a Pod with more than max resources 01/03/24 12:12:34.152
    STEP: Updating a LimitRange 01/03/24 12:12:34.168
    STEP: Verifying LimitRange updating is effective 01/03/24 12:12:34.19
    STEP: Creating a Pod with less than former min resources 01/03/24 12:12:36.213
    STEP: Failing to create a Pod with more than max resources 01/03/24 12:12:36.237
    STEP: Deleting a LimitRange 01/03/24 12:12:36.255
    STEP: Verifying the LimitRange was deleted 01/03/24 12:12:36.281
    Jan  3 12:12:41.305: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/03/24 12:12:41.305
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:12:41.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5718" for this suite. 01/03/24 12:12:41.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:12:41.414
Jan  3 12:12:41.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename job 01/03/24 12:12:41.416
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:41.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:41.502
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 01/03/24 12:12:41.548
STEP: Patching the Job 01/03/24 12:12:41.569
STEP: Watching for Job to be patched 01/03/24 12:12:41.595
Jan  3 12:12:41.611: INFO: Event ADDED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan  3 12:12:41.611: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan  3 12:12:41.612: INFO: Event MODIFIED found for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/03/24 12:12:41.612
STEP: Watching for Job to be updated 01/03/24 12:12:41.653
Jan  3 12:12:41.667: INFO: Event MODIFIED found for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  3 12:12:41.667: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/03/24 12:12:41.667
Jan  3 12:12:41.685: INFO: Job: e2e-z2228 as labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched]
STEP: Waiting for job to complete 01/03/24 12:12:41.685
STEP: Delete a job collection with a labelselector 01/03/24 12:12:53.706
STEP: Watching for Job to be deleted 01/03/24 12:12:53.732
Jan  3 12:12:53.747: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  3 12:12:53.747: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  3 12:12:53.747: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  3 12:12:53.747: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  3 12:12:53.748: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  3 12:12:53.748: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  3 12:12:53.757: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  3 12:12:53.758: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  3 12:12:53.758: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  3 12:12:53.758: INFO: Event DELETED found for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/03/24 12:12:53.758
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  3 12:12:53.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6595" for this suite. 01/03/24 12:12:53.811
------------------------------
• [SLOW TEST] [12.432 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:12:41.414
    Jan  3 12:12:41.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename job 01/03/24 12:12:41.416
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:41.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:41.502
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 01/03/24 12:12:41.548
    STEP: Patching the Job 01/03/24 12:12:41.569
    STEP: Watching for Job to be patched 01/03/24 12:12:41.595
    Jan  3 12:12:41.611: INFO: Event ADDED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan  3 12:12:41.611: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan  3 12:12:41.612: INFO: Event MODIFIED found for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/03/24 12:12:41.612
    STEP: Watching for Job to be updated 01/03/24 12:12:41.653
    Jan  3 12:12:41.667: INFO: Event MODIFIED found for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  3 12:12:41.667: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/03/24 12:12:41.667
    Jan  3 12:12:41.685: INFO: Job: e2e-z2228 as labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched]
    STEP: Waiting for job to complete 01/03/24 12:12:41.685
    STEP: Delete a job collection with a labelselector 01/03/24 12:12:53.706
    STEP: Watching for Job to be deleted 01/03/24 12:12:53.732
    Jan  3 12:12:53.747: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  3 12:12:53.747: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  3 12:12:53.747: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  3 12:12:53.747: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  3 12:12:53.748: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  3 12:12:53.748: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  3 12:12:53.757: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  3 12:12:53.758: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  3 12:12:53.758: INFO: Event MODIFIED observed for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  3 12:12:53.758: INFO: Event DELETED found for Job e2e-z2228 in namespace job-6595 with labels: map[e2e-job-label:e2e-z2228 e2e-z2228:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/03/24 12:12:53.758
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:12:53.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6595" for this suite. 01/03/24 12:12:53.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:12:53.847
Jan  3 12:12:53.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pods 01/03/24 12:12:53.85
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:53.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:53.942
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 01/03/24 12:12:53.97
STEP: setting up watch 01/03/24 12:12:53.97
STEP: submitting the pod to kubernetes 01/03/24 12:12:54.089
STEP: verifying the pod is in kubernetes 01/03/24 12:12:54.12
STEP: verifying pod creation was observed 01/03/24 12:12:54.139
Jan  3 12:12:54.139: INFO: Waiting up to 5m0s for pod "pod-submit-remove-778a38b1-2c1b-48b9-8ae8-2d3e6fef5a7e" in namespace "pods-5402" to be "running"
Jan  3 12:12:54.174: INFO: Pod "pod-submit-remove-778a38b1-2c1b-48b9-8ae8-2d3e6fef5a7e": Phase="Pending", Reason="", readiness=false. Elapsed: 34.631759ms
Jan  3 12:12:56.194: INFO: Pod "pod-submit-remove-778a38b1-2c1b-48b9-8ae8-2d3e6fef5a7e": Phase="Running", Reason="", readiness=true. Elapsed: 2.054460932s
Jan  3 12:12:56.194: INFO: Pod "pod-submit-remove-778a38b1-2c1b-48b9-8ae8-2d3e6fef5a7e" satisfied condition "running"
STEP: deleting the pod gracefully 01/03/24 12:12:56.221
STEP: verifying pod deletion was observed 01/03/24 12:12:56.248
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  3 12:12:59.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5402" for this suite. 01/03/24 12:12:59.629
------------------------------
• [SLOW TEST] [5.807 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:12:53.847
    Jan  3 12:12:53.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pods 01/03/24 12:12:53.85
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:53.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:53.942
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 01/03/24 12:12:53.97
    STEP: setting up watch 01/03/24 12:12:53.97
    STEP: submitting the pod to kubernetes 01/03/24 12:12:54.089
    STEP: verifying the pod is in kubernetes 01/03/24 12:12:54.12
    STEP: verifying pod creation was observed 01/03/24 12:12:54.139
    Jan  3 12:12:54.139: INFO: Waiting up to 5m0s for pod "pod-submit-remove-778a38b1-2c1b-48b9-8ae8-2d3e6fef5a7e" in namespace "pods-5402" to be "running"
    Jan  3 12:12:54.174: INFO: Pod "pod-submit-remove-778a38b1-2c1b-48b9-8ae8-2d3e6fef5a7e": Phase="Pending", Reason="", readiness=false. Elapsed: 34.631759ms
    Jan  3 12:12:56.194: INFO: Pod "pod-submit-remove-778a38b1-2c1b-48b9-8ae8-2d3e6fef5a7e": Phase="Running", Reason="", readiness=true. Elapsed: 2.054460932s
    Jan  3 12:12:56.194: INFO: Pod "pod-submit-remove-778a38b1-2c1b-48b9-8ae8-2d3e6fef5a7e" satisfied condition "running"
    STEP: deleting the pod gracefully 01/03/24 12:12:56.221
    STEP: verifying pod deletion was observed 01/03/24 12:12:56.248
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:12:59.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5402" for this suite. 01/03/24 12:12:59.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:12:59.657
Jan  3 12:12:59.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename replicaset 01/03/24 12:12:59.659
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:59.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:59.76
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan  3 12:12:59.846: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  3 12:13:04.869: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/03/24 12:13:04.869
STEP: Scaling up "test-rs" replicaset  01/03/24 12:13:04.869
Jan  3 12:13:04.906: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/03/24 12:13:04.906
W0103 12:13:04.941745      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  3 12:13:04.955: INFO: observed ReplicaSet test-rs in namespace replicaset-4479 with ReadyReplicas 1, AvailableReplicas 1
Jan  3 12:13:04.959: INFO: observed ReplicaSet test-rs in namespace replicaset-4479 with ReadyReplicas 1, AvailableReplicas 1
Jan  3 12:13:04.969: INFO: observed ReplicaSet test-rs in namespace replicaset-4479 with ReadyReplicas 1, AvailableReplicas 1
Jan  3 12:13:06.529: INFO: observed ReplicaSet test-rs in namespace replicaset-4479 with ReadyReplicas 2, AvailableReplicas 2
Jan  3 12:13:06.615: INFO: observed Replicaset test-rs in namespace replicaset-4479 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  3 12:13:06.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4479" for this suite. 01/03/24 12:13:06.651
------------------------------
• [SLOW TEST] [7.033 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:12:59.657
    Jan  3 12:12:59.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename replicaset 01/03/24 12:12:59.659
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:12:59.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:12:59.76
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan  3 12:12:59.846: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  3 12:13:04.869: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/03/24 12:13:04.869
    STEP: Scaling up "test-rs" replicaset  01/03/24 12:13:04.869
    Jan  3 12:13:04.906: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/03/24 12:13:04.906
    W0103 12:13:04.941745      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  3 12:13:04.955: INFO: observed ReplicaSet test-rs in namespace replicaset-4479 with ReadyReplicas 1, AvailableReplicas 1
    Jan  3 12:13:04.959: INFO: observed ReplicaSet test-rs in namespace replicaset-4479 with ReadyReplicas 1, AvailableReplicas 1
    Jan  3 12:13:04.969: INFO: observed ReplicaSet test-rs in namespace replicaset-4479 with ReadyReplicas 1, AvailableReplicas 1
    Jan  3 12:13:06.529: INFO: observed ReplicaSet test-rs in namespace replicaset-4479 with ReadyReplicas 2, AvailableReplicas 2
    Jan  3 12:13:06.615: INFO: observed Replicaset test-rs in namespace replicaset-4479 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:13:06.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4479" for this suite. 01/03/24 12:13:06.651
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:13:06.69
Jan  3 12:13:06.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename subpath 01/03/24 12:13:06.693
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:13:06.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:13:06.775
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/03/24 12:13:06.804
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-v8kb 01/03/24 12:13:06.848
STEP: Creating a pod to test atomic-volume-subpath 01/03/24 12:13:06.848
Jan  3 12:13:06.904: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-v8kb" in namespace "subpath-356" to be "Succeeded or Failed"
Jan  3 12:13:06.922: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.304018ms
Jan  3 12:13:08.943: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 2.039000127s
Jan  3 12:13:10.944: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 4.039994982s
Jan  3 12:13:12.943: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 6.039071426s
Jan  3 12:13:14.944: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 8.040175679s
Jan  3 12:13:16.949: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 10.044631699s
Jan  3 12:13:18.945: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 12.041089286s
Jan  3 12:13:20.952: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 14.048356759s
Jan  3 12:13:22.944: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 16.039889474s
Jan  3 12:13:24.948: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 18.043774731s
Jan  3 12:13:26.947: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 20.042913187s
Jan  3 12:13:28.947: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=false. Elapsed: 22.042857171s
Jan  3 12:13:30.942: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.03771204s
STEP: Saw pod success 01/03/24 12:13:30.942
Jan  3 12:13:30.942: INFO: Pod "pod-subpath-test-configmap-v8kb" satisfied condition "Succeeded or Failed"
Jan  3 12:13:30.964: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-subpath-test-configmap-v8kb container test-container-subpath-configmap-v8kb: <nil>
STEP: delete the pod 01/03/24 12:13:31.02
Jan  3 12:13:31.070: INFO: Waiting for pod pod-subpath-test-configmap-v8kb to disappear
Jan  3 12:13:31.088: INFO: Pod pod-subpath-test-configmap-v8kb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-v8kb 01/03/24 12:13:31.088
Jan  3 12:13:31.088: INFO: Deleting pod "pod-subpath-test-configmap-v8kb" in namespace "subpath-356"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  3 12:13:31.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-356" for this suite. 01/03/24 12:13:31.137
------------------------------
• [SLOW TEST] [24.474 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:13:06.69
    Jan  3 12:13:06.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename subpath 01/03/24 12:13:06.693
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:13:06.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:13:06.775
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/03/24 12:13:06.804
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-v8kb 01/03/24 12:13:06.848
    STEP: Creating a pod to test atomic-volume-subpath 01/03/24 12:13:06.848
    Jan  3 12:13:06.904: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-v8kb" in namespace "subpath-356" to be "Succeeded or Failed"
    Jan  3 12:13:06.922: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.304018ms
    Jan  3 12:13:08.943: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 2.039000127s
    Jan  3 12:13:10.944: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 4.039994982s
    Jan  3 12:13:12.943: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 6.039071426s
    Jan  3 12:13:14.944: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 8.040175679s
    Jan  3 12:13:16.949: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 10.044631699s
    Jan  3 12:13:18.945: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 12.041089286s
    Jan  3 12:13:20.952: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 14.048356759s
    Jan  3 12:13:22.944: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 16.039889474s
    Jan  3 12:13:24.948: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 18.043774731s
    Jan  3 12:13:26.947: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=true. Elapsed: 20.042913187s
    Jan  3 12:13:28.947: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Running", Reason="", readiness=false. Elapsed: 22.042857171s
    Jan  3 12:13:30.942: INFO: Pod "pod-subpath-test-configmap-v8kb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.03771204s
    STEP: Saw pod success 01/03/24 12:13:30.942
    Jan  3 12:13:30.942: INFO: Pod "pod-subpath-test-configmap-v8kb" satisfied condition "Succeeded or Failed"
    Jan  3 12:13:30.964: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-subpath-test-configmap-v8kb container test-container-subpath-configmap-v8kb: <nil>
    STEP: delete the pod 01/03/24 12:13:31.02
    Jan  3 12:13:31.070: INFO: Waiting for pod pod-subpath-test-configmap-v8kb to disappear
    Jan  3 12:13:31.088: INFO: Pod pod-subpath-test-configmap-v8kb no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-v8kb 01/03/24 12:13:31.088
    Jan  3 12:13:31.088: INFO: Deleting pod "pod-subpath-test-configmap-v8kb" in namespace "subpath-356"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:13:31.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-356" for this suite. 01/03/24 12:13:31.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:13:31.17
Jan  3 12:13:31.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename security-context 01/03/24 12:13:31.172
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:13:31.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:13:31.292
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/03/24 12:13:31.321
Jan  3 12:13:31.349: INFO: Waiting up to 5m0s for pod "security-context-88b7fc61-d585-472b-b541-adb58ddfb38f" in namespace "security-context-462" to be "Succeeded or Failed"
Jan  3 12:13:31.369: INFO: Pod "security-context-88b7fc61-d585-472b-b541-adb58ddfb38f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.992954ms
Jan  3 12:13:33.389: INFO: Pod "security-context-88b7fc61-d585-472b-b541-adb58ddfb38f": Phase="Running", Reason="", readiness=true. Elapsed: 2.039363092s
Jan  3 12:13:35.390: INFO: Pod "security-context-88b7fc61-d585-472b-b541-adb58ddfb38f": Phase="Running", Reason="", readiness=false. Elapsed: 4.040703938s
Jan  3 12:13:37.389: INFO: Pod "security-context-88b7fc61-d585-472b-b541-adb58ddfb38f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039628938s
STEP: Saw pod success 01/03/24 12:13:37.389
Jan  3 12:13:37.389: INFO: Pod "security-context-88b7fc61-d585-472b-b541-adb58ddfb38f" satisfied condition "Succeeded or Failed"
Jan  3 12:13:37.408: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod security-context-88b7fc61-d585-472b-b541-adb58ddfb38f container test-container: <nil>
STEP: delete the pod 01/03/24 12:13:37.45
Jan  3 12:13:37.489: INFO: Waiting for pod security-context-88b7fc61-d585-472b-b541-adb58ddfb38f to disappear
Jan  3 12:13:37.508: INFO: Pod security-context-88b7fc61-d585-472b-b541-adb58ddfb38f no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  3 12:13:37.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-462" for this suite. 01/03/24 12:13:37.542
------------------------------
• [SLOW TEST] [6.421 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:13:31.17
    Jan  3 12:13:31.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename security-context 01/03/24 12:13:31.172
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:13:31.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:13:31.292
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/03/24 12:13:31.321
    Jan  3 12:13:31.349: INFO: Waiting up to 5m0s for pod "security-context-88b7fc61-d585-472b-b541-adb58ddfb38f" in namespace "security-context-462" to be "Succeeded or Failed"
    Jan  3 12:13:31.369: INFO: Pod "security-context-88b7fc61-d585-472b-b541-adb58ddfb38f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.992954ms
    Jan  3 12:13:33.389: INFO: Pod "security-context-88b7fc61-d585-472b-b541-adb58ddfb38f": Phase="Running", Reason="", readiness=true. Elapsed: 2.039363092s
    Jan  3 12:13:35.390: INFO: Pod "security-context-88b7fc61-d585-472b-b541-adb58ddfb38f": Phase="Running", Reason="", readiness=false. Elapsed: 4.040703938s
    Jan  3 12:13:37.389: INFO: Pod "security-context-88b7fc61-d585-472b-b541-adb58ddfb38f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039628938s
    STEP: Saw pod success 01/03/24 12:13:37.389
    Jan  3 12:13:37.389: INFO: Pod "security-context-88b7fc61-d585-472b-b541-adb58ddfb38f" satisfied condition "Succeeded or Failed"
    Jan  3 12:13:37.408: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod security-context-88b7fc61-d585-472b-b541-adb58ddfb38f container test-container: <nil>
    STEP: delete the pod 01/03/24 12:13:37.45
    Jan  3 12:13:37.489: INFO: Waiting for pod security-context-88b7fc61-d585-472b-b541-adb58ddfb38f to disappear
    Jan  3 12:13:37.508: INFO: Pod security-context-88b7fc61-d585-472b-b541-adb58ddfb38f no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:13:37.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-462" for this suite. 01/03/24 12:13:37.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:13:37.592
Jan  3 12:13:37.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename taint-multiple-pods 01/03/24 12:13:37.593
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:13:37.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:13:37.679
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jan  3 12:13:37.708: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  3 12:14:37.840: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jan  3 12:14:37.861: INFO: Starting informer...
STEP: Starting pods... 01/03/24 12:14:37.861
Jan  3 12:14:38.149: INFO: Pod1 is running on jb-1-26-np-64kerjapxk. Tainting Node
Jan  3 12:14:38.398: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-4237" to be "running"
Jan  3 12:14:38.419: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.56804ms
Jan  3 12:14:40.440: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.041410856s
Jan  3 12:14:40.440: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan  3 12:14:40.440: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-4237" to be "running"
Jan  3 12:14:40.458: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 18.650535ms
Jan  3 12:14:40.459: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan  3 12:14:40.459: INFO: Pod2 is running on jb-1-26-np-64kerjapxk. Tainting Node
STEP: Trying to apply a taint on the Node 01/03/24 12:14:40.459
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/24 12:14:40.508
STEP: Waiting for Pod1 and Pod2 to be deleted 01/03/24 12:14:40.528
Jan  3 12:14:46.840: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan  3 12:15:06.872: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/24 12:15:06.921
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:15:06.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-4237" for this suite. 01/03/24 12:15:06.97
------------------------------
• [SLOW TEST] [89.405 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:13:37.592
    Jan  3 12:13:37.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename taint-multiple-pods 01/03/24 12:13:37.593
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:13:37.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:13:37.679
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jan  3 12:13:37.708: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  3 12:14:37.840: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jan  3 12:14:37.861: INFO: Starting informer...
    STEP: Starting pods... 01/03/24 12:14:37.861
    Jan  3 12:14:38.149: INFO: Pod1 is running on jb-1-26-np-64kerjapxk. Tainting Node
    Jan  3 12:14:38.398: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-4237" to be "running"
    Jan  3 12:14:38.419: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.56804ms
    Jan  3 12:14:40.440: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.041410856s
    Jan  3 12:14:40.440: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan  3 12:14:40.440: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-4237" to be "running"
    Jan  3 12:14:40.458: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 18.650535ms
    Jan  3 12:14:40.459: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan  3 12:14:40.459: INFO: Pod2 is running on jb-1-26-np-64kerjapxk. Tainting Node
    STEP: Trying to apply a taint on the Node 01/03/24 12:14:40.459
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/24 12:14:40.508
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/03/24 12:14:40.528
    Jan  3 12:14:46.840: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan  3 12:15:06.872: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/24 12:15:06.921
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:15:06.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-4237" for this suite. 01/03/24 12:15:06.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:15:07
Jan  3 12:15:07.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 12:15:07.001
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:15:07.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:15:07.085
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 12:15:07.163
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 12:15:07.52
STEP: Deploying the webhook pod 01/03/24 12:15:07.545
STEP: Wait for the deployment to be ready 01/03/24 12:15:07.586
Jan  3 12:15:07.629: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/24 12:15:09.684
STEP: Verifying the service has paired with the endpoint 01/03/24 12:15:09.719
Jan  3 12:15:10.719: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 01/03/24 12:15:10.742
STEP: create a pod that should be denied by the webhook 01/03/24 12:15:10.873
STEP: create a pod that causes the webhook to hang 01/03/24 12:15:11.004
STEP: create a configmap that should be denied by the webhook 01/03/24 12:15:21.051
STEP: create a configmap that should be admitted by the webhook 01/03/24 12:15:21.198
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/03/24 12:15:21.301
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/03/24 12:15:21.356
STEP: create a namespace that bypass the webhook 01/03/24 12:15:21.393
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/03/24 12:15:21.423
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:15:21.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5787" for this suite. 01/03/24 12:15:21.734
STEP: Destroying namespace "webhook-5787-markers" for this suite. 01/03/24 12:15:21.758
------------------------------
• [SLOW TEST] [14.785 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:15:07
    Jan  3 12:15:07.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 12:15:07.001
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:15:07.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:15:07.085
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 12:15:07.163
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 12:15:07.52
    STEP: Deploying the webhook pod 01/03/24 12:15:07.545
    STEP: Wait for the deployment to be ready 01/03/24 12:15:07.586
    Jan  3 12:15:07.629: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/24 12:15:09.684
    STEP: Verifying the service has paired with the endpoint 01/03/24 12:15:09.719
    Jan  3 12:15:10.719: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 01/03/24 12:15:10.742
    STEP: create a pod that should be denied by the webhook 01/03/24 12:15:10.873
    STEP: create a pod that causes the webhook to hang 01/03/24 12:15:11.004
    STEP: create a configmap that should be denied by the webhook 01/03/24 12:15:21.051
    STEP: create a configmap that should be admitted by the webhook 01/03/24 12:15:21.198
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/03/24 12:15:21.301
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/03/24 12:15:21.356
    STEP: create a namespace that bypass the webhook 01/03/24 12:15:21.393
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/03/24 12:15:21.423
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:15:21.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5787" for this suite. 01/03/24 12:15:21.734
    STEP: Destroying namespace "webhook-5787-markers" for this suite. 01/03/24 12:15:21.758
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:15:21.785
Jan  3 12:15:21.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename daemonsets 01/03/24 12:15:21.788
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:15:21.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:15:21.88
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
Jan  3 12:15:22.033: INFO: Create a RollingUpdate DaemonSet
Jan  3 12:15:22.053: INFO: Check that daemon pods launch on every node of the cluster
Jan  3 12:15:22.095: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:15:22.095: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:15:23.145: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:15:23.145: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:15:24.171: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 12:15:24.171: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jan  3 12:15:24.171: INFO: Update the DaemonSet to trigger a rollout
Jan  3 12:15:24.214: INFO: Updating DaemonSet daemon-set
Jan  3 12:15:28.309: INFO: Roll back the DaemonSet before rollout is complete
Jan  3 12:15:28.351: INFO: Updating DaemonSet daemon-set
Jan  3 12:15:28.351: INFO: Make sure DaemonSet rollback is complete
Jan  3 12:15:28.371: INFO: Wrong image for pod: daemon-set-tvjmm. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Jan  3 12:15:28.371: INFO: Pod daemon-set-tvjmm is not available
Jan  3 12:15:34.417: INFO: Pod daemon-set-58mrm is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 01/03/24 12:15:34.492
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3820, will wait for the garbage collector to delete the pods 01/03/24 12:15:34.492
Jan  3 12:15:34.585: INFO: Deleting DaemonSet.extensions daemon-set took: 23.288043ms
Jan  3 12:15:34.687: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.01802ms
Jan  3 12:15:38.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:15:38.006: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  3 12:15:38.024: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33980965473"},"items":null}

Jan  3 12:15:38.045: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33980965475"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:15:38.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3820" for this suite. 01/03/24 12:15:38.15
------------------------------
• [SLOW TEST] [16.403 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:15:21.785
    Jan  3 12:15:21.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename daemonsets 01/03/24 12:15:21.788
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:15:21.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:15:21.88
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:443
    Jan  3 12:15:22.033: INFO: Create a RollingUpdate DaemonSet
    Jan  3 12:15:22.053: INFO: Check that daemon pods launch on every node of the cluster
    Jan  3 12:15:22.095: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:15:22.095: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:15:23.145: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:15:23.145: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:15:24.171: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 12:15:24.171: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Jan  3 12:15:24.171: INFO: Update the DaemonSet to trigger a rollout
    Jan  3 12:15:24.214: INFO: Updating DaemonSet daemon-set
    Jan  3 12:15:28.309: INFO: Roll back the DaemonSet before rollout is complete
    Jan  3 12:15:28.351: INFO: Updating DaemonSet daemon-set
    Jan  3 12:15:28.351: INFO: Make sure DaemonSet rollback is complete
    Jan  3 12:15:28.371: INFO: Wrong image for pod: daemon-set-tvjmm. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Jan  3 12:15:28.371: INFO: Pod daemon-set-tvjmm is not available
    Jan  3 12:15:34.417: INFO: Pod daemon-set-58mrm is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 01/03/24 12:15:34.492
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3820, will wait for the garbage collector to delete the pods 01/03/24 12:15:34.492
    Jan  3 12:15:34.585: INFO: Deleting DaemonSet.extensions daemon-set took: 23.288043ms
    Jan  3 12:15:34.687: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.01802ms
    Jan  3 12:15:38.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:15:38.006: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  3 12:15:38.024: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33980965473"},"items":null}

    Jan  3 12:15:38.045: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33980965475"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:15:38.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3820" for this suite. 01/03/24 12:15:38.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:15:38.193
Jan  3 12:15:38.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 12:15:38.196
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:15:38.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:15:38.277
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-3748 01/03/24 12:15:38.307
STEP: creating replication controller nodeport-test in namespace services-3748 01/03/24 12:15:38.349
I0103 12:15:38.370025      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3748, replica count: 2
I0103 12:15:41.421974      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 12:15:41.422: INFO: Creating new exec pod
Jan  3 12:15:41.452: INFO: Waiting up to 5m0s for pod "execpod95qqn" in namespace "services-3748" to be "running"
Jan  3 12:15:41.470: INFO: Pod "execpod95qqn": Phase="Pending", Reason="", readiness=false. Elapsed: 18.100358ms
Jan  3 12:15:43.492: INFO: Pod "execpod95qqn": Phase="Running", Reason="", readiness=true. Elapsed: 2.040182951s
Jan  3 12:15:43.492: INFO: Pod "execpod95qqn" satisfied condition "running"
Jan  3 12:15:44.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3748 exec execpod95qqn -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jan  3 12:15:44.988: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan  3 12:15:44.988: INFO: stdout: ""
Jan  3 12:15:44.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3748 exec execpod95qqn -- /bin/sh -x -c nc -v -z -w 2 10.233.10.25 80'
Jan  3 12:15:45.441: INFO: stderr: "+ nc -v -z -w 2 10.233.10.25 80\nConnection to 10.233.10.25 80 port [tcp/http] succeeded!\n"
Jan  3 12:15:45.441: INFO: stdout: ""
Jan  3 12:15:45.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3748 exec execpod95qqn -- /bin/sh -x -c nc -v -z -w 2 85.215.162.129 30842'
Jan  3 12:15:45.928: INFO: stderr: "+ nc -v -z -w 2 85.215.162.129 30842\nConnection to 85.215.162.129 30842 port [tcp/*] succeeded!\n"
Jan  3 12:15:45.928: INFO: stdout: ""
Jan  3 12:15:45.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3748 exec execpod95qqn -- /bin/sh -x -c nc -v -z -w 2 85.215.218.90 30842'
Jan  3 12:15:46.375: INFO: stderr: "+ nc -v -z -w 2 85.215.218.90 30842\nConnection to 85.215.218.90 30842 port [tcp/*] succeeded!\n"
Jan  3 12:15:46.375: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 12:15:46.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3748" for this suite. 01/03/24 12:15:46.407
------------------------------
• [SLOW TEST] [8.239 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:15:38.193
    Jan  3 12:15:38.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 12:15:38.196
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:15:38.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:15:38.277
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-3748 01/03/24 12:15:38.307
    STEP: creating replication controller nodeport-test in namespace services-3748 01/03/24 12:15:38.349
    I0103 12:15:38.370025      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3748, replica count: 2
    I0103 12:15:41.421974      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 12:15:41.422: INFO: Creating new exec pod
    Jan  3 12:15:41.452: INFO: Waiting up to 5m0s for pod "execpod95qqn" in namespace "services-3748" to be "running"
    Jan  3 12:15:41.470: INFO: Pod "execpod95qqn": Phase="Pending", Reason="", readiness=false. Elapsed: 18.100358ms
    Jan  3 12:15:43.492: INFO: Pod "execpod95qqn": Phase="Running", Reason="", readiness=true. Elapsed: 2.040182951s
    Jan  3 12:15:43.492: INFO: Pod "execpod95qqn" satisfied condition "running"
    Jan  3 12:15:44.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3748 exec execpod95qqn -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jan  3 12:15:44.988: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan  3 12:15:44.988: INFO: stdout: ""
    Jan  3 12:15:44.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3748 exec execpod95qqn -- /bin/sh -x -c nc -v -z -w 2 10.233.10.25 80'
    Jan  3 12:15:45.441: INFO: stderr: "+ nc -v -z -w 2 10.233.10.25 80\nConnection to 10.233.10.25 80 port [tcp/http] succeeded!\n"
    Jan  3 12:15:45.441: INFO: stdout: ""
    Jan  3 12:15:45.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3748 exec execpod95qqn -- /bin/sh -x -c nc -v -z -w 2 85.215.162.129 30842'
    Jan  3 12:15:45.928: INFO: stderr: "+ nc -v -z -w 2 85.215.162.129 30842\nConnection to 85.215.162.129 30842 port [tcp/*] succeeded!\n"
    Jan  3 12:15:45.928: INFO: stdout: ""
    Jan  3 12:15:45.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3748 exec execpod95qqn -- /bin/sh -x -c nc -v -z -w 2 85.215.218.90 30842'
    Jan  3 12:15:46.375: INFO: stderr: "+ nc -v -z -w 2 85.215.218.90 30842\nConnection to 85.215.218.90 30842 port [tcp/*] succeeded!\n"
    Jan  3 12:15:46.375: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:15:46.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3748" for this suite. 01/03/24 12:15:46.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:15:46.434
Jan  3 12:15:46.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename secrets 01/03/24 12:15:46.436
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:15:46.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:15:46.52
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-8c638a3b-fdef-4e34-88f7-1f707c59385c 01/03/24 12:15:46.548
STEP: Creating a pod to test consume secrets 01/03/24 12:15:46.567
Jan  3 12:15:46.596: INFO: Waiting up to 5m0s for pod "pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6" in namespace "secrets-6415" to be "Succeeded or Failed"
Jan  3 12:15:46.616: INFO: Pod "pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.614766ms
Jan  3 12:15:48.636: INFO: Pod "pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.04013866s
Jan  3 12:15:50.637: INFO: Pod "pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6": Phase="Running", Reason="", readiness=false. Elapsed: 4.040650734s
Jan  3 12:15:52.641: INFO: Pod "pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045047133s
STEP: Saw pod success 01/03/24 12:15:52.641
Jan  3 12:15:52.641: INFO: Pod "pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6" satisfied condition "Succeeded or Failed"
Jan  3 12:15:52.662: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6 container secret-volume-test: <nil>
STEP: delete the pod 01/03/24 12:15:52.816
Jan  3 12:15:52.848: INFO: Waiting for pod pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6 to disappear
Jan  3 12:15:52.865: INFO: Pod pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  3 12:15:52.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6415" for this suite. 01/03/24 12:15:52.897
------------------------------
• [SLOW TEST] [6.491 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:15:46.434
    Jan  3 12:15:46.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename secrets 01/03/24 12:15:46.436
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:15:46.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:15:46.52
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-8c638a3b-fdef-4e34-88f7-1f707c59385c 01/03/24 12:15:46.548
    STEP: Creating a pod to test consume secrets 01/03/24 12:15:46.567
    Jan  3 12:15:46.596: INFO: Waiting up to 5m0s for pod "pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6" in namespace "secrets-6415" to be "Succeeded or Failed"
    Jan  3 12:15:46.616: INFO: Pod "pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.614766ms
    Jan  3 12:15:48.636: INFO: Pod "pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.04013866s
    Jan  3 12:15:50.637: INFO: Pod "pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6": Phase="Running", Reason="", readiness=false. Elapsed: 4.040650734s
    Jan  3 12:15:52.641: INFO: Pod "pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045047133s
    STEP: Saw pod success 01/03/24 12:15:52.641
    Jan  3 12:15:52.641: INFO: Pod "pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6" satisfied condition "Succeeded or Failed"
    Jan  3 12:15:52.662: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6 container secret-volume-test: <nil>
    STEP: delete the pod 01/03/24 12:15:52.816
    Jan  3 12:15:52.848: INFO: Waiting for pod pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6 to disappear
    Jan  3 12:15:52.865: INFO: Pod pod-secrets-b12f88dd-b3ec-4aa3-a963-c1aed33d59e6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:15:52.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6415" for this suite. 01/03/24 12:15:52.897
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:15:52.925
Jan  3 12:15:52.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 12:15:52.928
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:15:52.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:15:53.011
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-fa9843d8-03d1-4884-a846-47b3f7ade066 01/03/24 12:15:53.04
STEP: Creating a pod to test consume configMaps 01/03/24 12:15:53.061
Jan  3 12:15:53.088: INFO: Waiting up to 5m0s for pod "pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1" in namespace "configmap-671" to be "Succeeded or Failed"
Jan  3 12:15:53.107: INFO: Pod "pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.316197ms
Jan  3 12:15:55.126: INFO: Pod "pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038305689s
Jan  3 12:15:57.128: INFO: Pod "pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040246988s
STEP: Saw pod success 01/03/24 12:15:57.128
Jan  3 12:15:57.129: INFO: Pod "pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1" satisfied condition "Succeeded or Failed"
Jan  3 12:15:57.153: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1 container agnhost-container: <nil>
STEP: delete the pod 01/03/24 12:15:57.197
Jan  3 12:15:57.252: INFO: Waiting for pod pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1 to disappear
Jan  3 12:15:57.270: INFO: Pod pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 12:15:57.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-671" for this suite. 01/03/24 12:15:57.308
------------------------------
• [4.407 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:15:52.925
    Jan  3 12:15:52.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 12:15:52.928
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:15:52.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:15:53.011
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-fa9843d8-03d1-4884-a846-47b3f7ade066 01/03/24 12:15:53.04
    STEP: Creating a pod to test consume configMaps 01/03/24 12:15:53.061
    Jan  3 12:15:53.088: INFO: Waiting up to 5m0s for pod "pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1" in namespace "configmap-671" to be "Succeeded or Failed"
    Jan  3 12:15:53.107: INFO: Pod "pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.316197ms
    Jan  3 12:15:55.126: INFO: Pod "pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038305689s
    Jan  3 12:15:57.128: INFO: Pod "pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040246988s
    STEP: Saw pod success 01/03/24 12:15:57.128
    Jan  3 12:15:57.129: INFO: Pod "pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1" satisfied condition "Succeeded or Failed"
    Jan  3 12:15:57.153: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1 container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 12:15:57.197
    Jan  3 12:15:57.252: INFO: Waiting for pod pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1 to disappear
    Jan  3 12:15:57.270: INFO: Pod pod-configmaps-965130ee-e8c0-464d-a022-36e75e063fc1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:15:57.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-671" for this suite. 01/03/24 12:15:57.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:15:57.333
Jan  3 12:15:57.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 12:15:57.336
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:15:57.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:15:57.425
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-74757de9-971f-49a3-81ab-f61d02e5b7f3 01/03/24 12:15:57.453
STEP: Creating a pod to test consume configMaps 01/03/24 12:15:57.472
Jan  3 12:15:57.499: INFO: Waiting up to 5m0s for pod "pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5" in namespace "configmap-2757" to be "Succeeded or Failed"
Jan  3 12:15:57.517: INFO: Pod "pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.174366ms
Jan  3 12:15:59.541: INFO: Pod "pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5": Phase="Running", Reason="", readiness=true. Elapsed: 2.041988237s
Jan  3 12:16:01.542: INFO: Pod "pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5": Phase="Running", Reason="", readiness=false. Elapsed: 4.042808755s
Jan  3 12:16:03.540: INFO: Pod "pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040622774s
STEP: Saw pod success 01/03/24 12:16:03.54
Jan  3 12:16:03.540: INFO: Pod "pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5" satisfied condition "Succeeded or Failed"
Jan  3 12:16:03.560: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5 container agnhost-container: <nil>
STEP: delete the pod 01/03/24 12:16:03.612
Jan  3 12:16:03.651: INFO: Waiting for pod pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5 to disappear
Jan  3 12:16:03.668: INFO: Pod pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 12:16:03.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2757" for this suite. 01/03/24 12:16:03.701
------------------------------
• [SLOW TEST] [6.395 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:15:57.333
    Jan  3 12:15:57.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 12:15:57.336
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:15:57.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:15:57.425
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-74757de9-971f-49a3-81ab-f61d02e5b7f3 01/03/24 12:15:57.453
    STEP: Creating a pod to test consume configMaps 01/03/24 12:15:57.472
    Jan  3 12:15:57.499: INFO: Waiting up to 5m0s for pod "pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5" in namespace "configmap-2757" to be "Succeeded or Failed"
    Jan  3 12:15:57.517: INFO: Pod "pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.174366ms
    Jan  3 12:15:59.541: INFO: Pod "pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5": Phase="Running", Reason="", readiness=true. Elapsed: 2.041988237s
    Jan  3 12:16:01.542: INFO: Pod "pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5": Phase="Running", Reason="", readiness=false. Elapsed: 4.042808755s
    Jan  3 12:16:03.540: INFO: Pod "pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040622774s
    STEP: Saw pod success 01/03/24 12:16:03.54
    Jan  3 12:16:03.540: INFO: Pod "pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5" satisfied condition "Succeeded or Failed"
    Jan  3 12:16:03.560: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5 container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 12:16:03.612
    Jan  3 12:16:03.651: INFO: Waiting for pod pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5 to disappear
    Jan  3 12:16:03.668: INFO: Pod pod-configmaps-c45e141e-44e7-4d6a-9790-dd1236b01ac5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:16:03.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2757" for this suite. 01/03/24 12:16:03.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:16:03.736
Jan  3 12:16:03.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename svcaccounts 01/03/24 12:16:03.739
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:16:03.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:16:03.832
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jan  3 12:16:03.878: INFO: Got root ca configmap in namespace "svcaccounts-2908"
Jan  3 12:16:03.899: INFO: Deleted root ca configmap in namespace "svcaccounts-2908"
STEP: waiting for a new root ca configmap created 01/03/24 12:16:04.4
Jan  3 12:16:04.419: INFO: Recreated root ca configmap in namespace "svcaccounts-2908"
Jan  3 12:16:04.439: INFO: Updated root ca configmap in namespace "svcaccounts-2908"
STEP: waiting for the root ca configmap reconciled 01/03/24 12:16:04.94
Jan  3 12:16:04.972: INFO: Reconciled root ca configmap in namespace "svcaccounts-2908"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  3 12:16:04.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2908" for this suite. 01/03/24 12:16:05.025
------------------------------
• [1.320 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:16:03.736
    Jan  3 12:16:03.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename svcaccounts 01/03/24 12:16:03.739
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:16:03.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:16:03.832
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jan  3 12:16:03.878: INFO: Got root ca configmap in namespace "svcaccounts-2908"
    Jan  3 12:16:03.899: INFO: Deleted root ca configmap in namespace "svcaccounts-2908"
    STEP: waiting for a new root ca configmap created 01/03/24 12:16:04.4
    Jan  3 12:16:04.419: INFO: Recreated root ca configmap in namespace "svcaccounts-2908"
    Jan  3 12:16:04.439: INFO: Updated root ca configmap in namespace "svcaccounts-2908"
    STEP: waiting for the root ca configmap reconciled 01/03/24 12:16:04.94
    Jan  3 12:16:04.972: INFO: Reconciled root ca configmap in namespace "svcaccounts-2908"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:16:04.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2908" for this suite. 01/03/24 12:16:05.025
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:16:05.059
Jan  3 12:16:05.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 12:16:05.061
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:16:05.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:16:05.148
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-695 01/03/24 12:16:05.177
STEP: creating service affinity-clusterip in namespace services-695 01/03/24 12:16:05.177
STEP: creating replication controller affinity-clusterip in namespace services-695 01/03/24 12:16:05.212
I0103 12:16:05.235912      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-695, replica count: 3
I0103 12:16:08.289276      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 12:16:08.325: INFO: Creating new exec pod
Jan  3 12:16:08.356: INFO: Waiting up to 5m0s for pod "execpod-affinityq84sc" in namespace "services-695" to be "running"
Jan  3 12:16:08.377: INFO: Pod "execpod-affinityq84sc": Phase="Pending", Reason="", readiness=false. Elapsed: 21.235912ms
Jan  3 12:16:10.400: INFO: Pod "execpod-affinityq84sc": Phase="Running", Reason="", readiness=true. Elapsed: 2.043964057s
Jan  3 12:16:10.400: INFO: Pod "execpod-affinityq84sc" satisfied condition "running"
Jan  3 12:16:11.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-695 exec execpod-affinityq84sc -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan  3 12:16:11.867: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan  3 12:16:11.867: INFO: stdout: ""
Jan  3 12:16:11.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-695 exec execpod-affinityq84sc -- /bin/sh -x -c nc -v -z -w 2 10.233.29.170 80'
Jan  3 12:16:12.337: INFO: stderr: "+ nc -v -z -w 2 10.233.29.170 80\nConnection to 10.233.29.170 80 port [tcp/http] succeeded!\n"
Jan  3 12:16:12.337: INFO: stdout: ""
Jan  3 12:16:12.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-695 exec execpod-affinityq84sc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.29.170:80/ ; done'
Jan  3 12:16:12.954: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n"
Jan  3 12:16:12.954: INFO: stdout: "\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv"
Jan  3 12:16:12.954: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.954: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.954: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.954: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.954: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
Jan  3 12:16:12.955: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-695, will wait for the garbage collector to delete the pods 01/03/24 12:16:12.994
Jan  3 12:16:13.088: INFO: Deleting ReplicationController affinity-clusterip took: 24.559791ms
Jan  3 12:16:13.189: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.334302ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 12:16:16.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-695" for this suite. 01/03/24 12:16:16.465
------------------------------
• [SLOW TEST] [11.434 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:16:05.059
    Jan  3 12:16:05.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 12:16:05.061
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:16:05.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:16:05.148
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-695 01/03/24 12:16:05.177
    STEP: creating service affinity-clusterip in namespace services-695 01/03/24 12:16:05.177
    STEP: creating replication controller affinity-clusterip in namespace services-695 01/03/24 12:16:05.212
    I0103 12:16:05.235912      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-695, replica count: 3
    I0103 12:16:08.289276      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 12:16:08.325: INFO: Creating new exec pod
    Jan  3 12:16:08.356: INFO: Waiting up to 5m0s for pod "execpod-affinityq84sc" in namespace "services-695" to be "running"
    Jan  3 12:16:08.377: INFO: Pod "execpod-affinityq84sc": Phase="Pending", Reason="", readiness=false. Elapsed: 21.235912ms
    Jan  3 12:16:10.400: INFO: Pod "execpod-affinityq84sc": Phase="Running", Reason="", readiness=true. Elapsed: 2.043964057s
    Jan  3 12:16:10.400: INFO: Pod "execpod-affinityq84sc" satisfied condition "running"
    Jan  3 12:16:11.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-695 exec execpod-affinityq84sc -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan  3 12:16:11.867: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan  3 12:16:11.867: INFO: stdout: ""
    Jan  3 12:16:11.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-695 exec execpod-affinityq84sc -- /bin/sh -x -c nc -v -z -w 2 10.233.29.170 80'
    Jan  3 12:16:12.337: INFO: stderr: "+ nc -v -z -w 2 10.233.29.170 80\nConnection to 10.233.29.170 80 port [tcp/http] succeeded!\n"
    Jan  3 12:16:12.337: INFO: stdout: ""
    Jan  3 12:16:12.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-695 exec execpod-affinityq84sc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.29.170:80/ ; done'
    Jan  3 12:16:12.954: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.29.170:80/\n"
    Jan  3 12:16:12.954: INFO: stdout: "\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv\naffinity-clusterip-6nhfv"
    Jan  3 12:16:12.954: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.954: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.954: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.954: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.954: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.955: INFO: Received response from host: affinity-clusterip-6nhfv
    Jan  3 12:16:12.955: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-695, will wait for the garbage collector to delete the pods 01/03/24 12:16:12.994
    Jan  3 12:16:13.088: INFO: Deleting ReplicationController affinity-clusterip took: 24.559791ms
    Jan  3 12:16:13.189: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.334302ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:16:16.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-695" for this suite. 01/03/24 12:16:16.465
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:16:16.494
Jan  3 12:16:16.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename namespaces 01/03/24 12:16:16.497
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:16:16.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:16:16.578
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 01/03/24 12:16:16.606
Jan  3 12:16:16.627: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/03/24 12:16:16.627
Jan  3 12:16:16.650: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/03/24 12:16:16.65
Jan  3 12:16:16.691: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:16:16.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4228" for this suite. 01/03/24 12:16:16.717
------------------------------
• [0.248 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:16:16.494
    Jan  3 12:16:16.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename namespaces 01/03/24 12:16:16.497
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:16:16.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:16:16.578
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 01/03/24 12:16:16.606
    Jan  3 12:16:16.627: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/03/24 12:16:16.627
    Jan  3 12:16:16.650: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/03/24 12:16:16.65
    Jan  3 12:16:16.691: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:16:16.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4228" for this suite. 01/03/24 12:16:16.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:16:16.745
Jan  3 12:16:16.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename replicaset 01/03/24 12:16:16.747
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:16:16.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:16:16.831
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/03/24 12:16:16.862
Jan  3 12:16:16.902: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/03/24 12:16:16.902
Jan  3 12:16:16.902: INFO: Waiting up to 5m0s for pod "test-rs-jtw8w" in namespace "replicaset-1775" to be "running"
Jan  3 12:16:16.924: INFO: Pod "test-rs-jtw8w": Phase="Pending", Reason="", readiness=false. Elapsed: 21.483001ms
Jan  3 12:16:18.944: INFO: Pod "test-rs-jtw8w": Phase="Running", Reason="", readiness=true. Elapsed: 2.042044146s
Jan  3 12:16:18.944: INFO: Pod "test-rs-jtw8w" satisfied condition "running"
STEP: getting scale subresource 01/03/24 12:16:18.944
STEP: updating a scale subresource 01/03/24 12:16:18.961
STEP: verifying the replicaset Spec.Replicas was modified 01/03/24 12:16:18.983
STEP: Patch a scale subresource 01/03/24 12:16:19.004
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  3 12:16:19.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1775" for this suite. 01/03/24 12:16:19.081
------------------------------
• [2.365 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:16:16.745
    Jan  3 12:16:16.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename replicaset 01/03/24 12:16:16.747
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:16:16.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:16:16.831
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/03/24 12:16:16.862
    Jan  3 12:16:16.902: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/03/24 12:16:16.902
    Jan  3 12:16:16.902: INFO: Waiting up to 5m0s for pod "test-rs-jtw8w" in namespace "replicaset-1775" to be "running"
    Jan  3 12:16:16.924: INFO: Pod "test-rs-jtw8w": Phase="Pending", Reason="", readiness=false. Elapsed: 21.483001ms
    Jan  3 12:16:18.944: INFO: Pod "test-rs-jtw8w": Phase="Running", Reason="", readiness=true. Elapsed: 2.042044146s
    Jan  3 12:16:18.944: INFO: Pod "test-rs-jtw8w" satisfied condition "running"
    STEP: getting scale subresource 01/03/24 12:16:18.944
    STEP: updating a scale subresource 01/03/24 12:16:18.961
    STEP: verifying the replicaset Spec.Replicas was modified 01/03/24 12:16:18.983
    STEP: Patch a scale subresource 01/03/24 12:16:19.004
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:16:19.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1775" for this suite. 01/03/24 12:16:19.081
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:16:19.11
Jan  3 12:16:19.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename subpath 01/03/24 12:16:19.111
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:16:19.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:16:19.187
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/03/24 12:16:19.216
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-mqgw 01/03/24 12:16:19.256
STEP: Creating a pod to test atomic-volume-subpath 01/03/24 12:16:19.256
Jan  3 12:16:19.289: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mqgw" in namespace "subpath-9894" to be "Succeeded or Failed"
Jan  3 12:16:19.308: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Pending", Reason="", readiness=false. Elapsed: 19.272638ms
Jan  3 12:16:21.327: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 2.03834875s
Jan  3 12:16:23.334: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 4.045317783s
Jan  3 12:16:25.332: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 6.042889234s
Jan  3 12:16:27.330: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 8.041119631s
Jan  3 12:16:29.328: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 10.038837976s
Jan  3 12:16:31.329: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 12.040649013s
Jan  3 12:16:33.329: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 14.039925893s
Jan  3 12:16:35.338: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 16.048889281s
Jan  3 12:16:37.335: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 18.046298919s
Jan  3 12:16:39.332: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 20.043347737s
Jan  3 12:16:41.329: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=false. Elapsed: 22.039865569s
Jan  3 12:16:43.330: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.041323929s
STEP: Saw pod success 01/03/24 12:16:43.33
Jan  3 12:16:43.330: INFO: Pod "pod-subpath-test-downwardapi-mqgw" satisfied condition "Succeeded or Failed"
Jan  3 12:16:43.352: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-subpath-test-downwardapi-mqgw container test-container-subpath-downwardapi-mqgw: <nil>
STEP: delete the pod 01/03/24 12:16:43.403
Jan  3 12:16:43.459: INFO: Waiting for pod pod-subpath-test-downwardapi-mqgw to disappear
Jan  3 12:16:43.477: INFO: Pod pod-subpath-test-downwardapi-mqgw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mqgw 01/03/24 12:16:43.477
Jan  3 12:16:43.477: INFO: Deleting pod "pod-subpath-test-downwardapi-mqgw" in namespace "subpath-9894"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  3 12:16:43.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9894" for this suite. 01/03/24 12:16:43.529
------------------------------
• [SLOW TEST] [24.447 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:16:19.11
    Jan  3 12:16:19.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename subpath 01/03/24 12:16:19.111
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:16:19.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:16:19.187
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/03/24 12:16:19.216
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-mqgw 01/03/24 12:16:19.256
    STEP: Creating a pod to test atomic-volume-subpath 01/03/24 12:16:19.256
    Jan  3 12:16:19.289: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mqgw" in namespace "subpath-9894" to be "Succeeded or Failed"
    Jan  3 12:16:19.308: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Pending", Reason="", readiness=false. Elapsed: 19.272638ms
    Jan  3 12:16:21.327: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 2.03834875s
    Jan  3 12:16:23.334: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 4.045317783s
    Jan  3 12:16:25.332: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 6.042889234s
    Jan  3 12:16:27.330: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 8.041119631s
    Jan  3 12:16:29.328: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 10.038837976s
    Jan  3 12:16:31.329: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 12.040649013s
    Jan  3 12:16:33.329: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 14.039925893s
    Jan  3 12:16:35.338: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 16.048889281s
    Jan  3 12:16:37.335: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 18.046298919s
    Jan  3 12:16:39.332: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=true. Elapsed: 20.043347737s
    Jan  3 12:16:41.329: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Running", Reason="", readiness=false. Elapsed: 22.039865569s
    Jan  3 12:16:43.330: INFO: Pod "pod-subpath-test-downwardapi-mqgw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.041323929s
    STEP: Saw pod success 01/03/24 12:16:43.33
    Jan  3 12:16:43.330: INFO: Pod "pod-subpath-test-downwardapi-mqgw" satisfied condition "Succeeded or Failed"
    Jan  3 12:16:43.352: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-subpath-test-downwardapi-mqgw container test-container-subpath-downwardapi-mqgw: <nil>
    STEP: delete the pod 01/03/24 12:16:43.403
    Jan  3 12:16:43.459: INFO: Waiting for pod pod-subpath-test-downwardapi-mqgw to disappear
    Jan  3 12:16:43.477: INFO: Pod pod-subpath-test-downwardapi-mqgw no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-mqgw 01/03/24 12:16:43.477
    Jan  3 12:16:43.477: INFO: Deleting pod "pod-subpath-test-downwardapi-mqgw" in namespace "subpath-9894"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:16:43.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9894" for this suite. 01/03/24 12:16:43.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:16:43.559
Jan  3 12:16:43.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 12:16:43.561
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:16:43.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:16:43.646
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/03/24 12:16:43.674
Jan  3 12:16:43.703: INFO: Waiting up to 5m0s for pod "pod-11502f62-bd60-4bdb-9075-df60a2603032" in namespace "emptydir-608" to be "Succeeded or Failed"
Jan  3 12:16:43.722: INFO: Pod "pod-11502f62-bd60-4bdb-9075-df60a2603032": Phase="Pending", Reason="", readiness=false. Elapsed: 18.926756ms
Jan  3 12:16:45.743: INFO: Pod "pod-11502f62-bd60-4bdb-9075-df60a2603032": Phase="Running", Reason="", readiness=true. Elapsed: 2.040460239s
Jan  3 12:16:47.743: INFO: Pod "pod-11502f62-bd60-4bdb-9075-df60a2603032": Phase="Running", Reason="", readiness=false. Elapsed: 4.040560754s
Jan  3 12:16:49.741: INFO: Pod "pod-11502f62-bd60-4bdb-9075-df60a2603032": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037895687s
STEP: Saw pod success 01/03/24 12:16:49.741
Jan  3 12:16:49.741: INFO: Pod "pod-11502f62-bd60-4bdb-9075-df60a2603032" satisfied condition "Succeeded or Failed"
Jan  3 12:16:49.760: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-11502f62-bd60-4bdb-9075-df60a2603032 container test-container: <nil>
STEP: delete the pod 01/03/24 12:16:49.803
Jan  3 12:16:49.844: INFO: Waiting for pod pod-11502f62-bd60-4bdb-9075-df60a2603032 to disappear
Jan  3 12:16:49.863: INFO: Pod pod-11502f62-bd60-4bdb-9075-df60a2603032 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:16:49.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-608" for this suite. 01/03/24 12:16:49.893
------------------------------
• [SLOW TEST] [6.361 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:16:43.559
    Jan  3 12:16:43.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 12:16:43.561
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:16:43.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:16:43.646
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/03/24 12:16:43.674
    Jan  3 12:16:43.703: INFO: Waiting up to 5m0s for pod "pod-11502f62-bd60-4bdb-9075-df60a2603032" in namespace "emptydir-608" to be "Succeeded or Failed"
    Jan  3 12:16:43.722: INFO: Pod "pod-11502f62-bd60-4bdb-9075-df60a2603032": Phase="Pending", Reason="", readiness=false. Elapsed: 18.926756ms
    Jan  3 12:16:45.743: INFO: Pod "pod-11502f62-bd60-4bdb-9075-df60a2603032": Phase="Running", Reason="", readiness=true. Elapsed: 2.040460239s
    Jan  3 12:16:47.743: INFO: Pod "pod-11502f62-bd60-4bdb-9075-df60a2603032": Phase="Running", Reason="", readiness=false. Elapsed: 4.040560754s
    Jan  3 12:16:49.741: INFO: Pod "pod-11502f62-bd60-4bdb-9075-df60a2603032": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037895687s
    STEP: Saw pod success 01/03/24 12:16:49.741
    Jan  3 12:16:49.741: INFO: Pod "pod-11502f62-bd60-4bdb-9075-df60a2603032" satisfied condition "Succeeded or Failed"
    Jan  3 12:16:49.760: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-11502f62-bd60-4bdb-9075-df60a2603032 container test-container: <nil>
    STEP: delete the pod 01/03/24 12:16:49.803
    Jan  3 12:16:49.844: INFO: Waiting for pod pod-11502f62-bd60-4bdb-9075-df60a2603032 to disappear
    Jan  3 12:16:49.863: INFO: Pod pod-11502f62-bd60-4bdb-9075-df60a2603032 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:16:49.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-608" for this suite. 01/03/24 12:16:49.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:16:49.921
Jan  3 12:16:49.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename dns 01/03/24 12:16:49.924
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:16:49.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:16:50.007
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/03/24 12:16:50.037
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3395.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3395.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3395.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3395.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 190.42.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.42.190_udp@PTR;check="$$(dig +tcp +noall +answer +search 190.42.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.42.190_tcp@PTR;sleep 1; done
 01/03/24 12:16:50.101
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3395.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3395.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3395.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3395.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 190.42.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.42.190_udp@PTR;check="$$(dig +tcp +noall +answer +search 190.42.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.42.190_tcp@PTR;sleep 1; done
 01/03/24 12:16:50.102
STEP: creating a pod to probe DNS 01/03/24 12:16:50.102
STEP: submitting the pod to kubernetes 01/03/24 12:16:50.102
Jan  3 12:16:50.142: INFO: Waiting up to 15m0s for pod "dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f" in namespace "dns-3395" to be "running"
Jan  3 12:16:50.165: INFO: Pod "dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.394091ms
Jan  3 12:16:52.185: INFO: Pod "dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f": Phase="Running", Reason="", readiness=true. Elapsed: 2.043001774s
Jan  3 12:16:52.186: INFO: Pod "dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f" satisfied condition "running"
STEP: retrieving the pod 01/03/24 12:16:52.186
STEP: looking for the results for each expected name from probers 01/03/24 12:16:52.204
Jan  3 12:16:52.320: INFO: Unable to read wheezy_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:52.365: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:52.400: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:52.441: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:52.635: INFO: Unable to read jessie_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:52.672: INFO: Unable to read jessie_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:52.710: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:52.757: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:52.898: INFO: Lookups using dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f failed for: [wheezy_udp@dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_udp@dns-test-service.dns-3395.svc.cluster.local jessie_tcp@dns-test-service.dns-3395.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local]

Jan  3 12:16:57.941: INFO: Unable to read wheezy_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:57.978: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:58.021: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:58.060: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:58.279: INFO: Unable to read jessie_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:58.314: INFO: Unable to read jessie_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:58.348: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:58.385: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:16:58.525: INFO: Lookups using dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f failed for: [wheezy_udp@dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_udp@dns-test-service.dns-3395.svc.cluster.local jessie_tcp@dns-test-service.dns-3395.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local]

Jan  3 12:17:02.937: INFO: Unable to read wheezy_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:02.988: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:03.021: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:03.056: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:03.237: INFO: Unable to read jessie_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:03.274: INFO: Unable to read jessie_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:03.311: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:03.347: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:03.502: INFO: Lookups using dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f failed for: [wheezy_udp@dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_udp@dns-test-service.dns-3395.svc.cluster.local jessie_tcp@dns-test-service.dns-3395.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local]

Jan  3 12:17:07.933: INFO: Unable to read wheezy_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:07.974: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:08.016: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:08.053: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:08.226: INFO: Unable to read jessie_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:08.260: INFO: Unable to read jessie_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:08.295: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:08.331: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:08.477: INFO: Lookups using dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f failed for: [wheezy_udp@dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_udp@dns-test-service.dns-3395.svc.cluster.local jessie_tcp@dns-test-service.dns-3395.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local]

Jan  3 12:17:12.942: INFO: Unable to read wheezy_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:12.976: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:13.015: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:13.049: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:13.241: INFO: Unable to read jessie_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:13.278: INFO: Unable to read jessie_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:13.315: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:13.349: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:13.534: INFO: Lookups using dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f failed for: [wheezy_udp@dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_udp@dns-test-service.dns-3395.svc.cluster.local jessie_tcp@dns-test-service.dns-3395.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local]

Jan  3 12:17:17.937: INFO: Unable to read wheezy_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:17.970: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:18.008: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:18.045: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:18.219: INFO: Unable to read jessie_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:18.256: INFO: Unable to read jessie_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:18.307: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:18.340: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
Jan  3 12:17:18.503: INFO: Lookups using dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f failed for: [wheezy_udp@dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_udp@dns-test-service.dns-3395.svc.cluster.local jessie_tcp@dns-test-service.dns-3395.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local]

Jan  3 12:17:23.480: INFO: DNS probes using dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f succeeded

STEP: deleting the pod 01/03/24 12:17:23.48
STEP: deleting the test service 01/03/24 12:17:23.527
STEP: deleting the test headless service 01/03/24 12:17:23.566
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  3 12:17:23.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3395" for this suite. 01/03/24 12:17:23.63
------------------------------
• [SLOW TEST] [33.740 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:16:49.921
    Jan  3 12:16:49.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename dns 01/03/24 12:16:49.924
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:16:49.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:16:50.007
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/03/24 12:16:50.037
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3395.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3395.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3395.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3395.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 190.42.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.42.190_udp@PTR;check="$$(dig +tcp +noall +answer +search 190.42.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.42.190_tcp@PTR;sleep 1; done
     01/03/24 12:16:50.101
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3395.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3395.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3395.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3395.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3395.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 190.42.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.42.190_udp@PTR;check="$$(dig +tcp +noall +answer +search 190.42.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.42.190_tcp@PTR;sleep 1; done
     01/03/24 12:16:50.102
    STEP: creating a pod to probe DNS 01/03/24 12:16:50.102
    STEP: submitting the pod to kubernetes 01/03/24 12:16:50.102
    Jan  3 12:16:50.142: INFO: Waiting up to 15m0s for pod "dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f" in namespace "dns-3395" to be "running"
    Jan  3 12:16:50.165: INFO: Pod "dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.394091ms
    Jan  3 12:16:52.185: INFO: Pod "dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f": Phase="Running", Reason="", readiness=true. Elapsed: 2.043001774s
    Jan  3 12:16:52.186: INFO: Pod "dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f" satisfied condition "running"
    STEP: retrieving the pod 01/03/24 12:16:52.186
    STEP: looking for the results for each expected name from probers 01/03/24 12:16:52.204
    Jan  3 12:16:52.320: INFO: Unable to read wheezy_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:52.365: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:52.400: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:52.441: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:52.635: INFO: Unable to read jessie_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:52.672: INFO: Unable to read jessie_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:52.710: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:52.757: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:52.898: INFO: Lookups using dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f failed for: [wheezy_udp@dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_udp@dns-test-service.dns-3395.svc.cluster.local jessie_tcp@dns-test-service.dns-3395.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local]

    Jan  3 12:16:57.941: INFO: Unable to read wheezy_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:57.978: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:58.021: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:58.060: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:58.279: INFO: Unable to read jessie_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:58.314: INFO: Unable to read jessie_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:58.348: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:58.385: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:16:58.525: INFO: Lookups using dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f failed for: [wheezy_udp@dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_udp@dns-test-service.dns-3395.svc.cluster.local jessie_tcp@dns-test-service.dns-3395.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local]

    Jan  3 12:17:02.937: INFO: Unable to read wheezy_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:02.988: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:03.021: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:03.056: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:03.237: INFO: Unable to read jessie_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:03.274: INFO: Unable to read jessie_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:03.311: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:03.347: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:03.502: INFO: Lookups using dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f failed for: [wheezy_udp@dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_udp@dns-test-service.dns-3395.svc.cluster.local jessie_tcp@dns-test-service.dns-3395.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local]

    Jan  3 12:17:07.933: INFO: Unable to read wheezy_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:07.974: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:08.016: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:08.053: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:08.226: INFO: Unable to read jessie_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:08.260: INFO: Unable to read jessie_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:08.295: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:08.331: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:08.477: INFO: Lookups using dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f failed for: [wheezy_udp@dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_udp@dns-test-service.dns-3395.svc.cluster.local jessie_tcp@dns-test-service.dns-3395.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local]

    Jan  3 12:17:12.942: INFO: Unable to read wheezy_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:12.976: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:13.015: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:13.049: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:13.241: INFO: Unable to read jessie_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:13.278: INFO: Unable to read jessie_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:13.315: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:13.349: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:13.534: INFO: Lookups using dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f failed for: [wheezy_udp@dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_udp@dns-test-service.dns-3395.svc.cluster.local jessie_tcp@dns-test-service.dns-3395.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local]

    Jan  3 12:17:17.937: INFO: Unable to read wheezy_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:17.970: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:18.008: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:18.045: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:18.219: INFO: Unable to read jessie_udp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:18.256: INFO: Unable to read jessie_tcp@dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:18.307: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:18.340: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local from pod dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f: the server could not find the requested resource (get pods dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f)
    Jan  3 12:17:18.503: INFO: Lookups using dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f failed for: [wheezy_udp@dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@dns-test-service.dns-3395.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_udp@dns-test-service.dns-3395.svc.cluster.local jessie_tcp@dns-test-service.dns-3395.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3395.svc.cluster.local]

    Jan  3 12:17:23.480: INFO: DNS probes using dns-3395/dns-test-f0621806-cc6d-4b99-bea2-1a47586b015f succeeded

    STEP: deleting the pod 01/03/24 12:17:23.48
    STEP: deleting the test service 01/03/24 12:17:23.527
    STEP: deleting the test headless service 01/03/24 12:17:23.566
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:17:23.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3395" for this suite. 01/03/24 12:17:23.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:17:23.668
Jan  3 12:17:23.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename svc-latency 01/03/24 12:17:23.67
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:17:23.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:17:23.755
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan  3 12:17:23.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5080 01/03/24 12:17:23.784
I0103 12:17:23.806195      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5080, replica count: 1
I0103 12:17:24.857557      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 12:17:25.857967      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 12:17:26.858660      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 12:17:26.991: INFO: Created: latency-svc-g69r4
Jan  3 12:17:26.999: INFO: Got endpoints: latency-svc-g69r4 [39.63622ms]
Jan  3 12:17:27.027: INFO: Created: latency-svc-vb6vn
Jan  3 12:17:27.037: INFO: Got endpoints: latency-svc-vb6vn [36.628705ms]
Jan  3 12:17:27.037: INFO: Created: latency-svc-pw629
Jan  3 12:17:27.040: INFO: Created: latency-svc-6p56m
Jan  3 12:17:27.044: INFO: Got endpoints: latency-svc-pw629 [43.972898ms]
Jan  3 12:17:27.047: INFO: Got endpoints: latency-svc-6p56m [47.464374ms]
Jan  3 12:17:27.048: INFO: Created: latency-svc-757ll
Jan  3 12:17:27.058: INFO: Created: latency-svc-jlwms
Jan  3 12:17:27.078: INFO: Created: latency-svc-7sdfw
Jan  3 12:17:27.078: INFO: Got endpoints: latency-svc-757ll [78.152006ms]
Jan  3 12:17:27.079: INFO: Got endpoints: latency-svc-jlwms [78.896906ms]
Jan  3 12:17:27.085: INFO: Created: latency-svc-q9nqd
Jan  3 12:17:27.087: INFO: Got endpoints: latency-svc-7sdfw [86.388088ms]
Jan  3 12:17:27.095: INFO: Created: latency-svc-6dt26
Jan  3 12:17:27.095: INFO: Got endpoints: latency-svc-q9nqd [94.188671ms]
Jan  3 12:17:27.099: INFO: Created: latency-svc-ljnvh
Jan  3 12:17:27.103: INFO: Got endpoints: latency-svc-6dt26 [100.805107ms]
Jan  3 12:17:27.106: INFO: Created: latency-svc-tfn5r
Jan  3 12:17:27.106: INFO: Got endpoints: latency-svc-ljnvh [104.93891ms]
Jan  3 12:17:27.111: INFO: Created: latency-svc-667x2
Jan  3 12:17:27.115: INFO: Got endpoints: latency-svc-tfn5r [113.919665ms]
Jan  3 12:17:27.117: INFO: Created: latency-svc-bz25p
Jan  3 12:17:27.120: INFO: Got endpoints: latency-svc-667x2 [118.639672ms]
Jan  3 12:17:27.123: INFO: Created: latency-svc-jq8xw
Jan  3 12:17:27.124: INFO: Got endpoints: latency-svc-bz25p [122.816783ms]
Jan  3 12:17:27.128: INFO: Created: latency-svc-fn4n8
Jan  3 12:17:27.134: INFO: Got endpoints: latency-svc-jq8xw [132.061339ms]
Jan  3 12:17:27.136: INFO: Created: latency-svc-5wbfg
Jan  3 12:17:27.137: INFO: Got endpoints: latency-svc-fn4n8 [134.960363ms]
Jan  3 12:17:27.143: INFO: Created: latency-svc-ms2p8
Jan  3 12:17:27.143: INFO: Got endpoints: latency-svc-5wbfg [143.727101ms]
Jan  3 12:17:27.148: INFO: Created: latency-svc-fw2tv
Jan  3 12:17:27.152: INFO: Got endpoints: latency-svc-ms2p8 [115.226768ms]
Jan  3 12:17:27.153: INFO: Created: latency-svc-ltpnh
Jan  3 12:17:27.155: INFO: Got endpoints: latency-svc-fw2tv [109.964609ms]
Jan  3 12:17:27.161: INFO: Created: latency-svc-crw6w
Jan  3 12:17:27.165: INFO: Got endpoints: latency-svc-ltpnh [117.451221ms]
Jan  3 12:17:27.168: INFO: Created: latency-svc-vt6p2
Jan  3 12:17:27.169: INFO: Got endpoints: latency-svc-crw6w [90.137819ms]
Jan  3 12:17:27.174: INFO: Created: latency-svc-5dgzk
Jan  3 12:17:27.174: INFO: Got endpoints: latency-svc-vt6p2 [95.114404ms]
Jan  3 12:17:27.189: INFO: Created: latency-svc-zs6n2
Jan  3 12:17:27.189: INFO: Got endpoints: latency-svc-5dgzk [101.699954ms]
Jan  3 12:17:27.199: INFO: Created: latency-svc-j7d2r
Jan  3 12:17:27.200: INFO: Got endpoints: latency-svc-zs6n2 [104.39863ms]
Jan  3 12:17:27.206: INFO: Got endpoints: latency-svc-j7d2r [103.700594ms]
Jan  3 12:17:27.208: INFO: Created: latency-svc-qfhd6
Jan  3 12:17:27.216: INFO: Got endpoints: latency-svc-qfhd6 [109.645674ms]
Jan  3 12:17:27.219: INFO: Created: latency-svc-q7j2l
Jan  3 12:17:27.228: INFO: Created: latency-svc-22z85
Jan  3 12:17:27.230: INFO: Got endpoints: latency-svc-q7j2l [115.443815ms]
Jan  3 12:17:27.236: INFO: Got endpoints: latency-svc-22z85 [115.886619ms]
Jan  3 12:17:27.237: INFO: Created: latency-svc-hn7dx
Jan  3 12:17:27.244: INFO: Created: latency-svc-8wb68
Jan  3 12:17:27.244: INFO: Got endpoints: latency-svc-hn7dx [120.368442ms]
Jan  3 12:17:27.248: INFO: Created: latency-svc-ltnxc
Jan  3 12:17:27.252: INFO: Got endpoints: latency-svc-8wb68 [117.987409ms]
Jan  3 12:17:27.256: INFO: Created: latency-svc-cc46q
Jan  3 12:17:27.257: INFO: Got endpoints: latency-svc-ltnxc [119.622815ms]
Jan  3 12:17:27.265: INFO: Got endpoints: latency-svc-cc46q [121.799346ms]
Jan  3 12:17:27.265: INFO: Created: latency-svc-446qz
Jan  3 12:17:27.270: INFO: Created: latency-svc-5tpjv
Jan  3 12:17:27.275: INFO: Got endpoints: latency-svc-446qz [122.535734ms]
Jan  3 12:17:27.278: INFO: Got endpoints: latency-svc-5tpjv [123.158619ms]
Jan  3 12:17:27.278: INFO: Created: latency-svc-f7llr
Jan  3 12:17:27.284: INFO: Created: latency-svc-4g896
Jan  3 12:17:27.289: INFO: Got endpoints: latency-svc-f7llr [124.03001ms]
Jan  3 12:17:27.291: INFO: Created: latency-svc-cplmh
Jan  3 12:17:27.295: INFO: Got endpoints: latency-svc-4g896 [126.024256ms]
Jan  3 12:17:27.300: INFO: Got endpoints: latency-svc-cplmh [125.733441ms]
Jan  3 12:17:27.300: INFO: Created: latency-svc-jp8mm
Jan  3 12:17:27.314: INFO: Created: latency-svc-56clf
Jan  3 12:17:27.322: INFO: Created: latency-svc-zn7bz
Jan  3 12:17:27.326: INFO: Created: latency-svc-7q62w
Jan  3 12:17:27.335: INFO: Created: latency-svc-xqm8g
Jan  3 12:17:27.345: INFO: Created: latency-svc-kl4rf
Jan  3 12:17:27.349: INFO: Created: latency-svc-hvdqj
Jan  3 12:17:27.350: INFO: Got endpoints: latency-svc-jp8mm [161.185246ms]
Jan  3 12:17:27.355: INFO: Created: latency-svc-gstq4
Jan  3 12:17:27.362: INFO: Created: latency-svc-2l9cf
Jan  3 12:17:27.369: INFO: Created: latency-svc-h7t2l
Jan  3 12:17:27.377: INFO: Created: latency-svc-7hkqs
Jan  3 12:17:27.380: INFO: Created: latency-svc-nmxqk
Jan  3 12:17:27.386: INFO: Created: latency-svc-x57vk
Jan  3 12:17:27.391: INFO: Created: latency-svc-qqk5b
Jan  3 12:17:27.397: INFO: Created: latency-svc-q4tdp
Jan  3 12:17:27.400: INFO: Got endpoints: latency-svc-56clf [200.104846ms]
Jan  3 12:17:27.402: INFO: Created: latency-svc-tlrkh
Jan  3 12:17:27.423: INFO: Created: latency-svc-qtx4g
Jan  3 12:17:27.450: INFO: Got endpoints: latency-svc-zn7bz [243.75319ms]
Jan  3 12:17:27.477: INFO: Created: latency-svc-p48hf
Jan  3 12:17:27.498: INFO: Got endpoints: latency-svc-7q62w [282.720295ms]
Jan  3 12:17:27.526: INFO: Created: latency-svc-wgg8s
Jan  3 12:17:27.551: INFO: Got endpoints: latency-svc-xqm8g [320.455318ms]
Jan  3 12:17:27.580: INFO: Created: latency-svc-wc9wk
Jan  3 12:17:27.600: INFO: Got endpoints: latency-svc-kl4rf [363.668023ms]
Jan  3 12:17:27.627: INFO: Created: latency-svc-9mp9m
Jan  3 12:17:27.651: INFO: Got endpoints: latency-svc-hvdqj [406.562505ms]
Jan  3 12:17:27.682: INFO: Created: latency-svc-ps5rb
Jan  3 12:17:27.699: INFO: Got endpoints: latency-svc-gstq4 [447.447731ms]
Jan  3 12:17:27.728: INFO: Created: latency-svc-lhpp9
Jan  3 12:17:27.750: INFO: Got endpoints: latency-svc-2l9cf [492.906811ms]
Jan  3 12:17:27.776: INFO: Created: latency-svc-w6l2c
Jan  3 12:17:27.808: INFO: Got endpoints: latency-svc-h7t2l [542.737979ms]
Jan  3 12:17:27.840: INFO: Created: latency-svc-rrkwv
Jan  3 12:17:27.850: INFO: Got endpoints: latency-svc-7hkqs [575.409913ms]
Jan  3 12:17:27.880: INFO: Created: latency-svc-f77kt
Jan  3 12:17:27.902: INFO: Got endpoints: latency-svc-nmxqk [623.667099ms]
Jan  3 12:17:27.929: INFO: Created: latency-svc-sfc86
Jan  3 12:17:27.949: INFO: Got endpoints: latency-svc-x57vk [660.714565ms]
Jan  3 12:17:27.980: INFO: Created: latency-svc-ztjlz
Jan  3 12:17:28.000: INFO: Got endpoints: latency-svc-qqk5b [704.416007ms]
Jan  3 12:17:28.047: INFO: Created: latency-svc-tmsqx
Jan  3 12:17:28.055: INFO: Got endpoints: latency-svc-q4tdp [754.31389ms]
Jan  3 12:17:28.082: INFO: Created: latency-svc-hgc6k
Jan  3 12:17:28.103: INFO: Got endpoints: latency-svc-tlrkh [752.319399ms]
Jan  3 12:17:28.131: INFO: Created: latency-svc-kbn8j
Jan  3 12:17:28.149: INFO: Got endpoints: latency-svc-qtx4g [748.997008ms]
Jan  3 12:17:28.176: INFO: Created: latency-svc-jtqgg
Jan  3 12:17:28.200: INFO: Got endpoints: latency-svc-p48hf [749.230057ms]
Jan  3 12:17:28.226: INFO: Created: latency-svc-7jzvx
Jan  3 12:17:28.249: INFO: Got endpoints: latency-svc-wgg8s [750.262781ms]
Jan  3 12:17:28.277: INFO: Created: latency-svc-75f27
Jan  3 12:17:28.300: INFO: Got endpoints: latency-svc-wc9wk [749.109376ms]
Jan  3 12:17:28.329: INFO: Created: latency-svc-p827q
Jan  3 12:17:28.349: INFO: Got endpoints: latency-svc-9mp9m [748.724855ms]
Jan  3 12:17:28.375: INFO: Created: latency-svc-sxjzg
Jan  3 12:17:28.400: INFO: Got endpoints: latency-svc-ps5rb [748.752748ms]
Jan  3 12:17:28.450: INFO: Got endpoints: latency-svc-lhpp9 [750.086049ms]
Jan  3 12:17:28.452: INFO: Created: latency-svc-5l7c7
Jan  3 12:17:28.476: INFO: Created: latency-svc-wsmws
Jan  3 12:17:28.500: INFO: Got endpoints: latency-svc-w6l2c [750.648135ms]
Jan  3 12:17:28.527: INFO: Created: latency-svc-rdm8r
Jan  3 12:17:28.552: INFO: Got endpoints: latency-svc-rrkwv [743.141467ms]
Jan  3 12:17:28.578: INFO: Created: latency-svc-mhz8l
Jan  3 12:17:28.601: INFO: Got endpoints: latency-svc-f77kt [750.507168ms]
Jan  3 12:17:28.630: INFO: Created: latency-svc-qfcb4
Jan  3 12:17:28.651: INFO: Got endpoints: latency-svc-sfc86 [749.550674ms]
Jan  3 12:17:28.687: INFO: Created: latency-svc-xhn8q
Jan  3 12:17:28.719: INFO: Got endpoints: latency-svc-ztjlz [769.403334ms]
Jan  3 12:17:28.764: INFO: Got endpoints: latency-svc-tmsqx [763.998836ms]
Jan  3 12:17:28.773: INFO: Created: latency-svc-s2wdh
Jan  3 12:17:28.790: INFO: Created: latency-svc-v7txm
Jan  3 12:17:28.799: INFO: Got endpoints: latency-svc-hgc6k [744.39161ms]
Jan  3 12:17:28.824: INFO: Created: latency-svc-xkk7x
Jan  3 12:17:28.861: INFO: Got endpoints: latency-svc-kbn8j [758.513425ms]
Jan  3 12:17:28.891: INFO: Created: latency-svc-ktgpw
Jan  3 12:17:28.906: INFO: Got endpoints: latency-svc-jtqgg [755.876862ms]
Jan  3 12:17:28.935: INFO: Created: latency-svc-g9vsw
Jan  3 12:17:28.951: INFO: Got endpoints: latency-svc-7jzvx [751.536179ms]
Jan  3 12:17:28.978: INFO: Created: latency-svc-q5nzd
Jan  3 12:17:29.005: INFO: Got endpoints: latency-svc-75f27 [755.626798ms]
Jan  3 12:17:29.032: INFO: Created: latency-svc-7n4jl
Jan  3 12:17:29.054: INFO: Got endpoints: latency-svc-p827q [753.485349ms]
Jan  3 12:17:29.088: INFO: Created: latency-svc-gz49z
Jan  3 12:17:29.099: INFO: Got endpoints: latency-svc-sxjzg [750.569928ms]
Jan  3 12:17:29.125: INFO: Created: latency-svc-b9khj
Jan  3 12:17:29.155: INFO: Got endpoints: latency-svc-5l7c7 [754.868955ms]
Jan  3 12:17:29.183: INFO: Created: latency-svc-6mfb6
Jan  3 12:17:29.202: INFO: Got endpoints: latency-svc-wsmws [752.205891ms]
Jan  3 12:17:29.228: INFO: Created: latency-svc-74grx
Jan  3 12:17:29.252: INFO: Got endpoints: latency-svc-rdm8r [751.691615ms]
Jan  3 12:17:29.283: INFO: Created: latency-svc-xgx89
Jan  3 12:17:29.314: INFO: Got endpoints: latency-svc-mhz8l [762.419313ms]
Jan  3 12:17:29.342: INFO: Created: latency-svc-jcnnk
Jan  3 12:17:29.349: INFO: Got endpoints: latency-svc-qfcb4 [747.871665ms]
Jan  3 12:17:29.379: INFO: Created: latency-svc-zwkpz
Jan  3 12:17:29.400: INFO: Got endpoints: latency-svc-xhn8q [749.097816ms]
Jan  3 12:17:29.427: INFO: Created: latency-svc-wj9m9
Jan  3 12:17:29.468: INFO: Got endpoints: latency-svc-s2wdh [749.181712ms]
Jan  3 12:17:29.495: INFO: Created: latency-svc-hlvzn
Jan  3 12:17:29.501: INFO: Got endpoints: latency-svc-v7txm [737.290949ms]
Jan  3 12:17:29.528: INFO: Created: latency-svc-2lpdg
Jan  3 12:17:29.551: INFO: Got endpoints: latency-svc-xkk7x [751.526843ms]
Jan  3 12:17:29.579: INFO: Created: latency-svc-66kng
Jan  3 12:17:29.601: INFO: Got endpoints: latency-svc-ktgpw [739.944124ms]
Jan  3 12:17:29.637: INFO: Created: latency-svc-chx7s
Jan  3 12:17:29.653: INFO: Got endpoints: latency-svc-g9vsw [747.692489ms]
Jan  3 12:17:29.678: INFO: Created: latency-svc-ngwjv
Jan  3 12:17:29.700: INFO: Got endpoints: latency-svc-q5nzd [748.591443ms]
Jan  3 12:17:29.726: INFO: Created: latency-svc-cjt72
Jan  3 12:17:29.751: INFO: Got endpoints: latency-svc-7n4jl [745.698861ms]
Jan  3 12:17:29.799: INFO: Created: latency-svc-7qp2z
Jan  3 12:17:29.810: INFO: Got endpoints: latency-svc-gz49z [756.396933ms]
Jan  3 12:17:29.836: INFO: Created: latency-svc-cdgw7
Jan  3 12:17:29.851: INFO: Got endpoints: latency-svc-b9khj [751.460608ms]
Jan  3 12:17:29.883: INFO: Created: latency-svc-lvflx
Jan  3 12:17:29.905: INFO: Got endpoints: latency-svc-6mfb6 [749.726351ms]
Jan  3 12:17:29.933: INFO: Created: latency-svc-fzwfm
Jan  3 12:17:29.950: INFO: Got endpoints: latency-svc-74grx [748.584096ms]
Jan  3 12:17:29.981: INFO: Created: latency-svc-jtmvb
Jan  3 12:17:30.003: INFO: Got endpoints: latency-svc-xgx89 [750.757267ms]
Jan  3 12:17:30.036: INFO: Created: latency-svc-pdfm9
Jan  3 12:17:30.058: INFO: Got endpoints: latency-svc-jcnnk [743.711631ms]
Jan  3 12:17:30.091: INFO: Created: latency-svc-j9phl
Jan  3 12:17:30.102: INFO: Got endpoints: latency-svc-zwkpz [752.892892ms]
Jan  3 12:17:30.130: INFO: Created: latency-svc-dnbhh
Jan  3 12:17:30.149: INFO: Got endpoints: latency-svc-wj9m9 [748.771664ms]
Jan  3 12:17:30.177: INFO: Created: latency-svc-ksxl4
Jan  3 12:17:30.200: INFO: Got endpoints: latency-svc-hlvzn [731.606572ms]
Jan  3 12:17:30.225: INFO: Created: latency-svc-6dwvq
Jan  3 12:17:30.249: INFO: Got endpoints: latency-svc-2lpdg [747.725278ms]
Jan  3 12:17:30.275: INFO: Created: latency-svc-2zpp2
Jan  3 12:17:30.300: INFO: Got endpoints: latency-svc-66kng [749.299122ms]
Jan  3 12:17:30.328: INFO: Created: latency-svc-9cjhm
Jan  3 12:17:30.348: INFO: Got endpoints: latency-svc-chx7s [747.014976ms]
Jan  3 12:17:30.375: INFO: Created: latency-svc-5lpsn
Jan  3 12:17:30.400: INFO: Got endpoints: latency-svc-ngwjv [746.835203ms]
Jan  3 12:17:30.425: INFO: Created: latency-svc-j4zdg
Jan  3 12:17:30.453: INFO: Got endpoints: latency-svc-cjt72 [752.992514ms]
Jan  3 12:17:30.493: INFO: Created: latency-svc-4kgbr
Jan  3 12:17:30.502: INFO: Got endpoints: latency-svc-7qp2z [751.505666ms]
Jan  3 12:17:30.527: INFO: Created: latency-svc-xh8l8
Jan  3 12:17:30.549: INFO: Got endpoints: latency-svc-cdgw7 [738.15773ms]
Jan  3 12:17:30.574: INFO: Created: latency-svc-7sg8q
Jan  3 12:17:30.600: INFO: Got endpoints: latency-svc-lvflx [748.691033ms]
Jan  3 12:17:30.627: INFO: Created: latency-svc-cpz2j
Jan  3 12:17:30.659: INFO: Got endpoints: latency-svc-fzwfm [753.947971ms]
Jan  3 12:17:30.689: INFO: Created: latency-svc-hmdjg
Jan  3 12:17:30.700: INFO: Got endpoints: latency-svc-jtmvb [749.156286ms]
Jan  3 12:17:30.728: INFO: Created: latency-svc-wgnl2
Jan  3 12:17:30.750: INFO: Got endpoints: latency-svc-pdfm9 [746.683503ms]
Jan  3 12:17:30.782: INFO: Created: latency-svc-2hm87
Jan  3 12:17:30.799: INFO: Got endpoints: latency-svc-j9phl [741.093938ms]
Jan  3 12:17:30.828: INFO: Created: latency-svc-j9vlk
Jan  3 12:17:30.849: INFO: Got endpoints: latency-svc-dnbhh [745.857197ms]
Jan  3 12:17:30.875: INFO: Created: latency-svc-w9zvs
Jan  3 12:17:30.903: INFO: Got endpoints: latency-svc-ksxl4 [753.562175ms]
Jan  3 12:17:30.936: INFO: Created: latency-svc-n2wbl
Jan  3 12:17:30.949: INFO: Got endpoints: latency-svc-6dwvq [748.979841ms]
Jan  3 12:17:30.979: INFO: Created: latency-svc-jfq4g
Jan  3 12:17:31.002: INFO: Got endpoints: latency-svc-2zpp2 [753.274968ms]
Jan  3 12:17:31.037: INFO: Created: latency-svc-v7zjp
Jan  3 12:17:31.048: INFO: Got endpoints: latency-svc-9cjhm [747.730071ms]
Jan  3 12:17:31.073: INFO: Created: latency-svc-tlwfw
Jan  3 12:17:31.103: INFO: Got endpoints: latency-svc-5lpsn [754.424834ms]
Jan  3 12:17:31.136: INFO: Created: latency-svc-l585l
Jan  3 12:17:31.150: INFO: Got endpoints: latency-svc-j4zdg [749.297125ms]
Jan  3 12:17:31.176: INFO: Created: latency-svc-vmnr7
Jan  3 12:17:31.203: INFO: Got endpoints: latency-svc-4kgbr [749.287431ms]
Jan  3 12:17:31.229: INFO: Created: latency-svc-vl767
Jan  3 12:17:31.249: INFO: Got endpoints: latency-svc-xh8l8 [746.636634ms]
Jan  3 12:17:31.276: INFO: Created: latency-svc-pk2cv
Jan  3 12:17:31.300: INFO: Got endpoints: latency-svc-7sg8q [751.464978ms]
Jan  3 12:17:31.328: INFO: Created: latency-svc-ccddk
Jan  3 12:17:31.353: INFO: Got endpoints: latency-svc-cpz2j [753.043544ms]
Jan  3 12:17:31.384: INFO: Created: latency-svc-9l7t4
Jan  3 12:17:31.400: INFO: Got endpoints: latency-svc-hmdjg [740.612699ms]
Jan  3 12:17:31.429: INFO: Created: latency-svc-kjs9r
Jan  3 12:17:31.449: INFO: Got endpoints: latency-svc-wgnl2 [749.16815ms]
Jan  3 12:17:31.482: INFO: Created: latency-svc-4gcl6
Jan  3 12:17:31.500: INFO: Got endpoints: latency-svc-2hm87 [749.513772ms]
Jan  3 12:17:31.527: INFO: Created: latency-svc-hzllf
Jan  3 12:17:31.549: INFO: Got endpoints: latency-svc-j9vlk [749.938344ms]
Jan  3 12:17:31.576: INFO: Created: latency-svc-8kpqs
Jan  3 12:17:31.599: INFO: Got endpoints: latency-svc-w9zvs [750.891701ms]
Jan  3 12:17:31.629: INFO: Created: latency-svc-lv99z
Jan  3 12:17:31.651: INFO: Got endpoints: latency-svc-n2wbl [747.478018ms]
Jan  3 12:17:31.676: INFO: Created: latency-svc-dthm7
Jan  3 12:17:31.702: INFO: Got endpoints: latency-svc-jfq4g [753.026764ms]
Jan  3 12:17:31.731: INFO: Created: latency-svc-hx8bb
Jan  3 12:17:31.750: INFO: Got endpoints: latency-svc-v7zjp [747.437466ms]
Jan  3 12:17:31.776: INFO: Created: latency-svc-v4hzt
Jan  3 12:17:31.800: INFO: Got endpoints: latency-svc-tlwfw [751.715092ms]
Jan  3 12:17:31.827: INFO: Created: latency-svc-fx44n
Jan  3 12:17:31.851: INFO: Got endpoints: latency-svc-l585l [748.209621ms]
Jan  3 12:17:31.878: INFO: Created: latency-svc-vzbf7
Jan  3 12:17:31.904: INFO: Got endpoints: latency-svc-vmnr7 [753.965205ms]
Jan  3 12:17:31.937: INFO: Created: latency-svc-kpz2s
Jan  3 12:17:31.953: INFO: Got endpoints: latency-svc-vl767 [750.545662ms]
Jan  3 12:17:31.977: INFO: Created: latency-svc-rq5p9
Jan  3 12:17:31.998: INFO: Got endpoints: latency-svc-pk2cv [748.811689ms]
Jan  3 12:17:32.024: INFO: Created: latency-svc-zvxpw
Jan  3 12:17:32.049: INFO: Got endpoints: latency-svc-ccddk [748.377453ms]
Jan  3 12:17:32.078: INFO: Created: latency-svc-9l5q4
Jan  3 12:17:32.100: INFO: Got endpoints: latency-svc-9l7t4 [747.189322ms]
Jan  3 12:17:32.127: INFO: Created: latency-svc-t6m9s
Jan  3 12:17:32.153: INFO: Got endpoints: latency-svc-kjs9r [752.980952ms]
Jan  3 12:17:32.184: INFO: Created: latency-svc-6gbbf
Jan  3 12:17:32.200: INFO: Got endpoints: latency-svc-4gcl6 [750.774452ms]
Jan  3 12:17:32.226: INFO: Created: latency-svc-tz8m5
Jan  3 12:17:32.251: INFO: Got endpoints: latency-svc-hzllf [750.9153ms]
Jan  3 12:17:32.277: INFO: Created: latency-svc-fwd5q
Jan  3 12:17:32.299: INFO: Got endpoints: latency-svc-8kpqs [749.3694ms]
Jan  3 12:17:32.326: INFO: Created: latency-svc-9qq9v
Jan  3 12:17:32.350: INFO: Got endpoints: latency-svc-lv99z [750.308178ms]
Jan  3 12:17:32.382: INFO: Created: latency-svc-qqtjf
Jan  3 12:17:32.399: INFO: Got endpoints: latency-svc-dthm7 [748.276455ms]
Jan  3 12:17:32.425: INFO: Created: latency-svc-946mm
Jan  3 12:17:32.456: INFO: Got endpoints: latency-svc-hx8bb [753.438602ms]
Jan  3 12:17:32.480: INFO: Created: latency-svc-qjxbg
Jan  3 12:17:32.499: INFO: Got endpoints: latency-svc-v4hzt [748.477391ms]
Jan  3 12:17:32.526: INFO: Created: latency-svc-72hmr
Jan  3 12:17:32.557: INFO: Got endpoints: latency-svc-fx44n [757.123657ms]
Jan  3 12:17:32.584: INFO: Created: latency-svc-7zdpw
Jan  3 12:17:32.600: INFO: Got endpoints: latency-svc-vzbf7 [748.748357ms]
Jan  3 12:17:32.626: INFO: Created: latency-svc-v9n5f
Jan  3 12:17:32.656: INFO: Got endpoints: latency-svc-kpz2s [752.182521ms]
Jan  3 12:17:32.687: INFO: Created: latency-svc-2pwsv
Jan  3 12:17:32.700: INFO: Got endpoints: latency-svc-rq5p9 [747.122963ms]
Jan  3 12:17:32.726: INFO: Created: latency-svc-59ggz
Jan  3 12:17:32.750: INFO: Got endpoints: latency-svc-zvxpw [752.00839ms]
Jan  3 12:17:32.781: INFO: Created: latency-svc-9zcd6
Jan  3 12:17:32.799: INFO: Got endpoints: latency-svc-9l5q4 [750.043743ms]
Jan  3 12:17:32.826: INFO: Created: latency-svc-s2bmt
Jan  3 12:17:32.853: INFO: Got endpoints: latency-svc-t6m9s [751.858902ms]
Jan  3 12:17:32.884: INFO: Created: latency-svc-nx75m
Jan  3 12:17:32.900: INFO: Got endpoints: latency-svc-6gbbf [746.875578ms]
Jan  3 12:17:32.929: INFO: Created: latency-svc-nfqj5
Jan  3 12:17:32.948: INFO: Got endpoints: latency-svc-tz8m5 [748.463061ms]
Jan  3 12:17:32.975: INFO: Created: latency-svc-s48hx
Jan  3 12:17:33.001: INFO: Got endpoints: latency-svc-fwd5q [749.452739ms]
Jan  3 12:17:33.030: INFO: Created: latency-svc-vb9w2
Jan  3 12:17:33.051: INFO: Got endpoints: latency-svc-9qq9v [751.711739ms]
Jan  3 12:17:33.076: INFO: Created: latency-svc-5sh8k
Jan  3 12:17:33.112: INFO: Got endpoints: latency-svc-qqtjf [761.944842ms]
Jan  3 12:17:33.138: INFO: Created: latency-svc-br68t
Jan  3 12:17:33.149: INFO: Got endpoints: latency-svc-946mm [749.989193ms]
Jan  3 12:17:33.175: INFO: Created: latency-svc-974bw
Jan  3 12:17:33.199: INFO: Got endpoints: latency-svc-qjxbg [743.325028ms]
Jan  3 12:17:33.227: INFO: Created: latency-svc-ds4p4
Jan  3 12:17:33.249: INFO: Got endpoints: latency-svc-72hmr [750.514589ms]
Jan  3 12:17:33.277: INFO: Created: latency-svc-ncxkb
Jan  3 12:17:33.306: INFO: Got endpoints: latency-svc-7zdpw [748.773231ms]
Jan  3 12:17:33.334: INFO: Created: latency-svc-hzv4t
Jan  3 12:17:33.354: INFO: Got endpoints: latency-svc-v9n5f [753.637ms]
Jan  3 12:17:33.384: INFO: Created: latency-svc-bjz54
Jan  3 12:17:33.400: INFO: Got endpoints: latency-svc-2pwsv [743.595501ms]
Jan  3 12:17:33.428: INFO: Created: latency-svc-sk9fm
Jan  3 12:17:33.451: INFO: Got endpoints: latency-svc-59ggz [750.175256ms]
Jan  3 12:17:33.480: INFO: Created: latency-svc-c4qm4
Jan  3 12:17:33.501: INFO: Got endpoints: latency-svc-9zcd6 [750.808017ms]
Jan  3 12:17:33.528: INFO: Created: latency-svc-hdc4x
Jan  3 12:17:33.553: INFO: Got endpoints: latency-svc-s2bmt [753.661452ms]
Jan  3 12:17:33.584: INFO: Created: latency-svc-8t5pv
Jan  3 12:17:33.597: INFO: Got endpoints: latency-svc-nx75m [744.263206ms]
Jan  3 12:17:33.626: INFO: Created: latency-svc-x6hvx
Jan  3 12:17:33.651: INFO: Got endpoints: latency-svc-nfqj5 [750.699718ms]
Jan  3 12:17:33.682: INFO: Created: latency-svc-pr6kk
Jan  3 12:17:33.700: INFO: Got endpoints: latency-svc-s48hx [751.163931ms]
Jan  3 12:17:33.731: INFO: Created: latency-svc-fbsbd
Jan  3 12:17:33.751: INFO: Got endpoints: latency-svc-vb9w2 [750.474769ms]
Jan  3 12:17:33.777: INFO: Created: latency-svc-t69lc
Jan  3 12:17:33.801: INFO: Got endpoints: latency-svc-5sh8k [749.337989ms]
Jan  3 12:17:33.837: INFO: Created: latency-svc-jc4tf
Jan  3 12:17:33.849: INFO: Got endpoints: latency-svc-br68t [736.518463ms]
Jan  3 12:17:33.877: INFO: Created: latency-svc-9t6fp
Jan  3 12:17:33.900: INFO: Got endpoints: latency-svc-974bw [750.328486ms]
Jan  3 12:17:33.925: INFO: Created: latency-svc-pmddq
Jan  3 12:17:33.953: INFO: Got endpoints: latency-svc-ds4p4 [753.424785ms]
Jan  3 12:17:33.988: INFO: Created: latency-svc-wm79r
Jan  3 12:17:34.000: INFO: Got endpoints: latency-svc-ncxkb [750.353563ms]
Jan  3 12:17:34.026: INFO: Created: latency-svc-kg27w
Jan  3 12:17:34.050: INFO: Got endpoints: latency-svc-hzv4t [743.811492ms]
Jan  3 12:17:34.079: INFO: Created: latency-svc-th7b4
Jan  3 12:17:34.102: INFO: Got endpoints: latency-svc-bjz54 [747.933354ms]
Jan  3 12:17:34.127: INFO: Created: latency-svc-lmv5r
Jan  3 12:17:34.151: INFO: Got endpoints: latency-svc-sk9fm [750.769309ms]
Jan  3 12:17:34.188: INFO: Created: latency-svc-4s5j6
Jan  3 12:17:34.202: INFO: Got endpoints: latency-svc-c4qm4 [750.613326ms]
Jan  3 12:17:34.227: INFO: Created: latency-svc-k5s2q
Jan  3 12:17:34.250: INFO: Got endpoints: latency-svc-hdc4x [748.442293ms]
Jan  3 12:17:34.279: INFO: Created: latency-svc-ldnkf
Jan  3 12:17:34.301: INFO: Got endpoints: latency-svc-8t5pv [747.664257ms]
Jan  3 12:17:34.330: INFO: Created: latency-svc-knv4b
Jan  3 12:17:34.354: INFO: Got endpoints: latency-svc-x6hvx [756.485843ms]
Jan  3 12:17:34.389: INFO: Created: latency-svc-h46sd
Jan  3 12:17:34.399: INFO: Got endpoints: latency-svc-pr6kk [748.0105ms]
Jan  3 12:17:34.424: INFO: Created: latency-svc-9mfpc
Jan  3 12:17:34.450: INFO: Got endpoints: latency-svc-fbsbd [750.679781ms]
Jan  3 12:17:34.481: INFO: Created: latency-svc-5c4cp
Jan  3 12:17:34.500: INFO: Got endpoints: latency-svc-t69lc [748.570656ms]
Jan  3 12:17:34.530: INFO: Created: latency-svc-lphr7
Jan  3 12:17:34.552: INFO: Got endpoints: latency-svc-jc4tf [751.08473ms]
Jan  3 12:17:34.592: INFO: Created: latency-svc-jcxkn
Jan  3 12:17:34.599: INFO: Got endpoints: latency-svc-9t6fp [750.378795ms]
Jan  3 12:17:34.629: INFO: Created: latency-svc-hmmss
Jan  3 12:17:34.668: INFO: Got endpoints: latency-svc-pmddq [767.524173ms]
Jan  3 12:17:34.701: INFO: Got endpoints: latency-svc-wm79r [748.739763ms]
Jan  3 12:17:34.704: INFO: Created: latency-svc-vzgj8
Jan  3 12:17:34.726: INFO: Created: latency-svc-l78xq
Jan  3 12:17:34.752: INFO: Got endpoints: latency-svc-kg27w [751.81582ms]
Jan  3 12:17:34.785: INFO: Created: latency-svc-m5gk8
Jan  3 12:17:34.804: INFO: Got endpoints: latency-svc-th7b4 [754.018709ms]
Jan  3 12:17:34.833: INFO: Created: latency-svc-qcs6z
Jan  3 12:17:34.850: INFO: Got endpoints: latency-svc-lmv5r [747.99794ms]
Jan  3 12:17:34.899: INFO: Got endpoints: latency-svc-4s5j6 [748.444157ms]
Jan  3 12:17:34.951: INFO: Got endpoints: latency-svc-k5s2q [748.674258ms]
Jan  3 12:17:35.000: INFO: Got endpoints: latency-svc-ldnkf [750.223781ms]
Jan  3 12:17:35.051: INFO: Got endpoints: latency-svc-knv4b [749.05071ms]
Jan  3 12:17:35.101: INFO: Got endpoints: latency-svc-h46sd [746.85687ms]
Jan  3 12:17:35.153: INFO: Got endpoints: latency-svc-9mfpc [753.248629ms]
Jan  3 12:17:35.202: INFO: Got endpoints: latency-svc-5c4cp [751.137993ms]
Jan  3 12:17:35.265: INFO: Got endpoints: latency-svc-lphr7 [764.813359ms]
Jan  3 12:17:35.299: INFO: Got endpoints: latency-svc-jcxkn [747.269472ms]
Jan  3 12:17:35.358: INFO: Got endpoints: latency-svc-hmmss [759.17322ms]
Jan  3 12:17:35.407: INFO: Got endpoints: latency-svc-vzgj8 [739.532966ms]
Jan  3 12:17:35.450: INFO: Got endpoints: latency-svc-l78xq [747.956468ms]
Jan  3 12:17:35.502: INFO: Got endpoints: latency-svc-m5gk8 [750.815858ms]
Jan  3 12:17:35.552: INFO: Got endpoints: latency-svc-qcs6z [747.654132ms]
Jan  3 12:17:35.552: INFO: Latencies: [36.628705ms 43.972898ms 47.464374ms 78.152006ms 78.896906ms 86.388088ms 90.137819ms 94.188671ms 95.114404ms 100.805107ms 101.699954ms 103.700594ms 104.39863ms 104.93891ms 109.645674ms 109.964609ms 113.919665ms 115.226768ms 115.443815ms 115.886619ms 117.451221ms 117.987409ms 118.639672ms 119.622815ms 120.368442ms 121.799346ms 122.535734ms 122.816783ms 123.158619ms 124.03001ms 125.733441ms 126.024256ms 132.061339ms 134.960363ms 143.727101ms 161.185246ms 200.104846ms 243.75319ms 282.720295ms 320.455318ms 363.668023ms 406.562505ms 447.447731ms 492.906811ms 542.737979ms 575.409913ms 623.667099ms 660.714565ms 704.416007ms 731.606572ms 736.518463ms 737.290949ms 738.15773ms 739.532966ms 739.944124ms 740.612699ms 741.093938ms 743.141467ms 743.325028ms 743.595501ms 743.711631ms 743.811492ms 744.263206ms 744.39161ms 745.698861ms 745.857197ms 746.636634ms 746.683503ms 746.835203ms 746.85687ms 746.875578ms 747.014976ms 747.122963ms 747.189322ms 747.269472ms 747.437466ms 747.478018ms 747.654132ms 747.664257ms 747.692489ms 747.725278ms 747.730071ms 747.871665ms 747.933354ms 747.956468ms 747.99794ms 748.0105ms 748.209621ms 748.276455ms 748.377453ms 748.442293ms 748.444157ms 748.463061ms 748.477391ms 748.570656ms 748.584096ms 748.591443ms 748.674258ms 748.691033ms 748.724855ms 748.739763ms 748.748357ms 748.752748ms 748.771664ms 748.773231ms 748.811689ms 748.979841ms 748.997008ms 749.05071ms 749.097816ms 749.109376ms 749.156286ms 749.16815ms 749.181712ms 749.230057ms 749.287431ms 749.297125ms 749.299122ms 749.337989ms 749.3694ms 749.452739ms 749.513772ms 749.550674ms 749.726351ms 749.938344ms 749.989193ms 750.043743ms 750.086049ms 750.175256ms 750.223781ms 750.262781ms 750.308178ms 750.328486ms 750.353563ms 750.378795ms 750.474769ms 750.507168ms 750.514589ms 750.545662ms 750.569928ms 750.613326ms 750.648135ms 750.679781ms 750.699718ms 750.757267ms 750.769309ms 750.774452ms 750.808017ms 750.815858ms 750.891701ms 750.9153ms 751.08473ms 751.137993ms 751.163931ms 751.460608ms 751.464978ms 751.505666ms 751.526843ms 751.536179ms 751.691615ms 751.711739ms 751.715092ms 751.81582ms 751.858902ms 752.00839ms 752.182521ms 752.205891ms 752.319399ms 752.892892ms 752.980952ms 752.992514ms 753.026764ms 753.043544ms 753.248629ms 753.274968ms 753.424785ms 753.438602ms 753.485349ms 753.562175ms 753.637ms 753.661452ms 753.947971ms 753.965205ms 754.018709ms 754.31389ms 754.424834ms 754.868955ms 755.626798ms 755.876862ms 756.396933ms 756.485843ms 757.123657ms 758.513425ms 759.17322ms 761.944842ms 762.419313ms 763.998836ms 764.813359ms 767.524173ms 769.403334ms]
Jan  3 12:17:35.553: INFO: 50 %ile: 748.739763ms
Jan  3 12:17:35.553: INFO: 90 %ile: 753.661452ms
Jan  3 12:17:35.553: INFO: 99 %ile: 767.524173ms
Jan  3 12:17:35.553: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jan  3 12:17:35.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-5080" for this suite. 01/03/24 12:17:35.591
------------------------------
• [SLOW TEST] [11.949 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:17:23.668
    Jan  3 12:17:23.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename svc-latency 01/03/24 12:17:23.67
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:17:23.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:17:23.755
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan  3 12:17:23.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5080 01/03/24 12:17:23.784
    I0103 12:17:23.806195      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5080, replica count: 1
    I0103 12:17:24.857557      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0103 12:17:25.857967      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0103 12:17:26.858660      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 12:17:26.991: INFO: Created: latency-svc-g69r4
    Jan  3 12:17:26.999: INFO: Got endpoints: latency-svc-g69r4 [39.63622ms]
    Jan  3 12:17:27.027: INFO: Created: latency-svc-vb6vn
    Jan  3 12:17:27.037: INFO: Got endpoints: latency-svc-vb6vn [36.628705ms]
    Jan  3 12:17:27.037: INFO: Created: latency-svc-pw629
    Jan  3 12:17:27.040: INFO: Created: latency-svc-6p56m
    Jan  3 12:17:27.044: INFO: Got endpoints: latency-svc-pw629 [43.972898ms]
    Jan  3 12:17:27.047: INFO: Got endpoints: latency-svc-6p56m [47.464374ms]
    Jan  3 12:17:27.048: INFO: Created: latency-svc-757ll
    Jan  3 12:17:27.058: INFO: Created: latency-svc-jlwms
    Jan  3 12:17:27.078: INFO: Created: latency-svc-7sdfw
    Jan  3 12:17:27.078: INFO: Got endpoints: latency-svc-757ll [78.152006ms]
    Jan  3 12:17:27.079: INFO: Got endpoints: latency-svc-jlwms [78.896906ms]
    Jan  3 12:17:27.085: INFO: Created: latency-svc-q9nqd
    Jan  3 12:17:27.087: INFO: Got endpoints: latency-svc-7sdfw [86.388088ms]
    Jan  3 12:17:27.095: INFO: Created: latency-svc-6dt26
    Jan  3 12:17:27.095: INFO: Got endpoints: latency-svc-q9nqd [94.188671ms]
    Jan  3 12:17:27.099: INFO: Created: latency-svc-ljnvh
    Jan  3 12:17:27.103: INFO: Got endpoints: latency-svc-6dt26 [100.805107ms]
    Jan  3 12:17:27.106: INFO: Created: latency-svc-tfn5r
    Jan  3 12:17:27.106: INFO: Got endpoints: latency-svc-ljnvh [104.93891ms]
    Jan  3 12:17:27.111: INFO: Created: latency-svc-667x2
    Jan  3 12:17:27.115: INFO: Got endpoints: latency-svc-tfn5r [113.919665ms]
    Jan  3 12:17:27.117: INFO: Created: latency-svc-bz25p
    Jan  3 12:17:27.120: INFO: Got endpoints: latency-svc-667x2 [118.639672ms]
    Jan  3 12:17:27.123: INFO: Created: latency-svc-jq8xw
    Jan  3 12:17:27.124: INFO: Got endpoints: latency-svc-bz25p [122.816783ms]
    Jan  3 12:17:27.128: INFO: Created: latency-svc-fn4n8
    Jan  3 12:17:27.134: INFO: Got endpoints: latency-svc-jq8xw [132.061339ms]
    Jan  3 12:17:27.136: INFO: Created: latency-svc-5wbfg
    Jan  3 12:17:27.137: INFO: Got endpoints: latency-svc-fn4n8 [134.960363ms]
    Jan  3 12:17:27.143: INFO: Created: latency-svc-ms2p8
    Jan  3 12:17:27.143: INFO: Got endpoints: latency-svc-5wbfg [143.727101ms]
    Jan  3 12:17:27.148: INFO: Created: latency-svc-fw2tv
    Jan  3 12:17:27.152: INFO: Got endpoints: latency-svc-ms2p8 [115.226768ms]
    Jan  3 12:17:27.153: INFO: Created: latency-svc-ltpnh
    Jan  3 12:17:27.155: INFO: Got endpoints: latency-svc-fw2tv [109.964609ms]
    Jan  3 12:17:27.161: INFO: Created: latency-svc-crw6w
    Jan  3 12:17:27.165: INFO: Got endpoints: latency-svc-ltpnh [117.451221ms]
    Jan  3 12:17:27.168: INFO: Created: latency-svc-vt6p2
    Jan  3 12:17:27.169: INFO: Got endpoints: latency-svc-crw6w [90.137819ms]
    Jan  3 12:17:27.174: INFO: Created: latency-svc-5dgzk
    Jan  3 12:17:27.174: INFO: Got endpoints: latency-svc-vt6p2 [95.114404ms]
    Jan  3 12:17:27.189: INFO: Created: latency-svc-zs6n2
    Jan  3 12:17:27.189: INFO: Got endpoints: latency-svc-5dgzk [101.699954ms]
    Jan  3 12:17:27.199: INFO: Created: latency-svc-j7d2r
    Jan  3 12:17:27.200: INFO: Got endpoints: latency-svc-zs6n2 [104.39863ms]
    Jan  3 12:17:27.206: INFO: Got endpoints: latency-svc-j7d2r [103.700594ms]
    Jan  3 12:17:27.208: INFO: Created: latency-svc-qfhd6
    Jan  3 12:17:27.216: INFO: Got endpoints: latency-svc-qfhd6 [109.645674ms]
    Jan  3 12:17:27.219: INFO: Created: latency-svc-q7j2l
    Jan  3 12:17:27.228: INFO: Created: latency-svc-22z85
    Jan  3 12:17:27.230: INFO: Got endpoints: latency-svc-q7j2l [115.443815ms]
    Jan  3 12:17:27.236: INFO: Got endpoints: latency-svc-22z85 [115.886619ms]
    Jan  3 12:17:27.237: INFO: Created: latency-svc-hn7dx
    Jan  3 12:17:27.244: INFO: Created: latency-svc-8wb68
    Jan  3 12:17:27.244: INFO: Got endpoints: latency-svc-hn7dx [120.368442ms]
    Jan  3 12:17:27.248: INFO: Created: latency-svc-ltnxc
    Jan  3 12:17:27.252: INFO: Got endpoints: latency-svc-8wb68 [117.987409ms]
    Jan  3 12:17:27.256: INFO: Created: latency-svc-cc46q
    Jan  3 12:17:27.257: INFO: Got endpoints: latency-svc-ltnxc [119.622815ms]
    Jan  3 12:17:27.265: INFO: Got endpoints: latency-svc-cc46q [121.799346ms]
    Jan  3 12:17:27.265: INFO: Created: latency-svc-446qz
    Jan  3 12:17:27.270: INFO: Created: latency-svc-5tpjv
    Jan  3 12:17:27.275: INFO: Got endpoints: latency-svc-446qz [122.535734ms]
    Jan  3 12:17:27.278: INFO: Got endpoints: latency-svc-5tpjv [123.158619ms]
    Jan  3 12:17:27.278: INFO: Created: latency-svc-f7llr
    Jan  3 12:17:27.284: INFO: Created: latency-svc-4g896
    Jan  3 12:17:27.289: INFO: Got endpoints: latency-svc-f7llr [124.03001ms]
    Jan  3 12:17:27.291: INFO: Created: latency-svc-cplmh
    Jan  3 12:17:27.295: INFO: Got endpoints: latency-svc-4g896 [126.024256ms]
    Jan  3 12:17:27.300: INFO: Got endpoints: latency-svc-cplmh [125.733441ms]
    Jan  3 12:17:27.300: INFO: Created: latency-svc-jp8mm
    Jan  3 12:17:27.314: INFO: Created: latency-svc-56clf
    Jan  3 12:17:27.322: INFO: Created: latency-svc-zn7bz
    Jan  3 12:17:27.326: INFO: Created: latency-svc-7q62w
    Jan  3 12:17:27.335: INFO: Created: latency-svc-xqm8g
    Jan  3 12:17:27.345: INFO: Created: latency-svc-kl4rf
    Jan  3 12:17:27.349: INFO: Created: latency-svc-hvdqj
    Jan  3 12:17:27.350: INFO: Got endpoints: latency-svc-jp8mm [161.185246ms]
    Jan  3 12:17:27.355: INFO: Created: latency-svc-gstq4
    Jan  3 12:17:27.362: INFO: Created: latency-svc-2l9cf
    Jan  3 12:17:27.369: INFO: Created: latency-svc-h7t2l
    Jan  3 12:17:27.377: INFO: Created: latency-svc-7hkqs
    Jan  3 12:17:27.380: INFO: Created: latency-svc-nmxqk
    Jan  3 12:17:27.386: INFO: Created: latency-svc-x57vk
    Jan  3 12:17:27.391: INFO: Created: latency-svc-qqk5b
    Jan  3 12:17:27.397: INFO: Created: latency-svc-q4tdp
    Jan  3 12:17:27.400: INFO: Got endpoints: latency-svc-56clf [200.104846ms]
    Jan  3 12:17:27.402: INFO: Created: latency-svc-tlrkh
    Jan  3 12:17:27.423: INFO: Created: latency-svc-qtx4g
    Jan  3 12:17:27.450: INFO: Got endpoints: latency-svc-zn7bz [243.75319ms]
    Jan  3 12:17:27.477: INFO: Created: latency-svc-p48hf
    Jan  3 12:17:27.498: INFO: Got endpoints: latency-svc-7q62w [282.720295ms]
    Jan  3 12:17:27.526: INFO: Created: latency-svc-wgg8s
    Jan  3 12:17:27.551: INFO: Got endpoints: latency-svc-xqm8g [320.455318ms]
    Jan  3 12:17:27.580: INFO: Created: latency-svc-wc9wk
    Jan  3 12:17:27.600: INFO: Got endpoints: latency-svc-kl4rf [363.668023ms]
    Jan  3 12:17:27.627: INFO: Created: latency-svc-9mp9m
    Jan  3 12:17:27.651: INFO: Got endpoints: latency-svc-hvdqj [406.562505ms]
    Jan  3 12:17:27.682: INFO: Created: latency-svc-ps5rb
    Jan  3 12:17:27.699: INFO: Got endpoints: latency-svc-gstq4 [447.447731ms]
    Jan  3 12:17:27.728: INFO: Created: latency-svc-lhpp9
    Jan  3 12:17:27.750: INFO: Got endpoints: latency-svc-2l9cf [492.906811ms]
    Jan  3 12:17:27.776: INFO: Created: latency-svc-w6l2c
    Jan  3 12:17:27.808: INFO: Got endpoints: latency-svc-h7t2l [542.737979ms]
    Jan  3 12:17:27.840: INFO: Created: latency-svc-rrkwv
    Jan  3 12:17:27.850: INFO: Got endpoints: latency-svc-7hkqs [575.409913ms]
    Jan  3 12:17:27.880: INFO: Created: latency-svc-f77kt
    Jan  3 12:17:27.902: INFO: Got endpoints: latency-svc-nmxqk [623.667099ms]
    Jan  3 12:17:27.929: INFO: Created: latency-svc-sfc86
    Jan  3 12:17:27.949: INFO: Got endpoints: latency-svc-x57vk [660.714565ms]
    Jan  3 12:17:27.980: INFO: Created: latency-svc-ztjlz
    Jan  3 12:17:28.000: INFO: Got endpoints: latency-svc-qqk5b [704.416007ms]
    Jan  3 12:17:28.047: INFO: Created: latency-svc-tmsqx
    Jan  3 12:17:28.055: INFO: Got endpoints: latency-svc-q4tdp [754.31389ms]
    Jan  3 12:17:28.082: INFO: Created: latency-svc-hgc6k
    Jan  3 12:17:28.103: INFO: Got endpoints: latency-svc-tlrkh [752.319399ms]
    Jan  3 12:17:28.131: INFO: Created: latency-svc-kbn8j
    Jan  3 12:17:28.149: INFO: Got endpoints: latency-svc-qtx4g [748.997008ms]
    Jan  3 12:17:28.176: INFO: Created: latency-svc-jtqgg
    Jan  3 12:17:28.200: INFO: Got endpoints: latency-svc-p48hf [749.230057ms]
    Jan  3 12:17:28.226: INFO: Created: latency-svc-7jzvx
    Jan  3 12:17:28.249: INFO: Got endpoints: latency-svc-wgg8s [750.262781ms]
    Jan  3 12:17:28.277: INFO: Created: latency-svc-75f27
    Jan  3 12:17:28.300: INFO: Got endpoints: latency-svc-wc9wk [749.109376ms]
    Jan  3 12:17:28.329: INFO: Created: latency-svc-p827q
    Jan  3 12:17:28.349: INFO: Got endpoints: latency-svc-9mp9m [748.724855ms]
    Jan  3 12:17:28.375: INFO: Created: latency-svc-sxjzg
    Jan  3 12:17:28.400: INFO: Got endpoints: latency-svc-ps5rb [748.752748ms]
    Jan  3 12:17:28.450: INFO: Got endpoints: latency-svc-lhpp9 [750.086049ms]
    Jan  3 12:17:28.452: INFO: Created: latency-svc-5l7c7
    Jan  3 12:17:28.476: INFO: Created: latency-svc-wsmws
    Jan  3 12:17:28.500: INFO: Got endpoints: latency-svc-w6l2c [750.648135ms]
    Jan  3 12:17:28.527: INFO: Created: latency-svc-rdm8r
    Jan  3 12:17:28.552: INFO: Got endpoints: latency-svc-rrkwv [743.141467ms]
    Jan  3 12:17:28.578: INFO: Created: latency-svc-mhz8l
    Jan  3 12:17:28.601: INFO: Got endpoints: latency-svc-f77kt [750.507168ms]
    Jan  3 12:17:28.630: INFO: Created: latency-svc-qfcb4
    Jan  3 12:17:28.651: INFO: Got endpoints: latency-svc-sfc86 [749.550674ms]
    Jan  3 12:17:28.687: INFO: Created: latency-svc-xhn8q
    Jan  3 12:17:28.719: INFO: Got endpoints: latency-svc-ztjlz [769.403334ms]
    Jan  3 12:17:28.764: INFO: Got endpoints: latency-svc-tmsqx [763.998836ms]
    Jan  3 12:17:28.773: INFO: Created: latency-svc-s2wdh
    Jan  3 12:17:28.790: INFO: Created: latency-svc-v7txm
    Jan  3 12:17:28.799: INFO: Got endpoints: latency-svc-hgc6k [744.39161ms]
    Jan  3 12:17:28.824: INFO: Created: latency-svc-xkk7x
    Jan  3 12:17:28.861: INFO: Got endpoints: latency-svc-kbn8j [758.513425ms]
    Jan  3 12:17:28.891: INFO: Created: latency-svc-ktgpw
    Jan  3 12:17:28.906: INFO: Got endpoints: latency-svc-jtqgg [755.876862ms]
    Jan  3 12:17:28.935: INFO: Created: latency-svc-g9vsw
    Jan  3 12:17:28.951: INFO: Got endpoints: latency-svc-7jzvx [751.536179ms]
    Jan  3 12:17:28.978: INFO: Created: latency-svc-q5nzd
    Jan  3 12:17:29.005: INFO: Got endpoints: latency-svc-75f27 [755.626798ms]
    Jan  3 12:17:29.032: INFO: Created: latency-svc-7n4jl
    Jan  3 12:17:29.054: INFO: Got endpoints: latency-svc-p827q [753.485349ms]
    Jan  3 12:17:29.088: INFO: Created: latency-svc-gz49z
    Jan  3 12:17:29.099: INFO: Got endpoints: latency-svc-sxjzg [750.569928ms]
    Jan  3 12:17:29.125: INFO: Created: latency-svc-b9khj
    Jan  3 12:17:29.155: INFO: Got endpoints: latency-svc-5l7c7 [754.868955ms]
    Jan  3 12:17:29.183: INFO: Created: latency-svc-6mfb6
    Jan  3 12:17:29.202: INFO: Got endpoints: latency-svc-wsmws [752.205891ms]
    Jan  3 12:17:29.228: INFO: Created: latency-svc-74grx
    Jan  3 12:17:29.252: INFO: Got endpoints: latency-svc-rdm8r [751.691615ms]
    Jan  3 12:17:29.283: INFO: Created: latency-svc-xgx89
    Jan  3 12:17:29.314: INFO: Got endpoints: latency-svc-mhz8l [762.419313ms]
    Jan  3 12:17:29.342: INFO: Created: latency-svc-jcnnk
    Jan  3 12:17:29.349: INFO: Got endpoints: latency-svc-qfcb4 [747.871665ms]
    Jan  3 12:17:29.379: INFO: Created: latency-svc-zwkpz
    Jan  3 12:17:29.400: INFO: Got endpoints: latency-svc-xhn8q [749.097816ms]
    Jan  3 12:17:29.427: INFO: Created: latency-svc-wj9m9
    Jan  3 12:17:29.468: INFO: Got endpoints: latency-svc-s2wdh [749.181712ms]
    Jan  3 12:17:29.495: INFO: Created: latency-svc-hlvzn
    Jan  3 12:17:29.501: INFO: Got endpoints: latency-svc-v7txm [737.290949ms]
    Jan  3 12:17:29.528: INFO: Created: latency-svc-2lpdg
    Jan  3 12:17:29.551: INFO: Got endpoints: latency-svc-xkk7x [751.526843ms]
    Jan  3 12:17:29.579: INFO: Created: latency-svc-66kng
    Jan  3 12:17:29.601: INFO: Got endpoints: latency-svc-ktgpw [739.944124ms]
    Jan  3 12:17:29.637: INFO: Created: latency-svc-chx7s
    Jan  3 12:17:29.653: INFO: Got endpoints: latency-svc-g9vsw [747.692489ms]
    Jan  3 12:17:29.678: INFO: Created: latency-svc-ngwjv
    Jan  3 12:17:29.700: INFO: Got endpoints: latency-svc-q5nzd [748.591443ms]
    Jan  3 12:17:29.726: INFO: Created: latency-svc-cjt72
    Jan  3 12:17:29.751: INFO: Got endpoints: latency-svc-7n4jl [745.698861ms]
    Jan  3 12:17:29.799: INFO: Created: latency-svc-7qp2z
    Jan  3 12:17:29.810: INFO: Got endpoints: latency-svc-gz49z [756.396933ms]
    Jan  3 12:17:29.836: INFO: Created: latency-svc-cdgw7
    Jan  3 12:17:29.851: INFO: Got endpoints: latency-svc-b9khj [751.460608ms]
    Jan  3 12:17:29.883: INFO: Created: latency-svc-lvflx
    Jan  3 12:17:29.905: INFO: Got endpoints: latency-svc-6mfb6 [749.726351ms]
    Jan  3 12:17:29.933: INFO: Created: latency-svc-fzwfm
    Jan  3 12:17:29.950: INFO: Got endpoints: latency-svc-74grx [748.584096ms]
    Jan  3 12:17:29.981: INFO: Created: latency-svc-jtmvb
    Jan  3 12:17:30.003: INFO: Got endpoints: latency-svc-xgx89 [750.757267ms]
    Jan  3 12:17:30.036: INFO: Created: latency-svc-pdfm9
    Jan  3 12:17:30.058: INFO: Got endpoints: latency-svc-jcnnk [743.711631ms]
    Jan  3 12:17:30.091: INFO: Created: latency-svc-j9phl
    Jan  3 12:17:30.102: INFO: Got endpoints: latency-svc-zwkpz [752.892892ms]
    Jan  3 12:17:30.130: INFO: Created: latency-svc-dnbhh
    Jan  3 12:17:30.149: INFO: Got endpoints: latency-svc-wj9m9 [748.771664ms]
    Jan  3 12:17:30.177: INFO: Created: latency-svc-ksxl4
    Jan  3 12:17:30.200: INFO: Got endpoints: latency-svc-hlvzn [731.606572ms]
    Jan  3 12:17:30.225: INFO: Created: latency-svc-6dwvq
    Jan  3 12:17:30.249: INFO: Got endpoints: latency-svc-2lpdg [747.725278ms]
    Jan  3 12:17:30.275: INFO: Created: latency-svc-2zpp2
    Jan  3 12:17:30.300: INFO: Got endpoints: latency-svc-66kng [749.299122ms]
    Jan  3 12:17:30.328: INFO: Created: latency-svc-9cjhm
    Jan  3 12:17:30.348: INFO: Got endpoints: latency-svc-chx7s [747.014976ms]
    Jan  3 12:17:30.375: INFO: Created: latency-svc-5lpsn
    Jan  3 12:17:30.400: INFO: Got endpoints: latency-svc-ngwjv [746.835203ms]
    Jan  3 12:17:30.425: INFO: Created: latency-svc-j4zdg
    Jan  3 12:17:30.453: INFO: Got endpoints: latency-svc-cjt72 [752.992514ms]
    Jan  3 12:17:30.493: INFO: Created: latency-svc-4kgbr
    Jan  3 12:17:30.502: INFO: Got endpoints: latency-svc-7qp2z [751.505666ms]
    Jan  3 12:17:30.527: INFO: Created: latency-svc-xh8l8
    Jan  3 12:17:30.549: INFO: Got endpoints: latency-svc-cdgw7 [738.15773ms]
    Jan  3 12:17:30.574: INFO: Created: latency-svc-7sg8q
    Jan  3 12:17:30.600: INFO: Got endpoints: latency-svc-lvflx [748.691033ms]
    Jan  3 12:17:30.627: INFO: Created: latency-svc-cpz2j
    Jan  3 12:17:30.659: INFO: Got endpoints: latency-svc-fzwfm [753.947971ms]
    Jan  3 12:17:30.689: INFO: Created: latency-svc-hmdjg
    Jan  3 12:17:30.700: INFO: Got endpoints: latency-svc-jtmvb [749.156286ms]
    Jan  3 12:17:30.728: INFO: Created: latency-svc-wgnl2
    Jan  3 12:17:30.750: INFO: Got endpoints: latency-svc-pdfm9 [746.683503ms]
    Jan  3 12:17:30.782: INFO: Created: latency-svc-2hm87
    Jan  3 12:17:30.799: INFO: Got endpoints: latency-svc-j9phl [741.093938ms]
    Jan  3 12:17:30.828: INFO: Created: latency-svc-j9vlk
    Jan  3 12:17:30.849: INFO: Got endpoints: latency-svc-dnbhh [745.857197ms]
    Jan  3 12:17:30.875: INFO: Created: latency-svc-w9zvs
    Jan  3 12:17:30.903: INFO: Got endpoints: latency-svc-ksxl4 [753.562175ms]
    Jan  3 12:17:30.936: INFO: Created: latency-svc-n2wbl
    Jan  3 12:17:30.949: INFO: Got endpoints: latency-svc-6dwvq [748.979841ms]
    Jan  3 12:17:30.979: INFO: Created: latency-svc-jfq4g
    Jan  3 12:17:31.002: INFO: Got endpoints: latency-svc-2zpp2 [753.274968ms]
    Jan  3 12:17:31.037: INFO: Created: latency-svc-v7zjp
    Jan  3 12:17:31.048: INFO: Got endpoints: latency-svc-9cjhm [747.730071ms]
    Jan  3 12:17:31.073: INFO: Created: latency-svc-tlwfw
    Jan  3 12:17:31.103: INFO: Got endpoints: latency-svc-5lpsn [754.424834ms]
    Jan  3 12:17:31.136: INFO: Created: latency-svc-l585l
    Jan  3 12:17:31.150: INFO: Got endpoints: latency-svc-j4zdg [749.297125ms]
    Jan  3 12:17:31.176: INFO: Created: latency-svc-vmnr7
    Jan  3 12:17:31.203: INFO: Got endpoints: latency-svc-4kgbr [749.287431ms]
    Jan  3 12:17:31.229: INFO: Created: latency-svc-vl767
    Jan  3 12:17:31.249: INFO: Got endpoints: latency-svc-xh8l8 [746.636634ms]
    Jan  3 12:17:31.276: INFO: Created: latency-svc-pk2cv
    Jan  3 12:17:31.300: INFO: Got endpoints: latency-svc-7sg8q [751.464978ms]
    Jan  3 12:17:31.328: INFO: Created: latency-svc-ccddk
    Jan  3 12:17:31.353: INFO: Got endpoints: latency-svc-cpz2j [753.043544ms]
    Jan  3 12:17:31.384: INFO: Created: latency-svc-9l7t4
    Jan  3 12:17:31.400: INFO: Got endpoints: latency-svc-hmdjg [740.612699ms]
    Jan  3 12:17:31.429: INFO: Created: latency-svc-kjs9r
    Jan  3 12:17:31.449: INFO: Got endpoints: latency-svc-wgnl2 [749.16815ms]
    Jan  3 12:17:31.482: INFO: Created: latency-svc-4gcl6
    Jan  3 12:17:31.500: INFO: Got endpoints: latency-svc-2hm87 [749.513772ms]
    Jan  3 12:17:31.527: INFO: Created: latency-svc-hzllf
    Jan  3 12:17:31.549: INFO: Got endpoints: latency-svc-j9vlk [749.938344ms]
    Jan  3 12:17:31.576: INFO: Created: latency-svc-8kpqs
    Jan  3 12:17:31.599: INFO: Got endpoints: latency-svc-w9zvs [750.891701ms]
    Jan  3 12:17:31.629: INFO: Created: latency-svc-lv99z
    Jan  3 12:17:31.651: INFO: Got endpoints: latency-svc-n2wbl [747.478018ms]
    Jan  3 12:17:31.676: INFO: Created: latency-svc-dthm7
    Jan  3 12:17:31.702: INFO: Got endpoints: latency-svc-jfq4g [753.026764ms]
    Jan  3 12:17:31.731: INFO: Created: latency-svc-hx8bb
    Jan  3 12:17:31.750: INFO: Got endpoints: latency-svc-v7zjp [747.437466ms]
    Jan  3 12:17:31.776: INFO: Created: latency-svc-v4hzt
    Jan  3 12:17:31.800: INFO: Got endpoints: latency-svc-tlwfw [751.715092ms]
    Jan  3 12:17:31.827: INFO: Created: latency-svc-fx44n
    Jan  3 12:17:31.851: INFO: Got endpoints: latency-svc-l585l [748.209621ms]
    Jan  3 12:17:31.878: INFO: Created: latency-svc-vzbf7
    Jan  3 12:17:31.904: INFO: Got endpoints: latency-svc-vmnr7 [753.965205ms]
    Jan  3 12:17:31.937: INFO: Created: latency-svc-kpz2s
    Jan  3 12:17:31.953: INFO: Got endpoints: latency-svc-vl767 [750.545662ms]
    Jan  3 12:17:31.977: INFO: Created: latency-svc-rq5p9
    Jan  3 12:17:31.998: INFO: Got endpoints: latency-svc-pk2cv [748.811689ms]
    Jan  3 12:17:32.024: INFO: Created: latency-svc-zvxpw
    Jan  3 12:17:32.049: INFO: Got endpoints: latency-svc-ccddk [748.377453ms]
    Jan  3 12:17:32.078: INFO: Created: latency-svc-9l5q4
    Jan  3 12:17:32.100: INFO: Got endpoints: latency-svc-9l7t4 [747.189322ms]
    Jan  3 12:17:32.127: INFO: Created: latency-svc-t6m9s
    Jan  3 12:17:32.153: INFO: Got endpoints: latency-svc-kjs9r [752.980952ms]
    Jan  3 12:17:32.184: INFO: Created: latency-svc-6gbbf
    Jan  3 12:17:32.200: INFO: Got endpoints: latency-svc-4gcl6 [750.774452ms]
    Jan  3 12:17:32.226: INFO: Created: latency-svc-tz8m5
    Jan  3 12:17:32.251: INFO: Got endpoints: latency-svc-hzllf [750.9153ms]
    Jan  3 12:17:32.277: INFO: Created: latency-svc-fwd5q
    Jan  3 12:17:32.299: INFO: Got endpoints: latency-svc-8kpqs [749.3694ms]
    Jan  3 12:17:32.326: INFO: Created: latency-svc-9qq9v
    Jan  3 12:17:32.350: INFO: Got endpoints: latency-svc-lv99z [750.308178ms]
    Jan  3 12:17:32.382: INFO: Created: latency-svc-qqtjf
    Jan  3 12:17:32.399: INFO: Got endpoints: latency-svc-dthm7 [748.276455ms]
    Jan  3 12:17:32.425: INFO: Created: latency-svc-946mm
    Jan  3 12:17:32.456: INFO: Got endpoints: latency-svc-hx8bb [753.438602ms]
    Jan  3 12:17:32.480: INFO: Created: latency-svc-qjxbg
    Jan  3 12:17:32.499: INFO: Got endpoints: latency-svc-v4hzt [748.477391ms]
    Jan  3 12:17:32.526: INFO: Created: latency-svc-72hmr
    Jan  3 12:17:32.557: INFO: Got endpoints: latency-svc-fx44n [757.123657ms]
    Jan  3 12:17:32.584: INFO: Created: latency-svc-7zdpw
    Jan  3 12:17:32.600: INFO: Got endpoints: latency-svc-vzbf7 [748.748357ms]
    Jan  3 12:17:32.626: INFO: Created: latency-svc-v9n5f
    Jan  3 12:17:32.656: INFO: Got endpoints: latency-svc-kpz2s [752.182521ms]
    Jan  3 12:17:32.687: INFO: Created: latency-svc-2pwsv
    Jan  3 12:17:32.700: INFO: Got endpoints: latency-svc-rq5p9 [747.122963ms]
    Jan  3 12:17:32.726: INFO: Created: latency-svc-59ggz
    Jan  3 12:17:32.750: INFO: Got endpoints: latency-svc-zvxpw [752.00839ms]
    Jan  3 12:17:32.781: INFO: Created: latency-svc-9zcd6
    Jan  3 12:17:32.799: INFO: Got endpoints: latency-svc-9l5q4 [750.043743ms]
    Jan  3 12:17:32.826: INFO: Created: latency-svc-s2bmt
    Jan  3 12:17:32.853: INFO: Got endpoints: latency-svc-t6m9s [751.858902ms]
    Jan  3 12:17:32.884: INFO: Created: latency-svc-nx75m
    Jan  3 12:17:32.900: INFO: Got endpoints: latency-svc-6gbbf [746.875578ms]
    Jan  3 12:17:32.929: INFO: Created: latency-svc-nfqj5
    Jan  3 12:17:32.948: INFO: Got endpoints: latency-svc-tz8m5 [748.463061ms]
    Jan  3 12:17:32.975: INFO: Created: latency-svc-s48hx
    Jan  3 12:17:33.001: INFO: Got endpoints: latency-svc-fwd5q [749.452739ms]
    Jan  3 12:17:33.030: INFO: Created: latency-svc-vb9w2
    Jan  3 12:17:33.051: INFO: Got endpoints: latency-svc-9qq9v [751.711739ms]
    Jan  3 12:17:33.076: INFO: Created: latency-svc-5sh8k
    Jan  3 12:17:33.112: INFO: Got endpoints: latency-svc-qqtjf [761.944842ms]
    Jan  3 12:17:33.138: INFO: Created: latency-svc-br68t
    Jan  3 12:17:33.149: INFO: Got endpoints: latency-svc-946mm [749.989193ms]
    Jan  3 12:17:33.175: INFO: Created: latency-svc-974bw
    Jan  3 12:17:33.199: INFO: Got endpoints: latency-svc-qjxbg [743.325028ms]
    Jan  3 12:17:33.227: INFO: Created: latency-svc-ds4p4
    Jan  3 12:17:33.249: INFO: Got endpoints: latency-svc-72hmr [750.514589ms]
    Jan  3 12:17:33.277: INFO: Created: latency-svc-ncxkb
    Jan  3 12:17:33.306: INFO: Got endpoints: latency-svc-7zdpw [748.773231ms]
    Jan  3 12:17:33.334: INFO: Created: latency-svc-hzv4t
    Jan  3 12:17:33.354: INFO: Got endpoints: latency-svc-v9n5f [753.637ms]
    Jan  3 12:17:33.384: INFO: Created: latency-svc-bjz54
    Jan  3 12:17:33.400: INFO: Got endpoints: latency-svc-2pwsv [743.595501ms]
    Jan  3 12:17:33.428: INFO: Created: latency-svc-sk9fm
    Jan  3 12:17:33.451: INFO: Got endpoints: latency-svc-59ggz [750.175256ms]
    Jan  3 12:17:33.480: INFO: Created: latency-svc-c4qm4
    Jan  3 12:17:33.501: INFO: Got endpoints: latency-svc-9zcd6 [750.808017ms]
    Jan  3 12:17:33.528: INFO: Created: latency-svc-hdc4x
    Jan  3 12:17:33.553: INFO: Got endpoints: latency-svc-s2bmt [753.661452ms]
    Jan  3 12:17:33.584: INFO: Created: latency-svc-8t5pv
    Jan  3 12:17:33.597: INFO: Got endpoints: latency-svc-nx75m [744.263206ms]
    Jan  3 12:17:33.626: INFO: Created: latency-svc-x6hvx
    Jan  3 12:17:33.651: INFO: Got endpoints: latency-svc-nfqj5 [750.699718ms]
    Jan  3 12:17:33.682: INFO: Created: latency-svc-pr6kk
    Jan  3 12:17:33.700: INFO: Got endpoints: latency-svc-s48hx [751.163931ms]
    Jan  3 12:17:33.731: INFO: Created: latency-svc-fbsbd
    Jan  3 12:17:33.751: INFO: Got endpoints: latency-svc-vb9w2 [750.474769ms]
    Jan  3 12:17:33.777: INFO: Created: latency-svc-t69lc
    Jan  3 12:17:33.801: INFO: Got endpoints: latency-svc-5sh8k [749.337989ms]
    Jan  3 12:17:33.837: INFO: Created: latency-svc-jc4tf
    Jan  3 12:17:33.849: INFO: Got endpoints: latency-svc-br68t [736.518463ms]
    Jan  3 12:17:33.877: INFO: Created: latency-svc-9t6fp
    Jan  3 12:17:33.900: INFO: Got endpoints: latency-svc-974bw [750.328486ms]
    Jan  3 12:17:33.925: INFO: Created: latency-svc-pmddq
    Jan  3 12:17:33.953: INFO: Got endpoints: latency-svc-ds4p4 [753.424785ms]
    Jan  3 12:17:33.988: INFO: Created: latency-svc-wm79r
    Jan  3 12:17:34.000: INFO: Got endpoints: latency-svc-ncxkb [750.353563ms]
    Jan  3 12:17:34.026: INFO: Created: latency-svc-kg27w
    Jan  3 12:17:34.050: INFO: Got endpoints: latency-svc-hzv4t [743.811492ms]
    Jan  3 12:17:34.079: INFO: Created: latency-svc-th7b4
    Jan  3 12:17:34.102: INFO: Got endpoints: latency-svc-bjz54 [747.933354ms]
    Jan  3 12:17:34.127: INFO: Created: latency-svc-lmv5r
    Jan  3 12:17:34.151: INFO: Got endpoints: latency-svc-sk9fm [750.769309ms]
    Jan  3 12:17:34.188: INFO: Created: latency-svc-4s5j6
    Jan  3 12:17:34.202: INFO: Got endpoints: latency-svc-c4qm4 [750.613326ms]
    Jan  3 12:17:34.227: INFO: Created: latency-svc-k5s2q
    Jan  3 12:17:34.250: INFO: Got endpoints: latency-svc-hdc4x [748.442293ms]
    Jan  3 12:17:34.279: INFO: Created: latency-svc-ldnkf
    Jan  3 12:17:34.301: INFO: Got endpoints: latency-svc-8t5pv [747.664257ms]
    Jan  3 12:17:34.330: INFO: Created: latency-svc-knv4b
    Jan  3 12:17:34.354: INFO: Got endpoints: latency-svc-x6hvx [756.485843ms]
    Jan  3 12:17:34.389: INFO: Created: latency-svc-h46sd
    Jan  3 12:17:34.399: INFO: Got endpoints: latency-svc-pr6kk [748.0105ms]
    Jan  3 12:17:34.424: INFO: Created: latency-svc-9mfpc
    Jan  3 12:17:34.450: INFO: Got endpoints: latency-svc-fbsbd [750.679781ms]
    Jan  3 12:17:34.481: INFO: Created: latency-svc-5c4cp
    Jan  3 12:17:34.500: INFO: Got endpoints: latency-svc-t69lc [748.570656ms]
    Jan  3 12:17:34.530: INFO: Created: latency-svc-lphr7
    Jan  3 12:17:34.552: INFO: Got endpoints: latency-svc-jc4tf [751.08473ms]
    Jan  3 12:17:34.592: INFO: Created: latency-svc-jcxkn
    Jan  3 12:17:34.599: INFO: Got endpoints: latency-svc-9t6fp [750.378795ms]
    Jan  3 12:17:34.629: INFO: Created: latency-svc-hmmss
    Jan  3 12:17:34.668: INFO: Got endpoints: latency-svc-pmddq [767.524173ms]
    Jan  3 12:17:34.701: INFO: Got endpoints: latency-svc-wm79r [748.739763ms]
    Jan  3 12:17:34.704: INFO: Created: latency-svc-vzgj8
    Jan  3 12:17:34.726: INFO: Created: latency-svc-l78xq
    Jan  3 12:17:34.752: INFO: Got endpoints: latency-svc-kg27w [751.81582ms]
    Jan  3 12:17:34.785: INFO: Created: latency-svc-m5gk8
    Jan  3 12:17:34.804: INFO: Got endpoints: latency-svc-th7b4 [754.018709ms]
    Jan  3 12:17:34.833: INFO: Created: latency-svc-qcs6z
    Jan  3 12:17:34.850: INFO: Got endpoints: latency-svc-lmv5r [747.99794ms]
    Jan  3 12:17:34.899: INFO: Got endpoints: latency-svc-4s5j6 [748.444157ms]
    Jan  3 12:17:34.951: INFO: Got endpoints: latency-svc-k5s2q [748.674258ms]
    Jan  3 12:17:35.000: INFO: Got endpoints: latency-svc-ldnkf [750.223781ms]
    Jan  3 12:17:35.051: INFO: Got endpoints: latency-svc-knv4b [749.05071ms]
    Jan  3 12:17:35.101: INFO: Got endpoints: latency-svc-h46sd [746.85687ms]
    Jan  3 12:17:35.153: INFO: Got endpoints: latency-svc-9mfpc [753.248629ms]
    Jan  3 12:17:35.202: INFO: Got endpoints: latency-svc-5c4cp [751.137993ms]
    Jan  3 12:17:35.265: INFO: Got endpoints: latency-svc-lphr7 [764.813359ms]
    Jan  3 12:17:35.299: INFO: Got endpoints: latency-svc-jcxkn [747.269472ms]
    Jan  3 12:17:35.358: INFO: Got endpoints: latency-svc-hmmss [759.17322ms]
    Jan  3 12:17:35.407: INFO: Got endpoints: latency-svc-vzgj8 [739.532966ms]
    Jan  3 12:17:35.450: INFO: Got endpoints: latency-svc-l78xq [747.956468ms]
    Jan  3 12:17:35.502: INFO: Got endpoints: latency-svc-m5gk8 [750.815858ms]
    Jan  3 12:17:35.552: INFO: Got endpoints: latency-svc-qcs6z [747.654132ms]
    Jan  3 12:17:35.552: INFO: Latencies: [36.628705ms 43.972898ms 47.464374ms 78.152006ms 78.896906ms 86.388088ms 90.137819ms 94.188671ms 95.114404ms 100.805107ms 101.699954ms 103.700594ms 104.39863ms 104.93891ms 109.645674ms 109.964609ms 113.919665ms 115.226768ms 115.443815ms 115.886619ms 117.451221ms 117.987409ms 118.639672ms 119.622815ms 120.368442ms 121.799346ms 122.535734ms 122.816783ms 123.158619ms 124.03001ms 125.733441ms 126.024256ms 132.061339ms 134.960363ms 143.727101ms 161.185246ms 200.104846ms 243.75319ms 282.720295ms 320.455318ms 363.668023ms 406.562505ms 447.447731ms 492.906811ms 542.737979ms 575.409913ms 623.667099ms 660.714565ms 704.416007ms 731.606572ms 736.518463ms 737.290949ms 738.15773ms 739.532966ms 739.944124ms 740.612699ms 741.093938ms 743.141467ms 743.325028ms 743.595501ms 743.711631ms 743.811492ms 744.263206ms 744.39161ms 745.698861ms 745.857197ms 746.636634ms 746.683503ms 746.835203ms 746.85687ms 746.875578ms 747.014976ms 747.122963ms 747.189322ms 747.269472ms 747.437466ms 747.478018ms 747.654132ms 747.664257ms 747.692489ms 747.725278ms 747.730071ms 747.871665ms 747.933354ms 747.956468ms 747.99794ms 748.0105ms 748.209621ms 748.276455ms 748.377453ms 748.442293ms 748.444157ms 748.463061ms 748.477391ms 748.570656ms 748.584096ms 748.591443ms 748.674258ms 748.691033ms 748.724855ms 748.739763ms 748.748357ms 748.752748ms 748.771664ms 748.773231ms 748.811689ms 748.979841ms 748.997008ms 749.05071ms 749.097816ms 749.109376ms 749.156286ms 749.16815ms 749.181712ms 749.230057ms 749.287431ms 749.297125ms 749.299122ms 749.337989ms 749.3694ms 749.452739ms 749.513772ms 749.550674ms 749.726351ms 749.938344ms 749.989193ms 750.043743ms 750.086049ms 750.175256ms 750.223781ms 750.262781ms 750.308178ms 750.328486ms 750.353563ms 750.378795ms 750.474769ms 750.507168ms 750.514589ms 750.545662ms 750.569928ms 750.613326ms 750.648135ms 750.679781ms 750.699718ms 750.757267ms 750.769309ms 750.774452ms 750.808017ms 750.815858ms 750.891701ms 750.9153ms 751.08473ms 751.137993ms 751.163931ms 751.460608ms 751.464978ms 751.505666ms 751.526843ms 751.536179ms 751.691615ms 751.711739ms 751.715092ms 751.81582ms 751.858902ms 752.00839ms 752.182521ms 752.205891ms 752.319399ms 752.892892ms 752.980952ms 752.992514ms 753.026764ms 753.043544ms 753.248629ms 753.274968ms 753.424785ms 753.438602ms 753.485349ms 753.562175ms 753.637ms 753.661452ms 753.947971ms 753.965205ms 754.018709ms 754.31389ms 754.424834ms 754.868955ms 755.626798ms 755.876862ms 756.396933ms 756.485843ms 757.123657ms 758.513425ms 759.17322ms 761.944842ms 762.419313ms 763.998836ms 764.813359ms 767.524173ms 769.403334ms]
    Jan  3 12:17:35.553: INFO: 50 %ile: 748.739763ms
    Jan  3 12:17:35.553: INFO: 90 %ile: 753.661452ms
    Jan  3 12:17:35.553: INFO: 99 %ile: 767.524173ms
    Jan  3 12:17:35.553: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:17:35.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-5080" for this suite. 01/03/24 12:17:35.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:17:35.619
Jan  3 12:17:35.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename replication-controller 01/03/24 12:17:35.623
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:17:35.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:17:35.72
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 01/03/24 12:17:35.748
Jan  3 12:17:35.775: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9827" to be "running and ready"
Jan  3 12:17:35.795: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004785ms
Jan  3 12:17:35.795: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:17:37.816: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.040720034s
Jan  3 12:17:37.816: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan  3 12:17:37.816: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/03/24 12:17:37.834
STEP: Then the orphan pod is adopted 01/03/24 12:17:37.857
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  3 12:17:37.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9827" for this suite. 01/03/24 12:17:37.909
------------------------------
• [2.316 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:17:35.619
    Jan  3 12:17:35.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename replication-controller 01/03/24 12:17:35.623
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:17:35.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:17:35.72
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/03/24 12:17:35.748
    Jan  3 12:17:35.775: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9827" to be "running and ready"
    Jan  3 12:17:35.795: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004785ms
    Jan  3 12:17:35.795: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:17:37.816: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.040720034s
    Jan  3 12:17:37.816: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan  3 12:17:37.816: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/03/24 12:17:37.834
    STEP: Then the orphan pod is adopted 01/03/24 12:17:37.857
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:17:37.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9827" for this suite. 01/03/24 12:17:37.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:17:37.938
Jan  3 12:17:37.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 12:17:37.94
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:17:37.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:17:38.028
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 01/03/24 12:17:38.058
Jan  3 12:17:38.058: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan  3 12:17:38.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 create -f -'
Jan  3 12:17:38.383: INFO: stderr: ""
Jan  3 12:17:38.383: INFO: stdout: "service/agnhost-replica created\n"
Jan  3 12:17:38.383: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan  3 12:17:38.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 create -f -'
Jan  3 12:17:38.975: INFO: stderr: ""
Jan  3 12:17:38.975: INFO: stdout: "service/agnhost-primary created\n"
Jan  3 12:17:38.975: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan  3 12:17:38.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 create -f -'
Jan  3 12:17:39.728: INFO: stderr: ""
Jan  3 12:17:39.728: INFO: stdout: "service/frontend created\n"
Jan  3 12:17:39.728: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan  3 12:17:39.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 create -f -'
Jan  3 12:17:40.944: INFO: stderr: ""
Jan  3 12:17:40.944: INFO: stdout: "deployment.apps/frontend created\n"
Jan  3 12:17:40.944: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan  3 12:17:40.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 create -f -'
Jan  3 12:17:41.592: INFO: stderr: ""
Jan  3 12:17:41.592: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan  3 12:17:41.592: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan  3 12:17:41.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 create -f -'
Jan  3 12:17:42.263: INFO: stderr: ""
Jan  3 12:17:42.263: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/03/24 12:17:42.263
Jan  3 12:17:42.263: INFO: Waiting for all frontend pods to be Running.
Jan  3 12:17:47.315: INFO: Waiting for frontend to serve content.
Jan  3 12:17:47.336: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s 

v1StatusW

   Failure-no endpoints available for service "frontend""ServiceUnavailable0 " 
Jan  3 12:17:52.459: INFO: Trying to add a new entry to the guestbook.
Jan  3 12:17:52.573: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/03/24 12:17:52.635
Jan  3 12:17:52.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 delete --grace-period=0 --force -f -'
Jan  3 12:17:52.807: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 12:17:52.807: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/03/24 12:17:52.807
Jan  3 12:17:52.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 delete --grace-period=0 --force -f -'
Jan  3 12:17:52.978: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 12:17:52.978: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/03/24 12:17:52.978
Jan  3 12:17:52.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 delete --grace-period=0 --force -f -'
Jan  3 12:17:53.155: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 12:17:53.155: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/03/24 12:17:53.156
Jan  3 12:17:53.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 delete --grace-period=0 --force -f -'
Jan  3 12:17:53.297: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 12:17:53.297: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/03/24 12:17:53.298
Jan  3 12:17:53.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 delete --grace-period=0 --force -f -'
Jan  3 12:17:53.461: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 12:17:53.461: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/03/24 12:17:53.461
Jan  3 12:17:53.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 delete --grace-period=0 --force -f -'
Jan  3 12:17:53.616: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 12:17:53.616: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 12:17:53.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3157" for this suite. 01/03/24 12:17:53.649
------------------------------
• [SLOW TEST] [15.758 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:17:37.938
    Jan  3 12:17:37.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 12:17:37.94
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:17:37.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:17:38.028
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 01/03/24 12:17:38.058
    Jan  3 12:17:38.058: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan  3 12:17:38.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 create -f -'
    Jan  3 12:17:38.383: INFO: stderr: ""
    Jan  3 12:17:38.383: INFO: stdout: "service/agnhost-replica created\n"
    Jan  3 12:17:38.383: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan  3 12:17:38.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 create -f -'
    Jan  3 12:17:38.975: INFO: stderr: ""
    Jan  3 12:17:38.975: INFO: stdout: "service/agnhost-primary created\n"
    Jan  3 12:17:38.975: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan  3 12:17:38.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 create -f -'
    Jan  3 12:17:39.728: INFO: stderr: ""
    Jan  3 12:17:39.728: INFO: stdout: "service/frontend created\n"
    Jan  3 12:17:39.728: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan  3 12:17:39.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 create -f -'
    Jan  3 12:17:40.944: INFO: stderr: ""
    Jan  3 12:17:40.944: INFO: stdout: "deployment.apps/frontend created\n"
    Jan  3 12:17:40.944: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan  3 12:17:40.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 create -f -'
    Jan  3 12:17:41.592: INFO: stderr: ""
    Jan  3 12:17:41.592: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan  3 12:17:41.592: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan  3 12:17:41.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 create -f -'
    Jan  3 12:17:42.263: INFO: stderr: ""
    Jan  3 12:17:42.263: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/03/24 12:17:42.263
    Jan  3 12:17:42.263: INFO: Waiting for all frontend pods to be Running.
    Jan  3 12:17:47.315: INFO: Waiting for frontend to serve content.
    Jan  3 12:17:47.336: INFO: Failed to get response from guestbook. err: the server is currently unable to handle the request (get services frontend), response: k8s 
    
    v1StatusW
    
       Failure-no endpoints available for service "frontend""ServiceUnavailable0 " 
    Jan  3 12:17:52.459: INFO: Trying to add a new entry to the guestbook.
    Jan  3 12:17:52.573: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/03/24 12:17:52.635
    Jan  3 12:17:52.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 delete --grace-period=0 --force -f -'
    Jan  3 12:17:52.807: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 12:17:52.807: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/03/24 12:17:52.807
    Jan  3 12:17:52.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 delete --grace-period=0 --force -f -'
    Jan  3 12:17:52.978: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 12:17:52.978: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/03/24 12:17:52.978
    Jan  3 12:17:52.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 delete --grace-period=0 --force -f -'
    Jan  3 12:17:53.155: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 12:17:53.155: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/03/24 12:17:53.156
    Jan  3 12:17:53.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 delete --grace-period=0 --force -f -'
    Jan  3 12:17:53.297: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 12:17:53.297: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/03/24 12:17:53.298
    Jan  3 12:17:53.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 delete --grace-period=0 --force -f -'
    Jan  3 12:17:53.461: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 12:17:53.461: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/03/24 12:17:53.461
    Jan  3 12:17:53.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3157 delete --grace-period=0 --force -f -'
    Jan  3 12:17:53.616: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 12:17:53.616: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:17:53.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3157" for this suite. 01/03/24 12:17:53.649
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:17:53.696
Jan  3 12:17:53.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename cronjob 01/03/24 12:17:53.698
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:17:53.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:17:53.78
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/03/24 12:17:53.808
STEP: Ensuring a job is scheduled 01/03/24 12:17:53.831
STEP: Ensuring exactly one is scheduled 01/03/24 12:18:01.853
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/03/24 12:18:01.872
STEP: Ensuring the job is replaced with a new one 01/03/24 12:18:01.89
STEP: Removing cronjob 01/03/24 12:19:01.908
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  3 12:19:01.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1437" for this suite. 01/03/24 12:19:01.966
------------------------------
• [SLOW TEST] [68.299 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:17:53.696
    Jan  3 12:17:53.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename cronjob 01/03/24 12:17:53.698
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:17:53.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:17:53.78
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/03/24 12:17:53.808
    STEP: Ensuring a job is scheduled 01/03/24 12:17:53.831
    STEP: Ensuring exactly one is scheduled 01/03/24 12:18:01.853
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/03/24 12:18:01.872
    STEP: Ensuring the job is replaced with a new one 01/03/24 12:18:01.89
    STEP: Removing cronjob 01/03/24 12:19:01.908
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:19:01.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1437" for this suite. 01/03/24 12:19:01.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:19:02.002
Jan  3 12:19:02.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename runtimeclass 01/03/24 12:19:02.004
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:02.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:02.091
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/03/24 12:19:02.121
STEP: getting /apis/node.k8s.io 01/03/24 12:19:02.148
STEP: getting /apis/node.k8s.io/v1 01/03/24 12:19:02.162
STEP: creating 01/03/24 12:19:02.176
STEP: watching 01/03/24 12:19:02.253
Jan  3 12:19:02.253: INFO: starting watch
STEP: getting 01/03/24 12:19:02.285
STEP: listing 01/03/24 12:19:02.303
STEP: patching 01/03/24 12:19:02.324
STEP: updating 01/03/24 12:19:02.344
Jan  3 12:19:02.364: INFO: waiting for watch events with expected annotations
STEP: deleting 01/03/24 12:19:02.364
STEP: deleting a collection 01/03/24 12:19:02.425
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  3 12:19:02.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2663" for this suite. 01/03/24 12:19:02.505
------------------------------
• [0.540 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:19:02.002
    Jan  3 12:19:02.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename runtimeclass 01/03/24 12:19:02.004
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:02.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:02.091
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/03/24 12:19:02.121
    STEP: getting /apis/node.k8s.io 01/03/24 12:19:02.148
    STEP: getting /apis/node.k8s.io/v1 01/03/24 12:19:02.162
    STEP: creating 01/03/24 12:19:02.176
    STEP: watching 01/03/24 12:19:02.253
    Jan  3 12:19:02.253: INFO: starting watch
    STEP: getting 01/03/24 12:19:02.285
    STEP: listing 01/03/24 12:19:02.303
    STEP: patching 01/03/24 12:19:02.324
    STEP: updating 01/03/24 12:19:02.344
    Jan  3 12:19:02.364: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/03/24 12:19:02.364
    STEP: deleting a collection 01/03/24 12:19:02.425
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:19:02.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2663" for this suite. 01/03/24 12:19:02.505
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:19:02.543
Jan  3 12:19:02.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename runtimeclass 01/03/24 12:19:02.546
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:02.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:02.642
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan  3 12:19:02.717: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2377 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  3 12:19:02.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2377" for this suite. 01/03/24 12:19:02.792
------------------------------
• [0.275 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:19:02.543
    Jan  3 12:19:02.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename runtimeclass 01/03/24 12:19:02.546
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:02.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:02.642
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan  3 12:19:02.717: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2377 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:19:02.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2377" for this suite. 01/03/24 12:19:02.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:19:02.826
Jan  3 12:19:02.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename custom-resource-definition 01/03/24 12:19:02.827
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:02.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:02.919
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan  3 12:19:02.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:19:10.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2036" for this suite. 01/03/24 12:19:10.097
------------------------------
• [SLOW TEST] [7.328 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:19:02.826
    Jan  3 12:19:02.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename custom-resource-definition 01/03/24 12:19:02.827
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:02.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:02.919
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan  3 12:19:02.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:19:10.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2036" for this suite. 01/03/24 12:19:10.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:19:10.158
Jan  3 12:19:10.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 12:19:10.159
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:10.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:10.262
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 01/03/24 12:19:10.29
Jan  3 12:19:10.290: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3810 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/03/24 12:19:10.362
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 12:19:10.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3810" for this suite. 01/03/24 12:19:10.45
------------------------------
• [0.316 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:19:10.158
    Jan  3 12:19:10.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 12:19:10.159
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:10.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:10.262
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 01/03/24 12:19:10.29
    Jan  3 12:19:10.290: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-3810 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/03/24 12:19:10.362
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:19:10.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3810" for this suite. 01/03/24 12:19:10.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:19:10.48
Jan  3 12:19:10.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 12:19:10.482
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:10.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:10.572
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 01/03/24 12:19:10.603
Jan  3 12:19:10.635: INFO: Waiting up to 5m0s for pod "pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a" in namespace "emptydir-4395" to be "Succeeded or Failed"
Jan  3 12:19:10.654: INFO: Pod "pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.926732ms
Jan  3 12:19:12.678: INFO: Pod "pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042499048s
Jan  3 12:19:14.674: INFO: Pod "pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038719425s
STEP: Saw pod success 01/03/24 12:19:14.674
Jan  3 12:19:14.674: INFO: Pod "pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a" satisfied condition "Succeeded or Failed"
Jan  3 12:19:14.694: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a container test-container: <nil>
STEP: delete the pod 01/03/24 12:19:14.849
Jan  3 12:19:14.899: INFO: Waiting for pod pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a to disappear
Jan  3 12:19:14.919: INFO: Pod pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:19:14.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4395" for this suite. 01/03/24 12:19:14.951
------------------------------
• [4.497 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:19:10.48
    Jan  3 12:19:10.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 12:19:10.482
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:10.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:10.572
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/03/24 12:19:10.603
    Jan  3 12:19:10.635: INFO: Waiting up to 5m0s for pod "pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a" in namespace "emptydir-4395" to be "Succeeded or Failed"
    Jan  3 12:19:10.654: INFO: Pod "pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.926732ms
    Jan  3 12:19:12.678: INFO: Pod "pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042499048s
    Jan  3 12:19:14.674: INFO: Pod "pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038719425s
    STEP: Saw pod success 01/03/24 12:19:14.674
    Jan  3 12:19:14.674: INFO: Pod "pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a" satisfied condition "Succeeded or Failed"
    Jan  3 12:19:14.694: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a container test-container: <nil>
    STEP: delete the pod 01/03/24 12:19:14.849
    Jan  3 12:19:14.899: INFO: Waiting for pod pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a to disappear
    Jan  3 12:19:14.919: INFO: Pod pod-a27ac65d-9d7b-4fd5-bf87-864e5e4e765a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:19:14.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4395" for this suite. 01/03/24 12:19:14.951
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:19:14.979
Jan  3 12:19:14.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 12:19:14.983
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:15.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:15.07
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 01/03/24 12:19:15.098
Jan  3 12:19:15.134: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c" in namespace "downward-api-4022" to be "Succeeded or Failed"
Jan  3 12:19:15.154: INFO: Pod "downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.032866ms
Jan  3 12:19:17.179: INFO: Pod "downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c": Phase="Running", Reason="", readiness=true. Elapsed: 2.045213789s
Jan  3 12:19:19.176: INFO: Pod "downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c": Phase="Running", Reason="", readiness=false. Elapsed: 4.042438404s
Jan  3 12:19:21.174: INFO: Pod "downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040425456s
STEP: Saw pod success 01/03/24 12:19:21.174
Jan  3 12:19:21.175: INFO: Pod "downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c" satisfied condition "Succeeded or Failed"
Jan  3 12:19:21.194: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c container client-container: <nil>
STEP: delete the pod 01/03/24 12:19:21.233
Jan  3 12:19:21.274: INFO: Waiting for pod downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c to disappear
Jan  3 12:19:21.293: INFO: Pod downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  3 12:19:21.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4022" for this suite. 01/03/24 12:19:21.324
------------------------------
• [SLOW TEST] [6.371 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:19:14.979
    Jan  3 12:19:14.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 12:19:14.983
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:15.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:15.07
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 01/03/24 12:19:15.098
    Jan  3 12:19:15.134: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c" in namespace "downward-api-4022" to be "Succeeded or Failed"
    Jan  3 12:19:15.154: INFO: Pod "downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.032866ms
    Jan  3 12:19:17.179: INFO: Pod "downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c": Phase="Running", Reason="", readiness=true. Elapsed: 2.045213789s
    Jan  3 12:19:19.176: INFO: Pod "downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c": Phase="Running", Reason="", readiness=false. Elapsed: 4.042438404s
    Jan  3 12:19:21.174: INFO: Pod "downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040425456s
    STEP: Saw pod success 01/03/24 12:19:21.174
    Jan  3 12:19:21.175: INFO: Pod "downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c" satisfied condition "Succeeded or Failed"
    Jan  3 12:19:21.194: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c container client-container: <nil>
    STEP: delete the pod 01/03/24 12:19:21.233
    Jan  3 12:19:21.274: INFO: Waiting for pod downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c to disappear
    Jan  3 12:19:21.293: INFO: Pod downwardapi-volume-ab841460-9689-4d74-933a-4ab7ca614a8c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:19:21.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4022" for this suite. 01/03/24 12:19:21.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:19:21.353
Jan  3 12:19:21.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename proxy 01/03/24 12:19:21.355
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:21.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:21.444
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan  3 12:19:21.472: INFO: Creating pod...
Jan  3 12:19:21.497: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6553" to be "running"
Jan  3 12:19:21.527: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 30.435575ms
Jan  3 12:19:23.548: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.050954317s
Jan  3 12:19:23.548: INFO: Pod "agnhost" satisfied condition "running"
Jan  3 12:19:23.548: INFO: Creating service...
Jan  3 12:19:23.579: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/pods/agnhost/proxy/some/path/with/DELETE
Jan  3 12:19:23.674: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  3 12:19:23.674: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/pods/agnhost/proxy/some/path/with/GET
Jan  3 12:19:23.707: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan  3 12:19:23.707: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/pods/agnhost/proxy/some/path/with/HEAD
Jan  3 12:19:23.738: INFO: http.Client request:HEAD | StatusCode:200
Jan  3 12:19:23.738: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/pods/agnhost/proxy/some/path/with/OPTIONS
Jan  3 12:19:23.772: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  3 12:19:23.772: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/pods/agnhost/proxy/some/path/with/PATCH
Jan  3 12:19:23.811: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  3 12:19:23.811: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/pods/agnhost/proxy/some/path/with/POST
Jan  3 12:19:23.846: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  3 12:19:23.846: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/pods/agnhost/proxy/some/path/with/PUT
Jan  3 12:19:23.881: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  3 12:19:23.881: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/services/test-service/proxy/some/path/with/DELETE
Jan  3 12:19:23.922: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  3 12:19:23.922: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/services/test-service/proxy/some/path/with/GET
Jan  3 12:19:23.961: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan  3 12:19:23.961: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/services/test-service/proxy/some/path/with/HEAD
Jan  3 12:19:23.999: INFO: http.Client request:HEAD | StatusCode:200
Jan  3 12:19:23.999: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/services/test-service/proxy/some/path/with/OPTIONS
Jan  3 12:19:24.039: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  3 12:19:24.039: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/services/test-service/proxy/some/path/with/PATCH
Jan  3 12:19:24.077: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  3 12:19:24.077: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/services/test-service/proxy/some/path/with/POST
Jan  3 12:19:24.115: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  3 12:19:24.115: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/services/test-service/proxy/some/path/with/PUT
Jan  3 12:19:24.153: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan  3 12:19:24.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6553" for this suite. 01/03/24 12:19:24.188
------------------------------
• [2.861 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:19:21.353
    Jan  3 12:19:21.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename proxy 01/03/24 12:19:21.355
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:21.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:21.444
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan  3 12:19:21.472: INFO: Creating pod...
    Jan  3 12:19:21.497: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6553" to be "running"
    Jan  3 12:19:21.527: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 30.435575ms
    Jan  3 12:19:23.548: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.050954317s
    Jan  3 12:19:23.548: INFO: Pod "agnhost" satisfied condition "running"
    Jan  3 12:19:23.548: INFO: Creating service...
    Jan  3 12:19:23.579: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/pods/agnhost/proxy/some/path/with/DELETE
    Jan  3 12:19:23.674: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  3 12:19:23.674: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/pods/agnhost/proxy/some/path/with/GET
    Jan  3 12:19:23.707: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan  3 12:19:23.707: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/pods/agnhost/proxy/some/path/with/HEAD
    Jan  3 12:19:23.738: INFO: http.Client request:HEAD | StatusCode:200
    Jan  3 12:19:23.738: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan  3 12:19:23.772: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  3 12:19:23.772: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/pods/agnhost/proxy/some/path/with/PATCH
    Jan  3 12:19:23.811: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  3 12:19:23.811: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/pods/agnhost/proxy/some/path/with/POST
    Jan  3 12:19:23.846: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  3 12:19:23.846: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/pods/agnhost/proxy/some/path/with/PUT
    Jan  3 12:19:23.881: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  3 12:19:23.881: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/services/test-service/proxy/some/path/with/DELETE
    Jan  3 12:19:23.922: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  3 12:19:23.922: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/services/test-service/proxy/some/path/with/GET
    Jan  3 12:19:23.961: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan  3 12:19:23.961: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/services/test-service/proxy/some/path/with/HEAD
    Jan  3 12:19:23.999: INFO: http.Client request:HEAD | StatusCode:200
    Jan  3 12:19:23.999: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/services/test-service/proxy/some/path/with/OPTIONS
    Jan  3 12:19:24.039: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  3 12:19:24.039: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/services/test-service/proxy/some/path/with/PATCH
    Jan  3 12:19:24.077: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  3 12:19:24.077: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/services/test-service/proxy/some/path/with/POST
    Jan  3 12:19:24.115: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  3 12:19:24.115: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6553/services/test-service/proxy/some/path/with/PUT
    Jan  3 12:19:24.153: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:19:24.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6553" for this suite. 01/03/24 12:19:24.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:19:24.221
Jan  3 12:19:24.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename resourcequota 01/03/24 12:19:24.224
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:24.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:24.311
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 01/03/24 12:19:24.341
STEP: Creating a ResourceQuota 01/03/24 12:19:29.365
STEP: Ensuring resource quota status is calculated 01/03/24 12:19:29.384
STEP: Creating a ReplicationController 01/03/24 12:19:31.405
STEP: Ensuring resource quota status captures replication controller creation 01/03/24 12:19:31.435
STEP: Deleting a ReplicationController 01/03/24 12:19:33.458
STEP: Ensuring resource quota status released usage 01/03/24 12:19:33.485
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  3 12:19:35.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-233" for this suite. 01/03/24 12:19:35.537
------------------------------
• [SLOW TEST] [11.342 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:19:24.221
    Jan  3 12:19:24.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename resourcequota 01/03/24 12:19:24.224
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:24.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:24.311
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 01/03/24 12:19:24.341
    STEP: Creating a ResourceQuota 01/03/24 12:19:29.365
    STEP: Ensuring resource quota status is calculated 01/03/24 12:19:29.384
    STEP: Creating a ReplicationController 01/03/24 12:19:31.405
    STEP: Ensuring resource quota status captures replication controller creation 01/03/24 12:19:31.435
    STEP: Deleting a ReplicationController 01/03/24 12:19:33.458
    STEP: Ensuring resource quota status released usage 01/03/24 12:19:33.485
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:19:35.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-233" for this suite. 01/03/24 12:19:35.537
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:19:35.568
Jan  3 12:19:35.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename statefulset 01/03/24 12:19:35.57
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:35.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:35.663
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9571 01/03/24 12:19:35.69
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 01/03/24 12:19:35.709
STEP: Creating stateful set ss in namespace statefulset-9571 01/03/24 12:19:35.727
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9571 01/03/24 12:19:35.748
Jan  3 12:19:35.767: INFO: Found 0 stateful pods, waiting for 1
Jan  3 12:19:45.789: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/03/24 12:19:45.789
Jan  3 12:19:45.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 12:19:46.275: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 12:19:46.275: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 12:19:46.275: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 12:19:46.297: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan  3 12:19:56.318: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 12:19:56.318: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 12:19:56.400: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999362s
Jan  3 12:19:57.423: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.980146613s
Jan  3 12:19:58.445: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.957010208s
Jan  3 12:19:59.467: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.936137752s
Jan  3 12:20:00.486: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.914252543s
Jan  3 12:20:01.512: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.894259115s
Jan  3 12:20:02.533: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.86877104s
Jan  3 12:20:03.555: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.848185517s
Jan  3 12:20:04.576: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.825031369s
Jan  3 12:20:05.597: INFO: Verifying statefulset ss doesn't scale past 1 for another 804.381094ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9571 01/03/24 12:20:06.597
Jan  3 12:20:06.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 12:20:07.087: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 12:20:07.087: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 12:20:07.087: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 12:20:07.107: INFO: Found 1 stateful pods, waiting for 3
Jan  3 12:20:17.129: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 12:20:17.129: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 12:20:17.129: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/03/24 12:20:17.129
STEP: Scale down will halt with unhealthy stateful pod 01/03/24 12:20:17.13
Jan  3 12:20:17.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 12:20:17.644: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 12:20:17.644: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 12:20:17.644: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 12:20:17.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 12:20:18.142: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 12:20:18.142: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 12:20:18.142: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 12:20:18.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 12:20:18.622: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 12:20:18.622: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 12:20:18.622: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 12:20:18.622: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 12:20:18.641: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan  3 12:20:28.682: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 12:20:28.682: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 12:20:28.682: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 12:20:28.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999467s
Jan  3 12:20:29.785: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.960403107s
Jan  3 12:20:30.808: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.940176597s
Jan  3 12:20:31.828: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.917117905s
Jan  3 12:20:32.852: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.896601246s
Jan  3 12:20:33.872: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.87253069s
Jan  3 12:20:34.892: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.852214698s
Jan  3 12:20:35.915: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.832648841s
Jan  3 12:20:36.935: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.810319193s
Jan  3 12:20:37.956: INFO: Verifying statefulset ss doesn't scale past 3 for another 790.013505ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9571 01/03/24 12:20:38.956
Jan  3 12:20:38.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 12:20:39.418: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 12:20:39.418: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 12:20:39.418: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 12:20:39.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 12:20:39.863: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 12:20:39.863: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 12:20:39.863: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 12:20:39.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 12:20:40.420: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 12:20:40.420: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 12:20:40.420: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 12:20:40.420: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/03/24 12:20:50.556
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  3 12:20:50.556: INFO: Deleting all statefulset in ns statefulset-9571
Jan  3 12:20:50.575: INFO: Scaling statefulset ss to 0
Jan  3 12:20:50.666: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 12:20:50.685: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  3 12:20:50.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9571" for this suite. 01/03/24 12:20:50.778
------------------------------
• [SLOW TEST] [75.238 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:19:35.568
    Jan  3 12:19:35.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename statefulset 01/03/24 12:19:35.57
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:19:35.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:19:35.663
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9571 01/03/24 12:19:35.69
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/03/24 12:19:35.709
    STEP: Creating stateful set ss in namespace statefulset-9571 01/03/24 12:19:35.727
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9571 01/03/24 12:19:35.748
    Jan  3 12:19:35.767: INFO: Found 0 stateful pods, waiting for 1
    Jan  3 12:19:45.789: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/03/24 12:19:45.789
    Jan  3 12:19:45.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 12:19:46.275: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 12:19:46.275: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 12:19:46.275: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 12:19:46.297: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan  3 12:19:56.318: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 12:19:56.318: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 12:19:56.400: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999362s
    Jan  3 12:19:57.423: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.980146613s
    Jan  3 12:19:58.445: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.957010208s
    Jan  3 12:19:59.467: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.936137752s
    Jan  3 12:20:00.486: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.914252543s
    Jan  3 12:20:01.512: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.894259115s
    Jan  3 12:20:02.533: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.86877104s
    Jan  3 12:20:03.555: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.848185517s
    Jan  3 12:20:04.576: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.825031369s
    Jan  3 12:20:05.597: INFO: Verifying statefulset ss doesn't scale past 1 for another 804.381094ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9571 01/03/24 12:20:06.597
    Jan  3 12:20:06.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 12:20:07.087: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  3 12:20:07.087: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 12:20:07.087: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 12:20:07.107: INFO: Found 1 stateful pods, waiting for 3
    Jan  3 12:20:17.129: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 12:20:17.129: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 12:20:17.129: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/03/24 12:20:17.129
    STEP: Scale down will halt with unhealthy stateful pod 01/03/24 12:20:17.13
    Jan  3 12:20:17.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 12:20:17.644: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 12:20:17.644: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 12:20:17.644: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 12:20:17.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 12:20:18.142: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 12:20:18.142: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 12:20:18.142: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 12:20:18.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 12:20:18.622: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 12:20:18.622: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 12:20:18.622: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 12:20:18.622: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 12:20:18.641: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan  3 12:20:28.682: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 12:20:28.682: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 12:20:28.682: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 12:20:28.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999467s
    Jan  3 12:20:29.785: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.960403107s
    Jan  3 12:20:30.808: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.940176597s
    Jan  3 12:20:31.828: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.917117905s
    Jan  3 12:20:32.852: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.896601246s
    Jan  3 12:20:33.872: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.87253069s
    Jan  3 12:20:34.892: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.852214698s
    Jan  3 12:20:35.915: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.832648841s
    Jan  3 12:20:36.935: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.810319193s
    Jan  3 12:20:37.956: INFO: Verifying statefulset ss doesn't scale past 3 for another 790.013505ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9571 01/03/24 12:20:38.956
    Jan  3 12:20:38.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 12:20:39.418: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  3 12:20:39.418: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 12:20:39.418: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 12:20:39.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 12:20:39.863: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  3 12:20:39.863: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 12:20:39.863: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 12:20:39.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-9571 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 12:20:40.420: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  3 12:20:40.420: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 12:20:40.420: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 12:20:40.420: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/03/24 12:20:50.556
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  3 12:20:50.556: INFO: Deleting all statefulset in ns statefulset-9571
    Jan  3 12:20:50.575: INFO: Scaling statefulset ss to 0
    Jan  3 12:20:50.666: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 12:20:50.685: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:20:50.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9571" for this suite. 01/03/24 12:20:50.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:20:50.813
Jan  3 12:20:50.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename deployment 01/03/24 12:20:50.814
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:20:50.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:20:50.902
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan  3 12:20:50.930: INFO: Creating simple deployment test-new-deployment
Jan  3 12:20:50.988: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 01/03/24 12:20:53.076
STEP: updating a scale subresource 01/03/24 12:20:53.092
STEP: verifying the deployment Spec.Replicas was modified 01/03/24 12:20:53.111
STEP: Patch a scale subresource 01/03/24 12:20:53.128
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 12:20:53.194: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-5948  6d1fb499-c491-43e3-b55d-6ebee80c0410 33981005462 3 2024-01-03 12:20:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2024-01-03 12:20:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:20:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005b37c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:3,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2024-01-03 12:20:52 +0000 UTC,LastTransitionTime:2024-01-03 12:20:50 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-03 12:20:53 +0000 UTC,LastTransitionTime:2024-01-03 12:20:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  3 12:20:53.215: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-5948  1a8d0c71-36e3-4d91-bb8e-008740cf0fdc 33981005470 3 2024-01-03 12:20:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 6d1fb499-c491-43e3-b55d-6ebee80c0410 0xc0041415c7 0xc0041415c8}] [] [{kube-controller-manager Update apps/v1 2024-01-03 12:20:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d1fb499-c491-43e3-b55d-6ebee80c0410\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:20:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004141658 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  3 12:20:53.233: INFO: Pod "test-new-deployment-7f5969cbc7-8ng7n" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-8ng7n test-new-deployment-7f5969cbc7- deployment-5948  5d3499b3-213e-4c45-a598-dd68dc5012dd 33981005471 0 2024-01-03 12:20:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 1a8d0c71-36e3-4d91-bb8e-008740cf0fdc 0xc00301e007 0xc00301e008}] [] [{kube-controller-manager Update v1 2024-01-03 12:20:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a8d0c71-36e3-4d91-bb8e-008740cf0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rzq7w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rzq7w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:20:53.234: INFO: Pod "test-new-deployment-7f5969cbc7-8v2zr" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-8v2zr test-new-deployment-7f5969cbc7- deployment-5948  ec9771b2-7bdb-483b-84ba-e25f2e5f8f1c 33981005396 0 2024-01-03 12:20:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:fc3ec8ad2220117e263553bf0040267576794ddfe9d0ee8245a8d590cc17425f cni.projectcalico.org/podIP:10.221.146.115/32 cni.projectcalico.org/podIPs:10.221.146.115/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 1a8d0c71-36e3-4d91-bb8e-008740cf0fdc 0xc00301e167 0xc00301e168}] [] [{kube-controller-manager Update v1 2024-01-03 12:20:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a8d0c71-36e3-4d91-bb8e-008740cf0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:20:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkz4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkz4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.115,StartTime:2024-01-03 12:20:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:20:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://31e923d4311999b87cd0b7dc50090675e6e2a859bc0a69db79e4775c3f51afe7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:20:53.234: INFO: Pod "test-new-deployment-7f5969cbc7-9vbcr" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-9vbcr test-new-deployment-7f5969cbc7- deployment-5948  619eb86e-1395-4a88-ba33-99945bde33e0 33981005469 0 2024-01-03 12:20:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 1a8d0c71-36e3-4d91-bb8e-008740cf0fdc 0xc00301e367 0xc00301e368}] [] [{kube-controller-manager Update v1 2024-01-03 12:20:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a8d0c71-36e3-4d91-bb8e-008740cf0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:20:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cpskn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cpskn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:20:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:20:53.235: INFO: Pod "test-new-deployment-7f5969cbc7-q5g5j" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-q5g5j test-new-deployment-7f5969cbc7- deployment-5948  cf27288c-b150-440d-8f05-b5a7f2f269ef 33981005466 0 2024-01-03 12:20:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 1a8d0c71-36e3-4d91-bb8e-008740cf0fdc 0xc00301e527 0xc00301e528}] [] [{kube-controller-manager Update v1 2024-01-03 12:20:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a8d0c71-36e3-4d91-bb8e-008740cf0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-phhhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-phhhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  3 12:20:53.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5948" for this suite. 01/03/24 12:20:53.264
------------------------------
• [2.477 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:20:50.813
    Jan  3 12:20:50.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename deployment 01/03/24 12:20:50.814
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:20:50.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:20:50.902
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan  3 12:20:50.930: INFO: Creating simple deployment test-new-deployment
    Jan  3 12:20:50.988: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 01/03/24 12:20:53.076
    STEP: updating a scale subresource 01/03/24 12:20:53.092
    STEP: verifying the deployment Spec.Replicas was modified 01/03/24 12:20:53.111
    STEP: Patch a scale subresource 01/03/24 12:20:53.128
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 12:20:53.194: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-5948  6d1fb499-c491-43e3-b55d-6ebee80c0410 33981005462 3 2024-01-03 12:20:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2024-01-03 12:20:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:20:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005b37c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:3,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2024-01-03 12:20:52 +0000 UTC,LastTransitionTime:2024-01-03 12:20:50 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-03 12:20:53 +0000 UTC,LastTransitionTime:2024-01-03 12:20:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  3 12:20:53.215: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-5948  1a8d0c71-36e3-4d91-bb8e-008740cf0fdc 33981005470 3 2024-01-03 12:20:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 6d1fb499-c491-43e3-b55d-6ebee80c0410 0xc0041415c7 0xc0041415c8}] [] [{kube-controller-manager Update apps/v1 2024-01-03 12:20:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d1fb499-c491-43e3-b55d-6ebee80c0410\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:20:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004141658 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 12:20:53.233: INFO: Pod "test-new-deployment-7f5969cbc7-8ng7n" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-8ng7n test-new-deployment-7f5969cbc7- deployment-5948  5d3499b3-213e-4c45-a598-dd68dc5012dd 33981005471 0 2024-01-03 12:20:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 1a8d0c71-36e3-4d91-bb8e-008740cf0fdc 0xc00301e007 0xc00301e008}] [] [{kube-controller-manager Update v1 2024-01-03 12:20:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a8d0c71-36e3-4d91-bb8e-008740cf0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rzq7w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rzq7w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:20:53.234: INFO: Pod "test-new-deployment-7f5969cbc7-8v2zr" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-8v2zr test-new-deployment-7f5969cbc7- deployment-5948  ec9771b2-7bdb-483b-84ba-e25f2e5f8f1c 33981005396 0 2024-01-03 12:20:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:fc3ec8ad2220117e263553bf0040267576794ddfe9d0ee8245a8d590cc17425f cni.projectcalico.org/podIP:10.221.146.115/32 cni.projectcalico.org/podIPs:10.221.146.115/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 1a8d0c71-36e3-4d91-bb8e-008740cf0fdc 0xc00301e167 0xc00301e168}] [] [{kube-controller-manager Update v1 2024-01-03 12:20:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a8d0c71-36e3-4d91-bb8e-008740cf0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:20:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkz4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkz4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.115,StartTime:2024-01-03 12:20:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:20:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://31e923d4311999b87cd0b7dc50090675e6e2a859bc0a69db79e4775c3f51afe7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:20:53.234: INFO: Pod "test-new-deployment-7f5969cbc7-9vbcr" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-9vbcr test-new-deployment-7f5969cbc7- deployment-5948  619eb86e-1395-4a88-ba33-99945bde33e0 33981005469 0 2024-01-03 12:20:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 1a8d0c71-36e3-4d91-bb8e-008740cf0fdc 0xc00301e367 0xc00301e368}] [] [{kube-controller-manager Update v1 2024-01-03 12:20:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a8d0c71-36e3-4d91-bb8e-008740cf0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:20:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cpskn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cpskn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:20:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:20:53.235: INFO: Pod "test-new-deployment-7f5969cbc7-q5g5j" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-q5g5j test-new-deployment-7f5969cbc7- deployment-5948  cf27288c-b150-440d-8f05-b5a7f2f269ef 33981005466 0 2024-01-03 12:20:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 1a8d0c71-36e3-4d91-bb8e-008740cf0fdc 0xc00301e527 0xc00301e528}] [] [{kube-controller-manager Update v1 2024-01-03 12:20:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a8d0c71-36e3-4d91-bb8e-008740cf0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-phhhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-phhhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:20:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:20:53.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5948" for this suite. 01/03/24 12:20:53.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:20:53.297
Jan  3 12:20:53.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/24 12:20:53.299
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:20:53.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:20:53.388
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/03/24 12:20:53.436
Jan  3 12:20:53.464: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5637" to be "running and ready"
Jan  3 12:20:53.481: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 17.498731ms
Jan  3 12:20:53.482: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:20:55.503: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.039394977s
Jan  3 12:20:55.503: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  3 12:20:55.503: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 01/03/24 12:20:55.522
Jan  3 12:20:55.544: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5637" to be "running and ready"
Jan  3 12:20:55.564: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 19.872304ms
Jan  3 12:20:55.564: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:20:57.584: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.039871074s
Jan  3 12:20:57.584: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan  3 12:20:57.584: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/03/24 12:20:57.602
STEP: delete the pod with lifecycle hook 01/03/24 12:20:57.831
Jan  3 12:20:57.859: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  3 12:20:57.877: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  3 12:20:59.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  3 12:20:59.899: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  3 12:21:01.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  3 12:21:01.897: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan  3 12:21:01.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5637" for this suite. 01/03/24 12:21:01.931
------------------------------
• [SLOW TEST] [8.660 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:20:53.297
    Jan  3 12:20:53.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/24 12:20:53.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:20:53.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:20:53.388
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/03/24 12:20:53.436
    Jan  3 12:20:53.464: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5637" to be "running and ready"
    Jan  3 12:20:53.481: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 17.498731ms
    Jan  3 12:20:53.482: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:20:55.503: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.039394977s
    Jan  3 12:20:55.503: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  3 12:20:55.503: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 01/03/24 12:20:55.522
    Jan  3 12:20:55.544: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5637" to be "running and ready"
    Jan  3 12:20:55.564: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 19.872304ms
    Jan  3 12:20:55.564: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:20:57.584: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.039871074s
    Jan  3 12:20:57.584: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan  3 12:20:57.584: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/03/24 12:20:57.602
    STEP: delete the pod with lifecycle hook 01/03/24 12:20:57.831
    Jan  3 12:20:57.859: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  3 12:20:57.877: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan  3 12:20:59.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  3 12:20:59.899: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan  3 12:21:01.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  3 12:21:01.897: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:21:01.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5637" for this suite. 01/03/24 12:21:01.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:21:01.959
Jan  3 12:21:01.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir-wrapper 01/03/24 12:21:01.962
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:02.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:02.042
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan  3 12:21:02.144: INFO: Waiting up to 5m0s for pod "pod-secrets-f5d2de0d-ba3d-4ab9-9069-e2f9c16bd024" in namespace "emptydir-wrapper-8919" to be "running and ready"
Jan  3 12:21:02.162: INFO: Pod "pod-secrets-f5d2de0d-ba3d-4ab9-9069-e2f9c16bd024": Phase="Pending", Reason="", readiness=false. Elapsed: 17.809217ms
Jan  3 12:21:02.162: INFO: The phase of Pod pod-secrets-f5d2de0d-ba3d-4ab9-9069-e2f9c16bd024 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:21:04.190: INFO: Pod "pod-secrets-f5d2de0d-ba3d-4ab9-9069-e2f9c16bd024": Phase="Running", Reason="", readiness=true. Elapsed: 2.046655273s
Jan  3 12:21:04.190: INFO: The phase of Pod pod-secrets-f5d2de0d-ba3d-4ab9-9069-e2f9c16bd024 is Running (Ready = true)
Jan  3 12:21:04.190: INFO: Pod "pod-secrets-f5d2de0d-ba3d-4ab9-9069-e2f9c16bd024" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/03/24 12:21:04.21
STEP: Cleaning up the configmap 01/03/24 12:21:04.232
STEP: Cleaning up the pod 01/03/24 12:21:04.254
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:21:04.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8919" for this suite. 01/03/24 12:21:04.325
------------------------------
• [2.391 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:21:01.959
    Jan  3 12:21:01.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir-wrapper 01/03/24 12:21:01.962
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:02.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:02.042
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan  3 12:21:02.144: INFO: Waiting up to 5m0s for pod "pod-secrets-f5d2de0d-ba3d-4ab9-9069-e2f9c16bd024" in namespace "emptydir-wrapper-8919" to be "running and ready"
    Jan  3 12:21:02.162: INFO: Pod "pod-secrets-f5d2de0d-ba3d-4ab9-9069-e2f9c16bd024": Phase="Pending", Reason="", readiness=false. Elapsed: 17.809217ms
    Jan  3 12:21:02.162: INFO: The phase of Pod pod-secrets-f5d2de0d-ba3d-4ab9-9069-e2f9c16bd024 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:21:04.190: INFO: Pod "pod-secrets-f5d2de0d-ba3d-4ab9-9069-e2f9c16bd024": Phase="Running", Reason="", readiness=true. Elapsed: 2.046655273s
    Jan  3 12:21:04.190: INFO: The phase of Pod pod-secrets-f5d2de0d-ba3d-4ab9-9069-e2f9c16bd024 is Running (Ready = true)
    Jan  3 12:21:04.190: INFO: Pod "pod-secrets-f5d2de0d-ba3d-4ab9-9069-e2f9c16bd024" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/03/24 12:21:04.21
    STEP: Cleaning up the configmap 01/03/24 12:21:04.232
    STEP: Cleaning up the pod 01/03/24 12:21:04.254
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:21:04.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8919" for this suite. 01/03/24 12:21:04.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:21:04.353
Jan  3 12:21:04.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pods 01/03/24 12:21:04.356
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:04.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:04.442
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jan  3 12:21:04.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: creating the pod 01/03/24 12:21:04.472
STEP: submitting the pod to kubernetes 01/03/24 12:21:04.473
Jan  3 12:21:04.502: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-a47cd746-85ee-4020-8f3c-f35a7bad69bd" in namespace "pods-2222" to be "running and ready"
Jan  3 12:21:04.520: INFO: Pod "pod-logs-websocket-a47cd746-85ee-4020-8f3c-f35a7bad69bd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.157243ms
Jan  3 12:21:04.520: INFO: The phase of Pod pod-logs-websocket-a47cd746-85ee-4020-8f3c-f35a7bad69bd is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:21:06.541: INFO: Pod "pod-logs-websocket-a47cd746-85ee-4020-8f3c-f35a7bad69bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.039104219s
Jan  3 12:21:06.541: INFO: The phase of Pod pod-logs-websocket-a47cd746-85ee-4020-8f3c-f35a7bad69bd is Running (Ready = true)
Jan  3 12:21:06.541: INFO: Pod "pod-logs-websocket-a47cd746-85ee-4020-8f3c-f35a7bad69bd" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  3 12:21:06.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2222" for this suite. 01/03/24 12:21:06.791
------------------------------
• [2.469 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:21:04.353
    Jan  3 12:21:04.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pods 01/03/24 12:21:04.356
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:04.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:04.442
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jan  3 12:21:04.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: creating the pod 01/03/24 12:21:04.472
    STEP: submitting the pod to kubernetes 01/03/24 12:21:04.473
    Jan  3 12:21:04.502: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-a47cd746-85ee-4020-8f3c-f35a7bad69bd" in namespace "pods-2222" to be "running and ready"
    Jan  3 12:21:04.520: INFO: Pod "pod-logs-websocket-a47cd746-85ee-4020-8f3c-f35a7bad69bd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.157243ms
    Jan  3 12:21:04.520: INFO: The phase of Pod pod-logs-websocket-a47cd746-85ee-4020-8f3c-f35a7bad69bd is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:21:06.541: INFO: Pod "pod-logs-websocket-a47cd746-85ee-4020-8f3c-f35a7bad69bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.039104219s
    Jan  3 12:21:06.541: INFO: The phase of Pod pod-logs-websocket-a47cd746-85ee-4020-8f3c-f35a7bad69bd is Running (Ready = true)
    Jan  3 12:21:06.541: INFO: Pod "pod-logs-websocket-a47cd746-85ee-4020-8f3c-f35a7bad69bd" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:21:06.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2222" for this suite. 01/03/24 12:21:06.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:21:06.828
Jan  3 12:21:06.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 12:21:06.83
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:06.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:06.922
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 01/03/24 12:21:06.952
Jan  3 12:21:06.982: INFO: Waiting up to 5m0s for pod "pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec" in namespace "emptydir-6672" to be "Succeeded or Failed"
Jan  3 12:21:07.000: INFO: Pod "pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec": Phase="Pending", Reason="", readiness=false. Elapsed: 18.064074ms
Jan  3 12:21:09.021: INFO: Pod "pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039077816s
Jan  3 12:21:11.026: INFO: Pod "pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044231808s
STEP: Saw pod success 01/03/24 12:21:11.026
Jan  3 12:21:11.027: INFO: Pod "pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec" satisfied condition "Succeeded or Failed"
Jan  3 12:21:11.050: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec container test-container: <nil>
STEP: delete the pod 01/03/24 12:21:11.225
Jan  3 12:21:11.262: INFO: Waiting for pod pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec to disappear
Jan  3 12:21:11.281: INFO: Pod pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:21:11.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6672" for this suite. 01/03/24 12:21:11.312
------------------------------
• [4.512 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:21:06.828
    Jan  3 12:21:06.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 12:21:06.83
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:06.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:06.922
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/03/24 12:21:06.952
    Jan  3 12:21:06.982: INFO: Waiting up to 5m0s for pod "pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec" in namespace "emptydir-6672" to be "Succeeded or Failed"
    Jan  3 12:21:07.000: INFO: Pod "pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec": Phase="Pending", Reason="", readiness=false. Elapsed: 18.064074ms
    Jan  3 12:21:09.021: INFO: Pod "pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039077816s
    Jan  3 12:21:11.026: INFO: Pod "pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044231808s
    STEP: Saw pod success 01/03/24 12:21:11.026
    Jan  3 12:21:11.027: INFO: Pod "pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec" satisfied condition "Succeeded or Failed"
    Jan  3 12:21:11.050: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec container test-container: <nil>
    STEP: delete the pod 01/03/24 12:21:11.225
    Jan  3 12:21:11.262: INFO: Waiting for pod pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec to disappear
    Jan  3 12:21:11.281: INFO: Pod pod-6a9e15e0-4ceb-47fc-a263-2decd1cbacec no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:21:11.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6672" for this suite. 01/03/24 12:21:11.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:21:11.341
Jan  3 12:21:11.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 12:21:11.344
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:11.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:11.427
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jan  3 12:21:11.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/03/24 12:21:14.271
Jan  3 12:21:14.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-7961 --namespace=crd-publish-openapi-7961 create -f -'
Jan  3 12:21:15.791: INFO: stderr: ""
Jan  3 12:21:15.791: INFO: stdout: "e2e-test-crd-publish-openapi-1946-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan  3 12:21:15.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-7961 --namespace=crd-publish-openapi-7961 delete e2e-test-crd-publish-openapi-1946-crds test-cr'
Jan  3 12:21:15.995: INFO: stderr: ""
Jan  3 12:21:15.995: INFO: stdout: "e2e-test-crd-publish-openapi-1946-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan  3 12:21:15.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-7961 --namespace=crd-publish-openapi-7961 apply -f -'
Jan  3 12:21:16.347: INFO: stderr: ""
Jan  3 12:21:16.347: INFO: stdout: "e2e-test-crd-publish-openapi-1946-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan  3 12:21:16.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-7961 --namespace=crd-publish-openapi-7961 delete e2e-test-crd-publish-openapi-1946-crds test-cr'
Jan  3 12:21:16.506: INFO: stderr: ""
Jan  3 12:21:16.506: INFO: stdout: "e2e-test-crd-publish-openapi-1946-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/03/24 12:21:16.506
Jan  3 12:21:16.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-7961 explain e2e-test-crd-publish-openapi-1946-crds'
Jan  3 12:21:17.313: INFO: stderr: ""
Jan  3 12:21:17.313: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1946-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:21:19.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7961" for this suite. 01/03/24 12:21:20.057
------------------------------
• [SLOW TEST] [8.770 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:21:11.341
    Jan  3 12:21:11.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 12:21:11.344
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:11.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:11.427
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jan  3 12:21:11.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/03/24 12:21:14.271
    Jan  3 12:21:14.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-7961 --namespace=crd-publish-openapi-7961 create -f -'
    Jan  3 12:21:15.791: INFO: stderr: ""
    Jan  3 12:21:15.791: INFO: stdout: "e2e-test-crd-publish-openapi-1946-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan  3 12:21:15.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-7961 --namespace=crd-publish-openapi-7961 delete e2e-test-crd-publish-openapi-1946-crds test-cr'
    Jan  3 12:21:15.995: INFO: stderr: ""
    Jan  3 12:21:15.995: INFO: stdout: "e2e-test-crd-publish-openapi-1946-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan  3 12:21:15.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-7961 --namespace=crd-publish-openapi-7961 apply -f -'
    Jan  3 12:21:16.347: INFO: stderr: ""
    Jan  3 12:21:16.347: INFO: stdout: "e2e-test-crd-publish-openapi-1946-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan  3 12:21:16.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-7961 --namespace=crd-publish-openapi-7961 delete e2e-test-crd-publish-openapi-1946-crds test-cr'
    Jan  3 12:21:16.506: INFO: stderr: ""
    Jan  3 12:21:16.506: INFO: stdout: "e2e-test-crd-publish-openapi-1946-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/03/24 12:21:16.506
    Jan  3 12:21:16.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-7961 explain e2e-test-crd-publish-openapi-1946-crds'
    Jan  3 12:21:17.313: INFO: stderr: ""
    Jan  3 12:21:17.313: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1946-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:21:19.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7961" for this suite. 01/03/24 12:21:20.057
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:21:20.113
Jan  3 12:21:20.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename cronjob 01/03/24 12:21:20.116
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:20.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:20.215
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/03/24 12:21:20.242
STEP: creating 01/03/24 12:21:20.242
STEP: getting 01/03/24 12:21:20.262
STEP: listing 01/03/24 12:21:20.28
STEP: watching 01/03/24 12:21:20.3
Jan  3 12:21:20.300: INFO: starting watch
STEP: cluster-wide listing 01/03/24 12:21:20.313
STEP: cluster-wide watching 01/03/24 12:21:20.337
Jan  3 12:21:20.337: INFO: starting watch
STEP: patching 01/03/24 12:21:20.35
STEP: updating 01/03/24 12:21:20.373
Jan  3 12:21:20.414: INFO: waiting for watch events with expected annotations
Jan  3 12:21:20.414: INFO: saw patched and updated annotations
STEP: patching /status 01/03/24 12:21:20.415
STEP: updating /status 01/03/24 12:21:20.437
STEP: get /status 01/03/24 12:21:20.48
STEP: deleting 01/03/24 12:21:20.499
STEP: deleting a collection 01/03/24 12:21:20.558
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  3 12:21:20.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6771" for this suite. 01/03/24 12:21:20.632
------------------------------
• [0.546 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:21:20.113
    Jan  3 12:21:20.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename cronjob 01/03/24 12:21:20.116
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:20.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:20.215
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/03/24 12:21:20.242
    STEP: creating 01/03/24 12:21:20.242
    STEP: getting 01/03/24 12:21:20.262
    STEP: listing 01/03/24 12:21:20.28
    STEP: watching 01/03/24 12:21:20.3
    Jan  3 12:21:20.300: INFO: starting watch
    STEP: cluster-wide listing 01/03/24 12:21:20.313
    STEP: cluster-wide watching 01/03/24 12:21:20.337
    Jan  3 12:21:20.337: INFO: starting watch
    STEP: patching 01/03/24 12:21:20.35
    STEP: updating 01/03/24 12:21:20.373
    Jan  3 12:21:20.414: INFO: waiting for watch events with expected annotations
    Jan  3 12:21:20.414: INFO: saw patched and updated annotations
    STEP: patching /status 01/03/24 12:21:20.415
    STEP: updating /status 01/03/24 12:21:20.437
    STEP: get /status 01/03/24 12:21:20.48
    STEP: deleting 01/03/24 12:21:20.499
    STEP: deleting a collection 01/03/24 12:21:20.558
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:21:20.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6771" for this suite. 01/03/24 12:21:20.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:21:20.663
Jan  3 12:21:20.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename disruption 01/03/24 12:21:20.666
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:20.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:20.747
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 01/03/24 12:21:20.774
STEP: Waiting for the pdb to be processed 01/03/24 12:21:20.793
STEP: updating the pdb 01/03/24 12:21:22.848
STEP: Waiting for the pdb to be processed 01/03/24 12:21:22.885
STEP: patching the pdb 01/03/24 12:21:22.901
STEP: Waiting for the pdb to be processed 01/03/24 12:21:22.94
STEP: Waiting for the pdb to be deleted 01/03/24 12:21:22.977
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  3 12:21:22.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5842" for this suite. 01/03/24 12:21:23.026
------------------------------
• [2.389 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:21:20.663
    Jan  3 12:21:20.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename disruption 01/03/24 12:21:20.666
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:20.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:20.747
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 01/03/24 12:21:20.774
    STEP: Waiting for the pdb to be processed 01/03/24 12:21:20.793
    STEP: updating the pdb 01/03/24 12:21:22.848
    STEP: Waiting for the pdb to be processed 01/03/24 12:21:22.885
    STEP: patching the pdb 01/03/24 12:21:22.901
    STEP: Waiting for the pdb to be processed 01/03/24 12:21:22.94
    STEP: Waiting for the pdb to be deleted 01/03/24 12:21:22.977
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:21:22.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5842" for this suite. 01/03/24 12:21:23.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:21:23.055
Jan  3 12:21:23.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename dns 01/03/24 12:21:23.058
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:23.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:23.136
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/03/24 12:21:23.164
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8891.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8891.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/03/24 12:21:23.183
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8891.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8891.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/03/24 12:21:23.183
STEP: creating a pod to probe DNS 01/03/24 12:21:23.183
STEP: submitting the pod to kubernetes 01/03/24 12:21:23.183
Jan  3 12:21:23.211: INFO: Waiting up to 15m0s for pod "dns-test-fbf7a77e-a165-4ef1-b2dd-7317ae518346" in namespace "dns-8891" to be "running"
Jan  3 12:21:23.229: INFO: Pod "dns-test-fbf7a77e-a165-4ef1-b2dd-7317ae518346": Phase="Pending", Reason="", readiness=false. Elapsed: 17.203009ms
Jan  3 12:21:25.248: INFO: Pod "dns-test-fbf7a77e-a165-4ef1-b2dd-7317ae518346": Phase="Running", Reason="", readiness=true. Elapsed: 2.036683858s
Jan  3 12:21:25.248: INFO: Pod "dns-test-fbf7a77e-a165-4ef1-b2dd-7317ae518346" satisfied condition "running"
STEP: retrieving the pod 01/03/24 12:21:25.248
STEP: looking for the results for each expected name from probers 01/03/24 12:21:25.266
Jan  3 12:21:25.463: INFO: DNS probes using dns-8891/dns-test-fbf7a77e-a165-4ef1-b2dd-7317ae518346 succeeded

STEP: deleting the pod 01/03/24 12:21:25.463
STEP: deleting the test headless service 01/03/24 12:21:25.504
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  3 12:21:25.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8891" for this suite. 01/03/24 12:21:25.562
------------------------------
• [2.530 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:21:23.055
    Jan  3 12:21:23.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename dns 01/03/24 12:21:23.058
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:23.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:23.136
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/03/24 12:21:23.164
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8891.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8891.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/03/24 12:21:23.183
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8891.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8891.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/03/24 12:21:23.183
    STEP: creating a pod to probe DNS 01/03/24 12:21:23.183
    STEP: submitting the pod to kubernetes 01/03/24 12:21:23.183
    Jan  3 12:21:23.211: INFO: Waiting up to 15m0s for pod "dns-test-fbf7a77e-a165-4ef1-b2dd-7317ae518346" in namespace "dns-8891" to be "running"
    Jan  3 12:21:23.229: INFO: Pod "dns-test-fbf7a77e-a165-4ef1-b2dd-7317ae518346": Phase="Pending", Reason="", readiness=false. Elapsed: 17.203009ms
    Jan  3 12:21:25.248: INFO: Pod "dns-test-fbf7a77e-a165-4ef1-b2dd-7317ae518346": Phase="Running", Reason="", readiness=true. Elapsed: 2.036683858s
    Jan  3 12:21:25.248: INFO: Pod "dns-test-fbf7a77e-a165-4ef1-b2dd-7317ae518346" satisfied condition "running"
    STEP: retrieving the pod 01/03/24 12:21:25.248
    STEP: looking for the results for each expected name from probers 01/03/24 12:21:25.266
    Jan  3 12:21:25.463: INFO: DNS probes using dns-8891/dns-test-fbf7a77e-a165-4ef1-b2dd-7317ae518346 succeeded

    STEP: deleting the pod 01/03/24 12:21:25.463
    STEP: deleting the test headless service 01/03/24 12:21:25.504
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:21:25.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8891" for this suite. 01/03/24 12:21:25.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:21:25.587
Jan  3 12:21:25.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename endpointslice 01/03/24 12:21:25.589
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:25.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:25.678
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jan  3 12:21:25.764: INFO: Endpoints addresses: [195.20.227.74 217.160.201.77] , ports: [12978]
Jan  3 12:21:25.764: INFO: EndpointSlices addresses: [195.20.227.74 217.160.201.77] , ports: [12978]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan  3 12:21:25.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3613" for this suite. 01/03/24 12:21:25.794
------------------------------
• [0.235 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:21:25.587
    Jan  3 12:21:25.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename endpointslice 01/03/24 12:21:25.589
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:25.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:25.678
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jan  3 12:21:25.764: INFO: Endpoints addresses: [195.20.227.74 217.160.201.77] , ports: [12978]
    Jan  3 12:21:25.764: INFO: EndpointSlices addresses: [195.20.227.74 217.160.201.77] , ports: [12978]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:21:25.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3613" for this suite. 01/03/24 12:21:25.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:21:25.825
Jan  3 12:21:25.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename resourcequota 01/03/24 12:21:25.827
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:25.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:25.915
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 01/03/24 12:21:25.943
STEP: Ensuring ResourceQuota status is calculated 01/03/24 12:21:25.965
STEP: Creating a ResourceQuota with not best effort scope 01/03/24 12:21:27.993
STEP: Ensuring ResourceQuota status is calculated 01/03/24 12:21:28.012
STEP: Creating a best-effort pod 01/03/24 12:21:30.038
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/03/24 12:21:30.126
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/03/24 12:21:32.146
STEP: Deleting the pod 01/03/24 12:21:34.169
STEP: Ensuring resource quota status released the pod usage 01/03/24 12:21:34.213
STEP: Creating a not best-effort pod 01/03/24 12:21:36.232
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/03/24 12:21:36.262
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/03/24 12:21:38.283
STEP: Deleting the pod 01/03/24 12:21:40.303
STEP: Ensuring resource quota status released the pod usage 01/03/24 12:21:40.339
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  3 12:21:42.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3959" for this suite. 01/03/24 12:21:42.392
------------------------------
• [SLOW TEST] [16.591 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:21:25.825
    Jan  3 12:21:25.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename resourcequota 01/03/24 12:21:25.827
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:25.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:25.915
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 01/03/24 12:21:25.943
    STEP: Ensuring ResourceQuota status is calculated 01/03/24 12:21:25.965
    STEP: Creating a ResourceQuota with not best effort scope 01/03/24 12:21:27.993
    STEP: Ensuring ResourceQuota status is calculated 01/03/24 12:21:28.012
    STEP: Creating a best-effort pod 01/03/24 12:21:30.038
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/03/24 12:21:30.126
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/03/24 12:21:32.146
    STEP: Deleting the pod 01/03/24 12:21:34.169
    STEP: Ensuring resource quota status released the pod usage 01/03/24 12:21:34.213
    STEP: Creating a not best-effort pod 01/03/24 12:21:36.232
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/03/24 12:21:36.262
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/03/24 12:21:38.283
    STEP: Deleting the pod 01/03/24 12:21:40.303
    STEP: Ensuring resource quota status released the pod usage 01/03/24 12:21:40.339
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:21:42.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3959" for this suite. 01/03/24 12:21:42.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:21:42.418
Jan  3 12:21:42.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-probe 01/03/24 12:21:42.421
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:42.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:42.505
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467 in namespace container-probe-6901 01/03/24 12:21:42.536
Jan  3 12:21:42.565: INFO: Waiting up to 5m0s for pod "busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467" in namespace "container-probe-6901" to be "not pending"
Jan  3 12:21:42.584: INFO: Pod "busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467": Phase="Pending", Reason="", readiness=false. Elapsed: 18.902459ms
Jan  3 12:21:44.624: INFO: Pod "busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059088793s
Jan  3 12:21:46.606: INFO: Pod "busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467": Phase="Running", Reason="", readiness=true. Elapsed: 4.040699874s
Jan  3 12:21:46.606: INFO: Pod "busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467" satisfied condition "not pending"
Jan  3 12:21:46.606: INFO: Started pod busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467 in namespace container-probe-6901
STEP: checking the pod's current state and verifying that restartCount is present 01/03/24 12:21:46.606
Jan  3 12:21:46.626: INFO: Initial restart count of pod busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467 is 0
STEP: deleting the pod 01/03/24 12:25:47.367
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  3 12:25:47.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6901" for this suite. 01/03/24 12:25:47.437
------------------------------
• [SLOW TEST] [245.046 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:21:42.418
    Jan  3 12:21:42.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-probe 01/03/24 12:21:42.421
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:21:42.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:21:42.505
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467 in namespace container-probe-6901 01/03/24 12:21:42.536
    Jan  3 12:21:42.565: INFO: Waiting up to 5m0s for pod "busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467" in namespace "container-probe-6901" to be "not pending"
    Jan  3 12:21:42.584: INFO: Pod "busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467": Phase="Pending", Reason="", readiness=false. Elapsed: 18.902459ms
    Jan  3 12:21:44.624: INFO: Pod "busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059088793s
    Jan  3 12:21:46.606: INFO: Pod "busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467": Phase="Running", Reason="", readiness=true. Elapsed: 4.040699874s
    Jan  3 12:21:46.606: INFO: Pod "busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467" satisfied condition "not pending"
    Jan  3 12:21:46.606: INFO: Started pod busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467 in namespace container-probe-6901
    STEP: checking the pod's current state and verifying that restartCount is present 01/03/24 12:21:46.606
    Jan  3 12:21:46.626: INFO: Initial restart count of pod busybox-4a4a40b3-3d20-4f84-85e0-d305d7aa7467 is 0
    STEP: deleting the pod 01/03/24 12:25:47.367
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:25:47.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6901" for this suite. 01/03/24 12:25:47.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:25:47.469
Jan  3 12:25:47.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename namespaces 01/03/24 12:25:47.471
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:25:47.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:25:47.556
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-2604" 01/03/24 12:25:47.583
Jan  3 12:25:47.640: INFO: Namespace "namespaces-2604" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"574f13e1-2ed7-447d-a3ec-af70b4fe9007", "kubernetes.io/metadata.name":"namespaces-2604", "namespaces-2604":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:25:47.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2604" for this suite. 01/03/24 12:25:47.66
------------------------------
• [0.215 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:25:47.469
    Jan  3 12:25:47.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename namespaces 01/03/24 12:25:47.471
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:25:47.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:25:47.556
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-2604" 01/03/24 12:25:47.583
    Jan  3 12:25:47.640: INFO: Namespace "namespaces-2604" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"574f13e1-2ed7-447d-a3ec-af70b4fe9007", "kubernetes.io/metadata.name":"namespaces-2604", "namespaces-2604":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:25:47.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2604" for this suite. 01/03/24 12:25:47.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:25:47.689
Jan  3 12:25:47.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 12:25:47.69
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:25:47.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:25:47.768
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-b76728ba-c632-48b2-af52-b6577c4ab189 01/03/24 12:25:47.795
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 12:25:47.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4353" for this suite. 01/03/24 12:25:47.829
------------------------------
• [0.163 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:25:47.689
    Jan  3 12:25:47.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 12:25:47.69
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:25:47.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:25:47.768
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-b76728ba-c632-48b2-af52-b6577c4ab189 01/03/24 12:25:47.795
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:25:47.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4353" for this suite. 01/03/24 12:25:47.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:25:47.854
Jan  3 12:25:47.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename statefulset 01/03/24 12:25:47.856
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:25:47.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:25:47.938
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8256 01/03/24 12:25:47.965
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 01/03/24 12:25:47.983
Jan  3 12:25:48.020: INFO: Found 0 stateful pods, waiting for 3
Jan  3 12:25:58.048: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 12:25:58.048: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 12:25:58.048: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 12:25:58.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-8256 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 12:25:58.608: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 12:25:58.608: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 12:25:58.608: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/03/24 12:26:08.703
Jan  3 12:26:08.755: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/03/24 12:26:08.755
STEP: Updating Pods in reverse ordinal order 01/03/24 12:26:18.832
Jan  3 12:26:18.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-8256 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 12:26:19.335: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 12:26:19.335: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 12:26:19.335: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 12:26:29.464: INFO: Waiting for StatefulSet statefulset-8256/ss2 to complete update
Jan  3 12:26:29.464: INFO: Waiting for Pod statefulset-8256/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jan  3 12:26:29.464: INFO: Waiting for Pod statefulset-8256/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jan  3 12:26:39.510: INFO: Waiting for StatefulSet statefulset-8256/ss2 to complete update
Jan  3 12:26:39.510: INFO: Waiting for Pod statefulset-8256/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jan  3 12:26:49.503: INFO: Waiting for StatefulSet statefulset-8256/ss2 to complete update
Jan  3 12:26:49.503: INFO: Waiting for Pod statefulset-8256/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jan  3 12:26:59.522: INFO: Waiting for StatefulSet statefulset-8256/ss2 to complete update
STEP: Rolling back to a previous revision 01/03/24 12:27:09.504
Jan  3 12:27:09.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-8256 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 12:27:09.995: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 12:27:09.995: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 12:27:09.995: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 12:27:20.200: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/03/24 12:27:30.282
Jan  3 12:27:30.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-8256 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 12:27:30.855: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 12:27:30.855: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 12:27:30.855: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 12:27:40.981: INFO: Waiting for StatefulSet statefulset-8256/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  3 12:27:51.029: INFO: Deleting all statefulset in ns statefulset-8256
Jan  3 12:27:51.048: INFO: Scaling statefulset ss2 to 0
Jan  3 12:28:01.137: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 12:28:01.183: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  3 12:28:01.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8256" for this suite. 01/03/24 12:28:01.276
------------------------------
• [SLOW TEST] [133.449 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:25:47.854
    Jan  3 12:25:47.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename statefulset 01/03/24 12:25:47.856
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:25:47.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:25:47.938
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8256 01/03/24 12:25:47.965
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 01/03/24 12:25:47.983
    Jan  3 12:25:48.020: INFO: Found 0 stateful pods, waiting for 3
    Jan  3 12:25:58.048: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 12:25:58.048: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 12:25:58.048: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 12:25:58.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-8256 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 12:25:58.608: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 12:25:58.608: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 12:25:58.608: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/03/24 12:26:08.703
    Jan  3 12:26:08.755: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/03/24 12:26:08.755
    STEP: Updating Pods in reverse ordinal order 01/03/24 12:26:18.832
    Jan  3 12:26:18.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-8256 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 12:26:19.335: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  3 12:26:19.335: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 12:26:19.335: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 12:26:29.464: INFO: Waiting for StatefulSet statefulset-8256/ss2 to complete update
    Jan  3 12:26:29.464: INFO: Waiting for Pod statefulset-8256/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jan  3 12:26:29.464: INFO: Waiting for Pod statefulset-8256/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jan  3 12:26:39.510: INFO: Waiting for StatefulSet statefulset-8256/ss2 to complete update
    Jan  3 12:26:39.510: INFO: Waiting for Pod statefulset-8256/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jan  3 12:26:49.503: INFO: Waiting for StatefulSet statefulset-8256/ss2 to complete update
    Jan  3 12:26:49.503: INFO: Waiting for Pod statefulset-8256/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jan  3 12:26:59.522: INFO: Waiting for StatefulSet statefulset-8256/ss2 to complete update
    STEP: Rolling back to a previous revision 01/03/24 12:27:09.504
    Jan  3 12:27:09.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-8256 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 12:27:09.995: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 12:27:09.995: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 12:27:09.995: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 12:27:20.200: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/03/24 12:27:30.282
    Jan  3 12:27:30.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-8256 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 12:27:30.855: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  3 12:27:30.855: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 12:27:30.855: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 12:27:40.981: INFO: Waiting for StatefulSet statefulset-8256/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  3 12:27:51.029: INFO: Deleting all statefulset in ns statefulset-8256
    Jan  3 12:27:51.048: INFO: Scaling statefulset ss2 to 0
    Jan  3 12:28:01.137: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 12:28:01.183: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:28:01.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8256" for this suite. 01/03/24 12:28:01.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:28:01.307
Jan  3 12:28:01.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename endpointslice 01/03/24 12:28:01.31
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:28:01.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:28:01.391
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 01/03/24 12:28:01.42
STEP: getting /apis/discovery.k8s.io 01/03/24 12:28:01.447
STEP: getting /apis/discovery.k8s.iov1 01/03/24 12:28:01.459
STEP: creating 01/03/24 12:28:01.476
STEP: getting 01/03/24 12:28:01.535
STEP: listing 01/03/24 12:28:01.552
STEP: watching 01/03/24 12:28:01.572
Jan  3 12:28:01.573: INFO: starting watch
STEP: cluster-wide listing 01/03/24 12:28:01.586
STEP: cluster-wide watching 01/03/24 12:28:01.604
Jan  3 12:28:01.605: INFO: starting watch
STEP: patching 01/03/24 12:28:01.618
STEP: updating 01/03/24 12:28:01.64
Jan  3 12:28:01.683: INFO: waiting for watch events with expected annotations
Jan  3 12:28:01.683: INFO: saw patched and updated annotations
STEP: deleting 01/03/24 12:28:01.684
STEP: deleting a collection 01/03/24 12:28:01.746
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan  3 12:28:01.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5732" for this suite. 01/03/24 12:28:01.822
------------------------------
• [0.536 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:28:01.307
    Jan  3 12:28:01.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename endpointslice 01/03/24 12:28:01.31
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:28:01.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:28:01.391
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 01/03/24 12:28:01.42
    STEP: getting /apis/discovery.k8s.io 01/03/24 12:28:01.447
    STEP: getting /apis/discovery.k8s.iov1 01/03/24 12:28:01.459
    STEP: creating 01/03/24 12:28:01.476
    STEP: getting 01/03/24 12:28:01.535
    STEP: listing 01/03/24 12:28:01.552
    STEP: watching 01/03/24 12:28:01.572
    Jan  3 12:28:01.573: INFO: starting watch
    STEP: cluster-wide listing 01/03/24 12:28:01.586
    STEP: cluster-wide watching 01/03/24 12:28:01.604
    Jan  3 12:28:01.605: INFO: starting watch
    STEP: patching 01/03/24 12:28:01.618
    STEP: updating 01/03/24 12:28:01.64
    Jan  3 12:28:01.683: INFO: waiting for watch events with expected annotations
    Jan  3 12:28:01.683: INFO: saw patched and updated annotations
    STEP: deleting 01/03/24 12:28:01.684
    STEP: deleting a collection 01/03/24 12:28:01.746
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:28:01.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5732" for this suite. 01/03/24 12:28:01.822
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:28:01.844
Jan  3 12:28:01.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 12:28:01.847
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:28:01.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:28:01.927
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 01/03/24 12:28:01.957
Jan  3 12:28:01.983: INFO: Waiting up to 5m0s for pod "downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec" in namespace "downward-api-5079" to be "Succeeded or Failed"
Jan  3 12:28:02.001: INFO: Pod "downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec": Phase="Pending", Reason="", readiness=false. Elapsed: 17.933959ms
Jan  3 12:28:04.020: INFO: Pod "downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec": Phase="Running", Reason="", readiness=true. Elapsed: 2.037370805s
Jan  3 12:28:06.020: INFO: Pod "downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec": Phase="Running", Reason="", readiness=false. Elapsed: 4.037077912s
Jan  3 12:28:08.023: INFO: Pod "downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040215722s
STEP: Saw pod success 01/03/24 12:28:08.023
Jan  3 12:28:08.024: INFO: Pod "downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec" satisfied condition "Succeeded or Failed"
Jan  3 12:28:08.044: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec container dapi-container: <nil>
STEP: delete the pod 01/03/24 12:28:08.208
Jan  3 12:28:08.265: INFO: Waiting for pod downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec to disappear
Jan  3 12:28:08.282: INFO: Pod downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  3 12:28:08.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5079" for this suite. 01/03/24 12:28:08.312
------------------------------
• [SLOW TEST] [6.495 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:28:01.844
    Jan  3 12:28:01.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 12:28:01.847
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:28:01.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:28:01.927
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 01/03/24 12:28:01.957
    Jan  3 12:28:01.983: INFO: Waiting up to 5m0s for pod "downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec" in namespace "downward-api-5079" to be "Succeeded or Failed"
    Jan  3 12:28:02.001: INFO: Pod "downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec": Phase="Pending", Reason="", readiness=false. Elapsed: 17.933959ms
    Jan  3 12:28:04.020: INFO: Pod "downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec": Phase="Running", Reason="", readiness=true. Elapsed: 2.037370805s
    Jan  3 12:28:06.020: INFO: Pod "downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec": Phase="Running", Reason="", readiness=false. Elapsed: 4.037077912s
    Jan  3 12:28:08.023: INFO: Pod "downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040215722s
    STEP: Saw pod success 01/03/24 12:28:08.023
    Jan  3 12:28:08.024: INFO: Pod "downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec" satisfied condition "Succeeded or Failed"
    Jan  3 12:28:08.044: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec container dapi-container: <nil>
    STEP: delete the pod 01/03/24 12:28:08.208
    Jan  3 12:28:08.265: INFO: Waiting for pod downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec to disappear
    Jan  3 12:28:08.282: INFO: Pod downward-api-d3c76945-33c1-4769-8806-7f79db6a3fec no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:28:08.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5079" for this suite. 01/03/24 12:28:08.312
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:28:08.344
Jan  3 12:28:08.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename dns 01/03/24 12:28:08.347
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:28:08.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:28:08.424
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/03/24 12:28:08.455
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8846 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8846;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8846 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8846;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8846.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8846.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8846.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8846.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8846.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8846.svc;check="$$(dig +notcp +noall +answer +search 48.27.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.27.48_udp@PTR;check="$$(dig +tcp +noall +answer +search 48.27.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.27.48_tcp@PTR;sleep 1; done
 01/03/24 12:28:08.502
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8846 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8846;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8846 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8846;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8846.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8846.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8846.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8846.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8846.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8846.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8846.svc;check="$$(dig +notcp +noall +answer +search 48.27.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.27.48_udp@PTR;check="$$(dig +tcp +noall +answer +search 48.27.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.27.48_tcp@PTR;sleep 1; done
 01/03/24 12:28:08.503
STEP: creating a pod to probe DNS 01/03/24 12:28:08.503
STEP: submitting the pod to kubernetes 01/03/24 12:28:08.503
Jan  3 12:28:08.530: INFO: Waiting up to 15m0s for pod "dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a" in namespace "dns-8846" to be "running"
Jan  3 12:28:08.547: INFO: Pod "dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.385601ms
Jan  3 12:28:10.571: INFO: Pod "dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041372795s
Jan  3 12:28:12.566: INFO: Pod "dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a": Phase="Running", Reason="", readiness=true. Elapsed: 4.036936955s
Jan  3 12:28:12.567: INFO: Pod "dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a" satisfied condition "running"
STEP: retrieving the pod 01/03/24 12:28:12.567
STEP: looking for the results for each expected name from probers 01/03/24 12:28:12.585
Jan  3 12:28:12.754: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:12.798: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:12.831: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:12.863: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:12.895: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:12.929: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:12.961: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:12.994: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:13.159: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:13.189: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:13.223: INFO: Unable to read jessie_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:13.256: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:13.288: INFO: Unable to read jessie_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:13.321: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:13.359: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:13.392: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:13.526: INFO: Lookups using dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8846 wheezy_tcp@dns-test-service.dns-8846 wheezy_udp@dns-test-service.dns-8846.svc wheezy_tcp@dns-test-service.dns-8846.svc wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8846 jessie_tcp@dns-test-service.dns-8846 jessie_udp@dns-test-service.dns-8846.svc jessie_tcp@dns-test-service.dns-8846.svc jessie_udp@_http._tcp.dns-test-service.dns-8846.svc jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc]

Jan  3 12:28:18.560: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:18.610: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:18.642: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:18.674: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:18.705: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:18.737: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:18.769: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:18.801: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:19.001: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:19.037: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:19.074: INFO: Unable to read jessie_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:19.108: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:19.140: INFO: Unable to read jessie_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:19.171: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:19.206: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:19.247: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:19.399: INFO: Lookups using dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8846 wheezy_tcp@dns-test-service.dns-8846 wheezy_udp@dns-test-service.dns-8846.svc wheezy_tcp@dns-test-service.dns-8846.svc wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8846 jessie_tcp@dns-test-service.dns-8846 jessie_udp@dns-test-service.dns-8846.svc jessie_tcp@dns-test-service.dns-8846.svc jessie_udp@_http._tcp.dns-test-service.dns-8846.svc jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc]

Jan  3 12:28:23.566: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:23.599: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:23.631: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:23.670: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:23.701: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:23.733: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:23.765: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:23.800: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:23.971: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:24.005: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:24.037: INFO: Unable to read jessie_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:24.069: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:24.103: INFO: Unable to read jessie_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:24.135: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:24.167: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:24.198: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:24.327: INFO: Lookups using dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8846 wheezy_tcp@dns-test-service.dns-8846 wheezy_udp@dns-test-service.dns-8846.svc wheezy_tcp@dns-test-service.dns-8846.svc wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8846 jessie_tcp@dns-test-service.dns-8846 jessie_udp@dns-test-service.dns-8846.svc jessie_tcp@dns-test-service.dns-8846.svc jessie_udp@_http._tcp.dns-test-service.dns-8846.svc jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc]

Jan  3 12:28:28.562: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:28.593: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:28.627: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:28.658: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:28.695: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:28.727: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:28.759: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:28.794: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:28.961: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:28.993: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:29.025: INFO: Unable to read jessie_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:29.072: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:29.109: INFO: Unable to read jessie_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:29.141: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:29.175: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:29.216: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:29.351: INFO: Lookups using dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8846 wheezy_tcp@dns-test-service.dns-8846 wheezy_udp@dns-test-service.dns-8846.svc wheezy_tcp@dns-test-service.dns-8846.svc wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8846 jessie_tcp@dns-test-service.dns-8846 jessie_udp@dns-test-service.dns-8846.svc jessie_tcp@dns-test-service.dns-8846.svc jessie_udp@_http._tcp.dns-test-service.dns-8846.svc jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc]

Jan  3 12:28:33.560: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:33.598: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:33.633: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:33.668: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:33.700: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:33.731: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:33.764: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:33.800: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:33.975: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:34.009: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:34.041: INFO: Unable to read jessie_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:34.072: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:34.105: INFO: Unable to read jessie_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:34.137: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:34.169: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:34.215: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:34.357: INFO: Lookups using dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8846 wheezy_tcp@dns-test-service.dns-8846 wheezy_udp@dns-test-service.dns-8846.svc wheezy_tcp@dns-test-service.dns-8846.svc wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8846 jessie_tcp@dns-test-service.dns-8846 jessie_udp@dns-test-service.dns-8846.svc jessie_tcp@dns-test-service.dns-8846.svc jessie_udp@_http._tcp.dns-test-service.dns-8846.svc jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc]

Jan  3 12:28:38.560: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:38.592: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:38.628: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:38.662: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:38.698: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:38.729: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:38.762: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:38.794: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:38.958: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:38.991: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:39.024: INFO: Unable to read jessie_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:39.063: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:39.095: INFO: Unable to read jessie_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:39.130: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:39.168: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:39.200: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
Jan  3 12:28:39.340: INFO: Lookups using dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8846 wheezy_tcp@dns-test-service.dns-8846 wheezy_udp@dns-test-service.dns-8846.svc wheezy_tcp@dns-test-service.dns-8846.svc wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8846 jessie_tcp@dns-test-service.dns-8846 jessie_udp@dns-test-service.dns-8846.svc jessie_tcp@dns-test-service.dns-8846.svc jessie_udp@_http._tcp.dns-test-service.dns-8846.svc jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc]

Jan  3 12:28:44.348: INFO: DNS probes using dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a succeeded

STEP: deleting the pod 01/03/24 12:28:44.348
STEP: deleting the test service 01/03/24 12:28:44.382
STEP: deleting the test headless service 01/03/24 12:28:44.421
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  3 12:28:44.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8846" for this suite. 01/03/24 12:28:44.479
------------------------------
• [SLOW TEST] [36.159 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:28:08.344
    Jan  3 12:28:08.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename dns 01/03/24 12:28:08.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:28:08.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:28:08.424
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/03/24 12:28:08.455
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8846 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8846;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8846 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8846;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8846.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8846.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8846.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8846.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8846.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8846.svc;check="$$(dig +notcp +noall +answer +search 48.27.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.27.48_udp@PTR;check="$$(dig +tcp +noall +answer +search 48.27.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.27.48_tcp@PTR;sleep 1; done
     01/03/24 12:28:08.502
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8846 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8846;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8846 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8846;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8846.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8846.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8846.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8846.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8846.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8846.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8846.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8846.svc;check="$$(dig +notcp +noall +answer +search 48.27.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.27.48_udp@PTR;check="$$(dig +tcp +noall +answer +search 48.27.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.27.48_tcp@PTR;sleep 1; done
     01/03/24 12:28:08.503
    STEP: creating a pod to probe DNS 01/03/24 12:28:08.503
    STEP: submitting the pod to kubernetes 01/03/24 12:28:08.503
    Jan  3 12:28:08.530: INFO: Waiting up to 15m0s for pod "dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a" in namespace "dns-8846" to be "running"
    Jan  3 12:28:08.547: INFO: Pod "dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.385601ms
    Jan  3 12:28:10.571: INFO: Pod "dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041372795s
    Jan  3 12:28:12.566: INFO: Pod "dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a": Phase="Running", Reason="", readiness=true. Elapsed: 4.036936955s
    Jan  3 12:28:12.567: INFO: Pod "dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a" satisfied condition "running"
    STEP: retrieving the pod 01/03/24 12:28:12.567
    STEP: looking for the results for each expected name from probers 01/03/24 12:28:12.585
    Jan  3 12:28:12.754: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:12.798: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:12.831: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:12.863: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:12.895: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:12.929: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:12.961: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:12.994: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:13.159: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:13.189: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:13.223: INFO: Unable to read jessie_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:13.256: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:13.288: INFO: Unable to read jessie_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:13.321: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:13.359: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:13.392: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:13.526: INFO: Lookups using dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8846 wheezy_tcp@dns-test-service.dns-8846 wheezy_udp@dns-test-service.dns-8846.svc wheezy_tcp@dns-test-service.dns-8846.svc wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8846 jessie_tcp@dns-test-service.dns-8846 jessie_udp@dns-test-service.dns-8846.svc jessie_tcp@dns-test-service.dns-8846.svc jessie_udp@_http._tcp.dns-test-service.dns-8846.svc jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc]

    Jan  3 12:28:18.560: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:18.610: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:18.642: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:18.674: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:18.705: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:18.737: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:18.769: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:18.801: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:19.001: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:19.037: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:19.074: INFO: Unable to read jessie_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:19.108: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:19.140: INFO: Unable to read jessie_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:19.171: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:19.206: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:19.247: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:19.399: INFO: Lookups using dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8846 wheezy_tcp@dns-test-service.dns-8846 wheezy_udp@dns-test-service.dns-8846.svc wheezy_tcp@dns-test-service.dns-8846.svc wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8846 jessie_tcp@dns-test-service.dns-8846 jessie_udp@dns-test-service.dns-8846.svc jessie_tcp@dns-test-service.dns-8846.svc jessie_udp@_http._tcp.dns-test-service.dns-8846.svc jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc]

    Jan  3 12:28:23.566: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:23.599: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:23.631: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:23.670: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:23.701: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:23.733: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:23.765: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:23.800: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:23.971: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:24.005: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:24.037: INFO: Unable to read jessie_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:24.069: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:24.103: INFO: Unable to read jessie_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:24.135: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:24.167: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:24.198: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:24.327: INFO: Lookups using dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8846 wheezy_tcp@dns-test-service.dns-8846 wheezy_udp@dns-test-service.dns-8846.svc wheezy_tcp@dns-test-service.dns-8846.svc wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8846 jessie_tcp@dns-test-service.dns-8846 jessie_udp@dns-test-service.dns-8846.svc jessie_tcp@dns-test-service.dns-8846.svc jessie_udp@_http._tcp.dns-test-service.dns-8846.svc jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc]

    Jan  3 12:28:28.562: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:28.593: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:28.627: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:28.658: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:28.695: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:28.727: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:28.759: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:28.794: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:28.961: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:28.993: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:29.025: INFO: Unable to read jessie_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:29.072: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:29.109: INFO: Unable to read jessie_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:29.141: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:29.175: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:29.216: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:29.351: INFO: Lookups using dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8846 wheezy_tcp@dns-test-service.dns-8846 wheezy_udp@dns-test-service.dns-8846.svc wheezy_tcp@dns-test-service.dns-8846.svc wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8846 jessie_tcp@dns-test-service.dns-8846 jessie_udp@dns-test-service.dns-8846.svc jessie_tcp@dns-test-service.dns-8846.svc jessie_udp@_http._tcp.dns-test-service.dns-8846.svc jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc]

    Jan  3 12:28:33.560: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:33.598: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:33.633: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:33.668: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:33.700: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:33.731: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:33.764: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:33.800: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:33.975: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:34.009: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:34.041: INFO: Unable to read jessie_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:34.072: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:34.105: INFO: Unable to read jessie_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:34.137: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:34.169: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:34.215: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:34.357: INFO: Lookups using dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8846 wheezy_tcp@dns-test-service.dns-8846 wheezy_udp@dns-test-service.dns-8846.svc wheezy_tcp@dns-test-service.dns-8846.svc wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8846 jessie_tcp@dns-test-service.dns-8846 jessie_udp@dns-test-service.dns-8846.svc jessie_tcp@dns-test-service.dns-8846.svc jessie_udp@_http._tcp.dns-test-service.dns-8846.svc jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc]

    Jan  3 12:28:38.560: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:38.592: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:38.628: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:38.662: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:38.698: INFO: Unable to read wheezy_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:38.729: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:38.762: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:38.794: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:38.958: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:38.991: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:39.024: INFO: Unable to read jessie_udp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:39.063: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846 from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:39.095: INFO: Unable to read jessie_udp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:39.130: INFO: Unable to read jessie_tcp@dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:39.168: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:39.200: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc from pod dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a: the server could not find the requested resource (get pods dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a)
    Jan  3 12:28:39.340: INFO: Lookups using dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8846 wheezy_tcp@dns-test-service.dns-8846 wheezy_udp@dns-test-service.dns-8846.svc wheezy_tcp@dns-test-service.dns-8846.svc wheezy_udp@_http._tcp.dns-test-service.dns-8846.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8846.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8846 jessie_tcp@dns-test-service.dns-8846 jessie_udp@dns-test-service.dns-8846.svc jessie_tcp@dns-test-service.dns-8846.svc jessie_udp@_http._tcp.dns-test-service.dns-8846.svc jessie_tcp@_http._tcp.dns-test-service.dns-8846.svc]

    Jan  3 12:28:44.348: INFO: DNS probes using dns-8846/dns-test-c21caada-6576-4c55-bcc6-c7b0590d471a succeeded

    STEP: deleting the pod 01/03/24 12:28:44.348
    STEP: deleting the test service 01/03/24 12:28:44.382
    STEP: deleting the test headless service 01/03/24 12:28:44.421
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:28:44.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8846" for this suite. 01/03/24 12:28:44.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:28:44.504
Jan  3 12:28:44.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:28:44.505
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:28:44.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:28:44.579
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 01/03/24 12:28:44.606
Jan  3 12:28:44.636: INFO: Waiting up to 5m0s for pod "labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0" in namespace "projected-9869" to be "running and ready"
Jan  3 12:28:44.655: INFO: Pod "labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.163424ms
Jan  3 12:28:44.655: INFO: The phase of Pod labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:28:46.675: INFO: Pod "labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038422826s
Jan  3 12:28:46.675: INFO: The phase of Pod labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:28:48.675: INFO: Pod "labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0": Phase="Running", Reason="", readiness=true. Elapsed: 4.038256854s
Jan  3 12:28:48.675: INFO: The phase of Pod labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0 is Running (Ready = true)
Jan  3 12:28:48.675: INFO: Pod "labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0" satisfied condition "running and ready"
Jan  3 12:28:49.290: INFO: Successfully updated pod "labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  3 12:28:51.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9869" for this suite. 01/03/24 12:28:51.417
------------------------------
• [SLOW TEST] [6.941 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:28:44.504
    Jan  3 12:28:44.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:28:44.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:28:44.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:28:44.579
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 01/03/24 12:28:44.606
    Jan  3 12:28:44.636: INFO: Waiting up to 5m0s for pod "labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0" in namespace "projected-9869" to be "running and ready"
    Jan  3 12:28:44.655: INFO: Pod "labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.163424ms
    Jan  3 12:28:44.655: INFO: The phase of Pod labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:28:46.675: INFO: Pod "labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038422826s
    Jan  3 12:28:46.675: INFO: The phase of Pod labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:28:48.675: INFO: Pod "labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0": Phase="Running", Reason="", readiness=true. Elapsed: 4.038256854s
    Jan  3 12:28:48.675: INFO: The phase of Pod labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0 is Running (Ready = true)
    Jan  3 12:28:48.675: INFO: Pod "labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0" satisfied condition "running and ready"
    Jan  3 12:28:49.290: INFO: Successfully updated pod "labelsupdate5696bbc6-9516-4483-ae53-53bdb94833e0"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:28:51.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9869" for this suite. 01/03/24 12:28:51.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:28:51.447
Jan  3 12:28:51.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 12:28:51.45
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:28:51.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:28:51.544
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/03/24 12:28:51.571
Jan  3 12:28:51.604: INFO: Waiting up to 5m0s for pod "pod-a78160cf-81c4-48dd-b160-e98437454b51" in namespace "emptydir-5616" to be "Succeeded or Failed"
Jan  3 12:28:51.621: INFO: Pod "pod-a78160cf-81c4-48dd-b160-e98437454b51": Phase="Pending", Reason="", readiness=false. Elapsed: 17.239155ms
Jan  3 12:28:53.652: INFO: Pod "pod-a78160cf-81c4-48dd-b160-e98437454b51": Phase="Running", Reason="", readiness=true. Elapsed: 2.047883427s
Jan  3 12:28:55.645: INFO: Pod "pod-a78160cf-81c4-48dd-b160-e98437454b51": Phase="Running", Reason="", readiness=false. Elapsed: 4.040960422s
Jan  3 12:28:57.648: INFO: Pod "pod-a78160cf-81c4-48dd-b160-e98437454b51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043802427s
STEP: Saw pod success 01/03/24 12:28:57.648
Jan  3 12:28:57.648: INFO: Pod "pod-a78160cf-81c4-48dd-b160-e98437454b51" satisfied condition "Succeeded or Failed"
Jan  3 12:28:57.666: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-a78160cf-81c4-48dd-b160-e98437454b51 container test-container: <nil>
STEP: delete the pod 01/03/24 12:28:57.703
Jan  3 12:28:57.751: INFO: Waiting for pod pod-a78160cf-81c4-48dd-b160-e98437454b51 to disappear
Jan  3 12:28:57.768: INFO: Pod pod-a78160cf-81c4-48dd-b160-e98437454b51 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:28:57.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5616" for this suite. 01/03/24 12:28:57.808
------------------------------
• [SLOW TEST] [6.391 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:28:51.447
    Jan  3 12:28:51.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 12:28:51.45
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:28:51.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:28:51.544
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/03/24 12:28:51.571
    Jan  3 12:28:51.604: INFO: Waiting up to 5m0s for pod "pod-a78160cf-81c4-48dd-b160-e98437454b51" in namespace "emptydir-5616" to be "Succeeded or Failed"
    Jan  3 12:28:51.621: INFO: Pod "pod-a78160cf-81c4-48dd-b160-e98437454b51": Phase="Pending", Reason="", readiness=false. Elapsed: 17.239155ms
    Jan  3 12:28:53.652: INFO: Pod "pod-a78160cf-81c4-48dd-b160-e98437454b51": Phase="Running", Reason="", readiness=true. Elapsed: 2.047883427s
    Jan  3 12:28:55.645: INFO: Pod "pod-a78160cf-81c4-48dd-b160-e98437454b51": Phase="Running", Reason="", readiness=false. Elapsed: 4.040960422s
    Jan  3 12:28:57.648: INFO: Pod "pod-a78160cf-81c4-48dd-b160-e98437454b51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043802427s
    STEP: Saw pod success 01/03/24 12:28:57.648
    Jan  3 12:28:57.648: INFO: Pod "pod-a78160cf-81c4-48dd-b160-e98437454b51" satisfied condition "Succeeded or Failed"
    Jan  3 12:28:57.666: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-a78160cf-81c4-48dd-b160-e98437454b51 container test-container: <nil>
    STEP: delete the pod 01/03/24 12:28:57.703
    Jan  3 12:28:57.751: INFO: Waiting for pod pod-a78160cf-81c4-48dd-b160-e98437454b51 to disappear
    Jan  3 12:28:57.768: INFO: Pod pod-a78160cf-81c4-48dd-b160-e98437454b51 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:28:57.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5616" for this suite. 01/03/24 12:28:57.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:28:57.847
Jan  3 12:28:57.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename namespaces 01/03/24 12:28:57.849
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:28:57.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:28:57.929
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 01/03/24 12:28:57.956
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:28:58.012
STEP: Creating a service in the namespace 01/03/24 12:28:58.039
STEP: Deleting the namespace 01/03/24 12:28:58.068
STEP: Waiting for the namespace to be removed. 01/03/24 12:28:58.092
STEP: Recreating the namespace 01/03/24 12:29:04.114
STEP: Verifying there is no service in the namespace 01/03/24 12:29:04.168
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:29:04.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2041" for this suite. 01/03/24 12:29:04.217
STEP: Destroying namespace "nsdeletetest-4524" for this suite. 01/03/24 12:29:04.242
Jan  3 12:29:04.275: INFO: Namespace nsdeletetest-4524 was already deleted
STEP: Destroying namespace "nsdeletetest-3996" for this suite. 01/03/24 12:29:04.275
------------------------------
• [SLOW TEST] [6.454 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:28:57.847
    Jan  3 12:28:57.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename namespaces 01/03/24 12:28:57.849
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:28:57.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:28:57.929
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 01/03/24 12:28:57.956
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:28:58.012
    STEP: Creating a service in the namespace 01/03/24 12:28:58.039
    STEP: Deleting the namespace 01/03/24 12:28:58.068
    STEP: Waiting for the namespace to be removed. 01/03/24 12:28:58.092
    STEP: Recreating the namespace 01/03/24 12:29:04.114
    STEP: Verifying there is no service in the namespace 01/03/24 12:29:04.168
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:29:04.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2041" for this suite. 01/03/24 12:29:04.217
    STEP: Destroying namespace "nsdeletetest-4524" for this suite. 01/03/24 12:29:04.242
    Jan  3 12:29:04.275: INFO: Namespace nsdeletetest-4524 was already deleted
    STEP: Destroying namespace "nsdeletetest-3996" for this suite. 01/03/24 12:29:04.275
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:29:04.303
Jan  3 12:29:04.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 12:29:04.305
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:04.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:04.398
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 01/03/24 12:29:04.444
STEP: waiting for available Endpoint 01/03/24 12:29:04.466
STEP: listing all Endpoints 01/03/24 12:29:04.48
STEP: updating the Endpoint 01/03/24 12:29:04.5
STEP: fetching the Endpoint 01/03/24 12:29:04.531
STEP: patching the Endpoint 01/03/24 12:29:04.549
STEP: fetching the Endpoint 01/03/24 12:29:04.587
STEP: deleting the Endpoint by Collection 01/03/24 12:29:04.606
STEP: waiting for Endpoint deletion 01/03/24 12:29:04.642
STEP: fetching the Endpoint 01/03/24 12:29:04.656
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 12:29:04.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1358" for this suite. 01/03/24 12:29:04.693
------------------------------
• [0.419 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:29:04.303
    Jan  3 12:29:04.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 12:29:04.305
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:04.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:04.398
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 01/03/24 12:29:04.444
    STEP: waiting for available Endpoint 01/03/24 12:29:04.466
    STEP: listing all Endpoints 01/03/24 12:29:04.48
    STEP: updating the Endpoint 01/03/24 12:29:04.5
    STEP: fetching the Endpoint 01/03/24 12:29:04.531
    STEP: patching the Endpoint 01/03/24 12:29:04.549
    STEP: fetching the Endpoint 01/03/24 12:29:04.587
    STEP: deleting the Endpoint by Collection 01/03/24 12:29:04.606
    STEP: waiting for Endpoint deletion 01/03/24 12:29:04.642
    STEP: fetching the Endpoint 01/03/24 12:29:04.656
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:29:04.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1358" for this suite. 01/03/24 12:29:04.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:29:04.723
Jan  3 12:29:04.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename secrets 01/03/24 12:29:04.725
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:04.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:04.808
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-ccc2e527-48c0-4525-925c-2058ff1d23cf 01/03/24 12:29:04.835
STEP: Creating a pod to test consume secrets 01/03/24 12:29:04.854
Jan  3 12:29:04.907: INFO: Waiting up to 5m0s for pod "pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0" in namespace "secrets-9162" to be "Succeeded or Failed"
Jan  3 12:29:04.927: INFO: Pod "pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.297312ms
Jan  3 12:29:06.949: INFO: Pod "pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042474003s
Jan  3 12:29:08.949: INFO: Pod "pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041803989s
STEP: Saw pod success 01/03/24 12:29:08.949
Jan  3 12:29:08.949: INFO: Pod "pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0" satisfied condition "Succeeded or Failed"
Jan  3 12:29:08.968: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0 container secret-volume-test: <nil>
STEP: delete the pod 01/03/24 12:29:09.006
Jan  3 12:29:09.046: INFO: Waiting for pod pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0 to disappear
Jan  3 12:29:09.071: INFO: Pod pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  3 12:29:09.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9162" for this suite. 01/03/24 12:29:09.1
------------------------------
• [4.402 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:29:04.723
    Jan  3 12:29:04.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename secrets 01/03/24 12:29:04.725
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:04.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:04.808
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-ccc2e527-48c0-4525-925c-2058ff1d23cf 01/03/24 12:29:04.835
    STEP: Creating a pod to test consume secrets 01/03/24 12:29:04.854
    Jan  3 12:29:04.907: INFO: Waiting up to 5m0s for pod "pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0" in namespace "secrets-9162" to be "Succeeded or Failed"
    Jan  3 12:29:04.927: INFO: Pod "pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.297312ms
    Jan  3 12:29:06.949: INFO: Pod "pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042474003s
    Jan  3 12:29:08.949: INFO: Pod "pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041803989s
    STEP: Saw pod success 01/03/24 12:29:08.949
    Jan  3 12:29:08.949: INFO: Pod "pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0" satisfied condition "Succeeded or Failed"
    Jan  3 12:29:08.968: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0 container secret-volume-test: <nil>
    STEP: delete the pod 01/03/24 12:29:09.006
    Jan  3 12:29:09.046: INFO: Waiting for pod pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0 to disappear
    Jan  3 12:29:09.071: INFO: Pod pod-secrets-eddeaaea-46d7-490d-9802-89077a65cae0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:29:09.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9162" for this suite. 01/03/24 12:29:09.1
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:29:09.126
Jan  3 12:29:09.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename conformance-tests 01/03/24 12:29:09.127
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:09.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:09.206
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/03/24 12:29:09.234
Jan  3 12:29:09.235: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jan  3 12:29:09.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-9071" for this suite. 01/03/24 12:29:09.288
------------------------------
• [0.187 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:29:09.126
    Jan  3 12:29:09.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename conformance-tests 01/03/24 12:29:09.127
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:09.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:09.206
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/03/24 12:29:09.234
    Jan  3 12:29:09.235: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:29:09.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-9071" for this suite. 01/03/24 12:29:09.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:29:09.32
Jan  3 12:29:09.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 12:29:09.322
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:09.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:09.403
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 01/03/24 12:29:09.43
Jan  3 12:29:09.456: INFO: Waiting up to 5m0s for pod "downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c" in namespace "downward-api-9431" to be "Succeeded or Failed"
Jan  3 12:29:09.478: INFO: Pod "downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.24845ms
Jan  3 12:29:11.499: INFO: Pod "downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c": Phase="Running", Reason="", readiness=true. Elapsed: 2.043455721s
Jan  3 12:29:13.498: INFO: Pod "downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c": Phase="Running", Reason="", readiness=false. Elapsed: 4.042085814s
Jan  3 12:29:15.499: INFO: Pod "downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042865678s
STEP: Saw pod success 01/03/24 12:29:15.499
Jan  3 12:29:15.499: INFO: Pod "downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c" satisfied condition "Succeeded or Failed"
Jan  3 12:29:15.518: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c container dapi-container: <nil>
STEP: delete the pod 01/03/24 12:29:15.558
Jan  3 12:29:15.601: INFO: Waiting for pod downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c to disappear
Jan  3 12:29:15.620: INFO: Pod downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  3 12:29:15.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9431" for this suite. 01/03/24 12:29:15.652
------------------------------
• [SLOW TEST] [6.356 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:29:09.32
    Jan  3 12:29:09.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 12:29:09.322
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:09.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:09.403
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 01/03/24 12:29:09.43
    Jan  3 12:29:09.456: INFO: Waiting up to 5m0s for pod "downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c" in namespace "downward-api-9431" to be "Succeeded or Failed"
    Jan  3 12:29:09.478: INFO: Pod "downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.24845ms
    Jan  3 12:29:11.499: INFO: Pod "downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c": Phase="Running", Reason="", readiness=true. Elapsed: 2.043455721s
    Jan  3 12:29:13.498: INFO: Pod "downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c": Phase="Running", Reason="", readiness=false. Elapsed: 4.042085814s
    Jan  3 12:29:15.499: INFO: Pod "downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042865678s
    STEP: Saw pod success 01/03/24 12:29:15.499
    Jan  3 12:29:15.499: INFO: Pod "downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c" satisfied condition "Succeeded or Failed"
    Jan  3 12:29:15.518: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c container dapi-container: <nil>
    STEP: delete the pod 01/03/24 12:29:15.558
    Jan  3 12:29:15.601: INFO: Waiting for pod downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c to disappear
    Jan  3 12:29:15.620: INFO: Pod downward-api-9a72d540-379a-44a1-b040-01f6d3a2410c no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:29:15.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9431" for this suite. 01/03/24 12:29:15.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:29:15.681
Jan  3 12:29:15.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename server-version 01/03/24 12:29:15.683
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:15.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:15.772
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/03/24 12:29:15.8
STEP: Confirm major version 01/03/24 12:29:15.814
Jan  3 12:29:15.814: INFO: Major version: 1
STEP: Confirm minor version 01/03/24 12:29:15.814
Jan  3 12:29:15.814: INFO: cleanMinorVersion: 26
Jan  3 12:29:15.814: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jan  3 12:29:15.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-5061" for this suite. 01/03/24 12:29:15.836
------------------------------
• [0.181 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:29:15.681
    Jan  3 12:29:15.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename server-version 01/03/24 12:29:15.683
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:15.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:15.772
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/03/24 12:29:15.8
    STEP: Confirm major version 01/03/24 12:29:15.814
    Jan  3 12:29:15.814: INFO: Major version: 1
    STEP: Confirm minor version 01/03/24 12:29:15.814
    Jan  3 12:29:15.814: INFO: cleanMinorVersion: 26
    Jan  3 12:29:15.814: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:29:15.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-5061" for this suite. 01/03/24 12:29:15.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:29:15.868
Jan  3 12:29:15.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 12:29:15.871
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:15.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:15.957
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 12:29:16.037
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 12:29:16.322
STEP: Deploying the webhook pod 01/03/24 12:29:16.347
STEP: Wait for the deployment to be ready 01/03/24 12:29:16.386
Jan  3 12:29:16.419: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/24 12:29:18.472
STEP: Verifying the service has paired with the endpoint 01/03/24 12:29:18.501
Jan  3 12:29:19.502: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jan  3 12:29:19.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7545-crds.webhook.example.com via the AdmissionRegistration API 01/03/24 12:29:20.079
STEP: Creating a custom resource that should be mutated by the webhook 01/03/24 12:29:20.29
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:29:23.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6794" for this suite. 01/03/24 12:29:23.211
STEP: Destroying namespace "webhook-6794-markers" for this suite. 01/03/24 12:29:23.238
------------------------------
• [SLOW TEST] [7.392 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:29:15.868
    Jan  3 12:29:15.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 12:29:15.871
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:15.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:15.957
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 12:29:16.037
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 12:29:16.322
    STEP: Deploying the webhook pod 01/03/24 12:29:16.347
    STEP: Wait for the deployment to be ready 01/03/24 12:29:16.386
    Jan  3 12:29:16.419: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/24 12:29:18.472
    STEP: Verifying the service has paired with the endpoint 01/03/24 12:29:18.501
    Jan  3 12:29:19.502: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jan  3 12:29:19.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7545-crds.webhook.example.com via the AdmissionRegistration API 01/03/24 12:29:20.079
    STEP: Creating a custom resource that should be mutated by the webhook 01/03/24 12:29:20.29
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:29:23.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6794" for this suite. 01/03/24 12:29:23.211
    STEP: Destroying namespace "webhook-6794-markers" for this suite. 01/03/24 12:29:23.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:29:23.267
Jan  3 12:29:23.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 12:29:23.269
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:23.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:23.356
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 01/03/24 12:29:23.382
Jan  3 12:29:23.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4748 api-versions'
Jan  3 12:29:23.576: INFO: stderr: ""
Jan  3 12:29:23.576: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 12:29:23.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4748" for this suite. 01/03/24 12:29:23.595
------------------------------
• [0.355 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:29:23.267
    Jan  3 12:29:23.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 12:29:23.269
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:23.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:23.356
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 01/03/24 12:29:23.382
    Jan  3 12:29:23.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4748 api-versions'
    Jan  3 12:29:23.576: INFO: stderr: ""
    Jan  3 12:29:23.576: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:29:23.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4748" for this suite. 01/03/24 12:29:23.595
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:29:23.622
Jan  3 12:29:23.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename var-expansion 01/03/24 12:29:23.625
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:23.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:23.705
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 01/03/24 12:29:23.732
Jan  3 12:29:23.761: INFO: Waiting up to 5m0s for pod "var-expansion-648f7155-2918-4c6f-acd3-64e337408d16" in namespace "var-expansion-1912" to be "Succeeded or Failed"
Jan  3 12:29:23.780: INFO: Pod "var-expansion-648f7155-2918-4c6f-acd3-64e337408d16": Phase="Pending", Reason="", readiness=false. Elapsed: 18.665758ms
Jan  3 12:29:25.811: INFO: Pod "var-expansion-648f7155-2918-4c6f-acd3-64e337408d16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050040659s
Jan  3 12:29:27.799: INFO: Pod "var-expansion-648f7155-2918-4c6f-acd3-64e337408d16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037408095s
Jan  3 12:29:29.800: INFO: Pod "var-expansion-648f7155-2918-4c6f-acd3-64e337408d16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039324099s
STEP: Saw pod success 01/03/24 12:29:29.801
Jan  3 12:29:29.801: INFO: Pod "var-expansion-648f7155-2918-4c6f-acd3-64e337408d16" satisfied condition "Succeeded or Failed"
Jan  3 12:29:29.821: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod var-expansion-648f7155-2918-4c6f-acd3-64e337408d16 container dapi-container: <nil>
STEP: delete the pod 01/03/24 12:29:29.867
Jan  3 12:29:29.905: INFO: Waiting for pod var-expansion-648f7155-2918-4c6f-acd3-64e337408d16 to disappear
Jan  3 12:29:29.925: INFO: Pod var-expansion-648f7155-2918-4c6f-acd3-64e337408d16 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  3 12:29:29.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1912" for this suite. 01/03/24 12:29:29.956
------------------------------
• [SLOW TEST] [6.374 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:29:23.622
    Jan  3 12:29:23.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename var-expansion 01/03/24 12:29:23.625
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:23.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:23.705
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 01/03/24 12:29:23.732
    Jan  3 12:29:23.761: INFO: Waiting up to 5m0s for pod "var-expansion-648f7155-2918-4c6f-acd3-64e337408d16" in namespace "var-expansion-1912" to be "Succeeded or Failed"
    Jan  3 12:29:23.780: INFO: Pod "var-expansion-648f7155-2918-4c6f-acd3-64e337408d16": Phase="Pending", Reason="", readiness=false. Elapsed: 18.665758ms
    Jan  3 12:29:25.811: INFO: Pod "var-expansion-648f7155-2918-4c6f-acd3-64e337408d16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050040659s
    Jan  3 12:29:27.799: INFO: Pod "var-expansion-648f7155-2918-4c6f-acd3-64e337408d16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037408095s
    Jan  3 12:29:29.800: INFO: Pod "var-expansion-648f7155-2918-4c6f-acd3-64e337408d16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039324099s
    STEP: Saw pod success 01/03/24 12:29:29.801
    Jan  3 12:29:29.801: INFO: Pod "var-expansion-648f7155-2918-4c6f-acd3-64e337408d16" satisfied condition "Succeeded or Failed"
    Jan  3 12:29:29.821: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod var-expansion-648f7155-2918-4c6f-acd3-64e337408d16 container dapi-container: <nil>
    STEP: delete the pod 01/03/24 12:29:29.867
    Jan  3 12:29:29.905: INFO: Waiting for pod var-expansion-648f7155-2918-4c6f-acd3-64e337408d16 to disappear
    Jan  3 12:29:29.925: INFO: Pod var-expansion-648f7155-2918-4c6f-acd3-64e337408d16 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:29:29.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1912" for this suite. 01/03/24 12:29:29.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:29:30.002
Jan  3 12:29:30.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 12:29:30.004
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:30.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:30.105
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 12:29:30.196
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 12:29:30.537
STEP: Deploying the webhook pod 01/03/24 12:29:30.558
STEP: Wait for the deployment to be ready 01/03/24 12:29:30.598
Jan  3 12:29:30.634: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/24 12:29:32.693
STEP: Verifying the service has paired with the endpoint 01/03/24 12:29:32.716
Jan  3 12:29:33.717: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/03/24 12:29:33.737
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/03/24 12:29:33.872
STEP: Creating a dummy validating-webhook-configuration object 01/03/24 12:29:34.013
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/03/24 12:29:34.064
STEP: Creating a dummy mutating-webhook-configuration object 01/03/24 12:29:34.088
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/03/24 12:29:34.142
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:29:34.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8601" for this suite. 01/03/24 12:29:34.348
STEP: Destroying namespace "webhook-8601-markers" for this suite. 01/03/24 12:29:34.378
------------------------------
• [4.399 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:29:30.002
    Jan  3 12:29:30.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 12:29:30.004
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:30.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:30.105
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 12:29:30.196
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 12:29:30.537
    STEP: Deploying the webhook pod 01/03/24 12:29:30.558
    STEP: Wait for the deployment to be ready 01/03/24 12:29:30.598
    Jan  3 12:29:30.634: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/24 12:29:32.693
    STEP: Verifying the service has paired with the endpoint 01/03/24 12:29:32.716
    Jan  3 12:29:33.717: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/03/24 12:29:33.737
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/03/24 12:29:33.872
    STEP: Creating a dummy validating-webhook-configuration object 01/03/24 12:29:34.013
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/03/24 12:29:34.064
    STEP: Creating a dummy mutating-webhook-configuration object 01/03/24 12:29:34.088
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/03/24 12:29:34.142
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:29:34.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8601" for this suite. 01/03/24 12:29:34.348
    STEP: Destroying namespace "webhook-8601-markers" for this suite. 01/03/24 12:29:34.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:29:34.408
Jan  3 12:29:34.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 12:29:34.411
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:34.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:34.491
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 12:29:34.565
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 12:29:34.944
STEP: Deploying the webhook pod 01/03/24 12:29:34.963
STEP: Wait for the deployment to be ready 01/03/24 12:29:35.002
Jan  3 12:29:35.039: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 12:29:37.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 29, 35, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 29, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 29, 35, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 29, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/03/24 12:29:39.114
STEP: Verifying the service has paired with the endpoint 01/03/24 12:29:39.14
Jan  3 12:29:40.143: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jan  3 12:29:40.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/03/24 12:29:40.714
STEP: Creating a custom resource that should be denied by the webhook 01/03/24 12:29:40.854
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/03/24 12:29:43.04
STEP: Updating the custom resource with disallowed data should be denied 01/03/24 12:29:43.079
STEP: Deleting the custom resource should be denied 01/03/24 12:29:43.14
STEP: Remove the offending key and value from the custom resource data 01/03/24 12:29:43.186
STEP: Deleting the updated custom resource should be successful 01/03/24 12:29:43.239
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:29:43.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4719" for this suite. 01/03/24 12:29:44
STEP: Destroying namespace "webhook-4719-markers" for this suite. 01/03/24 12:29:44.022
------------------------------
• [SLOW TEST] [9.637 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:29:34.408
    Jan  3 12:29:34.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 12:29:34.411
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:34.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:34.491
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 12:29:34.565
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 12:29:34.944
    STEP: Deploying the webhook pod 01/03/24 12:29:34.963
    STEP: Wait for the deployment to be ready 01/03/24 12:29:35.002
    Jan  3 12:29:35.039: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan  3 12:29:37.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 29, 35, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 29, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 29, 35, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 29, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/03/24 12:29:39.114
    STEP: Verifying the service has paired with the endpoint 01/03/24 12:29:39.14
    Jan  3 12:29:40.143: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jan  3 12:29:40.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/03/24 12:29:40.714
    STEP: Creating a custom resource that should be denied by the webhook 01/03/24 12:29:40.854
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/03/24 12:29:43.04
    STEP: Updating the custom resource with disallowed data should be denied 01/03/24 12:29:43.079
    STEP: Deleting the custom resource should be denied 01/03/24 12:29:43.14
    STEP: Remove the offending key and value from the custom resource data 01/03/24 12:29:43.186
    STEP: Deleting the updated custom resource should be successful 01/03/24 12:29:43.239
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:29:43.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4719" for this suite. 01/03/24 12:29:44
    STEP: Destroying namespace "webhook-4719-markers" for this suite. 01/03/24 12:29:44.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:29:44.051
Jan  3 12:29:44.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename custom-resource-definition 01/03/24 12:29:44.053
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:44.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:44.134
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan  3 12:29:44.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:29:45.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1361" for this suite. 01/03/24 12:29:45.3
------------------------------
• [1.276 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:29:44.051
    Jan  3 12:29:44.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename custom-resource-definition 01/03/24 12:29:44.053
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:44.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:44.134
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan  3 12:29:44.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:29:45.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1361" for this suite. 01/03/24 12:29:45.3
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:29:45.328
Jan  3 12:29:45.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename runtimeclass 01/03/24 12:29:45.331
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:45.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:45.408
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-3089-delete-me 01/03/24 12:29:45.453
STEP: Waiting for the RuntimeClass to disappear 01/03/24 12:29:45.48
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  3 12:29:45.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3089" for this suite. 01/03/24 12:29:45.545
------------------------------
• [0.241 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:29:45.328
    Jan  3 12:29:45.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename runtimeclass 01/03/24 12:29:45.331
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:45.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:45.408
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-3089-delete-me 01/03/24 12:29:45.453
    STEP: Waiting for the RuntimeClass to disappear 01/03/24 12:29:45.48
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:29:45.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3089" for this suite. 01/03/24 12:29:45.545
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:29:45.57
Jan  3 12:29:45.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename sysctl 01/03/24 12:29:45.572
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:45.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:45.663
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/03/24 12:29:45.69
STEP: Watching for error events or started pod 01/03/24 12:29:45.719
STEP: Waiting for pod completion 01/03/24 12:29:47.741
Jan  3 12:29:47.741: INFO: Waiting up to 3m0s for pod "sysctl-994a43f2-8ac0-461b-8678-f4acb026df1a" in namespace "sysctl-3805" to be "completed"
Jan  3 12:29:47.761: INFO: Pod "sysctl-994a43f2-8ac0-461b-8678-f4acb026df1a": Phase="Running", Reason="", readiness=true. Elapsed: 19.244466ms
Jan  3 12:29:49.781: INFO: Pod "sysctl-994a43f2-8ac0-461b-8678-f4acb026df1a": Phase="Running", Reason="", readiness=false. Elapsed: 2.039805635s
Jan  3 12:29:51.784: INFO: Pod "sysctl-994a43f2-8ac0-461b-8678-f4acb026df1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042684545s
Jan  3 12:29:51.784: INFO: Pod "sysctl-994a43f2-8ac0-461b-8678-f4acb026df1a" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/03/24 12:29:51.803
STEP: Getting logs from the pod 01/03/24 12:29:51.803
STEP: Checking that the sysctl is actually updated 01/03/24 12:29:51.84
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:29:51.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-3805" for this suite. 01/03/24 12:29:51.874
------------------------------
• [SLOW TEST] [6.331 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:29:45.57
    Jan  3 12:29:45.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename sysctl 01/03/24 12:29:45.572
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:45.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:45.663
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/03/24 12:29:45.69
    STEP: Watching for error events or started pod 01/03/24 12:29:45.719
    STEP: Waiting for pod completion 01/03/24 12:29:47.741
    Jan  3 12:29:47.741: INFO: Waiting up to 3m0s for pod "sysctl-994a43f2-8ac0-461b-8678-f4acb026df1a" in namespace "sysctl-3805" to be "completed"
    Jan  3 12:29:47.761: INFO: Pod "sysctl-994a43f2-8ac0-461b-8678-f4acb026df1a": Phase="Running", Reason="", readiness=true. Elapsed: 19.244466ms
    Jan  3 12:29:49.781: INFO: Pod "sysctl-994a43f2-8ac0-461b-8678-f4acb026df1a": Phase="Running", Reason="", readiness=false. Elapsed: 2.039805635s
    Jan  3 12:29:51.784: INFO: Pod "sysctl-994a43f2-8ac0-461b-8678-f4acb026df1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042684545s
    Jan  3 12:29:51.784: INFO: Pod "sysctl-994a43f2-8ac0-461b-8678-f4acb026df1a" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/03/24 12:29:51.803
    STEP: Getting logs from the pod 01/03/24 12:29:51.803
    STEP: Checking that the sysctl is actually updated 01/03/24 12:29:51.84
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:29:51.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-3805" for this suite. 01/03/24 12:29:51.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:29:51.903
Jan  3 12:29:51.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-probe 01/03/24 12:29:51.905
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:51.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:51.987
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-7e59d2a5-c41c-4315-8f7b-ab40fbb7fa3d in namespace container-probe-628 01/03/24 12:29:52.023
Jan  3 12:29:52.051: INFO: Waiting up to 5m0s for pod "test-webserver-7e59d2a5-c41c-4315-8f7b-ab40fbb7fa3d" in namespace "container-probe-628" to be "not pending"
Jan  3 12:29:52.068: INFO: Pod "test-webserver-7e59d2a5-c41c-4315-8f7b-ab40fbb7fa3d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.564186ms
Jan  3 12:29:54.090: INFO: Pod "test-webserver-7e59d2a5-c41c-4315-8f7b-ab40fbb7fa3d": Phase="Running", Reason="", readiness=true. Elapsed: 2.039581763s
Jan  3 12:29:54.090: INFO: Pod "test-webserver-7e59d2a5-c41c-4315-8f7b-ab40fbb7fa3d" satisfied condition "not pending"
Jan  3 12:29:54.091: INFO: Started pod test-webserver-7e59d2a5-c41c-4315-8f7b-ab40fbb7fa3d in namespace container-probe-628
STEP: checking the pod's current state and verifying that restartCount is present 01/03/24 12:29:54.091
Jan  3 12:29:54.110: INFO: Initial restart count of pod test-webserver-7e59d2a5-c41c-4315-8f7b-ab40fbb7fa3d is 0
STEP: deleting the pod 01/03/24 12:33:54.814
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  3 12:33:54.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-628" for this suite. 01/03/24 12:33:54.883
------------------------------
• [SLOW TEST] [243.004 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:29:51.903
    Jan  3 12:29:51.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-probe 01/03/24 12:29:51.905
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:29:51.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:29:51.987
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-7e59d2a5-c41c-4315-8f7b-ab40fbb7fa3d in namespace container-probe-628 01/03/24 12:29:52.023
    Jan  3 12:29:52.051: INFO: Waiting up to 5m0s for pod "test-webserver-7e59d2a5-c41c-4315-8f7b-ab40fbb7fa3d" in namespace "container-probe-628" to be "not pending"
    Jan  3 12:29:52.068: INFO: Pod "test-webserver-7e59d2a5-c41c-4315-8f7b-ab40fbb7fa3d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.564186ms
    Jan  3 12:29:54.090: INFO: Pod "test-webserver-7e59d2a5-c41c-4315-8f7b-ab40fbb7fa3d": Phase="Running", Reason="", readiness=true. Elapsed: 2.039581763s
    Jan  3 12:29:54.090: INFO: Pod "test-webserver-7e59d2a5-c41c-4315-8f7b-ab40fbb7fa3d" satisfied condition "not pending"
    Jan  3 12:29:54.091: INFO: Started pod test-webserver-7e59d2a5-c41c-4315-8f7b-ab40fbb7fa3d in namespace container-probe-628
    STEP: checking the pod's current state and verifying that restartCount is present 01/03/24 12:29:54.091
    Jan  3 12:29:54.110: INFO: Initial restart count of pod test-webserver-7e59d2a5-c41c-4315-8f7b-ab40fbb7fa3d is 0
    STEP: deleting the pod 01/03/24 12:33:54.814
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:33:54.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-628" for this suite. 01/03/24 12:33:54.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:33:54.909
Jan  3 12:33:54.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:33:54.912
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:33:54.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:33:55.004
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-337ca598-80b6-4f96-abf0-095a3977c91f 01/03/24 12:33:55.049
STEP: Creating the pod 01/03/24 12:33:55.068
Jan  3 12:33:55.097: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0dde409f-bd22-42e3-a049-228498a70887" in namespace "projected-4392" to be "running and ready"
Jan  3 12:33:55.119: INFO: Pod "pod-projected-configmaps-0dde409f-bd22-42e3-a049-228498a70887": Phase="Pending", Reason="", readiness=false. Elapsed: 21.38184ms
Jan  3 12:33:55.119: INFO: The phase of Pod pod-projected-configmaps-0dde409f-bd22-42e3-a049-228498a70887 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:33:57.140: INFO: Pod "pod-projected-configmaps-0dde409f-bd22-42e3-a049-228498a70887": Phase="Running", Reason="", readiness=true. Elapsed: 2.042025698s
Jan  3 12:33:57.140: INFO: The phase of Pod pod-projected-configmaps-0dde409f-bd22-42e3-a049-228498a70887 is Running (Ready = true)
Jan  3 12:33:57.140: INFO: Pod "pod-projected-configmaps-0dde409f-bd22-42e3-a049-228498a70887" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-337ca598-80b6-4f96-abf0-095a3977c91f 01/03/24 12:33:57.324
STEP: waiting to observe update in volume 01/03/24 12:33:57.342
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  3 12:33:59.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4392" for this suite. 01/03/24 12:33:59.454
------------------------------
• [4.573 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:33:54.909
    Jan  3 12:33:54.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:33:54.912
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:33:54.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:33:55.004
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-337ca598-80b6-4f96-abf0-095a3977c91f 01/03/24 12:33:55.049
    STEP: Creating the pod 01/03/24 12:33:55.068
    Jan  3 12:33:55.097: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0dde409f-bd22-42e3-a049-228498a70887" in namespace "projected-4392" to be "running and ready"
    Jan  3 12:33:55.119: INFO: Pod "pod-projected-configmaps-0dde409f-bd22-42e3-a049-228498a70887": Phase="Pending", Reason="", readiness=false. Elapsed: 21.38184ms
    Jan  3 12:33:55.119: INFO: The phase of Pod pod-projected-configmaps-0dde409f-bd22-42e3-a049-228498a70887 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:33:57.140: INFO: Pod "pod-projected-configmaps-0dde409f-bd22-42e3-a049-228498a70887": Phase="Running", Reason="", readiness=true. Elapsed: 2.042025698s
    Jan  3 12:33:57.140: INFO: The phase of Pod pod-projected-configmaps-0dde409f-bd22-42e3-a049-228498a70887 is Running (Ready = true)
    Jan  3 12:33:57.140: INFO: Pod "pod-projected-configmaps-0dde409f-bd22-42e3-a049-228498a70887" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-337ca598-80b6-4f96-abf0-095a3977c91f 01/03/24 12:33:57.324
    STEP: waiting to observe update in volume 01/03/24 12:33:57.342
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:33:59.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4392" for this suite. 01/03/24 12:33:59.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:33:59.483
Jan  3 12:33:59.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 12:33:59.486
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:33:59.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:33:59.567
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4912 01/03/24 12:33:59.594
STEP: changing the ExternalName service to type=NodePort 01/03/24 12:33:59.612
STEP: creating replication controller externalname-service in namespace services-4912 01/03/24 12:33:59.666
I0103 12:33:59.687505      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4912, replica count: 2
I0103 12:34:02.739140      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 12:34:02.739: INFO: Creating new exec pod
Jan  3 12:34:02.774: INFO: Waiting up to 5m0s for pod "execpodxndbf" in namespace "services-4912" to be "running"
Jan  3 12:34:02.792: INFO: Pod "execpodxndbf": Phase="Pending", Reason="", readiness=false. Elapsed: 17.588401ms
Jan  3 12:34:04.814: INFO: Pod "execpodxndbf": Phase="Running", Reason="", readiness=true. Elapsed: 2.038915238s
Jan  3 12:34:04.814: INFO: Pod "execpodxndbf" satisfied condition "running"
Jan  3 12:34:05.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-4912 exec execpodxndbf -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan  3 12:34:06.340: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  3 12:34:06.340: INFO: stdout: ""
Jan  3 12:34:06.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-4912 exec execpodxndbf -- /bin/sh -x -c nc -v -z -w 2 10.233.25.71 80'
Jan  3 12:34:06.789: INFO: stderr: "+ nc -v -z -w 2 10.233.25.71 80\nConnection to 10.233.25.71 80 port [tcp/http] succeeded!\n"
Jan  3 12:34:06.790: INFO: stdout: ""
Jan  3 12:34:06.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-4912 exec execpodxndbf -- /bin/sh -x -c nc -v -z -w 2 85.215.218.90 31501'
Jan  3 12:34:07.227: INFO: stderr: "+ nc -v -z -w 2 85.215.218.90 31501\nConnection to 85.215.218.90 31501 port [tcp/*] succeeded!\n"
Jan  3 12:34:07.227: INFO: stdout: ""
Jan  3 12:34:07.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-4912 exec execpodxndbf -- /bin/sh -x -c nc -v -z -w 2 185.132.46.116 31501'
Jan  3 12:34:07.677: INFO: stderr: "+ nc -v -z -w 2 185.132.46.116 31501\nConnection to 185.132.46.116 31501 port [tcp/*] succeeded!\n"
Jan  3 12:34:07.677: INFO: stdout: ""
Jan  3 12:34:07.677: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 12:34:07.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4912" for this suite. 01/03/24 12:34:07.757
------------------------------
• [SLOW TEST] [8.298 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:33:59.483
    Jan  3 12:33:59.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 12:33:59.486
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:33:59.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:33:59.567
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4912 01/03/24 12:33:59.594
    STEP: changing the ExternalName service to type=NodePort 01/03/24 12:33:59.612
    STEP: creating replication controller externalname-service in namespace services-4912 01/03/24 12:33:59.666
    I0103 12:33:59.687505      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4912, replica count: 2
    I0103 12:34:02.739140      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 12:34:02.739: INFO: Creating new exec pod
    Jan  3 12:34:02.774: INFO: Waiting up to 5m0s for pod "execpodxndbf" in namespace "services-4912" to be "running"
    Jan  3 12:34:02.792: INFO: Pod "execpodxndbf": Phase="Pending", Reason="", readiness=false. Elapsed: 17.588401ms
    Jan  3 12:34:04.814: INFO: Pod "execpodxndbf": Phase="Running", Reason="", readiness=true. Elapsed: 2.038915238s
    Jan  3 12:34:04.814: INFO: Pod "execpodxndbf" satisfied condition "running"
    Jan  3 12:34:05.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-4912 exec execpodxndbf -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan  3 12:34:06.340: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan  3 12:34:06.340: INFO: stdout: ""
    Jan  3 12:34:06.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-4912 exec execpodxndbf -- /bin/sh -x -c nc -v -z -w 2 10.233.25.71 80'
    Jan  3 12:34:06.789: INFO: stderr: "+ nc -v -z -w 2 10.233.25.71 80\nConnection to 10.233.25.71 80 port [tcp/http] succeeded!\n"
    Jan  3 12:34:06.790: INFO: stdout: ""
    Jan  3 12:34:06.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-4912 exec execpodxndbf -- /bin/sh -x -c nc -v -z -w 2 85.215.218.90 31501'
    Jan  3 12:34:07.227: INFO: stderr: "+ nc -v -z -w 2 85.215.218.90 31501\nConnection to 85.215.218.90 31501 port [tcp/*] succeeded!\n"
    Jan  3 12:34:07.227: INFO: stdout: ""
    Jan  3 12:34:07.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-4912 exec execpodxndbf -- /bin/sh -x -c nc -v -z -w 2 185.132.46.116 31501'
    Jan  3 12:34:07.677: INFO: stderr: "+ nc -v -z -w 2 185.132.46.116 31501\nConnection to 185.132.46.116 31501 port [tcp/*] succeeded!\n"
    Jan  3 12:34:07.677: INFO: stdout: ""
    Jan  3 12:34:07.677: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:34:07.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4912" for this suite. 01/03/24 12:34:07.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:34:07.782
Jan  3 12:34:07.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:34:07.785
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:34:07.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:34:07.867
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-c38cfea4-db63-45c7-8efb-53104f5ce5f3 01/03/24 12:34:07.895
STEP: Creating a pod to test consume configMaps 01/03/24 12:34:07.927
Jan  3 12:34:07.965: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d" in namespace "projected-4461" to be "Succeeded or Failed"
Jan  3 12:34:07.988: INFO: Pod "pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d": Phase="Pending", Reason="", readiness=false. Elapsed: 22.817442ms
Jan  3 12:34:10.011: INFO: Pod "pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.045821375s
Jan  3 12:34:12.011: INFO: Pod "pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d": Phase="Running", Reason="", readiness=false. Elapsed: 4.046381044s
Jan  3 12:34:14.021: INFO: Pod "pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055603502s
STEP: Saw pod success 01/03/24 12:34:14.021
Jan  3 12:34:14.021: INFO: Pod "pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d" satisfied condition "Succeeded or Failed"
Jan  3 12:34:14.042: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d container agnhost-container: <nil>
STEP: delete the pod 01/03/24 12:34:14.079
Jan  3 12:34:14.111: INFO: Waiting for pod pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d to disappear
Jan  3 12:34:14.129: INFO: Pod pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  3 12:34:14.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4461" for this suite. 01/03/24 12:34:14.16
------------------------------
• [SLOW TEST] [6.411 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:34:07.782
    Jan  3 12:34:07.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:34:07.785
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:34:07.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:34:07.867
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-c38cfea4-db63-45c7-8efb-53104f5ce5f3 01/03/24 12:34:07.895
    STEP: Creating a pod to test consume configMaps 01/03/24 12:34:07.927
    Jan  3 12:34:07.965: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d" in namespace "projected-4461" to be "Succeeded or Failed"
    Jan  3 12:34:07.988: INFO: Pod "pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d": Phase="Pending", Reason="", readiness=false. Elapsed: 22.817442ms
    Jan  3 12:34:10.011: INFO: Pod "pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.045821375s
    Jan  3 12:34:12.011: INFO: Pod "pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d": Phase="Running", Reason="", readiness=false. Elapsed: 4.046381044s
    Jan  3 12:34:14.021: INFO: Pod "pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055603502s
    STEP: Saw pod success 01/03/24 12:34:14.021
    Jan  3 12:34:14.021: INFO: Pod "pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d" satisfied condition "Succeeded or Failed"
    Jan  3 12:34:14.042: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 12:34:14.079
    Jan  3 12:34:14.111: INFO: Waiting for pod pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d to disappear
    Jan  3 12:34:14.129: INFO: Pod pod-projected-configmaps-d08ba337-4d5f-4060-92d8-8f45ef894a9d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:34:14.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4461" for this suite. 01/03/24 12:34:14.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:34:14.198
Jan  3 12:34:14.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename statefulset 01/03/24 12:34:14.202
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:34:14.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:34:14.294
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7930 01/03/24 12:34:14.321
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-7930 01/03/24 12:34:14.34
Jan  3 12:34:14.380: INFO: Found 0 stateful pods, waiting for 1
Jan  3 12:34:24.407: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/03/24 12:34:24.446
STEP: updating a scale subresource 01/03/24 12:34:24.464
STEP: verifying the statefulset Spec.Replicas was modified 01/03/24 12:34:24.485
STEP: Patch a scale subresource 01/03/24 12:34:24.503
STEP: verifying the statefulset Spec.Replicas was modified 01/03/24 12:34:24.523
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  3 12:34:24.543: INFO: Deleting all statefulset in ns statefulset-7930
Jan  3 12:34:24.561: INFO: Scaling statefulset ss to 0
Jan  3 12:34:34.645: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 12:34:34.662: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  3 12:34:34.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7930" for this suite. 01/03/24 12:34:34.769
------------------------------
• [SLOW TEST] [20.598 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:34:14.198
    Jan  3 12:34:14.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename statefulset 01/03/24 12:34:14.202
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:34:14.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:34:14.294
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7930 01/03/24 12:34:14.321
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-7930 01/03/24 12:34:14.34
    Jan  3 12:34:14.380: INFO: Found 0 stateful pods, waiting for 1
    Jan  3 12:34:24.407: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/03/24 12:34:24.446
    STEP: updating a scale subresource 01/03/24 12:34:24.464
    STEP: verifying the statefulset Spec.Replicas was modified 01/03/24 12:34:24.485
    STEP: Patch a scale subresource 01/03/24 12:34:24.503
    STEP: verifying the statefulset Spec.Replicas was modified 01/03/24 12:34:24.523
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  3 12:34:24.543: INFO: Deleting all statefulset in ns statefulset-7930
    Jan  3 12:34:24.561: INFO: Scaling statefulset ss to 0
    Jan  3 12:34:34.645: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 12:34:34.662: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:34:34.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7930" for this suite. 01/03/24 12:34:34.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:34:34.8
Jan  3 12:34:34.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename disruption 01/03/24 12:34:34.802
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:34:34.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:34:34.894
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 01/03/24 12:34:34.95
STEP: Updating PodDisruptionBudget status 01/03/24 12:34:36.987
STEP: Waiting for all pods to be running 01/03/24 12:34:37.015
Jan  3 12:34:37.035: INFO: running pods: 0 < 1
STEP: locating a running pod 01/03/24 12:34:39.058
STEP: Waiting for the pdb to be processed 01/03/24 12:34:39.116
STEP: Patching PodDisruptionBudget status 01/03/24 12:34:39.156
STEP: Waiting for the pdb to be processed 01/03/24 12:34:39.202
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  3 12:34:39.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5976" for this suite. 01/03/24 12:34:39.254
------------------------------
• [4.481 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:34:34.8
    Jan  3 12:34:34.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename disruption 01/03/24 12:34:34.802
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:34:34.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:34:34.894
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 01/03/24 12:34:34.95
    STEP: Updating PodDisruptionBudget status 01/03/24 12:34:36.987
    STEP: Waiting for all pods to be running 01/03/24 12:34:37.015
    Jan  3 12:34:37.035: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/03/24 12:34:39.058
    STEP: Waiting for the pdb to be processed 01/03/24 12:34:39.116
    STEP: Patching PodDisruptionBudget status 01/03/24 12:34:39.156
    STEP: Waiting for the pdb to be processed 01/03/24 12:34:39.202
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:34:39.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5976" for this suite. 01/03/24 12:34:39.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:34:39.289
Jan  3 12:34:39.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename certificates 01/03/24 12:34:39.291
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:34:39.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:34:39.369
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/03/24 12:34:40.181
STEP: getting /apis/certificates.k8s.io 01/03/24 12:34:40.206
STEP: getting /apis/certificates.k8s.io/v1 01/03/24 12:34:40.22
STEP: creating 01/03/24 12:34:40.234
STEP: getting 01/03/24 12:34:40.299
STEP: listing 01/03/24 12:34:40.369
STEP: watching 01/03/24 12:34:40.392
Jan  3 12:34:40.392: INFO: starting watch
STEP: patching 01/03/24 12:34:40.405
STEP: updating 01/03/24 12:34:40.431
Jan  3 12:34:40.453: INFO: waiting for watch events with expected annotations
Jan  3 12:34:40.453: INFO: saw patched and updated annotations
STEP: getting /approval 01/03/24 12:34:40.454
STEP: patching /approval 01/03/24 12:34:40.472
STEP: updating /approval 01/03/24 12:34:40.499
STEP: getting /status 01/03/24 12:34:40.523
STEP: patching /status 01/03/24 12:34:40.541
STEP: updating /status 01/03/24 12:34:40.566
STEP: deleting 01/03/24 12:34:40.59
STEP: deleting a collection 01/03/24 12:34:40.654
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:34:40.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-6656" for this suite. 01/03/24 12:34:40.741
------------------------------
• [1.517 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:34:39.289
    Jan  3 12:34:39.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename certificates 01/03/24 12:34:39.291
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:34:39.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:34:39.369
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/03/24 12:34:40.181
    STEP: getting /apis/certificates.k8s.io 01/03/24 12:34:40.206
    STEP: getting /apis/certificates.k8s.io/v1 01/03/24 12:34:40.22
    STEP: creating 01/03/24 12:34:40.234
    STEP: getting 01/03/24 12:34:40.299
    STEP: listing 01/03/24 12:34:40.369
    STEP: watching 01/03/24 12:34:40.392
    Jan  3 12:34:40.392: INFO: starting watch
    STEP: patching 01/03/24 12:34:40.405
    STEP: updating 01/03/24 12:34:40.431
    Jan  3 12:34:40.453: INFO: waiting for watch events with expected annotations
    Jan  3 12:34:40.453: INFO: saw patched and updated annotations
    STEP: getting /approval 01/03/24 12:34:40.454
    STEP: patching /approval 01/03/24 12:34:40.472
    STEP: updating /approval 01/03/24 12:34:40.499
    STEP: getting /status 01/03/24 12:34:40.523
    STEP: patching /status 01/03/24 12:34:40.541
    STEP: updating /status 01/03/24 12:34:40.566
    STEP: deleting 01/03/24 12:34:40.59
    STEP: deleting a collection 01/03/24 12:34:40.654
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:34:40.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-6656" for this suite. 01/03/24 12:34:40.741
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:34:40.806
Jan  3 12:34:40.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 12:34:40.809
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:34:40.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:34:40.903
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jan  3 12:34:40.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/03/24 12:34:43.58
Jan  3 12:34:43.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 create -f -'
Jan  3 12:34:45.409: INFO: stderr: ""
Jan  3 12:34:45.409: INFO: stdout: "e2e-test-crd-publish-openapi-7687-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan  3 12:34:45.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 delete e2e-test-crd-publish-openapi-7687-crds test-foo'
Jan  3 12:34:45.657: INFO: stderr: ""
Jan  3 12:34:45.657: INFO: stdout: "e2e-test-crd-publish-openapi-7687-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan  3 12:34:45.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 apply -f -'
Jan  3 12:34:45.997: INFO: stderr: ""
Jan  3 12:34:45.997: INFO: stdout: "e2e-test-crd-publish-openapi-7687-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan  3 12:34:45.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 delete e2e-test-crd-publish-openapi-7687-crds test-foo'
Jan  3 12:34:46.162: INFO: stderr: ""
Jan  3 12:34:46.162: INFO: stdout: "e2e-test-crd-publish-openapi-7687-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/03/24 12:34:46.162
Jan  3 12:34:46.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 create -f -'
Jan  3 12:34:46.760: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/03/24 12:34:46.76
Jan  3 12:34:46.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 create -f -'
Jan  3 12:34:47.477: INFO: rc: 1
Jan  3 12:34:47.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 apply -f -'
Jan  3 12:34:48.458: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/03/24 12:34:48.458
Jan  3 12:34:48.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 create -f -'
Jan  3 12:34:49.328: INFO: rc: 1
Jan  3 12:34:49.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 apply -f -'
Jan  3 12:34:50.015: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/03/24 12:34:50.015
Jan  3 12:34:50.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 explain e2e-test-crd-publish-openapi-7687-crds'
Jan  3 12:34:50.754: INFO: stderr: ""
Jan  3 12:34:50.754: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7687-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/03/24 12:34:50.754
Jan  3 12:34:50.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 explain e2e-test-crd-publish-openapi-7687-crds.metadata'
Jan  3 12:34:51.339: INFO: stderr: ""
Jan  3 12:34:51.339: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7687-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan  3 12:34:51.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 explain e2e-test-crd-publish-openapi-7687-crds.spec'
Jan  3 12:34:52.072: INFO: stderr: ""
Jan  3 12:34:52.072: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7687-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan  3 12:34:52.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 explain e2e-test-crd-publish-openapi-7687-crds.spec.bars'
Jan  3 12:34:52.792: INFO: stderr: ""
Jan  3 12:34:52.792: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7687-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/03/24 12:34:52.793
Jan  3 12:34:52.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 explain e2e-test-crd-publish-openapi-7687-crds.spec.bars2'
Jan  3 12:34:53.524: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:34:56.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3434" for this suite. 01/03/24 12:34:56.268
------------------------------
• [SLOW TEST] [15.488 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:34:40.806
    Jan  3 12:34:40.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 12:34:40.809
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:34:40.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:34:40.903
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jan  3 12:34:40.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/03/24 12:34:43.58
    Jan  3 12:34:43.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 create -f -'
    Jan  3 12:34:45.409: INFO: stderr: ""
    Jan  3 12:34:45.409: INFO: stdout: "e2e-test-crd-publish-openapi-7687-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan  3 12:34:45.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 delete e2e-test-crd-publish-openapi-7687-crds test-foo'
    Jan  3 12:34:45.657: INFO: stderr: ""
    Jan  3 12:34:45.657: INFO: stdout: "e2e-test-crd-publish-openapi-7687-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan  3 12:34:45.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 apply -f -'
    Jan  3 12:34:45.997: INFO: stderr: ""
    Jan  3 12:34:45.997: INFO: stdout: "e2e-test-crd-publish-openapi-7687-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan  3 12:34:45.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 delete e2e-test-crd-publish-openapi-7687-crds test-foo'
    Jan  3 12:34:46.162: INFO: stderr: ""
    Jan  3 12:34:46.162: INFO: stdout: "e2e-test-crd-publish-openapi-7687-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/03/24 12:34:46.162
    Jan  3 12:34:46.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 create -f -'
    Jan  3 12:34:46.760: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/03/24 12:34:46.76
    Jan  3 12:34:46.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 create -f -'
    Jan  3 12:34:47.477: INFO: rc: 1
    Jan  3 12:34:47.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 apply -f -'
    Jan  3 12:34:48.458: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/03/24 12:34:48.458
    Jan  3 12:34:48.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 create -f -'
    Jan  3 12:34:49.328: INFO: rc: 1
    Jan  3 12:34:49.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 --namespace=crd-publish-openapi-3434 apply -f -'
    Jan  3 12:34:50.015: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/03/24 12:34:50.015
    Jan  3 12:34:50.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 explain e2e-test-crd-publish-openapi-7687-crds'
    Jan  3 12:34:50.754: INFO: stderr: ""
    Jan  3 12:34:50.754: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7687-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/03/24 12:34:50.754
    Jan  3 12:34:50.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 explain e2e-test-crd-publish-openapi-7687-crds.metadata'
    Jan  3 12:34:51.339: INFO: stderr: ""
    Jan  3 12:34:51.339: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7687-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan  3 12:34:51.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 explain e2e-test-crd-publish-openapi-7687-crds.spec'
    Jan  3 12:34:52.072: INFO: stderr: ""
    Jan  3 12:34:52.072: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7687-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan  3 12:34:52.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 explain e2e-test-crd-publish-openapi-7687-crds.spec.bars'
    Jan  3 12:34:52.792: INFO: stderr: ""
    Jan  3 12:34:52.792: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7687-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/03/24 12:34:52.793
    Jan  3 12:34:52.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=crd-publish-openapi-3434 explain e2e-test-crd-publish-openapi-7687-crds.spec.bars2'
    Jan  3 12:34:53.524: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:34:56.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3434" for this suite. 01/03/24 12:34:56.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:34:56.299
Jan  3 12:34:56.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 12:34:56.303
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:34:56.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:34:56.4
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 01/03/24 12:34:56.428
Jan  3 12:34:56.454: INFO: Waiting up to 5m0s for pod "pod-066139d8-bee5-4700-9e96-717bb682d41f" in namespace "emptydir-895" to be "Succeeded or Failed"
Jan  3 12:34:56.476: INFO: Pod "pod-066139d8-bee5-4700-9e96-717bb682d41f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.678667ms
Jan  3 12:34:58.496: INFO: Pod "pod-066139d8-bee5-4700-9e96-717bb682d41f": Phase="Running", Reason="", readiness=true. Elapsed: 2.041692864s
Jan  3 12:35:00.496: INFO: Pod "pod-066139d8-bee5-4700-9e96-717bb682d41f": Phase="Running", Reason="", readiness=false. Elapsed: 4.041270997s
Jan  3 12:35:02.498: INFO: Pod "pod-066139d8-bee5-4700-9e96-717bb682d41f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043709141s
STEP: Saw pod success 01/03/24 12:35:02.498
Jan  3 12:35:02.498: INFO: Pod "pod-066139d8-bee5-4700-9e96-717bb682d41f" satisfied condition "Succeeded or Failed"
Jan  3 12:35:02.518: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-066139d8-bee5-4700-9e96-717bb682d41f container test-container: <nil>
STEP: delete the pod 01/03/24 12:35:02.737
Jan  3 12:35:02.779: INFO: Waiting for pod pod-066139d8-bee5-4700-9e96-717bb682d41f to disappear
Jan  3 12:35:02.798: INFO: Pod pod-066139d8-bee5-4700-9e96-717bb682d41f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:35:02.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-895" for this suite. 01/03/24 12:35:02.832
------------------------------
• [SLOW TEST] [6.559 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:34:56.299
    Jan  3 12:34:56.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 12:34:56.303
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:34:56.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:34:56.4
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/03/24 12:34:56.428
    Jan  3 12:34:56.454: INFO: Waiting up to 5m0s for pod "pod-066139d8-bee5-4700-9e96-717bb682d41f" in namespace "emptydir-895" to be "Succeeded or Failed"
    Jan  3 12:34:56.476: INFO: Pod "pod-066139d8-bee5-4700-9e96-717bb682d41f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.678667ms
    Jan  3 12:34:58.496: INFO: Pod "pod-066139d8-bee5-4700-9e96-717bb682d41f": Phase="Running", Reason="", readiness=true. Elapsed: 2.041692864s
    Jan  3 12:35:00.496: INFO: Pod "pod-066139d8-bee5-4700-9e96-717bb682d41f": Phase="Running", Reason="", readiness=false. Elapsed: 4.041270997s
    Jan  3 12:35:02.498: INFO: Pod "pod-066139d8-bee5-4700-9e96-717bb682d41f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043709141s
    STEP: Saw pod success 01/03/24 12:35:02.498
    Jan  3 12:35:02.498: INFO: Pod "pod-066139d8-bee5-4700-9e96-717bb682d41f" satisfied condition "Succeeded or Failed"
    Jan  3 12:35:02.518: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-066139d8-bee5-4700-9e96-717bb682d41f container test-container: <nil>
    STEP: delete the pod 01/03/24 12:35:02.737
    Jan  3 12:35:02.779: INFO: Waiting for pod pod-066139d8-bee5-4700-9e96-717bb682d41f to disappear
    Jan  3 12:35:02.798: INFO: Pod pod-066139d8-bee5-4700-9e96-717bb682d41f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:35:02.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-895" for this suite. 01/03/24 12:35:02.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:35:02.862
Jan  3 12:35:02.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename resourcequota 01/03/24 12:35:02.865
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:02.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:02.949
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 01/03/24 12:35:02.977
STEP: Getting a ResourceQuota 01/03/24 12:35:02.998
STEP: Listing all ResourceQuotas with LabelSelector 01/03/24 12:35:03.016
STEP: Patching the ResourceQuota 01/03/24 12:35:03.034
STEP: Deleting a Collection of ResourceQuotas 01/03/24 12:35:03.057
STEP: Verifying the deleted ResourceQuota 01/03/24 12:35:03.085
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  3 12:35:03.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1064" for this suite. 01/03/24 12:35:03.127
------------------------------
• [0.297 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:35:02.862
    Jan  3 12:35:02.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename resourcequota 01/03/24 12:35:02.865
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:02.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:02.949
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 01/03/24 12:35:02.977
    STEP: Getting a ResourceQuota 01/03/24 12:35:02.998
    STEP: Listing all ResourceQuotas with LabelSelector 01/03/24 12:35:03.016
    STEP: Patching the ResourceQuota 01/03/24 12:35:03.034
    STEP: Deleting a Collection of ResourceQuotas 01/03/24 12:35:03.057
    STEP: Verifying the deleted ResourceQuota 01/03/24 12:35:03.085
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:35:03.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1064" for this suite. 01/03/24 12:35:03.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:35:03.163
Jan  3 12:35:03.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 12:35:03.165
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:03.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:03.249
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 12:35:03.324
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 12:35:03.548
STEP: Deploying the webhook pod 01/03/24 12:35:03.573
STEP: Wait for the deployment to be ready 01/03/24 12:35:03.624
Jan  3 12:35:03.657: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 12:35:05.713: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 35, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 35, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 35, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 35, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/03/24 12:35:07.733
STEP: Verifying the service has paired with the endpoint 01/03/24 12:35:07.764
Jan  3 12:35:08.766: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jan  3 12:35:08.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6069-crds.webhook.example.com via the AdmissionRegistration API 01/03/24 12:35:09.35
STEP: Creating a custom resource that should be mutated by the webhook 01/03/24 12:35:09.494
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:35:12.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1563" for this suite. 01/03/24 12:35:12.498
STEP: Destroying namespace "webhook-1563-markers" for this suite. 01/03/24 12:35:12.522
------------------------------
• [SLOW TEST] [9.384 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:35:03.163
    Jan  3 12:35:03.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 12:35:03.165
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:03.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:03.249
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 12:35:03.324
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 12:35:03.548
    STEP: Deploying the webhook pod 01/03/24 12:35:03.573
    STEP: Wait for the deployment to be ready 01/03/24 12:35:03.624
    Jan  3 12:35:03.657: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan  3 12:35:05.713: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 12, 35, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 35, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 12, 35, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 12, 35, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/03/24 12:35:07.733
    STEP: Verifying the service has paired with the endpoint 01/03/24 12:35:07.764
    Jan  3 12:35:08.766: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jan  3 12:35:08.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6069-crds.webhook.example.com via the AdmissionRegistration API 01/03/24 12:35:09.35
    STEP: Creating a custom resource that should be mutated by the webhook 01/03/24 12:35:09.494
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:35:12.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1563" for this suite. 01/03/24 12:35:12.498
    STEP: Destroying namespace "webhook-1563-markers" for this suite. 01/03/24 12:35:12.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:35:12.55
Jan  3 12:35:12.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename secrets 01/03/24 12:35:12.551
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:12.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:12.649
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  3 12:35:12.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3092" for this suite. 01/03/24 12:35:12.882
------------------------------
• [0.371 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:35:12.55
    Jan  3 12:35:12.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename secrets 01/03/24 12:35:12.551
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:12.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:12.649
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:35:12.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3092" for this suite. 01/03/24 12:35:12.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:35:12.925
Jan  3 12:35:12.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename watch 01/03/24 12:35:12.927
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:12.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:13.008
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/03/24 12:35:13.051
STEP: starting a background goroutine to produce watch events 01/03/24 12:35:13.081
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/03/24 12:35:13.081
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  3 12:35:15.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7158" for this suite. 01/03/24 12:35:15.798
------------------------------
• [2.929 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:35:12.925
    Jan  3 12:35:12.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename watch 01/03/24 12:35:12.927
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:12.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:13.008
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/03/24 12:35:13.051
    STEP: starting a background goroutine to produce watch events 01/03/24 12:35:13.081
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/03/24 12:35:13.081
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:35:15.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7158" for this suite. 01/03/24 12:35:15.798
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:35:15.854
Jan  3 12:35:15.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename secrets 01/03/24 12:35:15.858
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:15.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:15.938
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-49aa7296-d2a5-4612-a07d-4cd65ddef361 01/03/24 12:35:15.965
STEP: Creating a pod to test consume secrets 01/03/24 12:35:15.985
Jan  3 12:35:16.013: INFO: Waiting up to 5m0s for pod "pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3" in namespace "secrets-8170" to be "Succeeded or Failed"
Jan  3 12:35:16.031: INFO: Pod "pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.89184ms
Jan  3 12:35:18.052: INFO: Pod "pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038987481s
Jan  3 12:35:20.070: INFO: Pod "pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056752935s
STEP: Saw pod success 01/03/24 12:35:20.07
Jan  3 12:35:20.070: INFO: Pod "pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3" satisfied condition "Succeeded or Failed"
Jan  3 12:35:20.113: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3 container secret-env-test: <nil>
STEP: delete the pod 01/03/24 12:35:20.171
Jan  3 12:35:20.212: INFO: Waiting for pod pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3 to disappear
Jan  3 12:35:20.230: INFO: Pod pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  3 12:35:20.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8170" for this suite. 01/03/24 12:35:20.275
------------------------------
• [4.446 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:35:15.854
    Jan  3 12:35:15.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename secrets 01/03/24 12:35:15.858
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:15.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:15.938
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-49aa7296-d2a5-4612-a07d-4cd65ddef361 01/03/24 12:35:15.965
    STEP: Creating a pod to test consume secrets 01/03/24 12:35:15.985
    Jan  3 12:35:16.013: INFO: Waiting up to 5m0s for pod "pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3" in namespace "secrets-8170" to be "Succeeded or Failed"
    Jan  3 12:35:16.031: INFO: Pod "pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.89184ms
    Jan  3 12:35:18.052: INFO: Pod "pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038987481s
    Jan  3 12:35:20.070: INFO: Pod "pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056752935s
    STEP: Saw pod success 01/03/24 12:35:20.07
    Jan  3 12:35:20.070: INFO: Pod "pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3" satisfied condition "Succeeded or Failed"
    Jan  3 12:35:20.113: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3 container secret-env-test: <nil>
    STEP: delete the pod 01/03/24 12:35:20.171
    Jan  3 12:35:20.212: INFO: Waiting for pod pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3 to disappear
    Jan  3 12:35:20.230: INFO: Pod pod-secrets-a532b234-f7dd-4a4d-b8f8-6afafa98b7d3 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:35:20.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8170" for this suite. 01/03/24 12:35:20.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:35:20.311
Jan  3 12:35:20.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename svcaccounts 01/03/24 12:35:20.312
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:20.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:20.406
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Jan  3 12:35:20.482: INFO: created pod
Jan  3 12:35:20.482: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1629" to be "Succeeded or Failed"
Jan  3 12:35:20.499: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.105081ms
Jan  3 12:35:22.519: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037024522s
Jan  3 12:35:24.522: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039855297s
Jan  3 12:35:26.519: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037312002s
STEP: Saw pod success 01/03/24 12:35:26.519
Jan  3 12:35:26.520: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan  3 12:35:56.522: INFO: polling logs
Jan  3 12:35:56.565: INFO: Pod logs: 
I0103 12:35:21.819185       1 log.go:198] OK: Got token
I0103 12:35:21.819231       1 log.go:198] validating with in-cluster discovery
I0103 12:35:21.819632       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0103 12:35:21.819668       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1629:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1704285920, NotBefore:1704285320, IssuedAt:1704285320, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1629", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"5e382626-343b-4e31-b21f-87f179de9edd"}}}
I0103 12:35:21.893620       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0103 12:35:21.986191       1 log.go:198] OK: Validated signature on JWT
I0103 12:35:21.986304       1 log.go:198] OK: Got valid claims from token!
I0103 12:35:21.986335       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1629:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1704285920, NotBefore:1704285320, IssuedAt:1704285320, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1629", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"5e382626-343b-4e31-b21f-87f179de9edd"}}}

Jan  3 12:35:56.565: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  3 12:35:56.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1629" for this suite. 01/03/24 12:35:56.622
------------------------------
• [SLOW TEST] [36.337 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:35:20.311
    Jan  3 12:35:20.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename svcaccounts 01/03/24 12:35:20.312
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:20.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:20.406
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Jan  3 12:35:20.482: INFO: created pod
    Jan  3 12:35:20.482: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1629" to be "Succeeded or Failed"
    Jan  3 12:35:20.499: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.105081ms
    Jan  3 12:35:22.519: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037024522s
    Jan  3 12:35:24.522: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039855297s
    Jan  3 12:35:26.519: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037312002s
    STEP: Saw pod success 01/03/24 12:35:26.519
    Jan  3 12:35:26.520: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan  3 12:35:56.522: INFO: polling logs
    Jan  3 12:35:56.565: INFO: Pod logs: 
    I0103 12:35:21.819185       1 log.go:198] OK: Got token
    I0103 12:35:21.819231       1 log.go:198] validating with in-cluster discovery
    I0103 12:35:21.819632       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0103 12:35:21.819668       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1629:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1704285920, NotBefore:1704285320, IssuedAt:1704285320, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1629", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"5e382626-343b-4e31-b21f-87f179de9edd"}}}
    I0103 12:35:21.893620       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0103 12:35:21.986191       1 log.go:198] OK: Validated signature on JWT
    I0103 12:35:21.986304       1 log.go:198] OK: Got valid claims from token!
    I0103 12:35:21.986335       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1629:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1704285920, NotBefore:1704285320, IssuedAt:1704285320, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1629", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"5e382626-343b-4e31-b21f-87f179de9edd"}}}

    Jan  3 12:35:56.565: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:35:56.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1629" for this suite. 01/03/24 12:35:56.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:35:56.653
Jan  3 12:35:56.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename dns 01/03/24 12:35:56.655
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:56.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:56.745
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/03/24 12:35:56.772
Jan  3 12:35:56.799: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9199  455757c5-e993-4b84-aecd-ac1585a4db1d 33981113414 0 2024-01-03 12:35:56 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2024-01-03 12:35:56 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kk42r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kk42r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:35:56.802: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9199" to be "running and ready"
Jan  3 12:35:56.821: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 18.997306ms
Jan  3 12:35:56.821: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:35:58.840: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.03796567s
Jan  3 12:35:58.840: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan  3 12:35:58.840: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/03/24 12:35:58.84
Jan  3 12:35:58.841: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9199 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:35:58.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:35:58.843: INFO: ExecWithOptions: Clientset creation
Jan  3 12:35:58.843: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-9199/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/03/24 12:35:59.2
Jan  3 12:35:59.200: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9199 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:35:59.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:35:59.202: INFO: ExecWithOptions: Clientset creation
Jan  3 12:35:59.202: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-9199/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  3 12:35:59.511: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  3 12:35:59.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9199" for this suite. 01/03/24 12:35:59.583
------------------------------
• [2.956 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:35:56.653
    Jan  3 12:35:56.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename dns 01/03/24 12:35:56.655
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:56.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:56.745
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/03/24 12:35:56.772
    Jan  3 12:35:56.799: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9199  455757c5-e993-4b84-aecd-ac1585a4db1d 33981113414 0 2024-01-03 12:35:56 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2024-01-03 12:35:56 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kk42r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kk42r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:35:56.802: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9199" to be "running and ready"
    Jan  3 12:35:56.821: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 18.997306ms
    Jan  3 12:35:56.821: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:35:58.840: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.03796567s
    Jan  3 12:35:58.840: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan  3 12:35:58.840: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/03/24 12:35:58.84
    Jan  3 12:35:58.841: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9199 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:35:58.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:35:58.843: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:35:58.843: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-9199/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/03/24 12:35:59.2
    Jan  3 12:35:59.200: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9199 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:35:59.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:35:59.202: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:35:59.202: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-9199/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  3 12:35:59.511: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:35:59.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9199" for this suite. 01/03/24 12:35:59.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:35:59.612
Jan  3 12:35:59.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 12:35:59.613
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:59.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:59.693
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/03/24 12:35:59.721
Jan  3 12:35:59.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:36:02.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:36:12.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3084" for this suite. 01/03/24 12:36:12.996
------------------------------
• [SLOW TEST] [13.413 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:35:59.612
    Jan  3 12:35:59.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 12:35:59.613
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:35:59.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:35:59.693
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/03/24 12:35:59.721
    Jan  3 12:35:59.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:36:02.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:36:12.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3084" for this suite. 01/03/24 12:36:12.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:36:13.032
Jan  3 12:36:13.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pods 01/03/24 12:36:13.033
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:36:13.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:36:13.116
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 01/03/24 12:36:13.145
STEP: submitting the pod to kubernetes 01/03/24 12:36:13.146
Jan  3 12:36:13.180: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f" in namespace "pods-3163" to be "running and ready"
Jan  3 12:36:13.198: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.721565ms
Jan  3 12:36:13.198: INFO: The phase of Pod pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:36:15.226: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.045929769s
Jan  3 12:36:15.226: INFO: The phase of Pod pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f is Running (Ready = true)
Jan  3 12:36:15.226: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/03/24 12:36:15.244
STEP: updating the pod 01/03/24 12:36:15.263
Jan  3 12:36:15.812: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f"
Jan  3 12:36:15.812: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f" in namespace "pods-3163" to be "terminated with reason DeadlineExceeded"
Jan  3 12:36:15.832: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f": Phase="Running", Reason="", readiness=true. Elapsed: 20.40815ms
Jan  3 12:36:17.858: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.046229924s
Jan  3 12:36:19.852: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f": Phase="Running", Reason="", readiness=false. Elapsed: 4.040116062s
Jan  3 12:36:21.851: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.039402231s
Jan  3 12:36:21.851: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  3 12:36:21.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3163" for this suite. 01/03/24 12:36:21.882
------------------------------
• [SLOW TEST] [8.876 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:36:13.032
    Jan  3 12:36:13.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pods 01/03/24 12:36:13.033
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:36:13.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:36:13.116
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 01/03/24 12:36:13.145
    STEP: submitting the pod to kubernetes 01/03/24 12:36:13.146
    Jan  3 12:36:13.180: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f" in namespace "pods-3163" to be "running and ready"
    Jan  3 12:36:13.198: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.721565ms
    Jan  3 12:36:13.198: INFO: The phase of Pod pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:36:15.226: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.045929769s
    Jan  3 12:36:15.226: INFO: The phase of Pod pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f is Running (Ready = true)
    Jan  3 12:36:15.226: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/03/24 12:36:15.244
    STEP: updating the pod 01/03/24 12:36:15.263
    Jan  3 12:36:15.812: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f"
    Jan  3 12:36:15.812: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f" in namespace "pods-3163" to be "terminated with reason DeadlineExceeded"
    Jan  3 12:36:15.832: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f": Phase="Running", Reason="", readiness=true. Elapsed: 20.40815ms
    Jan  3 12:36:17.858: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.046229924s
    Jan  3 12:36:19.852: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f": Phase="Running", Reason="", readiness=false. Elapsed: 4.040116062s
    Jan  3 12:36:21.851: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.039402231s
    Jan  3 12:36:21.851: INFO: Pod "pod-update-activedeadlineseconds-c5485f35-19d4-4467-bdd7-7971f84e4d9f" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:36:21.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3163" for this suite. 01/03/24 12:36:21.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:36:21.911
Jan  3 12:36:21.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 12:36:21.913
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:36:21.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:36:21.996
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 01/03/24 12:36:22.027
Jan  3 12:36:22.062: INFO: Waiting up to 5m0s for pod "pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f" in namespace "emptydir-8303" to be "Succeeded or Failed"
Jan  3 12:36:22.082: INFO: Pod "pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.309301ms
Jan  3 12:36:24.111: INFO: Pod "pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048782574s
Jan  3 12:36:26.101: INFO: Pod "pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038871229s
STEP: Saw pod success 01/03/24 12:36:26.101
Jan  3 12:36:26.102: INFO: Pod "pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f" satisfied condition "Succeeded or Failed"
Jan  3 12:36:26.122: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f container test-container: <nil>
STEP: delete the pod 01/03/24 12:36:26.288
Jan  3 12:36:26.341: INFO: Waiting for pod pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f to disappear
Jan  3 12:36:26.362: INFO: Pod pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:36:26.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8303" for this suite. 01/03/24 12:36:26.393
------------------------------
• [4.508 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:36:21.911
    Jan  3 12:36:21.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 12:36:21.913
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:36:21.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:36:21.996
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/03/24 12:36:22.027
    Jan  3 12:36:22.062: INFO: Waiting up to 5m0s for pod "pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f" in namespace "emptydir-8303" to be "Succeeded or Failed"
    Jan  3 12:36:22.082: INFO: Pod "pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.309301ms
    Jan  3 12:36:24.111: INFO: Pod "pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048782574s
    Jan  3 12:36:26.101: INFO: Pod "pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038871229s
    STEP: Saw pod success 01/03/24 12:36:26.101
    Jan  3 12:36:26.102: INFO: Pod "pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f" satisfied condition "Succeeded or Failed"
    Jan  3 12:36:26.122: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f container test-container: <nil>
    STEP: delete the pod 01/03/24 12:36:26.288
    Jan  3 12:36:26.341: INFO: Waiting for pod pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f to disappear
    Jan  3 12:36:26.362: INFO: Pod pod-3fe08517-bdd6-4b5a-8cd7-50a7ed9cb23f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:36:26.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8303" for this suite. 01/03/24 12:36:26.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:36:26.423
Jan  3 12:36:26.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pods 01/03/24 12:36:26.425
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:36:26.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:36:26.507
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 01/03/24 12:36:26.562
STEP: watching for Pod to be ready 01/03/24 12:36:26.591
Jan  3 12:36:26.605: INFO: observed Pod pod-test in namespace pods-2744 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan  3 12:36:26.616: INFO: observed Pod pod-test in namespace pods-2744 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC  }]
Jan  3 12:36:26.666: INFO: observed Pod pod-test in namespace pods-2744 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC  }]
Jan  3 12:36:27.756: INFO: observed Pod pod-test in namespace pods-2744 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC  }]
Jan  3 12:36:28.834: INFO: Found Pod pod-test in namespace pods-2744 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/03/24 12:36:28.854
STEP: getting the Pod and ensuring that it's patched 01/03/24 12:36:28.892
STEP: replacing the Pod's status Ready condition to False 01/03/24 12:36:28.91
STEP: check the Pod again to ensure its Ready conditions are False 01/03/24 12:36:28.956
STEP: deleting the Pod via a Collection with a LabelSelector 01/03/24 12:36:28.956
STEP: watching for the Pod to be deleted 01/03/24 12:36:28.985
Jan  3 12:36:29.000: INFO: observed event type MODIFIED
Jan  3 12:36:30.841: INFO: observed event type MODIFIED
Jan  3 12:36:31.589: INFO: observed event type MODIFIED
Jan  3 12:36:32.846: INFO: observed event type MODIFIED
Jan  3 12:36:32.875: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  3 12:36:32.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2744" for this suite. 01/03/24 12:36:32.939
------------------------------
• [SLOW TEST] [6.543 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:36:26.423
    Jan  3 12:36:26.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pods 01/03/24 12:36:26.425
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:36:26.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:36:26.507
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 01/03/24 12:36:26.562
    STEP: watching for Pod to be ready 01/03/24 12:36:26.591
    Jan  3 12:36:26.605: INFO: observed Pod pod-test in namespace pods-2744 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan  3 12:36:26.616: INFO: observed Pod pod-test in namespace pods-2744 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC  }]
    Jan  3 12:36:26.666: INFO: observed Pod pod-test in namespace pods-2744 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC  }]
    Jan  3 12:36:27.756: INFO: observed Pod pod-test in namespace pods-2744 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC  }]
    Jan  3 12:36:28.834: INFO: Found Pod pod-test in namespace pods-2744 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 12:36:26 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/03/24 12:36:28.854
    STEP: getting the Pod and ensuring that it's patched 01/03/24 12:36:28.892
    STEP: replacing the Pod's status Ready condition to False 01/03/24 12:36:28.91
    STEP: check the Pod again to ensure its Ready conditions are False 01/03/24 12:36:28.956
    STEP: deleting the Pod via a Collection with a LabelSelector 01/03/24 12:36:28.956
    STEP: watching for the Pod to be deleted 01/03/24 12:36:28.985
    Jan  3 12:36:29.000: INFO: observed event type MODIFIED
    Jan  3 12:36:30.841: INFO: observed event type MODIFIED
    Jan  3 12:36:31.589: INFO: observed event type MODIFIED
    Jan  3 12:36:32.846: INFO: observed event type MODIFIED
    Jan  3 12:36:32.875: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:36:32.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2744" for this suite. 01/03/24 12:36:32.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:36:32.969
Jan  3 12:36:32.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 12:36:32.971
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:36:33.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:36:33.058
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 01/03/24 12:36:33.087
Jan  3 12:36:33.117: INFO: Waiting up to 5m0s for pod "downward-api-7c35c456-1825-42a3-832f-7f5a444125ba" in namespace "downward-api-409" to be "Succeeded or Failed"
Jan  3 12:36:33.135: INFO: Pod "downward-api-7c35c456-1825-42a3-832f-7f5a444125ba": Phase="Pending", Reason="", readiness=false. Elapsed: 17.268174ms
Jan  3 12:36:35.154: INFO: Pod "downward-api-7c35c456-1825-42a3-832f-7f5a444125ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036552548s
Jan  3 12:36:37.155: INFO: Pod "downward-api-7c35c456-1825-42a3-832f-7f5a444125ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037603077s
STEP: Saw pod success 01/03/24 12:36:37.155
Jan  3 12:36:37.156: INFO: Pod "downward-api-7c35c456-1825-42a3-832f-7f5a444125ba" satisfied condition "Succeeded or Failed"
Jan  3 12:36:37.174: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downward-api-7c35c456-1825-42a3-832f-7f5a444125ba container dapi-container: <nil>
STEP: delete the pod 01/03/24 12:36:37.219
Jan  3 12:36:37.259: INFO: Waiting for pod downward-api-7c35c456-1825-42a3-832f-7f5a444125ba to disappear
Jan  3 12:36:37.277: INFO: Pod downward-api-7c35c456-1825-42a3-832f-7f5a444125ba no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  3 12:36:37.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-409" for this suite. 01/03/24 12:36:37.313
------------------------------
• [4.370 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:36:32.969
    Jan  3 12:36:32.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 12:36:32.971
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:36:33.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:36:33.058
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 01/03/24 12:36:33.087
    Jan  3 12:36:33.117: INFO: Waiting up to 5m0s for pod "downward-api-7c35c456-1825-42a3-832f-7f5a444125ba" in namespace "downward-api-409" to be "Succeeded or Failed"
    Jan  3 12:36:33.135: INFO: Pod "downward-api-7c35c456-1825-42a3-832f-7f5a444125ba": Phase="Pending", Reason="", readiness=false. Elapsed: 17.268174ms
    Jan  3 12:36:35.154: INFO: Pod "downward-api-7c35c456-1825-42a3-832f-7f5a444125ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036552548s
    Jan  3 12:36:37.155: INFO: Pod "downward-api-7c35c456-1825-42a3-832f-7f5a444125ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037603077s
    STEP: Saw pod success 01/03/24 12:36:37.155
    Jan  3 12:36:37.156: INFO: Pod "downward-api-7c35c456-1825-42a3-832f-7f5a444125ba" satisfied condition "Succeeded or Failed"
    Jan  3 12:36:37.174: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downward-api-7c35c456-1825-42a3-832f-7f5a444125ba container dapi-container: <nil>
    STEP: delete the pod 01/03/24 12:36:37.219
    Jan  3 12:36:37.259: INFO: Waiting for pod downward-api-7c35c456-1825-42a3-832f-7f5a444125ba to disappear
    Jan  3 12:36:37.277: INFO: Pod downward-api-7c35c456-1825-42a3-832f-7f5a444125ba no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:36:37.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-409" for this suite. 01/03/24 12:36:37.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:36:37.342
Jan  3 12:36:37.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 12:36:37.344
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:36:37.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:36:37.425
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-a8f99660-1572-4e55-aca1-aa0420bc9cac 01/03/24 12:36:37.453
STEP: Creating a pod to test consume configMaps 01/03/24 12:36:37.472
Jan  3 12:36:37.502: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a" in namespace "configmap-8294" to be "Succeeded or Failed"
Jan  3 12:36:37.523: INFO: Pod "pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.129744ms
Jan  3 12:36:39.545: INFO: Pod "pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a": Phase="Running", Reason="", readiness=true. Elapsed: 2.042689204s
Jan  3 12:36:41.545: INFO: Pod "pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a": Phase="Running", Reason="", readiness=false. Elapsed: 4.043137675s
Jan  3 12:36:43.548: INFO: Pod "pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045749596s
STEP: Saw pod success 01/03/24 12:36:43.548
Jan  3 12:36:43.548: INFO: Pod "pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a" satisfied condition "Succeeded or Failed"
Jan  3 12:36:43.567: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a container agnhost-container: <nil>
STEP: delete the pod 01/03/24 12:36:43.605
Jan  3 12:36:43.645: INFO: Waiting for pod pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a to disappear
Jan  3 12:36:43.662: INFO: Pod pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 12:36:43.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8294" for this suite. 01/03/24 12:36:43.693
------------------------------
• [SLOW TEST] [6.378 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:36:37.342
    Jan  3 12:36:37.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 12:36:37.344
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:36:37.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:36:37.425
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-a8f99660-1572-4e55-aca1-aa0420bc9cac 01/03/24 12:36:37.453
    STEP: Creating a pod to test consume configMaps 01/03/24 12:36:37.472
    Jan  3 12:36:37.502: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a" in namespace "configmap-8294" to be "Succeeded or Failed"
    Jan  3 12:36:37.523: INFO: Pod "pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.129744ms
    Jan  3 12:36:39.545: INFO: Pod "pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a": Phase="Running", Reason="", readiness=true. Elapsed: 2.042689204s
    Jan  3 12:36:41.545: INFO: Pod "pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a": Phase="Running", Reason="", readiness=false. Elapsed: 4.043137675s
    Jan  3 12:36:43.548: INFO: Pod "pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045749596s
    STEP: Saw pod success 01/03/24 12:36:43.548
    Jan  3 12:36:43.548: INFO: Pod "pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a" satisfied condition "Succeeded or Failed"
    Jan  3 12:36:43.567: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 12:36:43.605
    Jan  3 12:36:43.645: INFO: Waiting for pod pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a to disappear
    Jan  3 12:36:43.662: INFO: Pod pod-configmaps-5a508b86-962f-4a1b-9bd5-53e2a1e1b09a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:36:43.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8294" for this suite. 01/03/24 12:36:43.693
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:36:43.719
Jan  3 12:36:43.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename taint-single-pod 01/03/24 12:36:43.72
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:36:43.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:36:43.813
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jan  3 12:36:43.841: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  3 12:37:43.974: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jan  3 12:37:43.993: INFO: Starting informer...
STEP: Starting pod... 01/03/24 12:37:43.993
Jan  3 12:37:44.046: INFO: Pod is running on jb-1-26-np-64kerjapxk. Tainting Node
STEP: Trying to apply a taint on the Node 01/03/24 12:37:44.046
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/24 12:37:44.09
STEP: Waiting short time to make sure Pod is queued for deletion 01/03/24 12:37:44.107
Jan  3 12:37:44.107: INFO: Pod wasn't evicted. Proceeding
Jan  3 12:37:44.107: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/24 12:37:44.15
STEP: Waiting some time to make sure that toleration time passed. 01/03/24 12:37:44.169
Jan  3 12:38:59.169: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:38:59.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-602" for this suite. 01/03/24 12:38:59.201
------------------------------
• [SLOW TEST] [135.506 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:36:43.719
    Jan  3 12:36:43.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename taint-single-pod 01/03/24 12:36:43.72
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:36:43.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:36:43.813
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jan  3 12:36:43.841: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  3 12:37:43.974: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jan  3 12:37:43.993: INFO: Starting informer...
    STEP: Starting pod... 01/03/24 12:37:43.993
    Jan  3 12:37:44.046: INFO: Pod is running on jb-1-26-np-64kerjapxk. Tainting Node
    STEP: Trying to apply a taint on the Node 01/03/24 12:37:44.046
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/24 12:37:44.09
    STEP: Waiting short time to make sure Pod is queued for deletion 01/03/24 12:37:44.107
    Jan  3 12:37:44.107: INFO: Pod wasn't evicted. Proceeding
    Jan  3 12:37:44.107: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/24 12:37:44.15
    STEP: Waiting some time to make sure that toleration time passed. 01/03/24 12:37:44.169
    Jan  3 12:38:59.169: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:38:59.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-602" for this suite. 01/03/24 12:38:59.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:38:59.234
Jan  3 12:38:59.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename daemonsets 01/03/24 12:38:59.237
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:38:59.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:38:59.325
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
STEP: Creating simple DaemonSet "daemon-set" 01/03/24 12:38:59.477
STEP: Check that daemon pods launch on every node of the cluster. 01/03/24 12:38:59.501
Jan  3 12:38:59.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:38:59.543: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:39:00.595: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:39:00.595: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:39:01.596: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 12:39:01.596: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 01/03/24 12:39:01.621
Jan  3 12:39:01.645: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/03/24 12:39:01.645
Jan  3 12:39:01.686: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/03/24 12:39:01.686
Jan  3 12:39:01.703: INFO: Observed &DaemonSet event: ADDED
Jan  3 12:39:01.704: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 12:39:01.704: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 12:39:01.705: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 12:39:01.705: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 12:39:01.705: INFO: Found daemon set daemon-set in namespace daemonsets-7015 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  3 12:39:01.705: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/03/24 12:39:01.705
STEP: watching for the daemon set status to be patched 01/03/24 12:39:01.729
Jan  3 12:39:01.745: INFO: Observed &DaemonSet event: ADDED
Jan  3 12:39:01.745: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 12:39:01.746: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 12:39:01.747: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 12:39:01.747: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 12:39:01.747: INFO: Observed daemon set daemon-set in namespace daemonsets-7015 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  3 12:39:01.748: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 12:39:01.748: INFO: Found daemon set daemon-set in namespace daemonsets-7015 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan  3 12:39:01.748: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 01/03/24 12:39:01.772
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7015, will wait for the garbage collector to delete the pods 01/03/24 12:39:01.772
Jan  3 12:39:01.887: INFO: Deleting DaemonSet.extensions daemon-set took: 23.184145ms
Jan  3 12:39:01.988: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.247843ms
Jan  3 12:39:05.412: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:39:05.413: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  3 12:39:05.431: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33981135766"},"items":null}

Jan  3 12:39:05.449: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33981135772"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:39:05.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7015" for this suite. 01/03/24 12:39:05.559
------------------------------
• [SLOW TEST] [6.348 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:38:59.234
    Jan  3 12:38:59.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename daemonsets 01/03/24 12:38:59.237
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:38:59.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:38:59.325
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:873
    STEP: Creating simple DaemonSet "daemon-set" 01/03/24 12:38:59.477
    STEP: Check that daemon pods launch on every node of the cluster. 01/03/24 12:38:59.501
    Jan  3 12:38:59.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:38:59.543: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:39:00.595: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:39:00.595: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:39:01.596: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 12:39:01.596: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 01/03/24 12:39:01.621
    Jan  3 12:39:01.645: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/03/24 12:39:01.645
    Jan  3 12:39:01.686: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/03/24 12:39:01.686
    Jan  3 12:39:01.703: INFO: Observed &DaemonSet event: ADDED
    Jan  3 12:39:01.704: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 12:39:01.704: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 12:39:01.705: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 12:39:01.705: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 12:39:01.705: INFO: Found daemon set daemon-set in namespace daemonsets-7015 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  3 12:39:01.705: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/03/24 12:39:01.705
    STEP: watching for the daemon set status to be patched 01/03/24 12:39:01.729
    Jan  3 12:39:01.745: INFO: Observed &DaemonSet event: ADDED
    Jan  3 12:39:01.745: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 12:39:01.746: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 12:39:01.747: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 12:39:01.747: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 12:39:01.747: INFO: Observed daemon set daemon-set in namespace daemonsets-7015 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  3 12:39:01.748: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 12:39:01.748: INFO: Found daemon set daemon-set in namespace daemonsets-7015 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan  3 12:39:01.748: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 01/03/24 12:39:01.772
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7015, will wait for the garbage collector to delete the pods 01/03/24 12:39:01.772
    Jan  3 12:39:01.887: INFO: Deleting DaemonSet.extensions daemon-set took: 23.184145ms
    Jan  3 12:39:01.988: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.247843ms
    Jan  3 12:39:05.412: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:39:05.413: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  3 12:39:05.431: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33981135766"},"items":null}

    Jan  3 12:39:05.449: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33981135772"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:39:05.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7015" for this suite. 01/03/24 12:39:05.559
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:39:05.583
Jan  3 12:39:05.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 12:39:05.586
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:39:05.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:39:05.686
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-350c8fd3-6f66-42db-b285-ebf0a673b510 01/03/24 12:39:05.716
STEP: Creating a pod to test consume configMaps 01/03/24 12:39:05.736
Jan  3 12:39:05.765: INFO: Waiting up to 5m0s for pod "pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22" in namespace "configmap-3486" to be "Succeeded or Failed"
Jan  3 12:39:05.782: INFO: Pod "pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22": Phase="Pending", Reason="", readiness=false. Elapsed: 17.326192ms
Jan  3 12:39:07.803: INFO: Pod "pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22": Phase="Running", Reason="", readiness=true. Elapsed: 2.038660783s
Jan  3 12:39:09.805: INFO: Pod "pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22": Phase="Running", Reason="", readiness=false. Elapsed: 4.040437342s
Jan  3 12:39:11.805: INFO: Pod "pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039999989s
STEP: Saw pod success 01/03/24 12:39:11.805
Jan  3 12:39:11.805: INFO: Pod "pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22" satisfied condition "Succeeded or Failed"
Jan  3 12:39:11.824: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22 container agnhost-container: <nil>
STEP: delete the pod 01/03/24 12:39:11.987
Jan  3 12:39:12.033: INFO: Waiting for pod pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22 to disappear
Jan  3 12:39:12.052: INFO: Pod pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 12:39:12.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3486" for this suite. 01/03/24 12:39:12.084
------------------------------
• [SLOW TEST] [6.530 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:39:05.583
    Jan  3 12:39:05.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 12:39:05.586
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:39:05.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:39:05.686
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-350c8fd3-6f66-42db-b285-ebf0a673b510 01/03/24 12:39:05.716
    STEP: Creating a pod to test consume configMaps 01/03/24 12:39:05.736
    Jan  3 12:39:05.765: INFO: Waiting up to 5m0s for pod "pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22" in namespace "configmap-3486" to be "Succeeded or Failed"
    Jan  3 12:39:05.782: INFO: Pod "pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22": Phase="Pending", Reason="", readiness=false. Elapsed: 17.326192ms
    Jan  3 12:39:07.803: INFO: Pod "pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22": Phase="Running", Reason="", readiness=true. Elapsed: 2.038660783s
    Jan  3 12:39:09.805: INFO: Pod "pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22": Phase="Running", Reason="", readiness=false. Elapsed: 4.040437342s
    Jan  3 12:39:11.805: INFO: Pod "pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039999989s
    STEP: Saw pod success 01/03/24 12:39:11.805
    Jan  3 12:39:11.805: INFO: Pod "pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22" satisfied condition "Succeeded or Failed"
    Jan  3 12:39:11.824: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22 container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 12:39:11.987
    Jan  3 12:39:12.033: INFO: Waiting for pod pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22 to disappear
    Jan  3 12:39:12.052: INFO: Pod pod-configmaps-f65f25c6-bc8b-4d2c-97a9-f51556b2dc22 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:39:12.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3486" for this suite. 01/03/24 12:39:12.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:39:12.118
Jan  3 12:39:12.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename secrets 01/03/24 12:39:12.12
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:39:12.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:39:12.208
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-ddaa103d-00cd-4121-8baa-6fdbb6255db2 01/03/24 12:39:12.24
STEP: Creating a pod to test consume secrets 01/03/24 12:39:12.258
Jan  3 12:39:12.285: INFO: Waiting up to 5m0s for pod "pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6" in namespace "secrets-6159" to be "Succeeded or Failed"
Jan  3 12:39:12.305: INFO: Pod "pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.720771ms
Jan  3 12:39:14.326: INFO: Pod "pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041013991s
Jan  3 12:39:16.332: INFO: Pod "pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046700117s
STEP: Saw pod success 01/03/24 12:39:16.332
Jan  3 12:39:16.333: INFO: Pod "pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6" satisfied condition "Succeeded or Failed"
Jan  3 12:39:16.360: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6 container secret-volume-test: <nil>
STEP: delete the pod 01/03/24 12:39:16.399
Jan  3 12:39:16.434: INFO: Waiting for pod pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6 to disappear
Jan  3 12:39:16.452: INFO: Pod pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  3 12:39:16.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6159" for this suite. 01/03/24 12:39:16.484
------------------------------
• [4.391 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:39:12.118
    Jan  3 12:39:12.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename secrets 01/03/24 12:39:12.12
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:39:12.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:39:12.208
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-ddaa103d-00cd-4121-8baa-6fdbb6255db2 01/03/24 12:39:12.24
    STEP: Creating a pod to test consume secrets 01/03/24 12:39:12.258
    Jan  3 12:39:12.285: INFO: Waiting up to 5m0s for pod "pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6" in namespace "secrets-6159" to be "Succeeded or Failed"
    Jan  3 12:39:12.305: INFO: Pod "pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.720771ms
    Jan  3 12:39:14.326: INFO: Pod "pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041013991s
    Jan  3 12:39:16.332: INFO: Pod "pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046700117s
    STEP: Saw pod success 01/03/24 12:39:16.332
    Jan  3 12:39:16.333: INFO: Pod "pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6" satisfied condition "Succeeded or Failed"
    Jan  3 12:39:16.360: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6 container secret-volume-test: <nil>
    STEP: delete the pod 01/03/24 12:39:16.399
    Jan  3 12:39:16.434: INFO: Waiting for pod pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6 to disappear
    Jan  3 12:39:16.452: INFO: Pod pod-secrets-dcd97f00-e42a-49cb-b578-e5f17841d5b6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:39:16.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6159" for this suite. 01/03/24 12:39:16.484
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:39:16.517
Jan  3 12:39:16.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename limitrange 01/03/24 12:39:16.519
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:39:16.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:39:16.602
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-zkskw" in namespace "limitrange-7610" 01/03/24 12:39:16.632
STEP: Creating another limitRange in another namespace 01/03/24 12:39:16.657
Jan  3 12:39:16.715: INFO: Namespace "e2e-limitrange-zkskw-8618" created
Jan  3 12:39:16.715: INFO: Creating LimitRange "e2e-limitrange-zkskw" in namespace "e2e-limitrange-zkskw-8618"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-zkskw" 01/03/24 12:39:16.737
Jan  3 12:39:16.759: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-zkskw" in "limitrange-7610" namespace 01/03/24 12:39:16.759
Jan  3 12:39:16.784: INFO: LimitRange "e2e-limitrange-zkskw" has been patched
STEP: Delete LimitRange "e2e-limitrange-zkskw" by Collection with labelSelector: "e2e-limitrange-zkskw=patched" 01/03/24 12:39:16.784
STEP: Confirm that the limitRange "e2e-limitrange-zkskw" has been deleted 01/03/24 12:39:16.815
Jan  3 12:39:16.816: INFO: Requesting list of LimitRange to confirm quantity
Jan  3 12:39:16.834: INFO: Found 0 LimitRange with label "e2e-limitrange-zkskw=patched"
Jan  3 12:39:16.834: INFO: LimitRange "e2e-limitrange-zkskw" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-zkskw" 01/03/24 12:39:16.834
Jan  3 12:39:16.853: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan  3 12:39:16.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-7610" for this suite. 01/03/24 12:39:16.886
STEP: Destroying namespace "e2e-limitrange-zkskw-8618" for this suite. 01/03/24 12:39:16.913
------------------------------
• [0.423 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:39:16.517
    Jan  3 12:39:16.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename limitrange 01/03/24 12:39:16.519
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:39:16.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:39:16.602
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-zkskw" in namespace "limitrange-7610" 01/03/24 12:39:16.632
    STEP: Creating another limitRange in another namespace 01/03/24 12:39:16.657
    Jan  3 12:39:16.715: INFO: Namespace "e2e-limitrange-zkskw-8618" created
    Jan  3 12:39:16.715: INFO: Creating LimitRange "e2e-limitrange-zkskw" in namespace "e2e-limitrange-zkskw-8618"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-zkskw" 01/03/24 12:39:16.737
    Jan  3 12:39:16.759: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-zkskw" in "limitrange-7610" namespace 01/03/24 12:39:16.759
    Jan  3 12:39:16.784: INFO: LimitRange "e2e-limitrange-zkskw" has been patched
    STEP: Delete LimitRange "e2e-limitrange-zkskw" by Collection with labelSelector: "e2e-limitrange-zkskw=patched" 01/03/24 12:39:16.784
    STEP: Confirm that the limitRange "e2e-limitrange-zkskw" has been deleted 01/03/24 12:39:16.815
    Jan  3 12:39:16.816: INFO: Requesting list of LimitRange to confirm quantity
    Jan  3 12:39:16.834: INFO: Found 0 LimitRange with label "e2e-limitrange-zkskw=patched"
    Jan  3 12:39:16.834: INFO: LimitRange "e2e-limitrange-zkskw" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-zkskw" 01/03/24 12:39:16.834
    Jan  3 12:39:16.853: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:39:16.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-7610" for this suite. 01/03/24 12:39:16.886
    STEP: Destroying namespace "e2e-limitrange-zkskw-8618" for this suite. 01/03/24 12:39:16.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:39:16.943
Jan  3 12:39:16.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir-wrapper 01/03/24 12:39:16.946
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:39:17.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:39:17.029
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/03/24 12:39:17.056
STEP: Creating RC which spawns configmap-volume pods 01/03/24 12:39:18.019
Jan  3 12:39:18.069: INFO: Pod name wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd: Found 0 pods out of 5
Jan  3 12:39:23.117: INFO: Pod name wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/03/24 12:39:23.117
Jan  3 12:39:23.118: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:39:23.137: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd": Phase="Pending", Reason="", readiness=false. Elapsed: 19.521714ms
Jan  3 12:39:25.175: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057884519s
Jan  3 12:39:27.173: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05551062s
Jan  3 12:39:29.170: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052773571s
Jan  3 12:39:31.170: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052619098s
Jan  3 12:39:33.170: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd": Phase="Running", Reason="", readiness=true. Elapsed: 10.052455808s
Jan  3 12:39:33.170: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd" satisfied condition "running"
Jan  3 12:39:33.170: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-9fvnw" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:39:33.189: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-9fvnw": Phase="Running", Reason="", readiness=true. Elapsed: 18.775503ms
Jan  3 12:39:33.189: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-9fvnw" satisfied condition "running"
Jan  3 12:39:33.189: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-m4wmk" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:39:33.211: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-m4wmk": Phase="Running", Reason="", readiness=true. Elapsed: 21.699098ms
Jan  3 12:39:33.211: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-m4wmk" satisfied condition "running"
Jan  3 12:39:33.211: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-nbdvw" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:39:33.231: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-nbdvw": Phase="Running", Reason="", readiness=true. Elapsed: 20.011236ms
Jan  3 12:39:33.231: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-nbdvw" satisfied condition "running"
Jan  3 12:39:33.231: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-rz266" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:39:33.251: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-rz266": Phase="Running", Reason="", readiness=true. Elapsed: 20.202606ms
Jan  3 12:39:33.251: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-rz266" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd in namespace emptydir-wrapper-8288, will wait for the garbage collector to delete the pods 01/03/24 12:39:33.251
Jan  3 12:39:33.349: INFO: Deleting ReplicationController wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd took: 24.71884ms
Jan  3 12:39:33.449: INFO: Terminating ReplicationController wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd pods took: 100.406153ms
STEP: Creating RC which spawns configmap-volume pods 01/03/24 12:39:37.888
Jan  3 12:39:37.953: INFO: Pod name wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd: Found 1 pods out of 5
Jan  3 12:39:43.013: INFO: Pod name wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/03/24 12:39:43.013
Jan  3 12:39:43.013: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:39:43.037: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7": Phase="Pending", Reason="", readiness=false. Elapsed: 23.942508ms
Jan  3 12:39:45.071: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058275629s
Jan  3 12:39:47.068: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054802117s
Jan  3 12:39:49.070: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05657205s
Jan  3 12:39:51.071: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.057555072s
Jan  3 12:39:53.073: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7": Phase="Running", Reason="", readiness=true. Elapsed: 10.059715785s
Jan  3 12:39:53.073: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7" satisfied condition "running"
Jan  3 12:39:53.073: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-4s5t8" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:39:53.094: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-4s5t8": Phase="Running", Reason="", readiness=true. Elapsed: 21.189066ms
Jan  3 12:39:53.094: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-4s5t8" satisfied condition "running"
Jan  3 12:39:53.094: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-7f5fl" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:39:53.114: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-7f5fl": Phase="Running", Reason="", readiness=true. Elapsed: 19.426972ms
Jan  3 12:39:53.114: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-7f5fl" satisfied condition "running"
Jan  3 12:39:53.114: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-gbjbf" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:39:53.134: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-gbjbf": Phase="Running", Reason="", readiness=true. Elapsed: 20.190791ms
Jan  3 12:39:53.134: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-gbjbf" satisfied condition "running"
Jan  3 12:39:53.134: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-hk5ff" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:39:53.155: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-hk5ff": Phase="Running", Reason="", readiness=true. Elapsed: 21.041904ms
Jan  3 12:39:53.155: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-hk5ff" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd in namespace emptydir-wrapper-8288, will wait for the garbage collector to delete the pods 01/03/24 12:39:53.155
Jan  3 12:39:53.256: INFO: Deleting ReplicationController wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd took: 29.891943ms
Jan  3 12:39:53.357: INFO: Terminating ReplicationController wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd pods took: 100.72452ms
STEP: Creating RC which spawns configmap-volume pods 01/03/24 12:39:57.98
Jan  3 12:39:58.066: INFO: Pod name wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332: Found 0 pods out of 5
Jan  3 12:40:03.116: INFO: Pod name wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/03/24 12:40:03.116
Jan  3 12:40:03.116: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:40:03.136: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl": Phase="Pending", Reason="", readiness=false. Elapsed: 19.956602ms
Jan  3 12:40:05.170: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053957455s
Jan  3 12:40:07.170: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053278148s
Jan  3 12:40:09.170: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053968173s
Jan  3 12:40:11.169: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052328822s
Jan  3 12:40:13.171: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl": Phase="Running", Reason="", readiness=true. Elapsed: 10.054748284s
Jan  3 12:40:13.171: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl" satisfied condition "running"
Jan  3 12:40:13.171: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-8pncc" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:40:13.196: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-8pncc": Phase="Running", Reason="", readiness=true. Elapsed: 24.48693ms
Jan  3 12:40:13.196: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-8pncc" satisfied condition "running"
Jan  3 12:40:13.196: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-jp4dc" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:40:13.220: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-jp4dc": Phase="Running", Reason="", readiness=true. Elapsed: 23.704036ms
Jan  3 12:40:13.220: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-jp4dc" satisfied condition "running"
Jan  3 12:40:13.220: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-mhdrk" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:40:13.242: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-mhdrk": Phase="Running", Reason="", readiness=true. Elapsed: 22.320958ms
Jan  3 12:40:13.242: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-mhdrk" satisfied condition "running"
Jan  3 12:40:13.242: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-xmpt7" in namespace "emptydir-wrapper-8288" to be "running"
Jan  3 12:40:13.263: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-xmpt7": Phase="Running", Reason="", readiness=true. Elapsed: 21.163252ms
Jan  3 12:40:13.263: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-xmpt7" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332 in namespace emptydir-wrapper-8288, will wait for the garbage collector to delete the pods 01/03/24 12:40:13.263
Jan  3 12:40:13.369: INFO: Deleting ReplicationController wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332 took: 27.793197ms
Jan  3 12:40:13.470: INFO: Terminating ReplicationController wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332 pods took: 100.534605ms
STEP: Cleaning up the configMaps 01/03/24 12:40:18.271
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:40:19.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8288" for this suite. 01/03/24 12:40:19.38
------------------------------
• [SLOW TEST] [62.464 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:39:16.943
    Jan  3 12:39:16.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir-wrapper 01/03/24 12:39:16.946
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:39:17.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:39:17.029
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/03/24 12:39:17.056
    STEP: Creating RC which spawns configmap-volume pods 01/03/24 12:39:18.019
    Jan  3 12:39:18.069: INFO: Pod name wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd: Found 0 pods out of 5
    Jan  3 12:39:23.117: INFO: Pod name wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/03/24 12:39:23.117
    Jan  3 12:39:23.118: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:39:23.137: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd": Phase="Pending", Reason="", readiness=false. Elapsed: 19.521714ms
    Jan  3 12:39:25.175: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057884519s
    Jan  3 12:39:27.173: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05551062s
    Jan  3 12:39:29.170: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052773571s
    Jan  3 12:39:31.170: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052619098s
    Jan  3 12:39:33.170: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd": Phase="Running", Reason="", readiness=true. Elapsed: 10.052455808s
    Jan  3 12:39:33.170: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-7qsdd" satisfied condition "running"
    Jan  3 12:39:33.170: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-9fvnw" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:39:33.189: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-9fvnw": Phase="Running", Reason="", readiness=true. Elapsed: 18.775503ms
    Jan  3 12:39:33.189: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-9fvnw" satisfied condition "running"
    Jan  3 12:39:33.189: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-m4wmk" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:39:33.211: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-m4wmk": Phase="Running", Reason="", readiness=true. Elapsed: 21.699098ms
    Jan  3 12:39:33.211: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-m4wmk" satisfied condition "running"
    Jan  3 12:39:33.211: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-nbdvw" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:39:33.231: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-nbdvw": Phase="Running", Reason="", readiness=true. Elapsed: 20.011236ms
    Jan  3 12:39:33.231: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-nbdvw" satisfied condition "running"
    Jan  3 12:39:33.231: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-rz266" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:39:33.251: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-rz266": Phase="Running", Reason="", readiness=true. Elapsed: 20.202606ms
    Jan  3 12:39:33.251: INFO: Pod "wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd-rz266" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd in namespace emptydir-wrapper-8288, will wait for the garbage collector to delete the pods 01/03/24 12:39:33.251
    Jan  3 12:39:33.349: INFO: Deleting ReplicationController wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd took: 24.71884ms
    Jan  3 12:39:33.449: INFO: Terminating ReplicationController wrapped-volume-race-3fcfea80-2e9d-442b-8818-29697d6ac4dd pods took: 100.406153ms
    STEP: Creating RC which spawns configmap-volume pods 01/03/24 12:39:37.888
    Jan  3 12:39:37.953: INFO: Pod name wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd: Found 1 pods out of 5
    Jan  3 12:39:43.013: INFO: Pod name wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/03/24 12:39:43.013
    Jan  3 12:39:43.013: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:39:43.037: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7": Phase="Pending", Reason="", readiness=false. Elapsed: 23.942508ms
    Jan  3 12:39:45.071: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058275629s
    Jan  3 12:39:47.068: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054802117s
    Jan  3 12:39:49.070: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05657205s
    Jan  3 12:39:51.071: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.057555072s
    Jan  3 12:39:53.073: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7": Phase="Running", Reason="", readiness=true. Elapsed: 10.059715785s
    Jan  3 12:39:53.073: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-2hsq7" satisfied condition "running"
    Jan  3 12:39:53.073: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-4s5t8" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:39:53.094: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-4s5t8": Phase="Running", Reason="", readiness=true. Elapsed: 21.189066ms
    Jan  3 12:39:53.094: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-4s5t8" satisfied condition "running"
    Jan  3 12:39:53.094: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-7f5fl" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:39:53.114: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-7f5fl": Phase="Running", Reason="", readiness=true. Elapsed: 19.426972ms
    Jan  3 12:39:53.114: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-7f5fl" satisfied condition "running"
    Jan  3 12:39:53.114: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-gbjbf" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:39:53.134: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-gbjbf": Phase="Running", Reason="", readiness=true. Elapsed: 20.190791ms
    Jan  3 12:39:53.134: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-gbjbf" satisfied condition "running"
    Jan  3 12:39:53.134: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-hk5ff" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:39:53.155: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-hk5ff": Phase="Running", Reason="", readiness=true. Elapsed: 21.041904ms
    Jan  3 12:39:53.155: INFO: Pod "wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd-hk5ff" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd in namespace emptydir-wrapper-8288, will wait for the garbage collector to delete the pods 01/03/24 12:39:53.155
    Jan  3 12:39:53.256: INFO: Deleting ReplicationController wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd took: 29.891943ms
    Jan  3 12:39:53.357: INFO: Terminating ReplicationController wrapped-volume-race-4e8f07df-8aa6-42ba-a382-1cd8e13839fd pods took: 100.72452ms
    STEP: Creating RC which spawns configmap-volume pods 01/03/24 12:39:57.98
    Jan  3 12:39:58.066: INFO: Pod name wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332: Found 0 pods out of 5
    Jan  3 12:40:03.116: INFO: Pod name wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/03/24 12:40:03.116
    Jan  3 12:40:03.116: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:40:03.136: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl": Phase="Pending", Reason="", readiness=false. Elapsed: 19.956602ms
    Jan  3 12:40:05.170: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053957455s
    Jan  3 12:40:07.170: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053278148s
    Jan  3 12:40:09.170: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053968173s
    Jan  3 12:40:11.169: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052328822s
    Jan  3 12:40:13.171: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl": Phase="Running", Reason="", readiness=true. Elapsed: 10.054748284s
    Jan  3 12:40:13.171: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-5lkgl" satisfied condition "running"
    Jan  3 12:40:13.171: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-8pncc" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:40:13.196: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-8pncc": Phase="Running", Reason="", readiness=true. Elapsed: 24.48693ms
    Jan  3 12:40:13.196: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-8pncc" satisfied condition "running"
    Jan  3 12:40:13.196: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-jp4dc" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:40:13.220: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-jp4dc": Phase="Running", Reason="", readiness=true. Elapsed: 23.704036ms
    Jan  3 12:40:13.220: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-jp4dc" satisfied condition "running"
    Jan  3 12:40:13.220: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-mhdrk" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:40:13.242: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-mhdrk": Phase="Running", Reason="", readiness=true. Elapsed: 22.320958ms
    Jan  3 12:40:13.242: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-mhdrk" satisfied condition "running"
    Jan  3 12:40:13.242: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-xmpt7" in namespace "emptydir-wrapper-8288" to be "running"
    Jan  3 12:40:13.263: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-xmpt7": Phase="Running", Reason="", readiness=true. Elapsed: 21.163252ms
    Jan  3 12:40:13.263: INFO: Pod "wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332-xmpt7" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332 in namespace emptydir-wrapper-8288, will wait for the garbage collector to delete the pods 01/03/24 12:40:13.263
    Jan  3 12:40:13.369: INFO: Deleting ReplicationController wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332 took: 27.793197ms
    Jan  3 12:40:13.470: INFO: Terminating ReplicationController wrapped-volume-race-ec84ed3f-bc8b-4d11-9615-b5616216e332 pods took: 100.534605ms
    STEP: Cleaning up the configMaps 01/03/24 12:40:18.271
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:40:19.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8288" for this suite. 01/03/24 12:40:19.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:40:19.41
Jan  3 12:40:19.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/03/24 12:40:19.414
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:40:19.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:40:19.495
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/03/24 12:40:19.523
STEP: Creating hostNetwork=false pod 01/03/24 12:40:19.524
Jan  3 12:40:19.556: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-6789" to be "running and ready"
Jan  3 12:40:19.578: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.400199ms
Jan  3 12:40:19.579: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:40:21.602: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.045915532s
Jan  3 12:40:21.602: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan  3 12:40:21.602: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/03/24 12:40:21.621
Jan  3 12:40:21.645: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-6789" to be "running and ready"
Jan  3 12:40:21.666: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.07456ms
Jan  3 12:40:21.666: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:40:23.688: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.042506787s
Jan  3 12:40:23.688: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan  3 12:40:23.688: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/03/24 12:40:23.706
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/03/24 12:40:23.707
Jan  3 12:40:23.707: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:40:23.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:40:23.708: INFO: ExecWithOptions: Clientset creation
Jan  3 12:40:23.708: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  3 12:40:24.030: INFO: Exec stderr: ""
Jan  3 12:40:24.030: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:40:24.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:40:24.031: INFO: ExecWithOptions: Clientset creation
Jan  3 12:40:24.032: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  3 12:40:24.344: INFO: Exec stderr: ""
Jan  3 12:40:24.344: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:40:24.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:40:24.346: INFO: ExecWithOptions: Clientset creation
Jan  3 12:40:24.346: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  3 12:40:24.647: INFO: Exec stderr: ""
Jan  3 12:40:24.648: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:40:24.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:40:24.649: INFO: ExecWithOptions: Clientset creation
Jan  3 12:40:24.649: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  3 12:40:24.941: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/03/24 12:40:24.942
Jan  3 12:40:24.942: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:40:24.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:40:24.944: INFO: ExecWithOptions: Clientset creation
Jan  3 12:40:24.944: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan  3 12:40:25.249: INFO: Exec stderr: ""
Jan  3 12:40:25.249: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:40:25.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:40:25.251: INFO: ExecWithOptions: Clientset creation
Jan  3 12:40:25.251: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan  3 12:40:25.563: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/03/24 12:40:25.563
Jan  3 12:40:25.564: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:40:25.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:40:25.565: INFO: ExecWithOptions: Clientset creation
Jan  3 12:40:25.565: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  3 12:40:25.877: INFO: Exec stderr: ""
Jan  3 12:40:25.877: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:40:25.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:40:25.879: INFO: ExecWithOptions: Clientset creation
Jan  3 12:40:25.879: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  3 12:40:26.178: INFO: Exec stderr: ""
Jan  3 12:40:26.179: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:40:26.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:40:26.180: INFO: ExecWithOptions: Clientset creation
Jan  3 12:40:26.180: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  3 12:40:26.482: INFO: Exec stderr: ""
Jan  3 12:40:26.482: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:40:26.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:40:26.484: INFO: ExecWithOptions: Clientset creation
Jan  3 12:40:26.484: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  3 12:40:26.811: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jan  3 12:40:26.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6789" for this suite. 01/03/24 12:40:26.846
------------------------------
• [SLOW TEST] [7.460 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:40:19.41
    Jan  3 12:40:19.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/03/24 12:40:19.414
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:40:19.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:40:19.495
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/03/24 12:40:19.523
    STEP: Creating hostNetwork=false pod 01/03/24 12:40:19.524
    Jan  3 12:40:19.556: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-6789" to be "running and ready"
    Jan  3 12:40:19.578: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.400199ms
    Jan  3 12:40:19.579: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:40:21.602: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.045915532s
    Jan  3 12:40:21.602: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan  3 12:40:21.602: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/03/24 12:40:21.621
    Jan  3 12:40:21.645: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-6789" to be "running and ready"
    Jan  3 12:40:21.666: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.07456ms
    Jan  3 12:40:21.666: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:40:23.688: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.042506787s
    Jan  3 12:40:23.688: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan  3 12:40:23.688: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/03/24 12:40:23.706
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/03/24 12:40:23.707
    Jan  3 12:40:23.707: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:40:23.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:40:23.708: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:40:23.708: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  3 12:40:24.030: INFO: Exec stderr: ""
    Jan  3 12:40:24.030: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:40:24.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:40:24.031: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:40:24.032: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  3 12:40:24.344: INFO: Exec stderr: ""
    Jan  3 12:40:24.344: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:40:24.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:40:24.346: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:40:24.346: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  3 12:40:24.647: INFO: Exec stderr: ""
    Jan  3 12:40:24.648: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:40:24.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:40:24.649: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:40:24.649: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  3 12:40:24.941: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/03/24 12:40:24.942
    Jan  3 12:40:24.942: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:40:24.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:40:24.944: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:40:24.944: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan  3 12:40:25.249: INFO: Exec stderr: ""
    Jan  3 12:40:25.249: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:40:25.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:40:25.251: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:40:25.251: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan  3 12:40:25.563: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/03/24 12:40:25.563
    Jan  3 12:40:25.564: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:40:25.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:40:25.565: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:40:25.565: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  3 12:40:25.877: INFO: Exec stderr: ""
    Jan  3 12:40:25.877: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:40:25.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:40:25.879: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:40:25.879: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  3 12:40:26.178: INFO: Exec stderr: ""
    Jan  3 12:40:26.179: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:40:26.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:40:26.180: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:40:26.180: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  3 12:40:26.482: INFO: Exec stderr: ""
    Jan  3 12:40:26.482: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6789 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:40:26.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:40:26.484: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:40:26.484: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6789/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  3 12:40:26.811: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:40:26.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-6789" for this suite. 01/03/24 12:40:26.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:40:26.872
Jan  3 12:40:26.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename replication-controller 01/03/24 12:40:26.875
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:40:26.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:40:26.957
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 01/03/24 12:40:26.985
STEP: When the matched label of one of its pods change 01/03/24 12:40:27.006
Jan  3 12:40:27.037: INFO: Pod name pod-release: Found 0 pods out of 1
Jan  3 12:40:32.060: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/03/24 12:40:32.104
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  3 12:40:32.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6261" for this suite. 01/03/24 12:40:32.155
------------------------------
• [SLOW TEST] [5.307 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:40:26.872
    Jan  3 12:40:26.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename replication-controller 01/03/24 12:40:26.875
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:40:26.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:40:26.957
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 01/03/24 12:40:26.985
    STEP: When the matched label of one of its pods change 01/03/24 12:40:27.006
    Jan  3 12:40:27.037: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan  3 12:40:32.060: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/03/24 12:40:32.104
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:40:32.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6261" for this suite. 01/03/24 12:40:32.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:40:32.18
Jan  3 12:40:32.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename watch 01/03/24 12:40:32.183
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:40:32.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:40:32.273
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/03/24 12:40:32.302
STEP: creating a watch on configmaps with label B 01/03/24 12:40:32.316
STEP: creating a watch on configmaps with label A or B 01/03/24 12:40:32.33
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/03/24 12:40:32.344
Jan  3 12:40:32.363: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147289 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 12:40:32.364: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147289 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/03/24 12:40:32.365
Jan  3 12:40:32.402: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147295 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 12:40:32.402: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147295 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/03/24 12:40:32.403
Jan  3 12:40:32.444: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147298 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 12:40:32.445: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147298 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/03/24 12:40:32.445
Jan  3 12:40:32.474: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147302 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 12:40:32.474: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147302 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/03/24 12:40:32.474
Jan  3 12:40:32.493: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5980  921349f2-65d8-479a-bc99-20ef31b7cb64 33981147304 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 12:40:32.494: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5980  921349f2-65d8-479a-bc99-20ef31b7cb64 33981147304 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/03/24 12:40:42.494
Jan  3 12:40:42.520: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5980  921349f2-65d8-479a-bc99-20ef31b7cb64 33981148505 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 12:40:42.520: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5980  921349f2-65d8-479a-bc99-20ef31b7cb64 33981148505 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  3 12:40:52.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5980" for this suite. 01/03/24 12:40:52.56
------------------------------
• [SLOW TEST] [20.405 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:40:32.18
    Jan  3 12:40:32.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename watch 01/03/24 12:40:32.183
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:40:32.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:40:32.273
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/03/24 12:40:32.302
    STEP: creating a watch on configmaps with label B 01/03/24 12:40:32.316
    STEP: creating a watch on configmaps with label A or B 01/03/24 12:40:32.33
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/03/24 12:40:32.344
    Jan  3 12:40:32.363: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147289 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 12:40:32.364: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147289 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/03/24 12:40:32.365
    Jan  3 12:40:32.402: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147295 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 12:40:32.402: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147295 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/03/24 12:40:32.403
    Jan  3 12:40:32.444: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147298 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 12:40:32.445: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147298 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/03/24 12:40:32.445
    Jan  3 12:40:32.474: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147302 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 12:40:32.474: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5980  6c28d7f2-258d-49e4-a7cb-c83e7dc9df7c 33981147302 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/03/24 12:40:32.474
    Jan  3 12:40:32.493: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5980  921349f2-65d8-479a-bc99-20ef31b7cb64 33981147304 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 12:40:32.494: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5980  921349f2-65d8-479a-bc99-20ef31b7cb64 33981147304 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/03/24 12:40:42.494
    Jan  3 12:40:42.520: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5980  921349f2-65d8-479a-bc99-20ef31b7cb64 33981148505 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 12:40:42.520: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5980  921349f2-65d8-479a-bc99-20ef31b7cb64 33981148505 0 2024-01-03 12:40:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-03 12:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:40:52.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5980" for this suite. 01/03/24 12:40:52.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:40:52.589
Jan  3 12:40:52.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename subpath 01/03/24 12:40:52.592
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:40:52.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:40:52.695
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/03/24 12:40:52.726
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-p99c 01/03/24 12:40:52.763
STEP: Creating a pod to test atomic-volume-subpath 01/03/24 12:40:52.763
Jan  3 12:40:52.790: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-p99c" in namespace "subpath-6847" to be "Succeeded or Failed"
Jan  3 12:40:52.811: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Pending", Reason="", readiness=false. Elapsed: 19.896579ms
Jan  3 12:40:54.848: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 2.057581354s
Jan  3 12:40:56.833: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 4.042444852s
Jan  3 12:40:58.831: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 6.0400489s
Jan  3 12:41:00.833: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 8.042017528s
Jan  3 12:41:02.834: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 10.043204843s
Jan  3 12:41:04.832: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 12.041447749s
Jan  3 12:41:06.832: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 14.04155207s
Jan  3 12:41:08.831: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 16.040393595s
Jan  3 12:41:10.832: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 18.041654678s
Jan  3 12:41:12.830: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 20.039300661s
Jan  3 12:41:14.841: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=false. Elapsed: 22.050201083s
Jan  3 12:41:16.837: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.046267107s
STEP: Saw pod success 01/03/24 12:41:16.837
Jan  3 12:41:16.837: INFO: Pod "pod-subpath-test-projected-p99c" satisfied condition "Succeeded or Failed"
Jan  3 12:41:16.863: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-subpath-test-projected-p99c container test-container-subpath-projected-p99c: <nil>
STEP: delete the pod 01/03/24 12:41:17.143
Jan  3 12:41:17.174: INFO: Waiting for pod pod-subpath-test-projected-p99c to disappear
Jan  3 12:41:17.192: INFO: Pod pod-subpath-test-projected-p99c no longer exists
STEP: Deleting pod pod-subpath-test-projected-p99c 01/03/24 12:41:17.192
Jan  3 12:41:17.192: INFO: Deleting pod "pod-subpath-test-projected-p99c" in namespace "subpath-6847"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  3 12:41:17.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6847" for this suite. 01/03/24 12:41:17.256
------------------------------
• [SLOW TEST] [24.692 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:40:52.589
    Jan  3 12:40:52.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename subpath 01/03/24 12:40:52.592
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:40:52.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:40:52.695
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/03/24 12:40:52.726
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-p99c 01/03/24 12:40:52.763
    STEP: Creating a pod to test atomic-volume-subpath 01/03/24 12:40:52.763
    Jan  3 12:40:52.790: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-p99c" in namespace "subpath-6847" to be "Succeeded or Failed"
    Jan  3 12:40:52.811: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Pending", Reason="", readiness=false. Elapsed: 19.896579ms
    Jan  3 12:40:54.848: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 2.057581354s
    Jan  3 12:40:56.833: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 4.042444852s
    Jan  3 12:40:58.831: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 6.0400489s
    Jan  3 12:41:00.833: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 8.042017528s
    Jan  3 12:41:02.834: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 10.043204843s
    Jan  3 12:41:04.832: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 12.041447749s
    Jan  3 12:41:06.832: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 14.04155207s
    Jan  3 12:41:08.831: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 16.040393595s
    Jan  3 12:41:10.832: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 18.041654678s
    Jan  3 12:41:12.830: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=true. Elapsed: 20.039300661s
    Jan  3 12:41:14.841: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Running", Reason="", readiness=false. Elapsed: 22.050201083s
    Jan  3 12:41:16.837: INFO: Pod "pod-subpath-test-projected-p99c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.046267107s
    STEP: Saw pod success 01/03/24 12:41:16.837
    Jan  3 12:41:16.837: INFO: Pod "pod-subpath-test-projected-p99c" satisfied condition "Succeeded or Failed"
    Jan  3 12:41:16.863: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-subpath-test-projected-p99c container test-container-subpath-projected-p99c: <nil>
    STEP: delete the pod 01/03/24 12:41:17.143
    Jan  3 12:41:17.174: INFO: Waiting for pod pod-subpath-test-projected-p99c to disappear
    Jan  3 12:41:17.192: INFO: Pod pod-subpath-test-projected-p99c no longer exists
    STEP: Deleting pod pod-subpath-test-projected-p99c 01/03/24 12:41:17.192
    Jan  3 12:41:17.192: INFO: Deleting pod "pod-subpath-test-projected-p99c" in namespace "subpath-6847"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:41:17.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6847" for this suite. 01/03/24 12:41:17.256
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:41:17.282
Jan  3 12:41:17.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename var-expansion 01/03/24 12:41:17.285
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:41:17.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:41:17.367
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 01/03/24 12:41:17.396
STEP: waiting for pod running 01/03/24 12:41:17.425
Jan  3 12:41:17.425: INFO: Waiting up to 2m0s for pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e" in namespace "var-expansion-8816" to be "running"
Jan  3 12:41:17.449: INFO: Pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.365721ms
Jan  3 12:41:19.468: INFO: Pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043003782s
Jan  3 12:41:21.468: INFO: Pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e": Phase="Running", Reason="", readiness=true. Elapsed: 4.042652052s
Jan  3 12:41:21.468: INFO: Pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e" satisfied condition "running"
STEP: creating a file in subpath 01/03/24 12:41:21.468
Jan  3 12:41:21.486: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8816 PodName:var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:41:21.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:41:21.488: INFO: ExecWithOptions: Clientset creation
Jan  3 12:41:21.488: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8816/pods/var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/03/24 12:41:21.803
Jan  3 12:41:21.824: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8816 PodName:var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:41:21.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:41:21.825: INFO: ExecWithOptions: Clientset creation
Jan  3 12:41:21.825: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8816/pods/var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/03/24 12:41:22.12
Jan  3 12:41:22.670: INFO: Successfully updated pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e"
STEP: waiting for annotated pod running 01/03/24 12:41:22.67
Jan  3 12:41:22.671: INFO: Waiting up to 2m0s for pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e" in namespace "var-expansion-8816" to be "running"
Jan  3 12:41:22.690: INFO: Pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e": Phase="Running", Reason="", readiness=true. Elapsed: 18.762227ms
Jan  3 12:41:22.690: INFO: Pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e" satisfied condition "running"
STEP: deleting the pod gracefully 01/03/24 12:41:22.69
Jan  3 12:41:22.690: INFO: Deleting pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e" in namespace "var-expansion-8816"
Jan  3 12:41:22.713: INFO: Wait up to 5m0s for pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  3 12:41:54.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8816" for this suite. 01/03/24 12:41:54.8
------------------------------
• [SLOW TEST] [37.544 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:41:17.282
    Jan  3 12:41:17.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename var-expansion 01/03/24 12:41:17.285
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:41:17.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:41:17.367
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 01/03/24 12:41:17.396
    STEP: waiting for pod running 01/03/24 12:41:17.425
    Jan  3 12:41:17.425: INFO: Waiting up to 2m0s for pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e" in namespace "var-expansion-8816" to be "running"
    Jan  3 12:41:17.449: INFO: Pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.365721ms
    Jan  3 12:41:19.468: INFO: Pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043003782s
    Jan  3 12:41:21.468: INFO: Pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e": Phase="Running", Reason="", readiness=true. Elapsed: 4.042652052s
    Jan  3 12:41:21.468: INFO: Pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e" satisfied condition "running"
    STEP: creating a file in subpath 01/03/24 12:41:21.468
    Jan  3 12:41:21.486: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8816 PodName:var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:41:21.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:41:21.488: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:41:21.488: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8816/pods/var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/03/24 12:41:21.803
    Jan  3 12:41:21.824: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8816 PodName:var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:41:21.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:41:21.825: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:41:21.825: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8816/pods/var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/03/24 12:41:22.12
    Jan  3 12:41:22.670: INFO: Successfully updated pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e"
    STEP: waiting for annotated pod running 01/03/24 12:41:22.67
    Jan  3 12:41:22.671: INFO: Waiting up to 2m0s for pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e" in namespace "var-expansion-8816" to be "running"
    Jan  3 12:41:22.690: INFO: Pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e": Phase="Running", Reason="", readiness=true. Elapsed: 18.762227ms
    Jan  3 12:41:22.690: INFO: Pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e" satisfied condition "running"
    STEP: deleting the pod gracefully 01/03/24 12:41:22.69
    Jan  3 12:41:22.690: INFO: Deleting pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e" in namespace "var-expansion-8816"
    Jan  3 12:41:22.713: INFO: Wait up to 5m0s for pod "var-expansion-1fb9af65-eb5d-4d7c-8dc7-04aa3fadfe2e" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:41:54.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8816" for this suite. 01/03/24 12:41:54.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:41:54.831
Jan  3 12:41:54.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pods 01/03/24 12:41:54.833
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:41:54.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:41:54.92
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jan  3 12:41:54.976: INFO: Waiting up to 5m0s for pod "server-envvars-c613a4fd-35f2-4eed-a6c1-a590f8680d55" in namespace "pods-6861" to be "running and ready"
Jan  3 12:41:54.994: INFO: Pod "server-envvars-c613a4fd-35f2-4eed-a6c1-a590f8680d55": Phase="Pending", Reason="", readiness=false. Elapsed: 17.131454ms
Jan  3 12:41:54.994: INFO: The phase of Pod server-envvars-c613a4fd-35f2-4eed-a6c1-a590f8680d55 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:41:57.017: INFO: Pod "server-envvars-c613a4fd-35f2-4eed-a6c1-a590f8680d55": Phase="Running", Reason="", readiness=true. Elapsed: 2.040496312s
Jan  3 12:41:57.017: INFO: The phase of Pod server-envvars-c613a4fd-35f2-4eed-a6c1-a590f8680d55 is Running (Ready = true)
Jan  3 12:41:57.017: INFO: Pod "server-envvars-c613a4fd-35f2-4eed-a6c1-a590f8680d55" satisfied condition "running and ready"
Jan  3 12:41:57.086: INFO: Waiting up to 5m0s for pod "client-envvars-a323066d-ed79-4668-830a-5d65379ebc77" in namespace "pods-6861" to be "Succeeded or Failed"
Jan  3 12:41:57.104: INFO: Pod "client-envvars-a323066d-ed79-4668-830a-5d65379ebc77": Phase="Pending", Reason="", readiness=false. Elapsed: 18.631679ms
Jan  3 12:41:59.125: INFO: Pod "client-envvars-a323066d-ed79-4668-830a-5d65379ebc77": Phase="Running", Reason="", readiness=true. Elapsed: 2.03928663s
Jan  3 12:42:01.123: INFO: Pod "client-envvars-a323066d-ed79-4668-830a-5d65379ebc77": Phase="Running", Reason="", readiness=false. Elapsed: 4.037539478s
Jan  3 12:42:03.126: INFO: Pod "client-envvars-a323066d-ed79-4668-830a-5d65379ebc77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039886168s
STEP: Saw pod success 01/03/24 12:42:03.126
Jan  3 12:42:03.126: INFO: Pod "client-envvars-a323066d-ed79-4668-830a-5d65379ebc77" satisfied condition "Succeeded or Failed"
Jan  3 12:42:03.146: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod client-envvars-a323066d-ed79-4668-830a-5d65379ebc77 container env3cont: <nil>
STEP: delete the pod 01/03/24 12:42:03.186
Jan  3 12:42:03.227: INFO: Waiting for pod client-envvars-a323066d-ed79-4668-830a-5d65379ebc77 to disappear
Jan  3 12:42:03.249: INFO: Pod client-envvars-a323066d-ed79-4668-830a-5d65379ebc77 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  3 12:42:03.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6861" for this suite. 01/03/24 12:42:03.281
------------------------------
• [SLOW TEST] [8.496 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:41:54.831
    Jan  3 12:41:54.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pods 01/03/24 12:41:54.833
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:41:54.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:41:54.92
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jan  3 12:41:54.976: INFO: Waiting up to 5m0s for pod "server-envvars-c613a4fd-35f2-4eed-a6c1-a590f8680d55" in namespace "pods-6861" to be "running and ready"
    Jan  3 12:41:54.994: INFO: Pod "server-envvars-c613a4fd-35f2-4eed-a6c1-a590f8680d55": Phase="Pending", Reason="", readiness=false. Elapsed: 17.131454ms
    Jan  3 12:41:54.994: INFO: The phase of Pod server-envvars-c613a4fd-35f2-4eed-a6c1-a590f8680d55 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:41:57.017: INFO: Pod "server-envvars-c613a4fd-35f2-4eed-a6c1-a590f8680d55": Phase="Running", Reason="", readiness=true. Elapsed: 2.040496312s
    Jan  3 12:41:57.017: INFO: The phase of Pod server-envvars-c613a4fd-35f2-4eed-a6c1-a590f8680d55 is Running (Ready = true)
    Jan  3 12:41:57.017: INFO: Pod "server-envvars-c613a4fd-35f2-4eed-a6c1-a590f8680d55" satisfied condition "running and ready"
    Jan  3 12:41:57.086: INFO: Waiting up to 5m0s for pod "client-envvars-a323066d-ed79-4668-830a-5d65379ebc77" in namespace "pods-6861" to be "Succeeded or Failed"
    Jan  3 12:41:57.104: INFO: Pod "client-envvars-a323066d-ed79-4668-830a-5d65379ebc77": Phase="Pending", Reason="", readiness=false. Elapsed: 18.631679ms
    Jan  3 12:41:59.125: INFO: Pod "client-envvars-a323066d-ed79-4668-830a-5d65379ebc77": Phase="Running", Reason="", readiness=true. Elapsed: 2.03928663s
    Jan  3 12:42:01.123: INFO: Pod "client-envvars-a323066d-ed79-4668-830a-5d65379ebc77": Phase="Running", Reason="", readiness=false. Elapsed: 4.037539478s
    Jan  3 12:42:03.126: INFO: Pod "client-envvars-a323066d-ed79-4668-830a-5d65379ebc77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039886168s
    STEP: Saw pod success 01/03/24 12:42:03.126
    Jan  3 12:42:03.126: INFO: Pod "client-envvars-a323066d-ed79-4668-830a-5d65379ebc77" satisfied condition "Succeeded or Failed"
    Jan  3 12:42:03.146: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod client-envvars-a323066d-ed79-4668-830a-5d65379ebc77 container env3cont: <nil>
    STEP: delete the pod 01/03/24 12:42:03.186
    Jan  3 12:42:03.227: INFO: Waiting for pod client-envvars-a323066d-ed79-4668-830a-5d65379ebc77 to disappear
    Jan  3 12:42:03.249: INFO: Pod client-envvars-a323066d-ed79-4668-830a-5d65379ebc77 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:42:03.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6861" for this suite. 01/03/24 12:42:03.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:42:03.33
Jan  3 12:42:03.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 12:42:03.333
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:03.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:03.427
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 12:42:03.504
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 12:42:03.702
STEP: Deploying the webhook pod 01/03/24 12:42:03.729
STEP: Wait for the deployment to be ready 01/03/24 12:42:03.771
Jan  3 12:42:03.805: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/24 12:42:05.86
STEP: Verifying the service has paired with the endpoint 01/03/24 12:42:05.889
Jan  3 12:42:06.890: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/03/24 12:42:06.909
STEP: create a pod that should be updated by the webhook 01/03/24 12:42:07.065
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:42:07.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9137" for this suite. 01/03/24 12:42:07.385
STEP: Destroying namespace "webhook-9137-markers" for this suite. 01/03/24 12:42:07.424
------------------------------
• [4.118 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:42:03.33
    Jan  3 12:42:03.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 12:42:03.333
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:03.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:03.427
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 12:42:03.504
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 12:42:03.702
    STEP: Deploying the webhook pod 01/03/24 12:42:03.729
    STEP: Wait for the deployment to be ready 01/03/24 12:42:03.771
    Jan  3 12:42:03.805: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/24 12:42:05.86
    STEP: Verifying the service has paired with the endpoint 01/03/24 12:42:05.889
    Jan  3 12:42:06.890: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/03/24 12:42:06.909
    STEP: create a pod that should be updated by the webhook 01/03/24 12:42:07.065
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:42:07.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9137" for this suite. 01/03/24 12:42:07.385
    STEP: Destroying namespace "webhook-9137-markers" for this suite. 01/03/24 12:42:07.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:42:07.452
Jan  3 12:42:07.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 12:42:07.454
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:07.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:07.537
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/03/24 12:42:07.564
Jan  3 12:42:07.591: INFO: Waiting up to 5m0s for pod "pod-094eb687-97e6-478e-a909-05a3cc9c8dd3" in namespace "emptydir-4045" to be "Succeeded or Failed"
Jan  3 12:42:07.610: INFO: Pod "pod-094eb687-97e6-478e-a909-05a3cc9c8dd3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.880058ms
Jan  3 12:42:09.631: INFO: Pod "pod-094eb687-97e6-478e-a909-05a3cc9c8dd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039062001s
Jan  3 12:42:11.630: INFO: Pod "pod-094eb687-97e6-478e-a909-05a3cc9c8dd3": Phase="Running", Reason="", readiness=false. Elapsed: 4.038809912s
Jan  3 12:42:13.633: INFO: Pod "pod-094eb687-97e6-478e-a909-05a3cc9c8dd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041880394s
STEP: Saw pod success 01/03/24 12:42:13.633
Jan  3 12:42:13.634: INFO: Pod "pod-094eb687-97e6-478e-a909-05a3cc9c8dd3" satisfied condition "Succeeded or Failed"
Jan  3 12:42:13.653: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-094eb687-97e6-478e-a909-05a3cc9c8dd3 container test-container: <nil>
STEP: delete the pod 01/03/24 12:42:13.695
Jan  3 12:42:13.735: INFO: Waiting for pod pod-094eb687-97e6-478e-a909-05a3cc9c8dd3 to disappear
Jan  3 12:42:13.756: INFO: Pod pod-094eb687-97e6-478e-a909-05a3cc9c8dd3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:42:13.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4045" for this suite. 01/03/24 12:42:13.788
------------------------------
• [SLOW TEST] [6.362 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:42:07.452
    Jan  3 12:42:07.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 12:42:07.454
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:07.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:07.537
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/03/24 12:42:07.564
    Jan  3 12:42:07.591: INFO: Waiting up to 5m0s for pod "pod-094eb687-97e6-478e-a909-05a3cc9c8dd3" in namespace "emptydir-4045" to be "Succeeded or Failed"
    Jan  3 12:42:07.610: INFO: Pod "pod-094eb687-97e6-478e-a909-05a3cc9c8dd3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.880058ms
    Jan  3 12:42:09.631: INFO: Pod "pod-094eb687-97e6-478e-a909-05a3cc9c8dd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039062001s
    Jan  3 12:42:11.630: INFO: Pod "pod-094eb687-97e6-478e-a909-05a3cc9c8dd3": Phase="Running", Reason="", readiness=false. Elapsed: 4.038809912s
    Jan  3 12:42:13.633: INFO: Pod "pod-094eb687-97e6-478e-a909-05a3cc9c8dd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041880394s
    STEP: Saw pod success 01/03/24 12:42:13.633
    Jan  3 12:42:13.634: INFO: Pod "pod-094eb687-97e6-478e-a909-05a3cc9c8dd3" satisfied condition "Succeeded or Failed"
    Jan  3 12:42:13.653: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-094eb687-97e6-478e-a909-05a3cc9c8dd3 container test-container: <nil>
    STEP: delete the pod 01/03/24 12:42:13.695
    Jan  3 12:42:13.735: INFO: Waiting for pod pod-094eb687-97e6-478e-a909-05a3cc9c8dd3 to disappear
    Jan  3 12:42:13.756: INFO: Pod pod-094eb687-97e6-478e-a909-05a3cc9c8dd3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:42:13.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4045" for this suite. 01/03/24 12:42:13.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:42:13.816
Jan  3 12:42:13.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 12:42:13.818
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:13.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:13.902
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-1207/configmap-test-b30c7f68-8eb7-4d42-98c6-45b9f07d90b1 01/03/24 12:42:13.931
STEP: Creating a pod to test consume configMaps 01/03/24 12:42:13.95
Jan  3 12:42:13.982: INFO: Waiting up to 5m0s for pod "pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a" in namespace "configmap-1207" to be "Succeeded or Failed"
Jan  3 12:42:14.004: INFO: Pod "pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.564077ms
Jan  3 12:42:16.026: INFO: Pod "pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a": Phase="Running", Reason="", readiness=true. Elapsed: 2.044251483s
Jan  3 12:42:18.035: INFO: Pod "pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a": Phase="Running", Reason="", readiness=false. Elapsed: 4.052729395s
Jan  3 12:42:20.032: INFO: Pod "pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049762223s
STEP: Saw pod success 01/03/24 12:42:20.032
Jan  3 12:42:20.032: INFO: Pod "pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a" satisfied condition "Succeeded or Failed"
Jan  3 12:42:20.090: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a container env-test: <nil>
STEP: delete the pod 01/03/24 12:42:20.165
Jan  3 12:42:20.202: INFO: Waiting for pod pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a to disappear
Jan  3 12:42:20.223: INFO: Pod pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 12:42:20.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1207" for this suite. 01/03/24 12:42:20.253
------------------------------
• [SLOW TEST] [6.467 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:42:13.816
    Jan  3 12:42:13.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 12:42:13.818
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:13.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:13.902
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-1207/configmap-test-b30c7f68-8eb7-4d42-98c6-45b9f07d90b1 01/03/24 12:42:13.931
    STEP: Creating a pod to test consume configMaps 01/03/24 12:42:13.95
    Jan  3 12:42:13.982: INFO: Waiting up to 5m0s for pod "pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a" in namespace "configmap-1207" to be "Succeeded or Failed"
    Jan  3 12:42:14.004: INFO: Pod "pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.564077ms
    Jan  3 12:42:16.026: INFO: Pod "pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a": Phase="Running", Reason="", readiness=true. Elapsed: 2.044251483s
    Jan  3 12:42:18.035: INFO: Pod "pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a": Phase="Running", Reason="", readiness=false. Elapsed: 4.052729395s
    Jan  3 12:42:20.032: INFO: Pod "pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049762223s
    STEP: Saw pod success 01/03/24 12:42:20.032
    Jan  3 12:42:20.032: INFO: Pod "pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a" satisfied condition "Succeeded or Failed"
    Jan  3 12:42:20.090: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a container env-test: <nil>
    STEP: delete the pod 01/03/24 12:42:20.165
    Jan  3 12:42:20.202: INFO: Waiting for pod pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a to disappear
    Jan  3 12:42:20.223: INFO: Pod pod-configmaps-a11674a0-a914-4032-b05a-92872f08d81a no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:42:20.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1207" for this suite. 01/03/24 12:42:20.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:42:20.284
Jan  3 12:42:20.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename sched-pred 01/03/24 12:42:20.287
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:20.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:20.375
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan  3 12:42:20.404: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  3 12:42:20.449: INFO: Waiting for terminating namespaces to be deleted...
Jan  3 12:42:20.477: INFO: 
Logging pods the apiserver thinks is on node jb-1-26-np-64kerjapxk before test
Jan  3 12:42:20.508: INFO: calico-node-r98wj from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.508: INFO: 	Container calico-node ready: true, restart count 2
Jan  3 12:42:20.508: INFO: csi-ionoscloud-t8qkm from kube-system started at 2024-01-03 11:27:28 +0000 UTC (2 container statuses recorded)
Jan  3 12:42:20.508: INFO: 	Container csi-ionoscloud ready: true, restart count 0
Jan  3 12:42:20.508: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  3 12:42:20.508: INFO: konnectivity-agent-gnnsp from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.508: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  3 12:42:20.508: INFO: kube-proxy-z7q4m from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.508: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 12:42:20.508: INFO: nginx-proxy-jb-1-26-np-64kerjapxk from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.508: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan  3 12:42:20.508: INFO: sonobuoy from sonobuoy started at 2024-01-03 11:39:58 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.508: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  3 12:42:20.508: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-4hltf from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 12:42:20.508: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 12:42:20.508: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 12:42:20.508: INFO: webhook-to-be-mutated from webhook-9137 started at 2024-01-03 12:42:07 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.508: INFO: 	Container example ready: false, restart count 0
Jan  3 12:42:20.508: INFO: 
Logging pods the apiserver thinks is on node jb-1-26-np-adtwo5cmi2 before test
Jan  3 12:42:20.560: INFO: calico-kube-controllers-54564bcfb4-kjb7m from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.560: INFO: 	Container calico-kube-controllers ready: true, restart count 2
Jan  3 12:42:20.560: INFO: calico-node-j4nfd from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.560: INFO: 	Container calico-node ready: true, restart count 1
Jan  3 12:42:20.560: INFO: calico-typha-8555b568f6-pqf6q from kube-system started at 2024-01-03 12:14:40 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.560: INFO: 	Container calico-typha ready: true, restart count 0
Jan  3 12:42:20.560: INFO: csi-ionoscloud-4z7q8 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (2 container statuses recorded)
Jan  3 12:42:20.560: INFO: 	Container csi-ionoscloud ready: true, restart count 0
Jan  3 12:42:20.560: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  3 12:42:20.560: INFO: konnectivity-agent-srxbt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.560: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  3 12:42:20.560: INFO: kube-proxy-ml4kt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.561: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 12:42:20.561: INFO: nginx-proxy-jb-1-26-np-adtwo5cmi2 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.561: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan  3 12:42:20.561: INFO: snapshot-validation-webhook-684799cdd5-x6hm4 from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.561: INFO: 	Container snapshot-validation ready: true, restart count 0
Jan  3 12:42:20.561: INFO: sonobuoy-e2e-job-61ed32d11fea4649 from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 12:42:20.561: INFO: 	Container e2e ready: true, restart count 0
Jan  3 12:42:20.561: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 12:42:20.561: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-cb8zx from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 12:42:20.561: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 12:42:20.561: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 12:42:20.561: INFO: 
Logging pods the apiserver thinks is on node jb-1-26-np-nqeu5xtrab before test
Jan  3 12:42:20.588: INFO: calico-node-t5pwv from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.588: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 12:42:20.588: INFO: calico-typha-8555b568f6-bdlz4 from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.588: INFO: 	Container calico-typha ready: true, restart count 0
Jan  3 12:42:20.588: INFO: coredns-89967fcdc-9jbpx from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.588: INFO: 	Container coredns ready: true, restart count 0
Jan  3 12:42:20.588: INFO: coredns-89967fcdc-zjgbq from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.588: INFO: 	Container coredns ready: true, restart count 0
Jan  3 12:42:20.588: INFO: csi-ionoscloud-kv8wz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (2 container statuses recorded)
Jan  3 12:42:20.588: INFO: 	Container csi-ionoscloud ready: true, restart count 0
Jan  3 12:42:20.588: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  3 12:42:20.588: INFO: ionos-policy-validator-64d9954f65-dpd5v from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.588: INFO: 	Container ionos-policy-validator ready: true, restart count 0
Jan  3 12:42:20.588: INFO: konnectivity-agent-v6hrz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.588: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  3 12:42:20.588: INFO: kube-proxy-hqxkb from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.588: INFO: 	Container kube-proxy ready: true, restart count 1
Jan  3 12:42:20.588: INFO: nginx-proxy-jb-1-26-np-nqeu5xtrab from kube-system started at 2024-01-03 11:30:18 +0000 UTC (1 container statuses recorded)
Jan  3 12:42:20.588: INFO: 	Container nginx-proxy ready: true, restart count 1
Jan  3 12:42:20.588: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-j7ffw from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 12:42:20.588: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 12:42:20.588: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/03/24 12:42:20.588
Jan  3 12:42:20.617: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7569" to be "running"
Jan  3 12:42:20.634: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 17.2299ms
Jan  3 12:42:22.656: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038690267s
Jan  3 12:42:24.654: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.036813427s
Jan  3 12:42:24.654: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/03/24 12:42:24.672
STEP: Trying to apply a random label on the found node. 01/03/24 12:42:24.707
STEP: verifying the node has the label kubernetes.io/e2e-71985eb0-5c81-4567-8c31-ae22229d0e46 42 01/03/24 12:42:24.737
STEP: Trying to relaunch the pod, now with labels. 01/03/24 12:42:24.754
Jan  3 12:42:24.780: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7569" to be "not pending"
Jan  3 12:42:24.802: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 22.489582ms
Jan  3 12:42:26.823: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.043796967s
Jan  3 12:42:26.823: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-71985eb0-5c81-4567-8c31-ae22229d0e46 off the node jb-1-26-np-64kerjapxk 01/03/24 12:42:26.842
STEP: verifying the node doesn't have the label kubernetes.io/e2e-71985eb0-5c81-4567-8c31-ae22229d0e46 01/03/24 12:42:26.891
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:42:26.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7569" for this suite. 01/03/24 12:42:26.939
------------------------------
• [SLOW TEST] [6.679 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:42:20.284
    Jan  3 12:42:20.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename sched-pred 01/03/24 12:42:20.287
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:20.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:20.375
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan  3 12:42:20.404: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  3 12:42:20.449: INFO: Waiting for terminating namespaces to be deleted...
    Jan  3 12:42:20.477: INFO: 
    Logging pods the apiserver thinks is on node jb-1-26-np-64kerjapxk before test
    Jan  3 12:42:20.508: INFO: calico-node-r98wj from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.508: INFO: 	Container calico-node ready: true, restart count 2
    Jan  3 12:42:20.508: INFO: csi-ionoscloud-t8qkm from kube-system started at 2024-01-03 11:27:28 +0000 UTC (2 container statuses recorded)
    Jan  3 12:42:20.508: INFO: 	Container csi-ionoscloud ready: true, restart count 0
    Jan  3 12:42:20.508: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  3 12:42:20.508: INFO: konnectivity-agent-gnnsp from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.508: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  3 12:42:20.508: INFO: kube-proxy-z7q4m from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.508: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 12:42:20.508: INFO: nginx-proxy-jb-1-26-np-64kerjapxk from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.508: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan  3 12:42:20.508: INFO: sonobuoy from sonobuoy started at 2024-01-03 11:39:58 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.508: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  3 12:42:20.508: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-4hltf from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 12:42:20.508: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 12:42:20.508: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 12:42:20.508: INFO: webhook-to-be-mutated from webhook-9137 started at 2024-01-03 12:42:07 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.508: INFO: 	Container example ready: false, restart count 0
    Jan  3 12:42:20.508: INFO: 
    Logging pods the apiserver thinks is on node jb-1-26-np-adtwo5cmi2 before test
    Jan  3 12:42:20.560: INFO: calico-kube-controllers-54564bcfb4-kjb7m from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.560: INFO: 	Container calico-kube-controllers ready: true, restart count 2
    Jan  3 12:42:20.560: INFO: calico-node-j4nfd from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.560: INFO: 	Container calico-node ready: true, restart count 1
    Jan  3 12:42:20.560: INFO: calico-typha-8555b568f6-pqf6q from kube-system started at 2024-01-03 12:14:40 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.560: INFO: 	Container calico-typha ready: true, restart count 0
    Jan  3 12:42:20.560: INFO: csi-ionoscloud-4z7q8 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (2 container statuses recorded)
    Jan  3 12:42:20.560: INFO: 	Container csi-ionoscloud ready: true, restart count 0
    Jan  3 12:42:20.560: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  3 12:42:20.560: INFO: konnectivity-agent-srxbt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.560: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  3 12:42:20.560: INFO: kube-proxy-ml4kt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.561: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 12:42:20.561: INFO: nginx-proxy-jb-1-26-np-adtwo5cmi2 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.561: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan  3 12:42:20.561: INFO: snapshot-validation-webhook-684799cdd5-x6hm4 from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.561: INFO: 	Container snapshot-validation ready: true, restart count 0
    Jan  3 12:42:20.561: INFO: sonobuoy-e2e-job-61ed32d11fea4649 from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 12:42:20.561: INFO: 	Container e2e ready: true, restart count 0
    Jan  3 12:42:20.561: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 12:42:20.561: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-cb8zx from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 12:42:20.561: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 12:42:20.561: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 12:42:20.561: INFO: 
    Logging pods the apiserver thinks is on node jb-1-26-np-nqeu5xtrab before test
    Jan  3 12:42:20.588: INFO: calico-node-t5pwv from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.588: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 12:42:20.588: INFO: calico-typha-8555b568f6-bdlz4 from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.588: INFO: 	Container calico-typha ready: true, restart count 0
    Jan  3 12:42:20.588: INFO: coredns-89967fcdc-9jbpx from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.588: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 12:42:20.588: INFO: coredns-89967fcdc-zjgbq from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.588: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 12:42:20.588: INFO: csi-ionoscloud-kv8wz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (2 container statuses recorded)
    Jan  3 12:42:20.588: INFO: 	Container csi-ionoscloud ready: true, restart count 0
    Jan  3 12:42:20.588: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  3 12:42:20.588: INFO: ionos-policy-validator-64d9954f65-dpd5v from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.588: INFO: 	Container ionos-policy-validator ready: true, restart count 0
    Jan  3 12:42:20.588: INFO: konnectivity-agent-v6hrz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.588: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  3 12:42:20.588: INFO: kube-proxy-hqxkb from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.588: INFO: 	Container kube-proxy ready: true, restart count 1
    Jan  3 12:42:20.588: INFO: nginx-proxy-jb-1-26-np-nqeu5xtrab from kube-system started at 2024-01-03 11:30:18 +0000 UTC (1 container statuses recorded)
    Jan  3 12:42:20.588: INFO: 	Container nginx-proxy ready: true, restart count 1
    Jan  3 12:42:20.588: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-j7ffw from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 12:42:20.588: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 12:42:20.588: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/03/24 12:42:20.588
    Jan  3 12:42:20.617: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7569" to be "running"
    Jan  3 12:42:20.634: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 17.2299ms
    Jan  3 12:42:22.656: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038690267s
    Jan  3 12:42:24.654: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.036813427s
    Jan  3 12:42:24.654: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/03/24 12:42:24.672
    STEP: Trying to apply a random label on the found node. 01/03/24 12:42:24.707
    STEP: verifying the node has the label kubernetes.io/e2e-71985eb0-5c81-4567-8c31-ae22229d0e46 42 01/03/24 12:42:24.737
    STEP: Trying to relaunch the pod, now with labels. 01/03/24 12:42:24.754
    Jan  3 12:42:24.780: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7569" to be "not pending"
    Jan  3 12:42:24.802: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 22.489582ms
    Jan  3 12:42:26.823: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.043796967s
    Jan  3 12:42:26.823: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-71985eb0-5c81-4567-8c31-ae22229d0e46 off the node jb-1-26-np-64kerjapxk 01/03/24 12:42:26.842
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-71985eb0-5c81-4567-8c31-ae22229d0e46 01/03/24 12:42:26.891
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:42:26.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7569" for this suite. 01/03/24 12:42:26.939
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:42:26.966
Jan  3 12:42:26.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename replication-controller 01/03/24 12:42:26.97
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:27.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:27.059
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jan  3 12:42:27.086: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/03/24 12:42:27.126
STEP: Checking rc "condition-test" has the desired failure condition set 01/03/24 12:42:27.151
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/03/24 12:42:28.198
Jan  3 12:42:28.238: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/03/24 12:42:28.238
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  3 12:42:28.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7548" for this suite. 01/03/24 12:42:28.287
------------------------------
• [1.347 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:42:26.966
    Jan  3 12:42:26.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename replication-controller 01/03/24 12:42:26.97
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:27.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:27.059
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jan  3 12:42:27.086: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/03/24 12:42:27.126
    STEP: Checking rc "condition-test" has the desired failure condition set 01/03/24 12:42:27.151
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/03/24 12:42:28.198
    Jan  3 12:42:28.238: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/03/24 12:42:28.238
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:42:28.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7548" for this suite. 01/03/24 12:42:28.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:42:28.314
Jan  3 12:42:28.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename watch 01/03/24 12:42:28.315
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:28.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:28.418
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/03/24 12:42:28.447
STEP: creating a new configmap 01/03/24 12:42:28.46
STEP: modifying the configmap once 01/03/24 12:42:28.479
STEP: changing the label value of the configmap 01/03/24 12:42:28.515
STEP: Expecting to observe a delete notification for the watched object 01/03/24 12:42:28.552
Jan  3 12:42:28.552: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9110  feb58292-016e-4e5f-8d7e-bcb79ba858c7 33981161201 0 2024-01-03 12:42:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-03 12:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 12:42:28.553: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9110  feb58292-016e-4e5f-8d7e-bcb79ba858c7 33981161206 0 2024-01-03 12:42:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-03 12:42:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 12:42:28.553: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9110  feb58292-016e-4e5f-8d7e-bcb79ba858c7 33981161213 0 2024-01-03 12:42:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-03 12:42:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/03/24 12:42:28.553
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/03/24 12:42:28.588
STEP: changing the label value of the configmap back 01/03/24 12:42:38.589
STEP: modifying the configmap a third time 01/03/24 12:42:38.64
STEP: deleting the configmap 01/03/24 12:42:38.676
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/03/24 12:42:38.698
Jan  3 12:42:38.699: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9110  feb58292-016e-4e5f-8d7e-bcb79ba858c7 33981162489 0 2024-01-03 12:42:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-03 12:42:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 12:42:38.699: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9110  feb58292-016e-4e5f-8d7e-bcb79ba858c7 33981162493 0 2024-01-03 12:42:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-03 12:42:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 12:42:38.699: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9110  feb58292-016e-4e5f-8d7e-bcb79ba858c7 33981162496 0 2024-01-03 12:42:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-03 12:42:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  3 12:42:38.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9110" for this suite. 01/03/24 12:42:38.73
------------------------------
• [SLOW TEST] [10.442 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:42:28.314
    Jan  3 12:42:28.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename watch 01/03/24 12:42:28.315
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:28.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:28.418
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/03/24 12:42:28.447
    STEP: creating a new configmap 01/03/24 12:42:28.46
    STEP: modifying the configmap once 01/03/24 12:42:28.479
    STEP: changing the label value of the configmap 01/03/24 12:42:28.515
    STEP: Expecting to observe a delete notification for the watched object 01/03/24 12:42:28.552
    Jan  3 12:42:28.552: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9110  feb58292-016e-4e5f-8d7e-bcb79ba858c7 33981161201 0 2024-01-03 12:42:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-03 12:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 12:42:28.553: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9110  feb58292-016e-4e5f-8d7e-bcb79ba858c7 33981161206 0 2024-01-03 12:42:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-03 12:42:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 12:42:28.553: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9110  feb58292-016e-4e5f-8d7e-bcb79ba858c7 33981161213 0 2024-01-03 12:42:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-03 12:42:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/03/24 12:42:28.553
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/03/24 12:42:28.588
    STEP: changing the label value of the configmap back 01/03/24 12:42:38.589
    STEP: modifying the configmap a third time 01/03/24 12:42:38.64
    STEP: deleting the configmap 01/03/24 12:42:38.676
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/03/24 12:42:38.698
    Jan  3 12:42:38.699: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9110  feb58292-016e-4e5f-8d7e-bcb79ba858c7 33981162489 0 2024-01-03 12:42:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-03 12:42:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 12:42:38.699: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9110  feb58292-016e-4e5f-8d7e-bcb79ba858c7 33981162493 0 2024-01-03 12:42:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-03 12:42:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 12:42:38.699: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9110  feb58292-016e-4e5f-8d7e-bcb79ba858c7 33981162496 0 2024-01-03 12:42:28 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-03 12:42:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:42:38.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9110" for this suite. 01/03/24 12:42:38.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:42:38.762
Jan  3 12:42:38.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 12:42:38.764
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:38.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:38.857
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 01/03/24 12:42:38.886
Jan  3 12:42:38.916: INFO: Waiting up to 5m0s for pod "annotationupdate4f60dd92-e5c3-4b1c-bcae-50e24dc400d6" in namespace "downward-api-8999" to be "running and ready"
Jan  3 12:42:38.942: INFO: Pod "annotationupdate4f60dd92-e5c3-4b1c-bcae-50e24dc400d6": Phase="Pending", Reason="", readiness=false. Elapsed: 26.478484ms
Jan  3 12:42:38.942: INFO: The phase of Pod annotationupdate4f60dd92-e5c3-4b1c-bcae-50e24dc400d6 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:42:40.962: INFO: Pod "annotationupdate4f60dd92-e5c3-4b1c-bcae-50e24dc400d6": Phase="Running", Reason="", readiness=true. Elapsed: 2.046122479s
Jan  3 12:42:40.962: INFO: The phase of Pod annotationupdate4f60dd92-e5c3-4b1c-bcae-50e24dc400d6 is Running (Ready = true)
Jan  3 12:42:40.962: INFO: Pod "annotationupdate4f60dd92-e5c3-4b1c-bcae-50e24dc400d6" satisfied condition "running and ready"
Jan  3 12:42:41.579: INFO: Successfully updated pod "annotationupdate4f60dd92-e5c3-4b1c-bcae-50e24dc400d6"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  3 12:42:43.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8999" for this suite. 01/03/24 12:42:43.692
------------------------------
• [4.964 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:42:38.762
    Jan  3 12:42:38.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 12:42:38.764
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:38.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:38.857
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 01/03/24 12:42:38.886
    Jan  3 12:42:38.916: INFO: Waiting up to 5m0s for pod "annotationupdate4f60dd92-e5c3-4b1c-bcae-50e24dc400d6" in namespace "downward-api-8999" to be "running and ready"
    Jan  3 12:42:38.942: INFO: Pod "annotationupdate4f60dd92-e5c3-4b1c-bcae-50e24dc400d6": Phase="Pending", Reason="", readiness=false. Elapsed: 26.478484ms
    Jan  3 12:42:38.942: INFO: The phase of Pod annotationupdate4f60dd92-e5c3-4b1c-bcae-50e24dc400d6 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:42:40.962: INFO: Pod "annotationupdate4f60dd92-e5c3-4b1c-bcae-50e24dc400d6": Phase="Running", Reason="", readiness=true. Elapsed: 2.046122479s
    Jan  3 12:42:40.962: INFO: The phase of Pod annotationupdate4f60dd92-e5c3-4b1c-bcae-50e24dc400d6 is Running (Ready = true)
    Jan  3 12:42:40.962: INFO: Pod "annotationupdate4f60dd92-e5c3-4b1c-bcae-50e24dc400d6" satisfied condition "running and ready"
    Jan  3 12:42:41.579: INFO: Successfully updated pod "annotationupdate4f60dd92-e5c3-4b1c-bcae-50e24dc400d6"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:42:43.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8999" for this suite. 01/03/24 12:42:43.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:42:43.729
Jan  3 12:42:43.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename namespaces 01/03/24 12:42:43.731
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:43.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:43.82
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 01/03/24 12:42:43.849
STEP: patching the Namespace 01/03/24 12:42:43.902
STEP: get the Namespace and ensuring it has the label 01/03/24 12:42:43.926
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:42:43.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7349" for this suite. 01/03/24 12:42:43.965
STEP: Destroying namespace "nspatchtest-8dc478f5-492d-4644-af2a-61626d4128a6-5169" for this suite. 01/03/24 12:42:43.992
------------------------------
• [0.300 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:42:43.729
    Jan  3 12:42:43.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename namespaces 01/03/24 12:42:43.731
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:43.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:43.82
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 01/03/24 12:42:43.849
    STEP: patching the Namespace 01/03/24 12:42:43.902
    STEP: get the Namespace and ensuring it has the label 01/03/24 12:42:43.926
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:42:43.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7349" for this suite. 01/03/24 12:42:43.965
    STEP: Destroying namespace "nspatchtest-8dc478f5-492d-4644-af2a-61626d4128a6-5169" for this suite. 01/03/24 12:42:43.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:42:44.033
Jan  3 12:42:44.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename gc 01/03/24 12:42:44.035
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:44.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:44.114
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/03/24 12:42:44.147
STEP: Wait for the Deployment to create new ReplicaSet 01/03/24 12:42:44.165
STEP: delete the deployment 01/03/24 12:42:44.342
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/03/24 12:42:44.388
STEP: Gathering metrics 01/03/24 12:42:45.001
W0103 12:42:45.043619      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  3 12:42:45.043: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  3 12:42:45.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4666" for this suite. 01/03/24 12:42:45.064
------------------------------
• [1.069 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:42:44.033
    Jan  3 12:42:44.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename gc 01/03/24 12:42:44.035
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:44.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:44.114
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/03/24 12:42:44.147
    STEP: Wait for the Deployment to create new ReplicaSet 01/03/24 12:42:44.165
    STEP: delete the deployment 01/03/24 12:42:44.342
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/03/24 12:42:44.388
    STEP: Gathering metrics 01/03/24 12:42:45.001
    W0103 12:42:45.043619      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  3 12:42:45.043: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:42:45.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4666" for this suite. 01/03/24 12:42:45.064
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:42:45.103
Jan  3 12:42:45.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 12:42:45.104
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:45.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:45.199
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/03/24 12:42:45.233
Jan  3 12:42:45.262: INFO: Waiting up to 5m0s for pod "pod-9aed350c-ae00-4824-826e-092cbc578e2c" in namespace "emptydir-684" to be "Succeeded or Failed"
Jan  3 12:42:45.281: INFO: Pod "pod-9aed350c-ae00-4824-826e-092cbc578e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.411447ms
Jan  3 12:42:47.302: INFO: Pod "pod-9aed350c-ae00-4824-826e-092cbc578e2c": Phase="Running", Reason="", readiness=true. Elapsed: 2.040251323s
Jan  3 12:42:49.306: INFO: Pod "pod-9aed350c-ae00-4824-826e-092cbc578e2c": Phase="Running", Reason="", readiness=false. Elapsed: 4.04350265s
Jan  3 12:42:51.301: INFO: Pod "pod-9aed350c-ae00-4824-826e-092cbc578e2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039302349s
STEP: Saw pod success 01/03/24 12:42:51.301
Jan  3 12:42:51.302: INFO: Pod "pod-9aed350c-ae00-4824-826e-092cbc578e2c" satisfied condition "Succeeded or Failed"
Jan  3 12:42:51.321: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-9aed350c-ae00-4824-826e-092cbc578e2c container test-container: <nil>
STEP: delete the pod 01/03/24 12:42:51.365
Jan  3 12:42:51.406: INFO: Waiting for pod pod-9aed350c-ae00-4824-826e-092cbc578e2c to disappear
Jan  3 12:42:51.429: INFO: Pod pod-9aed350c-ae00-4824-826e-092cbc578e2c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:42:51.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-684" for this suite. 01/03/24 12:42:51.46
------------------------------
• [SLOW TEST] [6.392 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:42:45.103
    Jan  3 12:42:45.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 12:42:45.104
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:45.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:45.199
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/03/24 12:42:45.233
    Jan  3 12:42:45.262: INFO: Waiting up to 5m0s for pod "pod-9aed350c-ae00-4824-826e-092cbc578e2c" in namespace "emptydir-684" to be "Succeeded or Failed"
    Jan  3 12:42:45.281: INFO: Pod "pod-9aed350c-ae00-4824-826e-092cbc578e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.411447ms
    Jan  3 12:42:47.302: INFO: Pod "pod-9aed350c-ae00-4824-826e-092cbc578e2c": Phase="Running", Reason="", readiness=true. Elapsed: 2.040251323s
    Jan  3 12:42:49.306: INFO: Pod "pod-9aed350c-ae00-4824-826e-092cbc578e2c": Phase="Running", Reason="", readiness=false. Elapsed: 4.04350265s
    Jan  3 12:42:51.301: INFO: Pod "pod-9aed350c-ae00-4824-826e-092cbc578e2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039302349s
    STEP: Saw pod success 01/03/24 12:42:51.301
    Jan  3 12:42:51.302: INFO: Pod "pod-9aed350c-ae00-4824-826e-092cbc578e2c" satisfied condition "Succeeded or Failed"
    Jan  3 12:42:51.321: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-9aed350c-ae00-4824-826e-092cbc578e2c container test-container: <nil>
    STEP: delete the pod 01/03/24 12:42:51.365
    Jan  3 12:42:51.406: INFO: Waiting for pod pod-9aed350c-ae00-4824-826e-092cbc578e2c to disappear
    Jan  3 12:42:51.429: INFO: Pod pod-9aed350c-ae00-4824-826e-092cbc578e2c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:42:51.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-684" for this suite. 01/03/24 12:42:51.46
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:42:51.496
Jan  3 12:42:51.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename replicaset 01/03/24 12:42:51.499
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:51.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:51.577
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/03/24 12:42:51.608
Jan  3 12:42:51.648: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-2163" to be "running and ready"
Jan  3 12:42:51.665: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 17.409431ms
Jan  3 12:42:51.665: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:42:53.686: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.038232762s
Jan  3 12:42:53.686: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan  3 12:42:53.686: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/03/24 12:42:53.707
STEP: Then the orphan pod is adopted 01/03/24 12:42:53.728
STEP: When the matched label of one of its pods change 01/03/24 12:42:53.746
Jan  3 12:42:53.766: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/03/24 12:42:53.816
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  3 12:42:53.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2163" for this suite. 01/03/24 12:42:53.866
------------------------------
• [2.393 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:42:51.496
    Jan  3 12:42:51.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename replicaset 01/03/24 12:42:51.499
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:51.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:51.577
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/03/24 12:42:51.608
    Jan  3 12:42:51.648: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-2163" to be "running and ready"
    Jan  3 12:42:51.665: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 17.409431ms
    Jan  3 12:42:51.665: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:42:53.686: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.038232762s
    Jan  3 12:42:53.686: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan  3 12:42:53.686: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/03/24 12:42:53.707
    STEP: Then the orphan pod is adopted 01/03/24 12:42:53.728
    STEP: When the matched label of one of its pods change 01/03/24 12:42:53.746
    Jan  3 12:42:53.766: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/03/24 12:42:53.816
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:42:53.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2163" for this suite. 01/03/24 12:42:53.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:42:53.891
Jan  3 12:42:53.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename gc 01/03/24 12:42:53.894
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:53.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:53.988
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/03/24 12:42:54.052
STEP: delete the rc 01/03/24 12:42:59.094
STEP: wait for the rc to be deleted 01/03/24 12:42:59.119
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/03/24 12:43:04.139
STEP: Gathering metrics 01/03/24 12:43:34.194
W0103 12:43:34.229731      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  3 12:43:34.229: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan  3 12:43:34.229: INFO: Deleting pod "simpletest.rc-2882k" in namespace "gc-6997"
Jan  3 12:43:34.293: INFO: Deleting pod "simpletest.rc-28jrr" in namespace "gc-6997"
Jan  3 12:43:34.328: INFO: Deleting pod "simpletest.rc-29phz" in namespace "gc-6997"
Jan  3 12:43:34.361: INFO: Deleting pod "simpletest.rc-2vv7b" in namespace "gc-6997"
Jan  3 12:43:34.399: INFO: Deleting pod "simpletest.rc-2xhvs" in namespace "gc-6997"
Jan  3 12:43:34.468: INFO: Deleting pod "simpletest.rc-4dvsp" in namespace "gc-6997"
Jan  3 12:43:34.502: INFO: Deleting pod "simpletest.rc-4f24m" in namespace "gc-6997"
Jan  3 12:43:34.531: INFO: Deleting pod "simpletest.rc-4flz9" in namespace "gc-6997"
Jan  3 12:43:34.566: INFO: Deleting pod "simpletest.rc-4v5ww" in namespace "gc-6997"
Jan  3 12:43:34.605: INFO: Deleting pod "simpletest.rc-5d8ss" in namespace "gc-6997"
Jan  3 12:43:34.643: INFO: Deleting pod "simpletest.rc-5djz4" in namespace "gc-6997"
Jan  3 12:43:34.675: INFO: Deleting pod "simpletest.rc-5fcx4" in namespace "gc-6997"
Jan  3 12:43:34.725: INFO: Deleting pod "simpletest.rc-5nq8d" in namespace "gc-6997"
Jan  3 12:43:34.780: INFO: Deleting pod "simpletest.rc-5pjzj" in namespace "gc-6997"
Jan  3 12:43:34.811: INFO: Deleting pod "simpletest.rc-5sxfs" in namespace "gc-6997"
Jan  3 12:43:34.851: INFO: Deleting pod "simpletest.rc-5vbvx" in namespace "gc-6997"
Jan  3 12:43:34.903: INFO: Deleting pod "simpletest.rc-5vv4l" in namespace "gc-6997"
Jan  3 12:43:34.947: INFO: Deleting pod "simpletest.rc-5xqxb" in namespace "gc-6997"
Jan  3 12:43:34.978: INFO: Deleting pod "simpletest.rc-669wx" in namespace "gc-6997"
Jan  3 12:43:35.014: INFO: Deleting pod "simpletest.rc-6zm6r" in namespace "gc-6997"
Jan  3 12:43:35.053: INFO: Deleting pod "simpletest.rc-78q55" in namespace "gc-6997"
Jan  3 12:43:35.091: INFO: Deleting pod "simpletest.rc-7hfvg" in namespace "gc-6997"
Jan  3 12:43:35.125: INFO: Deleting pod "simpletest.rc-7jfjj" in namespace "gc-6997"
Jan  3 12:43:35.163: INFO: Deleting pod "simpletest.rc-7vkv5" in namespace "gc-6997"
Jan  3 12:43:35.217: INFO: Deleting pod "simpletest.rc-82dnt" in namespace "gc-6997"
Jan  3 12:43:35.257: INFO: Deleting pod "simpletest.rc-82qjg" in namespace "gc-6997"
Jan  3 12:43:35.294: INFO: Deleting pod "simpletest.rc-8fs66" in namespace "gc-6997"
Jan  3 12:43:35.334: INFO: Deleting pod "simpletest.rc-8gtn8" in namespace "gc-6997"
Jan  3 12:43:35.378: INFO: Deleting pod "simpletest.rc-8rt8f" in namespace "gc-6997"
Jan  3 12:43:35.418: INFO: Deleting pod "simpletest.rc-8sp9r" in namespace "gc-6997"
Jan  3 12:43:35.465: INFO: Deleting pod "simpletest.rc-97lth" in namespace "gc-6997"
Jan  3 12:43:35.500: INFO: Deleting pod "simpletest.rc-9fn66" in namespace "gc-6997"
Jan  3 12:43:35.539: INFO: Deleting pod "simpletest.rc-b8q4z" in namespace "gc-6997"
Jan  3 12:43:35.571: INFO: Deleting pod "simpletest.rc-b8qpp" in namespace "gc-6997"
Jan  3 12:43:35.610: INFO: Deleting pod "simpletest.rc-br4qr" in namespace "gc-6997"
Jan  3 12:43:35.651: INFO: Deleting pod "simpletest.rc-bth5p" in namespace "gc-6997"
Jan  3 12:43:35.686: INFO: Deleting pod "simpletest.rc-bvfhb" in namespace "gc-6997"
Jan  3 12:43:35.730: INFO: Deleting pod "simpletest.rc-bvqt6" in namespace "gc-6997"
Jan  3 12:43:35.764: INFO: Deleting pod "simpletest.rc-cbv9q" in namespace "gc-6997"
Jan  3 12:43:35.795: INFO: Deleting pod "simpletest.rc-ckb88" in namespace "gc-6997"
Jan  3 12:43:35.825: INFO: Deleting pod "simpletest.rc-cxmbd" in namespace "gc-6997"
Jan  3 12:43:35.855: INFO: Deleting pod "simpletest.rc-dgn25" in namespace "gc-6997"
Jan  3 12:43:35.891: INFO: Deleting pod "simpletest.rc-dljdm" in namespace "gc-6997"
Jan  3 12:43:35.926: INFO: Deleting pod "simpletest.rc-dzkgm" in namespace "gc-6997"
Jan  3 12:43:35.969: INFO: Deleting pod "simpletest.rc-f5lws" in namespace "gc-6997"
Jan  3 12:43:36.002: INFO: Deleting pod "simpletest.rc-f7hck" in namespace "gc-6997"
Jan  3 12:43:36.040: INFO: Deleting pod "simpletest.rc-fc6zh" in namespace "gc-6997"
Jan  3 12:43:36.072: INFO: Deleting pod "simpletest.rc-frqnb" in namespace "gc-6997"
Jan  3 12:43:36.103: INFO: Deleting pod "simpletest.rc-g9jb8" in namespace "gc-6997"
Jan  3 12:43:36.142: INFO: Deleting pod "simpletest.rc-grkg5" in namespace "gc-6997"
Jan  3 12:43:36.182: INFO: Deleting pod "simpletest.rc-gxln4" in namespace "gc-6997"
Jan  3 12:43:36.212: INFO: Deleting pod "simpletest.rc-h57ss" in namespace "gc-6997"
Jan  3 12:43:36.254: INFO: Deleting pod "simpletest.rc-hjwbj" in namespace "gc-6997"
Jan  3 12:43:36.285: INFO: Deleting pod "simpletest.rc-hvhpv" in namespace "gc-6997"
Jan  3 12:43:36.354: INFO: Deleting pod "simpletest.rc-hw5zt" in namespace "gc-6997"
Jan  3 12:43:36.397: INFO: Deleting pod "simpletest.rc-jgg6r" in namespace "gc-6997"
Jan  3 12:43:36.434: INFO: Deleting pod "simpletest.rc-jhrqj" in namespace "gc-6997"
Jan  3 12:43:36.472: INFO: Deleting pod "simpletest.rc-jlw78" in namespace "gc-6997"
Jan  3 12:43:36.508: INFO: Deleting pod "simpletest.rc-jrmbv" in namespace "gc-6997"
Jan  3 12:43:36.546: INFO: Deleting pod "simpletest.rc-jt8vt" in namespace "gc-6997"
Jan  3 12:43:36.586: INFO: Deleting pod "simpletest.rc-jz7gl" in namespace "gc-6997"
Jan  3 12:43:36.638: INFO: Deleting pod "simpletest.rc-k67hb" in namespace "gc-6997"
Jan  3 12:43:36.679: INFO: Deleting pod "simpletest.rc-kck4c" in namespace "gc-6997"
Jan  3 12:43:36.719: INFO: Deleting pod "simpletest.rc-khjmf" in namespace "gc-6997"
Jan  3 12:43:36.764: INFO: Deleting pod "simpletest.rc-kvx9v" in namespace "gc-6997"
Jan  3 12:43:36.800: INFO: Deleting pod "simpletest.rc-l6w7z" in namespace "gc-6997"
Jan  3 12:43:36.834: INFO: Deleting pod "simpletest.rc-lcjzv" in namespace "gc-6997"
Jan  3 12:43:36.872: INFO: Deleting pod "simpletest.rc-lk4tt" in namespace "gc-6997"
Jan  3 12:43:36.912: INFO: Deleting pod "simpletest.rc-lrct9" in namespace "gc-6997"
Jan  3 12:43:36.946: INFO: Deleting pod "simpletest.rc-lx78c" in namespace "gc-6997"
Jan  3 12:43:36.980: INFO: Deleting pod "simpletest.rc-m4mrg" in namespace "gc-6997"
Jan  3 12:43:37.014: INFO: Deleting pod "simpletest.rc-m6h27" in namespace "gc-6997"
Jan  3 12:43:37.056: INFO: Deleting pod "simpletest.rc-m8btt" in namespace "gc-6997"
Jan  3 12:43:37.092: INFO: Deleting pod "simpletest.rc-mc9kw" in namespace "gc-6997"
Jan  3 12:43:37.126: INFO: Deleting pod "simpletest.rc-nghsr" in namespace "gc-6997"
Jan  3 12:43:37.164: INFO: Deleting pod "simpletest.rc-p2vxx" in namespace "gc-6997"
Jan  3 12:43:37.205: INFO: Deleting pod "simpletest.rc-p4psp" in namespace "gc-6997"
Jan  3 12:43:37.246: INFO: Deleting pod "simpletest.rc-pm5cz" in namespace "gc-6997"
Jan  3 12:43:37.287: INFO: Deleting pod "simpletest.rc-q9lwd" in namespace "gc-6997"
Jan  3 12:43:37.333: INFO: Deleting pod "simpletest.rc-qscz2" in namespace "gc-6997"
Jan  3 12:43:37.384: INFO: Deleting pod "simpletest.rc-qsm75" in namespace "gc-6997"
Jan  3 12:43:37.422: INFO: Deleting pod "simpletest.rc-qzz25" in namespace "gc-6997"
Jan  3 12:43:37.459: INFO: Deleting pod "simpletest.rc-sfflh" in namespace "gc-6997"
Jan  3 12:43:37.488: INFO: Deleting pod "simpletest.rc-slq9v" in namespace "gc-6997"
Jan  3 12:43:37.522: INFO: Deleting pod "simpletest.rc-ss5cd" in namespace "gc-6997"
Jan  3 12:43:37.555: INFO: Deleting pod "simpletest.rc-sxf9q" in namespace "gc-6997"
Jan  3 12:43:37.587: INFO: Deleting pod "simpletest.rc-v4b8k" in namespace "gc-6997"
Jan  3 12:43:37.625: INFO: Deleting pod "simpletest.rc-vb9w8" in namespace "gc-6997"
Jan  3 12:43:37.674: INFO: Deleting pod "simpletest.rc-vj96h" in namespace "gc-6997"
Jan  3 12:43:37.714: INFO: Deleting pod "simpletest.rc-vkzmp" in namespace "gc-6997"
Jan  3 12:43:37.747: INFO: Deleting pod "simpletest.rc-w78m7" in namespace "gc-6997"
Jan  3 12:43:37.786: INFO: Deleting pod "simpletest.rc-w7vwv" in namespace "gc-6997"
Jan  3 12:43:37.820: INFO: Deleting pod "simpletest.rc-wf2d6" in namespace "gc-6997"
Jan  3 12:43:37.858: INFO: Deleting pod "simpletest.rc-wkvwp" in namespace "gc-6997"
Jan  3 12:43:37.908: INFO: Deleting pod "simpletest.rc-wz7k9" in namespace "gc-6997"
Jan  3 12:43:37.939: INFO: Deleting pod "simpletest.rc-x552w" in namespace "gc-6997"
Jan  3 12:43:37.975: INFO: Deleting pod "simpletest.rc-x6gp6" in namespace "gc-6997"
Jan  3 12:43:38.005: INFO: Deleting pod "simpletest.rc-xz28m" in namespace "gc-6997"
Jan  3 12:43:38.037: INFO: Deleting pod "simpletest.rc-z5b8s" in namespace "gc-6997"
Jan  3 12:43:38.070: INFO: Deleting pod "simpletest.rc-zx24w" in namespace "gc-6997"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  3 12:43:38.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6997" for this suite. 01/03/24 12:43:38.126
------------------------------
• [SLOW TEST] [44.259 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:42:53.891
    Jan  3 12:42:53.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename gc 01/03/24 12:42:53.894
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:42:53.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:42:53.988
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/03/24 12:42:54.052
    STEP: delete the rc 01/03/24 12:42:59.094
    STEP: wait for the rc to be deleted 01/03/24 12:42:59.119
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/03/24 12:43:04.139
    STEP: Gathering metrics 01/03/24 12:43:34.194
    W0103 12:43:34.229731      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  3 12:43:34.229: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan  3 12:43:34.229: INFO: Deleting pod "simpletest.rc-2882k" in namespace "gc-6997"
    Jan  3 12:43:34.293: INFO: Deleting pod "simpletest.rc-28jrr" in namespace "gc-6997"
    Jan  3 12:43:34.328: INFO: Deleting pod "simpletest.rc-29phz" in namespace "gc-6997"
    Jan  3 12:43:34.361: INFO: Deleting pod "simpletest.rc-2vv7b" in namespace "gc-6997"
    Jan  3 12:43:34.399: INFO: Deleting pod "simpletest.rc-2xhvs" in namespace "gc-6997"
    Jan  3 12:43:34.468: INFO: Deleting pod "simpletest.rc-4dvsp" in namespace "gc-6997"
    Jan  3 12:43:34.502: INFO: Deleting pod "simpletest.rc-4f24m" in namespace "gc-6997"
    Jan  3 12:43:34.531: INFO: Deleting pod "simpletest.rc-4flz9" in namespace "gc-6997"
    Jan  3 12:43:34.566: INFO: Deleting pod "simpletest.rc-4v5ww" in namespace "gc-6997"
    Jan  3 12:43:34.605: INFO: Deleting pod "simpletest.rc-5d8ss" in namespace "gc-6997"
    Jan  3 12:43:34.643: INFO: Deleting pod "simpletest.rc-5djz4" in namespace "gc-6997"
    Jan  3 12:43:34.675: INFO: Deleting pod "simpletest.rc-5fcx4" in namespace "gc-6997"
    Jan  3 12:43:34.725: INFO: Deleting pod "simpletest.rc-5nq8d" in namespace "gc-6997"
    Jan  3 12:43:34.780: INFO: Deleting pod "simpletest.rc-5pjzj" in namespace "gc-6997"
    Jan  3 12:43:34.811: INFO: Deleting pod "simpletest.rc-5sxfs" in namespace "gc-6997"
    Jan  3 12:43:34.851: INFO: Deleting pod "simpletest.rc-5vbvx" in namespace "gc-6997"
    Jan  3 12:43:34.903: INFO: Deleting pod "simpletest.rc-5vv4l" in namespace "gc-6997"
    Jan  3 12:43:34.947: INFO: Deleting pod "simpletest.rc-5xqxb" in namespace "gc-6997"
    Jan  3 12:43:34.978: INFO: Deleting pod "simpletest.rc-669wx" in namespace "gc-6997"
    Jan  3 12:43:35.014: INFO: Deleting pod "simpletest.rc-6zm6r" in namespace "gc-6997"
    Jan  3 12:43:35.053: INFO: Deleting pod "simpletest.rc-78q55" in namespace "gc-6997"
    Jan  3 12:43:35.091: INFO: Deleting pod "simpletest.rc-7hfvg" in namespace "gc-6997"
    Jan  3 12:43:35.125: INFO: Deleting pod "simpletest.rc-7jfjj" in namespace "gc-6997"
    Jan  3 12:43:35.163: INFO: Deleting pod "simpletest.rc-7vkv5" in namespace "gc-6997"
    Jan  3 12:43:35.217: INFO: Deleting pod "simpletest.rc-82dnt" in namespace "gc-6997"
    Jan  3 12:43:35.257: INFO: Deleting pod "simpletest.rc-82qjg" in namespace "gc-6997"
    Jan  3 12:43:35.294: INFO: Deleting pod "simpletest.rc-8fs66" in namespace "gc-6997"
    Jan  3 12:43:35.334: INFO: Deleting pod "simpletest.rc-8gtn8" in namespace "gc-6997"
    Jan  3 12:43:35.378: INFO: Deleting pod "simpletest.rc-8rt8f" in namespace "gc-6997"
    Jan  3 12:43:35.418: INFO: Deleting pod "simpletest.rc-8sp9r" in namespace "gc-6997"
    Jan  3 12:43:35.465: INFO: Deleting pod "simpletest.rc-97lth" in namespace "gc-6997"
    Jan  3 12:43:35.500: INFO: Deleting pod "simpletest.rc-9fn66" in namespace "gc-6997"
    Jan  3 12:43:35.539: INFO: Deleting pod "simpletest.rc-b8q4z" in namespace "gc-6997"
    Jan  3 12:43:35.571: INFO: Deleting pod "simpletest.rc-b8qpp" in namespace "gc-6997"
    Jan  3 12:43:35.610: INFO: Deleting pod "simpletest.rc-br4qr" in namespace "gc-6997"
    Jan  3 12:43:35.651: INFO: Deleting pod "simpletest.rc-bth5p" in namespace "gc-6997"
    Jan  3 12:43:35.686: INFO: Deleting pod "simpletest.rc-bvfhb" in namespace "gc-6997"
    Jan  3 12:43:35.730: INFO: Deleting pod "simpletest.rc-bvqt6" in namespace "gc-6997"
    Jan  3 12:43:35.764: INFO: Deleting pod "simpletest.rc-cbv9q" in namespace "gc-6997"
    Jan  3 12:43:35.795: INFO: Deleting pod "simpletest.rc-ckb88" in namespace "gc-6997"
    Jan  3 12:43:35.825: INFO: Deleting pod "simpletest.rc-cxmbd" in namespace "gc-6997"
    Jan  3 12:43:35.855: INFO: Deleting pod "simpletest.rc-dgn25" in namespace "gc-6997"
    Jan  3 12:43:35.891: INFO: Deleting pod "simpletest.rc-dljdm" in namespace "gc-6997"
    Jan  3 12:43:35.926: INFO: Deleting pod "simpletest.rc-dzkgm" in namespace "gc-6997"
    Jan  3 12:43:35.969: INFO: Deleting pod "simpletest.rc-f5lws" in namespace "gc-6997"
    Jan  3 12:43:36.002: INFO: Deleting pod "simpletest.rc-f7hck" in namespace "gc-6997"
    Jan  3 12:43:36.040: INFO: Deleting pod "simpletest.rc-fc6zh" in namespace "gc-6997"
    Jan  3 12:43:36.072: INFO: Deleting pod "simpletest.rc-frqnb" in namespace "gc-6997"
    Jan  3 12:43:36.103: INFO: Deleting pod "simpletest.rc-g9jb8" in namespace "gc-6997"
    Jan  3 12:43:36.142: INFO: Deleting pod "simpletest.rc-grkg5" in namespace "gc-6997"
    Jan  3 12:43:36.182: INFO: Deleting pod "simpletest.rc-gxln4" in namespace "gc-6997"
    Jan  3 12:43:36.212: INFO: Deleting pod "simpletest.rc-h57ss" in namespace "gc-6997"
    Jan  3 12:43:36.254: INFO: Deleting pod "simpletest.rc-hjwbj" in namespace "gc-6997"
    Jan  3 12:43:36.285: INFO: Deleting pod "simpletest.rc-hvhpv" in namespace "gc-6997"
    Jan  3 12:43:36.354: INFO: Deleting pod "simpletest.rc-hw5zt" in namespace "gc-6997"
    Jan  3 12:43:36.397: INFO: Deleting pod "simpletest.rc-jgg6r" in namespace "gc-6997"
    Jan  3 12:43:36.434: INFO: Deleting pod "simpletest.rc-jhrqj" in namespace "gc-6997"
    Jan  3 12:43:36.472: INFO: Deleting pod "simpletest.rc-jlw78" in namespace "gc-6997"
    Jan  3 12:43:36.508: INFO: Deleting pod "simpletest.rc-jrmbv" in namespace "gc-6997"
    Jan  3 12:43:36.546: INFO: Deleting pod "simpletest.rc-jt8vt" in namespace "gc-6997"
    Jan  3 12:43:36.586: INFO: Deleting pod "simpletest.rc-jz7gl" in namespace "gc-6997"
    Jan  3 12:43:36.638: INFO: Deleting pod "simpletest.rc-k67hb" in namespace "gc-6997"
    Jan  3 12:43:36.679: INFO: Deleting pod "simpletest.rc-kck4c" in namespace "gc-6997"
    Jan  3 12:43:36.719: INFO: Deleting pod "simpletest.rc-khjmf" in namespace "gc-6997"
    Jan  3 12:43:36.764: INFO: Deleting pod "simpletest.rc-kvx9v" in namespace "gc-6997"
    Jan  3 12:43:36.800: INFO: Deleting pod "simpletest.rc-l6w7z" in namespace "gc-6997"
    Jan  3 12:43:36.834: INFO: Deleting pod "simpletest.rc-lcjzv" in namespace "gc-6997"
    Jan  3 12:43:36.872: INFO: Deleting pod "simpletest.rc-lk4tt" in namespace "gc-6997"
    Jan  3 12:43:36.912: INFO: Deleting pod "simpletest.rc-lrct9" in namespace "gc-6997"
    Jan  3 12:43:36.946: INFO: Deleting pod "simpletest.rc-lx78c" in namespace "gc-6997"
    Jan  3 12:43:36.980: INFO: Deleting pod "simpletest.rc-m4mrg" in namespace "gc-6997"
    Jan  3 12:43:37.014: INFO: Deleting pod "simpletest.rc-m6h27" in namespace "gc-6997"
    Jan  3 12:43:37.056: INFO: Deleting pod "simpletest.rc-m8btt" in namespace "gc-6997"
    Jan  3 12:43:37.092: INFO: Deleting pod "simpletest.rc-mc9kw" in namespace "gc-6997"
    Jan  3 12:43:37.126: INFO: Deleting pod "simpletest.rc-nghsr" in namespace "gc-6997"
    Jan  3 12:43:37.164: INFO: Deleting pod "simpletest.rc-p2vxx" in namespace "gc-6997"
    Jan  3 12:43:37.205: INFO: Deleting pod "simpletest.rc-p4psp" in namespace "gc-6997"
    Jan  3 12:43:37.246: INFO: Deleting pod "simpletest.rc-pm5cz" in namespace "gc-6997"
    Jan  3 12:43:37.287: INFO: Deleting pod "simpletest.rc-q9lwd" in namespace "gc-6997"
    Jan  3 12:43:37.333: INFO: Deleting pod "simpletest.rc-qscz2" in namespace "gc-6997"
    Jan  3 12:43:37.384: INFO: Deleting pod "simpletest.rc-qsm75" in namespace "gc-6997"
    Jan  3 12:43:37.422: INFO: Deleting pod "simpletest.rc-qzz25" in namespace "gc-6997"
    Jan  3 12:43:37.459: INFO: Deleting pod "simpletest.rc-sfflh" in namespace "gc-6997"
    Jan  3 12:43:37.488: INFO: Deleting pod "simpletest.rc-slq9v" in namespace "gc-6997"
    Jan  3 12:43:37.522: INFO: Deleting pod "simpletest.rc-ss5cd" in namespace "gc-6997"
    Jan  3 12:43:37.555: INFO: Deleting pod "simpletest.rc-sxf9q" in namespace "gc-6997"
    Jan  3 12:43:37.587: INFO: Deleting pod "simpletest.rc-v4b8k" in namespace "gc-6997"
    Jan  3 12:43:37.625: INFO: Deleting pod "simpletest.rc-vb9w8" in namespace "gc-6997"
    Jan  3 12:43:37.674: INFO: Deleting pod "simpletest.rc-vj96h" in namespace "gc-6997"
    Jan  3 12:43:37.714: INFO: Deleting pod "simpletest.rc-vkzmp" in namespace "gc-6997"
    Jan  3 12:43:37.747: INFO: Deleting pod "simpletest.rc-w78m7" in namespace "gc-6997"
    Jan  3 12:43:37.786: INFO: Deleting pod "simpletest.rc-w7vwv" in namespace "gc-6997"
    Jan  3 12:43:37.820: INFO: Deleting pod "simpletest.rc-wf2d6" in namespace "gc-6997"
    Jan  3 12:43:37.858: INFO: Deleting pod "simpletest.rc-wkvwp" in namespace "gc-6997"
    Jan  3 12:43:37.908: INFO: Deleting pod "simpletest.rc-wz7k9" in namespace "gc-6997"
    Jan  3 12:43:37.939: INFO: Deleting pod "simpletest.rc-x552w" in namespace "gc-6997"
    Jan  3 12:43:37.975: INFO: Deleting pod "simpletest.rc-x6gp6" in namespace "gc-6997"
    Jan  3 12:43:38.005: INFO: Deleting pod "simpletest.rc-xz28m" in namespace "gc-6997"
    Jan  3 12:43:38.037: INFO: Deleting pod "simpletest.rc-z5b8s" in namespace "gc-6997"
    Jan  3 12:43:38.070: INFO: Deleting pod "simpletest.rc-zx24w" in namespace "gc-6997"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:43:38.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6997" for this suite. 01/03/24 12:43:38.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:43:38.151
Jan  3 12:43:38.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename lease-test 01/03/24 12:43:38.152
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:43:38.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:43:38.249
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jan  3 12:43:38.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-7963" for this suite. 01/03/24 12:43:38.557
------------------------------
• [0.430 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:43:38.151
    Jan  3 12:43:38.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename lease-test 01/03/24 12:43:38.152
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:43:38.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:43:38.249
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:43:38.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-7963" for this suite. 01/03/24 12:43:38.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:43:38.582
Jan  3 12:43:38.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 12:43:38.585
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:43:38.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:43:38.666
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 01/03/24 12:43:38.696
Jan  3 12:43:38.725: INFO: Waiting up to 5m0s for pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd" in namespace "downward-api-4441" to be "running and ready"
Jan  3 12:43:38.743: INFO: Pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd": Phase="Pending", Reason="", readiness=false. Elapsed: 17.942307ms
Jan  3 12:43:38.743: INFO: The phase of Pod labelsupdate5800e205-429e-4948-a260-4f1062175acd is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:43:40.768: INFO: Pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043505053s
Jan  3 12:43:40.769: INFO: The phase of Pod labelsupdate5800e205-429e-4948-a260-4f1062175acd is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:43:42.767: INFO: Pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042400662s
Jan  3 12:43:42.767: INFO: The phase of Pod labelsupdate5800e205-429e-4948-a260-4f1062175acd is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:43:44.763: INFO: Pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038001197s
Jan  3 12:43:44.763: INFO: The phase of Pod labelsupdate5800e205-429e-4948-a260-4f1062175acd is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:43:46.763: INFO: Pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd": Phase="Running", Reason="", readiness=true. Elapsed: 8.038077307s
Jan  3 12:43:46.763: INFO: The phase of Pod labelsupdate5800e205-429e-4948-a260-4f1062175acd is Running (Ready = true)
Jan  3 12:43:46.763: INFO: Pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd" satisfied condition "running and ready"
Jan  3 12:43:47.383: INFO: Successfully updated pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  3 12:43:49.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4441" for this suite. 01/03/24 12:43:49.502
------------------------------
• [SLOW TEST] [10.949 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:43:38.582
    Jan  3 12:43:38.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 12:43:38.585
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:43:38.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:43:38.666
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 01/03/24 12:43:38.696
    Jan  3 12:43:38.725: INFO: Waiting up to 5m0s for pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd" in namespace "downward-api-4441" to be "running and ready"
    Jan  3 12:43:38.743: INFO: Pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd": Phase="Pending", Reason="", readiness=false. Elapsed: 17.942307ms
    Jan  3 12:43:38.743: INFO: The phase of Pod labelsupdate5800e205-429e-4948-a260-4f1062175acd is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:43:40.768: INFO: Pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043505053s
    Jan  3 12:43:40.769: INFO: The phase of Pod labelsupdate5800e205-429e-4948-a260-4f1062175acd is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:43:42.767: INFO: Pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042400662s
    Jan  3 12:43:42.767: INFO: The phase of Pod labelsupdate5800e205-429e-4948-a260-4f1062175acd is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:43:44.763: INFO: Pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038001197s
    Jan  3 12:43:44.763: INFO: The phase of Pod labelsupdate5800e205-429e-4948-a260-4f1062175acd is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:43:46.763: INFO: Pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd": Phase="Running", Reason="", readiness=true. Elapsed: 8.038077307s
    Jan  3 12:43:46.763: INFO: The phase of Pod labelsupdate5800e205-429e-4948-a260-4f1062175acd is Running (Ready = true)
    Jan  3 12:43:46.763: INFO: Pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd" satisfied condition "running and ready"
    Jan  3 12:43:47.383: INFO: Successfully updated pod "labelsupdate5800e205-429e-4948-a260-4f1062175acd"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:43:49.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4441" for this suite. 01/03/24 12:43:49.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:43:49.533
Jan  3 12:43:49.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename csiinlinevolumes 01/03/24 12:43:49.537
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:43:49.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:43:49.617
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 01/03/24 12:43:49.644
STEP: getting 01/03/24 12:43:49.72
STEP: listing 01/03/24 12:43:49.759
STEP: deleting 01/03/24 12:43:49.777
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:43:49.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-6249" for this suite. 01/03/24 12:43:49.903
------------------------------
• [0.420 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:43:49.533
    Jan  3 12:43:49.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename csiinlinevolumes 01/03/24 12:43:49.537
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:43:49.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:43:49.617
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 01/03/24 12:43:49.644
    STEP: getting 01/03/24 12:43:49.72
    STEP: listing 01/03/24 12:43:49.759
    STEP: deleting 01/03/24 12:43:49.777
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:43:49.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-6249" for this suite. 01/03/24 12:43:49.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:43:49.954
Jan  3 12:43:49.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename runtimeclass 01/03/24 12:43:49.957
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:43:50.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:43:50.045
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan  3 12:43:50.153: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1187 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  3 12:43:50.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1187" for this suite. 01/03/24 12:43:50.225
------------------------------
• [0.295 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:43:49.954
    Jan  3 12:43:49.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename runtimeclass 01/03/24 12:43:49.957
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:43:50.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:43:50.045
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan  3 12:43:50.153: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1187 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:43:50.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1187" for this suite. 01/03/24 12:43:50.225
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:43:50.252
Jan  3 12:43:50.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename proxy 01/03/24 12:43:50.255
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:43:50.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:43:50.336
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/03/24 12:43:50.392
STEP: creating replication controller proxy-service-65x2w in namespace proxy-7614 01/03/24 12:43:50.394
I0103 12:43:50.415997      22 runners.go:193] Created replication controller with name: proxy-service-65x2w, namespace: proxy-7614, replica count: 1
I0103 12:43:51.467358      22 runners.go:193] proxy-service-65x2w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 12:43:52.468647      22 runners.go:193] proxy-service-65x2w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 12:43:53.469504      22 runners.go:193] proxy-service-65x2w Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 12:43:53.491: INFO: setup took 3.127047894s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/03/24 12:43:53.491
Jan  3 12:43:53.616: INFO: (0) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 124.182041ms)
Jan  3 12:43:53.624: INFO: (0) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 132.437123ms)
Jan  3 12:43:53.624: INFO: (0) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 132.224325ms)
Jan  3 12:43:53.629: INFO: (0) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 137.086991ms)
Jan  3 12:43:53.632: INFO: (0) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 140.159171ms)
Jan  3 12:43:53.642: INFO: (0) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 149.802585ms)
Jan  3 12:43:53.642: INFO: (0) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 149.888452ms)
Jan  3 12:43:53.642: INFO: (0) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 150.325476ms)
Jan  3 12:43:53.642: INFO: (0) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 150.091253ms)
Jan  3 12:43:53.642: INFO: (0) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 149.970082ms)
Jan  3 12:43:53.645: INFO: (0) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 153.777495ms)
Jan  3 12:43:53.645: INFO: (0) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 153.282622ms)
Jan  3 12:43:53.649: INFO: (0) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 157.379444ms)
Jan  3 12:43:53.660: INFO: (0) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 167.850211ms)
Jan  3 12:43:53.669: INFO: (0) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 176.755033ms)
Jan  3 12:43:53.669: INFO: (0) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 176.679036ms)
Jan  3 12:43:53.709: INFO: (1) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 39.381092ms)
Jan  3 12:43:53.710: INFO: (1) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 41.36401ms)
Jan  3 12:43:53.710: INFO: (1) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 41.106331ms)
Jan  3 12:43:53.710: INFO: (1) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 40.90798ms)
Jan  3 12:43:53.710: INFO: (1) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 41.543157ms)
Jan  3 12:43:53.710: INFO: (1) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 41.363524ms)
Jan  3 12:43:53.711: INFO: (1) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 41.299245ms)
Jan  3 12:43:53.711: INFO: (1) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 41.279759ms)
Jan  3 12:43:53.711: INFO: (1) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 41.143358ms)
Jan  3 12:43:53.711: INFO: (1) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 41.454434ms)
Jan  3 12:43:53.714: INFO: (1) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 44.694583ms)
Jan  3 12:43:53.714: INFO: (1) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 44.465857ms)
Jan  3 12:43:53.725: INFO: (1) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 55.481515ms)
Jan  3 12:43:53.725: INFO: (1) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 55.764184ms)
Jan  3 12:43:53.725: INFO: (1) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 55.815717ms)
Jan  3 12:43:53.725: INFO: (1) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 55.520028ms)
Jan  3 12:43:53.773: INFO: (2) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 47.690305ms)
Jan  3 12:43:53.773: INFO: (2) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 47.437264ms)
Jan  3 12:43:53.774: INFO: (2) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 47.62009ms)
Jan  3 12:43:53.774: INFO: (2) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 47.845805ms)
Jan  3 12:43:53.775: INFO: (2) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 48.741667ms)
Jan  3 12:43:53.775: INFO: (2) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 48.492484ms)
Jan  3 12:43:53.775: INFO: (2) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 49.67867ms)
Jan  3 12:43:53.775: INFO: (2) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 48.688362ms)
Jan  3 12:43:53.775: INFO: (2) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 48.848333ms)
Jan  3 12:43:53.776: INFO: (2) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 50.728396ms)
Jan  3 12:43:53.776: INFO: (2) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 49.749866ms)
Jan  3 12:43:53.776: INFO: (2) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 50.492541ms)
Jan  3 12:43:53.788: INFO: (2) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 61.579209ms)
Jan  3 12:43:53.788: INFO: (2) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 61.711884ms)
Jan  3 12:43:53.799: INFO: (2) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 73.389273ms)
Jan  3 12:43:53.799: INFO: (2) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 73.275419ms)
Jan  3 12:43:53.835: INFO: (3) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 35.460867ms)
Jan  3 12:43:53.851: INFO: (3) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 50.465105ms)
Jan  3 12:43:53.851: INFO: (3) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 51.186159ms)
Jan  3 12:43:53.851: INFO: (3) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 51.485581ms)
Jan  3 12:43:53.852: INFO: (3) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 52.010041ms)
Jan  3 12:43:53.852: INFO: (3) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 51.701333ms)
Jan  3 12:43:53.852: INFO: (3) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 51.806524ms)
Jan  3 12:43:53.852: INFO: (3) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 51.935649ms)
Jan  3 12:43:53.852: INFO: (3) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 52.640842ms)
Jan  3 12:43:53.852: INFO: (3) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 52.304946ms)
Jan  3 12:43:53.853: INFO: (3) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 52.544641ms)
Jan  3 12:43:53.853: INFO: (3) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 52.795472ms)
Jan  3 12:43:53.853: INFO: (3) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 52.732424ms)
Jan  3 12:43:53.873: INFO: (3) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 72.934794ms)
Jan  3 12:43:53.873: INFO: (3) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 73.15526ms)
Jan  3 12:43:53.873: INFO: (3) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 73.539469ms)
Jan  3 12:43:53.912: INFO: (4) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 38.236879ms)
Jan  3 12:43:53.912: INFO: (4) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 38.318245ms)
Jan  3 12:43:53.912: INFO: (4) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 38.18156ms)
Jan  3 12:43:53.912: INFO: (4) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 38.125838ms)
Jan  3 12:43:53.912: INFO: (4) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 38.023467ms)
Jan  3 12:43:53.912: INFO: (4) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 38.523133ms)
Jan  3 12:43:53.922: INFO: (4) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 47.879481ms)
Jan  3 12:43:53.922: INFO: (4) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 47.884322ms)
Jan  3 12:43:53.922: INFO: (4) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 48.606062ms)
Jan  3 12:43:53.922: INFO: (4) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 47.952136ms)
Jan  3 12:43:53.922: INFO: (4) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 48.036007ms)
Jan  3 12:43:53.922: INFO: (4) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 48.280552ms)
Jan  3 12:43:53.934: INFO: (4) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 60.664474ms)
Jan  3 12:43:53.934: INFO: (4) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 60.050851ms)
Jan  3 12:43:53.948: INFO: (4) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 74.494538ms)
Jan  3 12:43:53.948: INFO: (4) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 73.927206ms)
Jan  3 12:43:53.990: INFO: (5) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 42.041386ms)
Jan  3 12:43:53.991: INFO: (5) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 42.025716ms)
Jan  3 12:43:53.991: INFO: (5) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 42.239709ms)
Jan  3 12:43:53.991: INFO: (5) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 42.322052ms)
Jan  3 12:43:53.991: INFO: (5) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 42.659558ms)
Jan  3 12:43:53.991: INFO: (5) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 42.388011ms)
Jan  3 12:43:53.991: INFO: (5) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 43.041925ms)
Jan  3 12:43:53.991: INFO: (5) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 42.707518ms)
Jan  3 12:43:53.992: INFO: (5) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 43.515227ms)
Jan  3 12:43:53.992: INFO: (5) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 43.456437ms)
Jan  3 12:43:53.994: INFO: (5) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 45.424913ms)
Jan  3 12:43:53.994: INFO: (5) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 46.109023ms)
Jan  3 12:43:54.005: INFO: (5) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 56.168998ms)
Jan  3 12:43:54.005: INFO: (5) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 55.997077ms)
Jan  3 12:43:54.005: INFO: (5) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 56.957019ms)
Jan  3 12:43:54.005: INFO: (5) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 56.908764ms)
Jan  3 12:43:54.042: INFO: (6) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 36.258218ms)
Jan  3 12:43:54.042: INFO: (6) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 36.47493ms)
Jan  3 12:43:54.043: INFO: (6) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 36.591412ms)
Jan  3 12:43:54.043: INFO: (6) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 36.393244ms)
Jan  3 12:43:54.043: INFO: (6) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 37.279077ms)
Jan  3 12:43:54.043: INFO: (6) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 37.093268ms)
Jan  3 12:43:54.043: INFO: (6) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 37.226138ms)
Jan  3 12:43:54.043: INFO: (6) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 37.534795ms)
Jan  3 12:43:54.043: INFO: (6) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 37.872968ms)
Jan  3 12:43:54.045: INFO: (6) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 39.053355ms)
Jan  3 12:43:54.053: INFO: (6) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 46.452228ms)
Jan  3 12:43:54.053: INFO: (6) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 46.445412ms)
Jan  3 12:43:54.060: INFO: (6) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 53.904608ms)
Jan  3 12:43:54.061: INFO: (6) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 55.019809ms)
Jan  3 12:43:54.066: INFO: (6) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 59.690548ms)
Jan  3 12:43:54.066: INFO: (6) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 59.76775ms)
Jan  3 12:43:54.098: INFO: (7) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 31.061478ms)
Jan  3 12:43:54.099: INFO: (7) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 32.05738ms)
Jan  3 12:43:54.100: INFO: (7) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 33.384087ms)
Jan  3 12:43:54.100: INFO: (7) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 33.944059ms)
Jan  3 12:43:54.100: INFO: (7) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 33.757627ms)
Jan  3 12:43:54.102: INFO: (7) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 34.814844ms)
Jan  3 12:43:54.102: INFO: (7) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 35.133927ms)
Jan  3 12:43:54.102: INFO: (7) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 35.561907ms)
Jan  3 12:43:54.103: INFO: (7) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 35.553333ms)
Jan  3 12:43:54.103: INFO: (7) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 36.399709ms)
Jan  3 12:43:54.107: INFO: (7) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 40.732833ms)
Jan  3 12:43:54.110: INFO: (7) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 42.503007ms)
Jan  3 12:43:54.114: INFO: (7) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 46.938284ms)
Jan  3 12:43:54.116: INFO: (7) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 48.645931ms)
Jan  3 12:43:54.117: INFO: (7) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 50.016337ms)
Jan  3 12:43:54.117: INFO: (7) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 50.240114ms)
Jan  3 12:43:54.152: INFO: (8) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 34.026446ms)
Jan  3 12:43:54.153: INFO: (8) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 34.868997ms)
Jan  3 12:43:54.171: INFO: (8) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 52.528361ms)
Jan  3 12:43:54.171: INFO: (8) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 52.650309ms)
Jan  3 12:43:54.171: INFO: (8) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 53.599811ms)
Jan  3 12:43:54.171: INFO: (8) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 53.027613ms)
Jan  3 12:43:54.172: INFO: (8) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 53.294827ms)
Jan  3 12:43:54.172: INFO: (8) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 53.698842ms)
Jan  3 12:43:54.172: INFO: (8) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 53.6463ms)
Jan  3 12:43:54.172: INFO: (8) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 53.838558ms)
Jan  3 12:43:54.172: INFO: (8) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 53.500014ms)
Jan  3 12:43:54.172: INFO: (8) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 53.678132ms)
Jan  3 12:43:54.188: INFO: (8) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 69.755439ms)
Jan  3 12:43:54.188: INFO: (8) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 69.567989ms)
Jan  3 12:43:54.188: INFO: (8) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 69.407108ms)
Jan  3 12:43:54.188: INFO: (8) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 69.689692ms)
Jan  3 12:43:54.229: INFO: (9) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 40.484528ms)
Jan  3 12:43:54.229: INFO: (9) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 40.5108ms)
Jan  3 12:43:54.229: INFO: (9) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 40.64758ms)
Jan  3 12:43:54.229: INFO: (9) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 40.602595ms)
Jan  3 12:43:54.229: INFO: (9) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 40.670569ms)
Jan  3 12:43:54.229: INFO: (9) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 40.700987ms)
Jan  3 12:43:54.229: INFO: (9) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 40.899342ms)
Jan  3 12:43:54.230: INFO: (9) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 41.123449ms)
Jan  3 12:43:54.230: INFO: (9) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 40.937294ms)
Jan  3 12:43:54.230: INFO: (9) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 41.070179ms)
Jan  3 12:43:54.230: INFO: (9) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 40.876772ms)
Jan  3 12:43:54.230: INFO: (9) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 40.714052ms)
Jan  3 12:43:54.257: INFO: (9) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 68.311601ms)
Jan  3 12:43:54.257: INFO: (9) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 68.461124ms)
Jan  3 12:43:54.257: INFO: (9) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 68.606967ms)
Jan  3 12:43:54.257: INFO: (9) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 68.560584ms)
Jan  3 12:43:54.297: INFO: (10) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 39.466031ms)
Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 40.245493ms)
Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 40.179065ms)
Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 40.322476ms)
Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 40.133348ms)
Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 40.727985ms)
Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 40.247712ms)
Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 40.118236ms)
Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 40.121176ms)
Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 40.746347ms)
Jan  3 12:43:54.310: INFO: (10) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 52.275665ms)
Jan  3 12:43:54.310: INFO: (10) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 52.206836ms)
Jan  3 12:43:54.312: INFO: (10) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 53.758985ms)
Jan  3 12:43:54.312: INFO: (10) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 53.563296ms)
Jan  3 12:43:54.313: INFO: (10) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 55.631406ms)
Jan  3 12:43:54.313: INFO: (10) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 55.348694ms)
Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 43.672058ms)
Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 44.039448ms)
Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 43.661237ms)
Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 44.00541ms)
Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 44.171082ms)
Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 44.459386ms)
Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 44.451632ms)
Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 44.591618ms)
Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 44.158964ms)
Jan  3 12:43:54.359: INFO: (11) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 45.01787ms)
Jan  3 12:43:54.375: INFO: (11) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 61.280592ms)
Jan  3 12:43:54.376: INFO: (11) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 61.571316ms)
Jan  3 12:43:54.376: INFO: (11) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 61.800326ms)
Jan  3 12:43:54.376: INFO: (11) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 61.876186ms)
Jan  3 12:43:54.376: INFO: (11) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 61.910291ms)
Jan  3 12:43:54.376: INFO: (11) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 62.02357ms)
Jan  3 12:43:54.409: INFO: (12) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 33.447694ms)
Jan  3 12:43:54.413: INFO: (12) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 36.091884ms)
Jan  3 12:43:54.413: INFO: (12) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 36.798663ms)
Jan  3 12:43:54.413: INFO: (12) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 36.484421ms)
Jan  3 12:43:54.413: INFO: (12) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 36.515814ms)
Jan  3 12:43:54.414: INFO: (12) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 37.352235ms)
Jan  3 12:43:54.414: INFO: (12) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 37.095735ms)
Jan  3 12:43:54.414: INFO: (12) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 37.074564ms)
Jan  3 12:43:54.414: INFO: (12) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 37.130245ms)
Jan  3 12:43:54.414: INFO: (12) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 37.075441ms)
Jan  3 12:43:54.422: INFO: (12) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 45.631335ms)
Jan  3 12:43:54.426: INFO: (12) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 49.555927ms)
Jan  3 12:43:54.428: INFO: (12) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 51.123669ms)
Jan  3 12:43:54.428: INFO: (12) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 51.301734ms)
Jan  3 12:43:54.429: INFO: (12) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 51.596564ms)
Jan  3 12:43:54.429: INFO: (12) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 51.823969ms)
Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 38.586936ms)
Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 38.170962ms)
Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 37.707572ms)
Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 38.339021ms)
Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 39.408227ms)
Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 39.159609ms)
Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 38.811167ms)
Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 39.302404ms)
Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 39.207229ms)
Jan  3 12:43:54.469: INFO: (13) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 38.646267ms)
Jan  3 12:43:54.481: INFO: (13) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 50.71693ms)
Jan  3 12:43:54.481: INFO: (13) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 50.987881ms)
Jan  3 12:43:54.481: INFO: (13) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 50.994011ms)
Jan  3 12:43:54.481: INFO: (13) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 51.695854ms)
Jan  3 12:43:54.482: INFO: (13) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 51.935891ms)
Jan  3 12:43:54.482: INFO: (13) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 53.199458ms)
Jan  3 12:43:54.529: INFO: (14) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 45.302553ms)
Jan  3 12:43:54.529: INFO: (14) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 45.94274ms)
Jan  3 12:43:54.529: INFO: (14) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 45.286178ms)
Jan  3 12:43:54.529: INFO: (14) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 46.148974ms)
Jan  3 12:43:54.529: INFO: (14) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 46.292197ms)
Jan  3 12:43:54.529: INFO: (14) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 46.131835ms)
Jan  3 12:43:54.529: INFO: (14) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 46.432025ms)
Jan  3 12:43:54.530: INFO: (14) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 46.692235ms)
Jan  3 12:43:54.530: INFO: (14) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 46.940401ms)
Jan  3 12:43:54.530: INFO: (14) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 46.762286ms)
Jan  3 12:43:54.530: INFO: (14) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 47.003486ms)
Jan  3 12:43:54.530: INFO: (14) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 47.26765ms)
Jan  3 12:43:54.551: INFO: (14) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 68.007974ms)
Jan  3 12:43:54.551: INFO: (14) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 67.58421ms)
Jan  3 12:43:54.551: INFO: (14) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 67.814174ms)
Jan  3 12:43:54.551: INFO: (14) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 67.777719ms)
Jan  3 12:43:54.586: INFO: (15) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 34.352574ms)
Jan  3 12:43:54.586: INFO: (15) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 34.196464ms)
Jan  3 12:43:54.586: INFO: (15) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 34.33714ms)
Jan  3 12:43:54.587: INFO: (15) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 35.246669ms)
Jan  3 12:43:54.587: INFO: (15) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 35.486559ms)
Jan  3 12:43:54.588: INFO: (15) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 35.77207ms)
Jan  3 12:43:54.588: INFO: (15) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 35.778ms)
Jan  3 12:43:54.588: INFO: (15) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 36.294222ms)
Jan  3 12:43:54.589: INFO: (15) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 36.967425ms)
Jan  3 12:43:54.589: INFO: (15) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 36.711767ms)
Jan  3 12:43:54.590: INFO: (15) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 38.427684ms)
Jan  3 12:43:54.592: INFO: (15) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 40.47874ms)
Jan  3 12:43:54.600: INFO: (15) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 47.851286ms)
Jan  3 12:43:54.600: INFO: (15) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 48.633271ms)
Jan  3 12:43:54.601: INFO: (15) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 48.3562ms)
Jan  3 12:43:54.603: INFO: (15) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 50.440675ms)
Jan  3 12:43:54.644: INFO: (16) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 41.086731ms)
Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 41.880999ms)
Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 41.181738ms)
Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 41.285804ms)
Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 41.285225ms)
Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 41.403891ms)
Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 41.603645ms)
Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 41.88439ms)
Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 41.405403ms)
Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 41.419467ms)
Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 42.033213ms)
Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 41.734827ms)
Jan  3 12:43:54.659: INFO: (16) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 56.064071ms)
Jan  3 12:43:54.659: INFO: (16) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 56.225885ms)
Jan  3 12:43:54.659: INFO: (16) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 55.84301ms)
Jan  3 12:43:54.659: INFO: (16) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 56.536184ms)
Jan  3 12:43:54.692: INFO: (17) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 32.201276ms)
Jan  3 12:43:54.692: INFO: (17) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 32.260476ms)
Jan  3 12:43:54.693: INFO: (17) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 33.952105ms)
Jan  3 12:43:54.694: INFO: (17) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 34.03239ms)
Jan  3 12:43:54.694: INFO: (17) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 34.164939ms)
Jan  3 12:43:54.695: INFO: (17) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 34.563621ms)
Jan  3 12:43:54.695: INFO: (17) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 35.220061ms)
Jan  3 12:43:54.696: INFO: (17) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 36.274147ms)
Jan  3 12:43:54.696: INFO: (17) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 36.320657ms)
Jan  3 12:43:54.696: INFO: (17) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 36.194915ms)
Jan  3 12:43:54.698: INFO: (17) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 38.092236ms)
Jan  3 12:43:54.702: INFO: (17) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 42.377659ms)
Jan  3 12:43:54.708: INFO: (17) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 48.017068ms)
Jan  3 12:43:54.708: INFO: (17) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 47.892472ms)
Jan  3 12:43:54.709: INFO: (17) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 48.484323ms)
Jan  3 12:43:54.710: INFO: (17) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 50.632938ms)
Jan  3 12:43:54.753: INFO: (18) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 42.838222ms)
Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 62.394269ms)
Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 62.477631ms)
Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 62.443371ms)
Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 62.55924ms)
Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 62.592713ms)
Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 62.412263ms)
Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 63.017082ms)
Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 62.589021ms)
Jan  3 12:43:54.774: INFO: (18) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 63.382041ms)
Jan  3 12:43:54.774: INFO: (18) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 63.190142ms)
Jan  3 12:43:54.775: INFO: (18) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 64.039362ms)
Jan  3 12:43:54.774: INFO: (18) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 63.284189ms)
Jan  3 12:43:54.798: INFO: (18) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 87.539899ms)
Jan  3 12:43:54.798: INFO: (18) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 87.551158ms)
Jan  3 12:43:54.798: INFO: (18) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 87.655426ms)
Jan  3 12:43:54.838: INFO: (19) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 38.65059ms)
Jan  3 12:43:54.838: INFO: (19) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 38.836007ms)
Jan  3 12:43:54.838: INFO: (19) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 39.441565ms)
Jan  3 12:43:54.838: INFO: (19) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 39.292227ms)
Jan  3 12:43:54.838: INFO: (19) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 38.915339ms)
Jan  3 12:43:54.839: INFO: (19) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 40.133725ms)
Jan  3 12:43:54.839: INFO: (19) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 40.198924ms)
Jan  3 12:43:54.839: INFO: (19) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 40.445085ms)
Jan  3 12:43:54.839: INFO: (19) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 40.655762ms)
Jan  3 12:43:54.839: INFO: (19) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 40.853634ms)
Jan  3 12:43:54.850: INFO: (19) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 50.766061ms)
Jan  3 12:43:54.850: INFO: (19) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 51.555587ms)
Jan  3 12:43:54.851: INFO: (19) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 52.031488ms)
Jan  3 12:43:54.851: INFO: (19) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 52.597846ms)
Jan  3 12:43:54.851: INFO: (19) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 52.995137ms)
Jan  3 12:43:54.852: INFO: (19) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 53.510709ms)
STEP: deleting ReplicationController proxy-service-65x2w in namespace proxy-7614, will wait for the garbage collector to delete the pods 01/03/24 12:43:54.852
Jan  3 12:43:54.960: INFO: Deleting ReplicationController proxy-service-65x2w took: 28.041324ms
Jan  3 12:43:55.060: INFO: Terminating ReplicationController proxy-service-65x2w pods took: 100.155191ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan  3 12:43:57.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7614" for this suite. 01/03/24 12:43:57.692
------------------------------
• [SLOW TEST] [7.465 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:43:50.252
    Jan  3 12:43:50.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename proxy 01/03/24 12:43:50.255
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:43:50.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:43:50.336
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/03/24 12:43:50.392
    STEP: creating replication controller proxy-service-65x2w in namespace proxy-7614 01/03/24 12:43:50.394
    I0103 12:43:50.415997      22 runners.go:193] Created replication controller with name: proxy-service-65x2w, namespace: proxy-7614, replica count: 1
    I0103 12:43:51.467358      22 runners.go:193] proxy-service-65x2w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0103 12:43:52.468647      22 runners.go:193] proxy-service-65x2w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0103 12:43:53.469504      22 runners.go:193] proxy-service-65x2w Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 12:43:53.491: INFO: setup took 3.127047894s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/03/24 12:43:53.491
    Jan  3 12:43:53.616: INFO: (0) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 124.182041ms)
    Jan  3 12:43:53.624: INFO: (0) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 132.437123ms)
    Jan  3 12:43:53.624: INFO: (0) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 132.224325ms)
    Jan  3 12:43:53.629: INFO: (0) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 137.086991ms)
    Jan  3 12:43:53.632: INFO: (0) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 140.159171ms)
    Jan  3 12:43:53.642: INFO: (0) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 149.802585ms)
    Jan  3 12:43:53.642: INFO: (0) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 149.888452ms)
    Jan  3 12:43:53.642: INFO: (0) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 150.325476ms)
    Jan  3 12:43:53.642: INFO: (0) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 150.091253ms)
    Jan  3 12:43:53.642: INFO: (0) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 149.970082ms)
    Jan  3 12:43:53.645: INFO: (0) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 153.777495ms)
    Jan  3 12:43:53.645: INFO: (0) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 153.282622ms)
    Jan  3 12:43:53.649: INFO: (0) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 157.379444ms)
    Jan  3 12:43:53.660: INFO: (0) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 167.850211ms)
    Jan  3 12:43:53.669: INFO: (0) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 176.755033ms)
    Jan  3 12:43:53.669: INFO: (0) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 176.679036ms)
    Jan  3 12:43:53.709: INFO: (1) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 39.381092ms)
    Jan  3 12:43:53.710: INFO: (1) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 41.36401ms)
    Jan  3 12:43:53.710: INFO: (1) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 41.106331ms)
    Jan  3 12:43:53.710: INFO: (1) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 40.90798ms)
    Jan  3 12:43:53.710: INFO: (1) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 41.543157ms)
    Jan  3 12:43:53.710: INFO: (1) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 41.363524ms)
    Jan  3 12:43:53.711: INFO: (1) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 41.299245ms)
    Jan  3 12:43:53.711: INFO: (1) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 41.279759ms)
    Jan  3 12:43:53.711: INFO: (1) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 41.143358ms)
    Jan  3 12:43:53.711: INFO: (1) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 41.454434ms)
    Jan  3 12:43:53.714: INFO: (1) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 44.694583ms)
    Jan  3 12:43:53.714: INFO: (1) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 44.465857ms)
    Jan  3 12:43:53.725: INFO: (1) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 55.481515ms)
    Jan  3 12:43:53.725: INFO: (1) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 55.764184ms)
    Jan  3 12:43:53.725: INFO: (1) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 55.815717ms)
    Jan  3 12:43:53.725: INFO: (1) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 55.520028ms)
    Jan  3 12:43:53.773: INFO: (2) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 47.690305ms)
    Jan  3 12:43:53.773: INFO: (2) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 47.437264ms)
    Jan  3 12:43:53.774: INFO: (2) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 47.62009ms)
    Jan  3 12:43:53.774: INFO: (2) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 47.845805ms)
    Jan  3 12:43:53.775: INFO: (2) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 48.741667ms)
    Jan  3 12:43:53.775: INFO: (2) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 48.492484ms)
    Jan  3 12:43:53.775: INFO: (2) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 49.67867ms)
    Jan  3 12:43:53.775: INFO: (2) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 48.688362ms)
    Jan  3 12:43:53.775: INFO: (2) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 48.848333ms)
    Jan  3 12:43:53.776: INFO: (2) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 50.728396ms)
    Jan  3 12:43:53.776: INFO: (2) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 49.749866ms)
    Jan  3 12:43:53.776: INFO: (2) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 50.492541ms)
    Jan  3 12:43:53.788: INFO: (2) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 61.579209ms)
    Jan  3 12:43:53.788: INFO: (2) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 61.711884ms)
    Jan  3 12:43:53.799: INFO: (2) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 73.389273ms)
    Jan  3 12:43:53.799: INFO: (2) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 73.275419ms)
    Jan  3 12:43:53.835: INFO: (3) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 35.460867ms)
    Jan  3 12:43:53.851: INFO: (3) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 50.465105ms)
    Jan  3 12:43:53.851: INFO: (3) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 51.186159ms)
    Jan  3 12:43:53.851: INFO: (3) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 51.485581ms)
    Jan  3 12:43:53.852: INFO: (3) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 52.010041ms)
    Jan  3 12:43:53.852: INFO: (3) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 51.701333ms)
    Jan  3 12:43:53.852: INFO: (3) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 51.806524ms)
    Jan  3 12:43:53.852: INFO: (3) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 51.935649ms)
    Jan  3 12:43:53.852: INFO: (3) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 52.640842ms)
    Jan  3 12:43:53.852: INFO: (3) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 52.304946ms)
    Jan  3 12:43:53.853: INFO: (3) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 52.544641ms)
    Jan  3 12:43:53.853: INFO: (3) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 52.795472ms)
    Jan  3 12:43:53.853: INFO: (3) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 52.732424ms)
    Jan  3 12:43:53.873: INFO: (3) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 72.934794ms)
    Jan  3 12:43:53.873: INFO: (3) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 73.15526ms)
    Jan  3 12:43:53.873: INFO: (3) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 73.539469ms)
    Jan  3 12:43:53.912: INFO: (4) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 38.236879ms)
    Jan  3 12:43:53.912: INFO: (4) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 38.318245ms)
    Jan  3 12:43:53.912: INFO: (4) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 38.18156ms)
    Jan  3 12:43:53.912: INFO: (4) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 38.125838ms)
    Jan  3 12:43:53.912: INFO: (4) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 38.023467ms)
    Jan  3 12:43:53.912: INFO: (4) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 38.523133ms)
    Jan  3 12:43:53.922: INFO: (4) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 47.879481ms)
    Jan  3 12:43:53.922: INFO: (4) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 47.884322ms)
    Jan  3 12:43:53.922: INFO: (4) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 48.606062ms)
    Jan  3 12:43:53.922: INFO: (4) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 47.952136ms)
    Jan  3 12:43:53.922: INFO: (4) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 48.036007ms)
    Jan  3 12:43:53.922: INFO: (4) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 48.280552ms)
    Jan  3 12:43:53.934: INFO: (4) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 60.664474ms)
    Jan  3 12:43:53.934: INFO: (4) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 60.050851ms)
    Jan  3 12:43:53.948: INFO: (4) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 74.494538ms)
    Jan  3 12:43:53.948: INFO: (4) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 73.927206ms)
    Jan  3 12:43:53.990: INFO: (5) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 42.041386ms)
    Jan  3 12:43:53.991: INFO: (5) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 42.025716ms)
    Jan  3 12:43:53.991: INFO: (5) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 42.239709ms)
    Jan  3 12:43:53.991: INFO: (5) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 42.322052ms)
    Jan  3 12:43:53.991: INFO: (5) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 42.659558ms)
    Jan  3 12:43:53.991: INFO: (5) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 42.388011ms)
    Jan  3 12:43:53.991: INFO: (5) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 43.041925ms)
    Jan  3 12:43:53.991: INFO: (5) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 42.707518ms)
    Jan  3 12:43:53.992: INFO: (5) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 43.515227ms)
    Jan  3 12:43:53.992: INFO: (5) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 43.456437ms)
    Jan  3 12:43:53.994: INFO: (5) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 45.424913ms)
    Jan  3 12:43:53.994: INFO: (5) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 46.109023ms)
    Jan  3 12:43:54.005: INFO: (5) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 56.168998ms)
    Jan  3 12:43:54.005: INFO: (5) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 55.997077ms)
    Jan  3 12:43:54.005: INFO: (5) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 56.957019ms)
    Jan  3 12:43:54.005: INFO: (5) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 56.908764ms)
    Jan  3 12:43:54.042: INFO: (6) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 36.258218ms)
    Jan  3 12:43:54.042: INFO: (6) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 36.47493ms)
    Jan  3 12:43:54.043: INFO: (6) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 36.591412ms)
    Jan  3 12:43:54.043: INFO: (6) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 36.393244ms)
    Jan  3 12:43:54.043: INFO: (6) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 37.279077ms)
    Jan  3 12:43:54.043: INFO: (6) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 37.093268ms)
    Jan  3 12:43:54.043: INFO: (6) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 37.226138ms)
    Jan  3 12:43:54.043: INFO: (6) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 37.534795ms)
    Jan  3 12:43:54.043: INFO: (6) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 37.872968ms)
    Jan  3 12:43:54.045: INFO: (6) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 39.053355ms)
    Jan  3 12:43:54.053: INFO: (6) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 46.452228ms)
    Jan  3 12:43:54.053: INFO: (6) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 46.445412ms)
    Jan  3 12:43:54.060: INFO: (6) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 53.904608ms)
    Jan  3 12:43:54.061: INFO: (6) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 55.019809ms)
    Jan  3 12:43:54.066: INFO: (6) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 59.690548ms)
    Jan  3 12:43:54.066: INFO: (6) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 59.76775ms)
    Jan  3 12:43:54.098: INFO: (7) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 31.061478ms)
    Jan  3 12:43:54.099: INFO: (7) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 32.05738ms)
    Jan  3 12:43:54.100: INFO: (7) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 33.384087ms)
    Jan  3 12:43:54.100: INFO: (7) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 33.944059ms)
    Jan  3 12:43:54.100: INFO: (7) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 33.757627ms)
    Jan  3 12:43:54.102: INFO: (7) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 34.814844ms)
    Jan  3 12:43:54.102: INFO: (7) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 35.133927ms)
    Jan  3 12:43:54.102: INFO: (7) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 35.561907ms)
    Jan  3 12:43:54.103: INFO: (7) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 35.553333ms)
    Jan  3 12:43:54.103: INFO: (7) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 36.399709ms)
    Jan  3 12:43:54.107: INFO: (7) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 40.732833ms)
    Jan  3 12:43:54.110: INFO: (7) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 42.503007ms)
    Jan  3 12:43:54.114: INFO: (7) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 46.938284ms)
    Jan  3 12:43:54.116: INFO: (7) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 48.645931ms)
    Jan  3 12:43:54.117: INFO: (7) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 50.016337ms)
    Jan  3 12:43:54.117: INFO: (7) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 50.240114ms)
    Jan  3 12:43:54.152: INFO: (8) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 34.026446ms)
    Jan  3 12:43:54.153: INFO: (8) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 34.868997ms)
    Jan  3 12:43:54.171: INFO: (8) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 52.528361ms)
    Jan  3 12:43:54.171: INFO: (8) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 52.650309ms)
    Jan  3 12:43:54.171: INFO: (8) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 53.599811ms)
    Jan  3 12:43:54.171: INFO: (8) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 53.027613ms)
    Jan  3 12:43:54.172: INFO: (8) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 53.294827ms)
    Jan  3 12:43:54.172: INFO: (8) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 53.698842ms)
    Jan  3 12:43:54.172: INFO: (8) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 53.6463ms)
    Jan  3 12:43:54.172: INFO: (8) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 53.838558ms)
    Jan  3 12:43:54.172: INFO: (8) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 53.500014ms)
    Jan  3 12:43:54.172: INFO: (8) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 53.678132ms)
    Jan  3 12:43:54.188: INFO: (8) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 69.755439ms)
    Jan  3 12:43:54.188: INFO: (8) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 69.567989ms)
    Jan  3 12:43:54.188: INFO: (8) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 69.407108ms)
    Jan  3 12:43:54.188: INFO: (8) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 69.689692ms)
    Jan  3 12:43:54.229: INFO: (9) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 40.484528ms)
    Jan  3 12:43:54.229: INFO: (9) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 40.5108ms)
    Jan  3 12:43:54.229: INFO: (9) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 40.64758ms)
    Jan  3 12:43:54.229: INFO: (9) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 40.602595ms)
    Jan  3 12:43:54.229: INFO: (9) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 40.670569ms)
    Jan  3 12:43:54.229: INFO: (9) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 40.700987ms)
    Jan  3 12:43:54.229: INFO: (9) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 40.899342ms)
    Jan  3 12:43:54.230: INFO: (9) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 41.123449ms)
    Jan  3 12:43:54.230: INFO: (9) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 40.937294ms)
    Jan  3 12:43:54.230: INFO: (9) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 41.070179ms)
    Jan  3 12:43:54.230: INFO: (9) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 40.876772ms)
    Jan  3 12:43:54.230: INFO: (9) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 40.714052ms)
    Jan  3 12:43:54.257: INFO: (9) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 68.311601ms)
    Jan  3 12:43:54.257: INFO: (9) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 68.461124ms)
    Jan  3 12:43:54.257: INFO: (9) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 68.606967ms)
    Jan  3 12:43:54.257: INFO: (9) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 68.560584ms)
    Jan  3 12:43:54.297: INFO: (10) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 39.466031ms)
    Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 40.245493ms)
    Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 40.179065ms)
    Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 40.322476ms)
    Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 40.133348ms)
    Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 40.727985ms)
    Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 40.247712ms)
    Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 40.118236ms)
    Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 40.121176ms)
    Jan  3 12:43:54.298: INFO: (10) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 40.746347ms)
    Jan  3 12:43:54.310: INFO: (10) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 52.275665ms)
    Jan  3 12:43:54.310: INFO: (10) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 52.206836ms)
    Jan  3 12:43:54.312: INFO: (10) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 53.758985ms)
    Jan  3 12:43:54.312: INFO: (10) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 53.563296ms)
    Jan  3 12:43:54.313: INFO: (10) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 55.631406ms)
    Jan  3 12:43:54.313: INFO: (10) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 55.348694ms)
    Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 43.672058ms)
    Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 44.039448ms)
    Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 43.661237ms)
    Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 44.00541ms)
    Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 44.171082ms)
    Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 44.459386ms)
    Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 44.451632ms)
    Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 44.591618ms)
    Jan  3 12:43:54.358: INFO: (11) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 44.158964ms)
    Jan  3 12:43:54.359: INFO: (11) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 45.01787ms)
    Jan  3 12:43:54.375: INFO: (11) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 61.280592ms)
    Jan  3 12:43:54.376: INFO: (11) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 61.571316ms)
    Jan  3 12:43:54.376: INFO: (11) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 61.800326ms)
    Jan  3 12:43:54.376: INFO: (11) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 61.876186ms)
    Jan  3 12:43:54.376: INFO: (11) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 61.910291ms)
    Jan  3 12:43:54.376: INFO: (11) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 62.02357ms)
    Jan  3 12:43:54.409: INFO: (12) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 33.447694ms)
    Jan  3 12:43:54.413: INFO: (12) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 36.091884ms)
    Jan  3 12:43:54.413: INFO: (12) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 36.798663ms)
    Jan  3 12:43:54.413: INFO: (12) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 36.484421ms)
    Jan  3 12:43:54.413: INFO: (12) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 36.515814ms)
    Jan  3 12:43:54.414: INFO: (12) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 37.352235ms)
    Jan  3 12:43:54.414: INFO: (12) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 37.095735ms)
    Jan  3 12:43:54.414: INFO: (12) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 37.074564ms)
    Jan  3 12:43:54.414: INFO: (12) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 37.130245ms)
    Jan  3 12:43:54.414: INFO: (12) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 37.075441ms)
    Jan  3 12:43:54.422: INFO: (12) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 45.631335ms)
    Jan  3 12:43:54.426: INFO: (12) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 49.555927ms)
    Jan  3 12:43:54.428: INFO: (12) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 51.123669ms)
    Jan  3 12:43:54.428: INFO: (12) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 51.301734ms)
    Jan  3 12:43:54.429: INFO: (12) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 51.596564ms)
    Jan  3 12:43:54.429: INFO: (12) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 51.823969ms)
    Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 38.586936ms)
    Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 38.170962ms)
    Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 37.707572ms)
    Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 38.339021ms)
    Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 39.408227ms)
    Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 39.159609ms)
    Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 38.811167ms)
    Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 39.302404ms)
    Jan  3 12:43:54.468: INFO: (13) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 39.207229ms)
    Jan  3 12:43:54.469: INFO: (13) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 38.646267ms)
    Jan  3 12:43:54.481: INFO: (13) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 50.71693ms)
    Jan  3 12:43:54.481: INFO: (13) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 50.987881ms)
    Jan  3 12:43:54.481: INFO: (13) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 50.994011ms)
    Jan  3 12:43:54.481: INFO: (13) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 51.695854ms)
    Jan  3 12:43:54.482: INFO: (13) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 51.935891ms)
    Jan  3 12:43:54.482: INFO: (13) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 53.199458ms)
    Jan  3 12:43:54.529: INFO: (14) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 45.302553ms)
    Jan  3 12:43:54.529: INFO: (14) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 45.94274ms)
    Jan  3 12:43:54.529: INFO: (14) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 45.286178ms)
    Jan  3 12:43:54.529: INFO: (14) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 46.148974ms)
    Jan  3 12:43:54.529: INFO: (14) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 46.292197ms)
    Jan  3 12:43:54.529: INFO: (14) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 46.131835ms)
    Jan  3 12:43:54.529: INFO: (14) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 46.432025ms)
    Jan  3 12:43:54.530: INFO: (14) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 46.692235ms)
    Jan  3 12:43:54.530: INFO: (14) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 46.940401ms)
    Jan  3 12:43:54.530: INFO: (14) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 46.762286ms)
    Jan  3 12:43:54.530: INFO: (14) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 47.003486ms)
    Jan  3 12:43:54.530: INFO: (14) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 47.26765ms)
    Jan  3 12:43:54.551: INFO: (14) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 68.007974ms)
    Jan  3 12:43:54.551: INFO: (14) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 67.58421ms)
    Jan  3 12:43:54.551: INFO: (14) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 67.814174ms)
    Jan  3 12:43:54.551: INFO: (14) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 67.777719ms)
    Jan  3 12:43:54.586: INFO: (15) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 34.352574ms)
    Jan  3 12:43:54.586: INFO: (15) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 34.196464ms)
    Jan  3 12:43:54.586: INFO: (15) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 34.33714ms)
    Jan  3 12:43:54.587: INFO: (15) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 35.246669ms)
    Jan  3 12:43:54.587: INFO: (15) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 35.486559ms)
    Jan  3 12:43:54.588: INFO: (15) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 35.77207ms)
    Jan  3 12:43:54.588: INFO: (15) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 35.778ms)
    Jan  3 12:43:54.588: INFO: (15) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 36.294222ms)
    Jan  3 12:43:54.589: INFO: (15) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 36.967425ms)
    Jan  3 12:43:54.589: INFO: (15) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 36.711767ms)
    Jan  3 12:43:54.590: INFO: (15) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 38.427684ms)
    Jan  3 12:43:54.592: INFO: (15) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 40.47874ms)
    Jan  3 12:43:54.600: INFO: (15) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 47.851286ms)
    Jan  3 12:43:54.600: INFO: (15) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 48.633271ms)
    Jan  3 12:43:54.601: INFO: (15) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 48.3562ms)
    Jan  3 12:43:54.603: INFO: (15) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 50.440675ms)
    Jan  3 12:43:54.644: INFO: (16) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 41.086731ms)
    Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 41.880999ms)
    Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 41.181738ms)
    Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 41.285804ms)
    Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 41.285225ms)
    Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 41.403891ms)
    Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 41.603645ms)
    Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 41.88439ms)
    Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 41.405403ms)
    Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 41.419467ms)
    Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 42.033213ms)
    Jan  3 12:43:54.645: INFO: (16) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 41.734827ms)
    Jan  3 12:43:54.659: INFO: (16) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 56.064071ms)
    Jan  3 12:43:54.659: INFO: (16) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 56.225885ms)
    Jan  3 12:43:54.659: INFO: (16) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 55.84301ms)
    Jan  3 12:43:54.659: INFO: (16) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 56.536184ms)
    Jan  3 12:43:54.692: INFO: (17) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 32.201276ms)
    Jan  3 12:43:54.692: INFO: (17) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 32.260476ms)
    Jan  3 12:43:54.693: INFO: (17) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 33.952105ms)
    Jan  3 12:43:54.694: INFO: (17) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 34.03239ms)
    Jan  3 12:43:54.694: INFO: (17) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 34.164939ms)
    Jan  3 12:43:54.695: INFO: (17) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 34.563621ms)
    Jan  3 12:43:54.695: INFO: (17) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 35.220061ms)
    Jan  3 12:43:54.696: INFO: (17) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 36.274147ms)
    Jan  3 12:43:54.696: INFO: (17) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 36.320657ms)
    Jan  3 12:43:54.696: INFO: (17) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 36.194915ms)
    Jan  3 12:43:54.698: INFO: (17) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 38.092236ms)
    Jan  3 12:43:54.702: INFO: (17) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 42.377659ms)
    Jan  3 12:43:54.708: INFO: (17) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 48.017068ms)
    Jan  3 12:43:54.708: INFO: (17) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 47.892472ms)
    Jan  3 12:43:54.709: INFO: (17) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 48.484323ms)
    Jan  3 12:43:54.710: INFO: (17) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 50.632938ms)
    Jan  3 12:43:54.753: INFO: (18) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 42.838222ms)
    Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 62.394269ms)
    Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 62.477631ms)
    Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 62.443371ms)
    Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 62.55924ms)
    Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 62.592713ms)
    Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 62.412263ms)
    Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 63.017082ms)
    Jan  3 12:43:54.773: INFO: (18) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 62.589021ms)
    Jan  3 12:43:54.774: INFO: (18) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 63.382041ms)
    Jan  3 12:43:54.774: INFO: (18) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 63.190142ms)
    Jan  3 12:43:54.775: INFO: (18) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 64.039362ms)
    Jan  3 12:43:54.774: INFO: (18) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 63.284189ms)
    Jan  3 12:43:54.798: INFO: (18) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 87.539899ms)
    Jan  3 12:43:54.798: INFO: (18) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 87.551158ms)
    Jan  3 12:43:54.798: INFO: (18) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 87.655426ms)
    Jan  3 12:43:54.838: INFO: (19) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:462/proxy/: tls qux (200; 38.65059ms)
    Jan  3 12:43:54.838: INFO: (19) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 38.836007ms)
    Jan  3 12:43:54.838: INFO: (19) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:443/proxy/tlsrewritem... (200; 39.441565ms)
    Jan  3 12:43:54.838: INFO: (19) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">... (200; 39.292227ms)
    Jan  3 12:43:54.838: INFO: (19) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 38.915339ms)
    Jan  3 12:43:54.839: INFO: (19) /api/v1/namespaces/proxy-7614/pods/https:proxy-service-65x2w-pgjvp:460/proxy/: tls baz (200; 40.133725ms)
    Jan  3 12:43:54.839: INFO: (19) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:160/proxy/: foo (200; 40.198924ms)
    Jan  3 12:43:54.839: INFO: (19) /api/v1/namespaces/proxy-7614/pods/http:proxy-service-65x2w-pgjvp:162/proxy/: bar (200; 40.445085ms)
    Jan  3 12:43:54.839: INFO: (19) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp/proxy/rewriteme">test</a> (200; 40.655762ms)
    Jan  3 12:43:54.839: INFO: (19) /api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7614/pods/proxy-service-65x2w-pgjvp:1080/proxy/rewriteme">test<... (200; 40.853634ms)
    Jan  3 12:43:54.850: INFO: (19) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname2/proxy/: tls qux (200; 50.766061ms)
    Jan  3 12:43:54.850: INFO: (19) /api/v1/namespaces/proxy-7614/services/https:proxy-service-65x2w:tlsportname1/proxy/: tls baz (200; 51.555587ms)
    Jan  3 12:43:54.851: INFO: (19) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname2/proxy/: bar (200; 52.031488ms)
    Jan  3 12:43:54.851: INFO: (19) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname1/proxy/: foo (200; 52.597846ms)
    Jan  3 12:43:54.851: INFO: (19) /api/v1/namespaces/proxy-7614/services/http:proxy-service-65x2w:portname2/proxy/: bar (200; 52.995137ms)
    Jan  3 12:43:54.852: INFO: (19) /api/v1/namespaces/proxy-7614/services/proxy-service-65x2w:portname1/proxy/: foo (200; 53.510709ms)
    STEP: deleting ReplicationController proxy-service-65x2w in namespace proxy-7614, will wait for the garbage collector to delete the pods 01/03/24 12:43:54.852
    Jan  3 12:43:54.960: INFO: Deleting ReplicationController proxy-service-65x2w took: 28.041324ms
    Jan  3 12:43:55.060: INFO: Terminating ReplicationController proxy-service-65x2w pods took: 100.155191ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:43:57.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7614" for this suite. 01/03/24 12:43:57.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:43:57.721
Jan  3 12:43:57.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename gc 01/03/24 12:43:57.722
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:43:57.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:43:57.805
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/03/24 12:43:57.834
STEP: delete the rc 01/03/24 12:44:02.883
STEP: wait for all pods to be garbage collected 01/03/24 12:44:02.909
STEP: Gathering metrics 01/03/24 12:44:07.946
W0103 12:44:07.983127      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  3 12:44:07.983: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  3 12:44:07.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1178" for this suite. 01/03/24 12:44:08.003
------------------------------
• [SLOW TEST] [10.310 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:43:57.721
    Jan  3 12:43:57.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename gc 01/03/24 12:43:57.722
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:43:57.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:43:57.805
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/03/24 12:43:57.834
    STEP: delete the rc 01/03/24 12:44:02.883
    STEP: wait for all pods to be garbage collected 01/03/24 12:44:02.909
    STEP: Gathering metrics 01/03/24 12:44:07.946
    W0103 12:44:07.983127      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  3 12:44:07.983: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:44:07.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1178" for this suite. 01/03/24 12:44:08.003
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:44:08.032
Jan  3 12:44:08.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubelet-test 01/03/24 12:44:08.034
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:44:08.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:44:08.116
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  3 12:44:12.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7931" for this suite. 01/03/24 12:44:12.243
------------------------------
• [4.240 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:44:08.032
    Jan  3 12:44:08.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubelet-test 01/03/24 12:44:08.034
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:44:08.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:44:08.116
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:44:12.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7931" for this suite. 01/03/24 12:44:12.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:44:12.273
Jan  3 12:44:12.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename deployment 01/03/24 12:44:12.274
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:44:12.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:44:12.37
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan  3 12:44:12.398: INFO: Creating deployment "webserver-deployment"
Jan  3 12:44:12.417: INFO: Waiting for observed generation 1
Jan  3 12:44:14.452: INFO: Waiting for all required pods to come up
Jan  3 12:44:14.483: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/03/24 12:44:14.483
Jan  3 12:44:14.483: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-w854v" in namespace "deployment-4643" to be "running"
Jan  3 12:44:14.483: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-8nfbr" in namespace "deployment-4643" to be "running"
Jan  3 12:44:14.483: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9qzq7" in namespace "deployment-4643" to be "running"
Jan  3 12:44:14.483: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fr8wx" in namespace "deployment-4643" to be "running"
Jan  3 12:44:14.483: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jm5p9" in namespace "deployment-4643" to be "running"
Jan  3 12:44:14.484: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-k5xtv" in namespace "deployment-4643" to be "running"
Jan  3 12:44:14.484: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jgjvj" in namespace "deployment-4643" to be "running"
Jan  3 12:44:14.484: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qb477" in namespace "deployment-4643" to be "running"
Jan  3 12:44:14.484: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nkq9h" in namespace "deployment-4643" to be "running"
Jan  3 12:44:14.501: INFO: Pod "webserver-deployment-7f5969cbc7-8nfbr": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013032ms
Jan  3 12:44:14.503: INFO: Pod "webserver-deployment-7f5969cbc7-fr8wx": Phase="Pending", Reason="", readiness=false. Elapsed: 19.750505ms
Jan  3 12:44:14.503: INFO: Pod "webserver-deployment-7f5969cbc7-9qzq7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.922261ms
Jan  3 12:44:14.527: INFO: Pod "webserver-deployment-7f5969cbc7-jgjvj": Phase="Pending", Reason="", readiness=false. Elapsed: 43.037901ms
Jan  3 12:44:14.533: INFO: Pod "webserver-deployment-7f5969cbc7-jm5p9": Phase="Pending", Reason="", readiness=false. Elapsed: 49.915474ms
Jan  3 12:44:14.534: INFO: Pod "webserver-deployment-7f5969cbc7-nkq9h": Phase="Pending", Reason="", readiness=false. Elapsed: 50.347265ms
Jan  3 12:44:14.534: INFO: Pod "webserver-deployment-7f5969cbc7-k5xtv": Phase="Pending", Reason="", readiness=false. Elapsed: 50.843501ms
Jan  3 12:44:14.534: INFO: Pod "webserver-deployment-7f5969cbc7-w854v": Phase="Running", Reason="", readiness=true. Elapsed: 51.169131ms
Jan  3 12:44:14.535: INFO: Pod "webserver-deployment-7f5969cbc7-w854v" satisfied condition "running"
Jan  3 12:44:14.534: INFO: Pod "webserver-deployment-7f5969cbc7-qb477": Phase="Pending", Reason="", readiness=false. Elapsed: 50.751722ms
Jan  3 12:44:16.527: INFO: Pod "webserver-deployment-7f5969cbc7-fr8wx": Phase="Running", Reason="", readiness=true. Elapsed: 2.043827305s
Jan  3 12:44:16.527: INFO: Pod "webserver-deployment-7f5969cbc7-fr8wx" satisfied condition "running"
Jan  3 12:44:16.530: INFO: Pod "webserver-deployment-7f5969cbc7-9qzq7": Phase="Running", Reason="", readiness=true. Elapsed: 2.046812493s
Jan  3 12:44:16.530: INFO: Pod "webserver-deployment-7f5969cbc7-9qzq7" satisfied condition "running"
Jan  3 12:44:16.538: INFO: Pod "webserver-deployment-7f5969cbc7-8nfbr": Phase="Running", Reason="", readiness=true. Elapsed: 2.054749186s
Jan  3 12:44:16.538: INFO: Pod "webserver-deployment-7f5969cbc7-8nfbr" satisfied condition "running"
Jan  3 12:44:16.544: INFO: Pod "webserver-deployment-7f5969cbc7-jgjvj": Phase="Running", Reason="", readiness=true. Elapsed: 2.060240865s
Jan  3 12:44:16.544: INFO: Pod "webserver-deployment-7f5969cbc7-jgjvj" satisfied condition "running"
Jan  3 12:44:16.551: INFO: Pod "webserver-deployment-7f5969cbc7-nkq9h": Phase="Running", Reason="", readiness=true. Elapsed: 2.067433792s
Jan  3 12:44:16.551: INFO: Pod "webserver-deployment-7f5969cbc7-nkq9h" satisfied condition "running"
Jan  3 12:44:16.554: INFO: Pod "webserver-deployment-7f5969cbc7-k5xtv": Phase="Running", Reason="", readiness=true. Elapsed: 2.07070952s
Jan  3 12:44:16.554: INFO: Pod "webserver-deployment-7f5969cbc7-k5xtv" satisfied condition "running"
Jan  3 12:44:16.555: INFO: Pod "webserver-deployment-7f5969cbc7-qb477": Phase="Running", Reason="", readiness=true. Elapsed: 2.070887547s
Jan  3 12:44:16.555: INFO: Pod "webserver-deployment-7f5969cbc7-qb477" satisfied condition "running"
Jan  3 12:44:16.555: INFO: Pod "webserver-deployment-7f5969cbc7-jm5p9": Phase="Running", Reason="", readiness=true. Elapsed: 2.07120475s
Jan  3 12:44:16.555: INFO: Pod "webserver-deployment-7f5969cbc7-jm5p9" satisfied condition "running"
Jan  3 12:44:16.555: INFO: Waiting for deployment "webserver-deployment" to complete
Jan  3 12:44:16.589: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan  3 12:44:16.633: INFO: Updating deployment webserver-deployment
Jan  3 12:44:16.633: INFO: Waiting for observed generation 2
Jan  3 12:44:18.677: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan  3 12:44:18.696: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan  3 12:44:18.714: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan  3 12:44:18.790: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan  3 12:44:18.790: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan  3 12:44:18.814: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan  3 12:44:18.847: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan  3 12:44:18.847: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan  3 12:44:18.893: INFO: Updating deployment webserver-deployment
Jan  3 12:44:18.893: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan  3 12:44:18.930: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan  3 12:44:20.968: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 12:44:21.006: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4643  ccd164a7-9712-48ce-8bfd-4d0c04bb2a18 33981177271 3 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00395cb88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-03 12:44:18 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2024-01-03 12:44:19 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan  3 12:44:21.022: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-4643  a337c5cd-a803-4508-9818-00829141903c 33981177263 3 2024-01-03 12:44:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment ccd164a7-9712-48ce-8bfd-4d0c04bb2a18 0xc0041aad37 0xc0041aad38}] [] [{kube-controller-manager Update apps/v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccd164a7-9712-48ce-8bfd-4d0c04bb2a18\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041aadd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 12:44:21.022: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan  3 12:44:21.023: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-4643  83ecf90b-98ef-4215-8624-653e5f2df47f 33981177256 3 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment ccd164a7-9712-48ce-8bfd-4d0c04bb2a18 0xc0041aaa87 0xc0041aaa88}] [] [{kube-controller-manager Update apps/v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccd164a7-9712-48ce-8bfd-4d0c04bb2a18\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041aacd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan  3 12:44:21.071: INFO: Pod "webserver-deployment-7f5969cbc7-8nfbr" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8nfbr webserver-deployment-7f5969cbc7- deployment-4643  cbb91d03-5bdf-46b7-a4ad-6ce13a2cfc58 33981176582 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8fc9e6b5510a9934213e6213ff33402e167889a748e7c241460829cb6f9c835e cni.projectcalico.org/podIP:10.221.146.101/32 cni.projectcalico.org/podIPs:10.221.146.101/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395cf97 0xc00395cf98}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hh5gk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hh5gk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.101,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://df465db4e8dc27e5dda0bdb7b096d8a5b267b459c02189ac109dd8260bcc0aa8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.071: INFO: Pod "webserver-deployment-7f5969cbc7-9f4bs" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9f4bs webserver-deployment-7f5969cbc7- deployment-4643  4f7820a4-3fdb-462f-bc01-36850e4da16b 33981177555 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d1e479210ac728af4b76562a5bf2704fd1ec7963909d64ce47bd1240984da324 cni.projectcalico.org/podIP:10.222.238.199/32 cni.projectcalico.org/podIPs:10.222.238.199/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395d197 0xc00395d198}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2h9x4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2h9x4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.071: INFO: Pod "webserver-deployment-7f5969cbc7-9qzq7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9qzq7 webserver-deployment-7f5969cbc7- deployment-4643  2f1b6df1-15ed-46b7-83c1-85d16cf7ea70 33981176654 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cf77b6465428e7fe44f7d7cf325762c9b8d03985e66af524153d3b0717cbffe9 cni.projectcalico.org/podIP:10.222.238.250/32 cni.projectcalico.org/podIPs:10.222.238.250/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395d377 0xc00395d378}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.222.238.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-plxn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-plxn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:10.222.238.250,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aa0f1514690837d184410b559e31b4721e5b5b2b83cd131571284974bf98b13b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.222.238.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.072: INFO: Pod "webserver-deployment-7f5969cbc7-c877s" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c877s webserver-deployment-7f5969cbc7- deployment-4643  1f6d26ae-b674-446b-8be4-5899ce53ee18 33981177578 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cd2d0e4dfe43dee8c595737c4904e450c7c60e10c9848db1628df95e5a698e99 cni.projectcalico.org/podIP:10.221.146.110/32 cni.projectcalico.org/podIPs:10.221.146.110/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395d587 0xc00395d588}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-599m9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-599m9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.072: INFO: Pod "webserver-deployment-7f5969cbc7-cbrxg" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cbrxg webserver-deployment-7f5969cbc7- deployment-4643  f3f77c7c-49ef-45bf-b8e8-186d5356d753 33981177516 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:201dc6e9fa56d340ca76eeb2e877a98d2d2ba226dc9e9413840d72edaa4f7dfa cni.projectcalico.org/podIP:10.222.238.255/32 cni.projectcalico.org/podIPs:10.222.238.255/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395d767 0xc00395d768}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jhskf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jhskf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.073: INFO: Pod "webserver-deployment-7f5969cbc7-fcmbt" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fcmbt webserver-deployment-7f5969cbc7- deployment-4643  58eaefaf-a945-4914-9083-4ad27964784c 33981177238 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395d947 0xc00395d948}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7j8n8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7j8n8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.073: INFO: Pod "webserver-deployment-7f5969cbc7-gdqnc" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gdqnc webserver-deployment-7f5969cbc7- deployment-4643  a30b9597-9786-4a54-883c-64d9ba311119 33981176503 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9eaa4bbfc389ab5d65909a7b68c1d992451e1234f2cbb4035c7f9616e87177ee cni.projectcalico.org/podIP:10.222.238.248/32 cni.projectcalico.org/podIPs:10.222.238.248/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395daa7 0xc00395daa8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.222.238.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxjkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxjkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:10.222.238.248,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://23c28a1a6a2b251d247e548904723697546418343d561e99ce6f3d3fcfddb57d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.222.238.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.073: INFO: Pod "webserver-deployment-7f5969cbc7-jgjvj" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jgjvj webserver-deployment-7f5969cbc7- deployment-4643  51c8ab54-32a7-434f-9012-052888081213 33981176821 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c1178943abccd8f1d18e39f43ab348ebbf2124f306a52a177a5c6c0af5a06104 cni.projectcalico.org/podIP:10.221.146.150/32 cni.projectcalico.org/podIPs:10.221.146.150/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395dca7 0xc00395dca8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4z8rx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4z8rx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:10.221.146.150,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aac5ba7b47677fa25b1b3c14260bf821ce50910177c2a0ef875a347dea12b5fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.074: INFO: Pod "webserver-deployment-7f5969cbc7-jm5p9" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jm5p9 webserver-deployment-7f5969cbc7- deployment-4643  9654bae1-34ce-474c-950e-3990942b6be0 33981176684 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ba5436917e1dfac6bbcfea4cab9183191eafaeb28158585176ccb294c660f6cc cni.projectcalico.org/podIP:10.221.146.149/32 cni.projectcalico.org/podIPs:10.221.146.149/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395ded7 0xc00395ded8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7qb6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7qb6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:10.221.146.149,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b6c8e6ada396ceca65cdc875b9fc70d66e601a400f4abe988f0ab8dbf6f4352e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.074: INFO: Pod "webserver-deployment-7f5969cbc7-k47xw" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k47xw webserver-deployment-7f5969cbc7- deployment-4643  200d5b98-959d-41f4-be66-be16f61c7eeb 33981177465 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:73cdabcea6c69008997cde6b03a8fdd18f4875266c24557304ef9bc399dee4fa cni.projectcalico.org/podIP:10.221.146.155/32 cni.projectcalico.org/podIPs:10.221.146.155/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301e0d7 0xc00301e0d8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jcp9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jcp9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.074: INFO: Pod "webserver-deployment-7f5969cbc7-k5xtv" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k5xtv webserver-deployment-7f5969cbc7- deployment-4643  cff69f71-ce02-4d01-b82f-ccf3950eecbe 33981176661 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:aa3b54e93646bd136a1588610a0624bbeae08cb413693e578b09a8d46d255393 cni.projectcalico.org/podIP:10.222.238.249/32 cni.projectcalico.org/podIPs:10.222.238.249/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301e2b7 0xc00301e2b8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.222.238.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nw82c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nw82c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:10.222.238.249,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://18c9890716827160312789b5bccc10a320206aa171843b877b88eaf0d21a358f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.222.238.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.074: INFO: Pod "webserver-deployment-7f5969cbc7-ph4sk" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ph4sk webserver-deployment-7f5969cbc7- deployment-4643  b5d5ddc2-5b77-4f4d-8731-fb80af88073f 33981177286 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301e4e7 0xc00301e4e8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jg2dj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jg2dj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.075: INFO: Pod "webserver-deployment-7f5969cbc7-pxsqp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pxsqp webserver-deployment-7f5969cbc7- deployment-4643  aae3ed13-7c4a-451e-8d4f-3bff74d0ee7d 33981177308 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301e6b7 0xc00301e6b8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8svhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8svhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:,StartTime:2024-01-03 12:44:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.075: INFO: Pod "webserver-deployment-7f5969cbc7-q8scm" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q8scm webserver-deployment-7f5969cbc7- deployment-4643  b71f7219-0694-4f11-8ad0-41790d9f70c1 33981177296 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301e887 0xc00301e888}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j97sf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j97sf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.075: INFO: Pod "webserver-deployment-7f5969cbc7-qb477" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qb477 webserver-deployment-7f5969cbc7- deployment-4643  e331f895-712b-4fd7-a58e-e928fe499426 33981176832 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:460f211d21c53a4ea302e69c28905d344e8c80e28ffccf9574a0a2a694e4dcc2 cni.projectcalico.org/podIP:10.221.146.151/32 cni.projectcalico.org/podIPs:10.221.146.151/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301ea67 0xc00301ea68}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ds2w4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ds2w4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:10.221.146.151,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8c864b413de8e0dd22d4c945526295223bee40d296ff47f7c355bab7320f701a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.075: INFO: Pod "webserver-deployment-7f5969cbc7-ssbb4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ssbb4 webserver-deployment-7f5969cbc7- deployment-4643  55addda2-5808-4925-a35a-39a62f9b6093 33981177395 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301ec67 0xc00301ec68}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wj7sm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wj7sm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.075: INFO: Pod "webserver-deployment-7f5969cbc7-w854v" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-w854v webserver-deployment-7f5969cbc7- deployment-4643  6bcf2902-6ce0-476d-b08d-ffa55154a07e 33981176574 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a5560ee24b472ec4c5dfff55f4b78de1ecf59042add19f95a3ecac5d7745c5a7 cni.projectcalico.org/podIP:10.221.146.102/32 cni.projectcalico.org/podIPs:10.221.146.102/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301ee27 0xc00301ee28}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qjll4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qjll4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.102,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://591dacd195e49aaade27231ac970324610b1d59a6a107fefed4bada30986b0d9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.076: INFO: Pod "webserver-deployment-7f5969cbc7-xkdx6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xkdx6 webserver-deployment-7f5969cbc7- deployment-4643  f3481d12-7fe1-4143-b778-1aae086cdd69 33981177530 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301f027 0xc00301f028}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8d4ql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8d4ql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.079: INFO: Pod "webserver-deployment-7f5969cbc7-z78mc" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-z78mc webserver-deployment-7f5969cbc7- deployment-4643  5f90ea4b-4c5a-4cde-a678-7d61279a04a6 33981177254 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301f1e7 0xc00301f1e8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lpfm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lpfm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.079: INFO: Pod "webserver-deployment-7f5969cbc7-zwmcp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zwmcp webserver-deployment-7f5969cbc7- deployment-4643  802922b7-53ef-4b3a-b3b6-c0119ef89e3b 33981177458 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301f3c7 0xc00301f3c8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfk5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfk5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:,StartTime:2024-01-03 12:44:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.079: INFO: Pod "webserver-deployment-d9f79cb5-5dc4m" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5dc4m webserver-deployment-d9f79cb5- deployment-4643  6f6855c6-f9ae-4653-89a6-667888130f8d 33981177251 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc00301f587 0xc00301f588}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7lqxq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7lqxq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.079: INFO: Pod "webserver-deployment-d9f79cb5-9hsx4" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9hsx4 webserver-deployment-d9f79cb5- deployment-4643  2eca47d1-7760-477b-9b9d-ade98a7c57fa 33981177405 0 2024-01-03 12:44:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:43c668a10b02892921b7e1e5750e7016e0e69829117e12f199e3c8b7bfe02500 cni.projectcalico.org/podIP:10.221.146.105/32 cni.projectcalico.org/podIPs:10.221.146.105/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc00301f6f7 0xc00301f6f8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcw7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcw7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.079: INFO: Pod "webserver-deployment-d9f79cb5-clbfw" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-clbfw webserver-deployment-d9f79cb5- deployment-4643  2f69e470-4ad7-4ecf-b47f-9d613c573c3d 33981177419 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f991b7982cc8d4dd1a9bbd850c800f43a0ee1e2b0e08aaa18320a381160d7c58 cni.projectcalico.org/podIP:10.222.238.253/32 cni.projectcalico.org/podIPs:10.222.238.253/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc00301f8f7 0xc00301f8f8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j5tp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j5tp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.080: INFO: Pod "webserver-deployment-d9f79cb5-lvnnz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lvnnz webserver-deployment-d9f79cb5- deployment-4643  44632ea4-be80-49a9-9661-24fab05a61a1 33981177364 0 2024-01-03 12:44:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:8342cea68f576b7e258757912a7cc0bea2f2346f0d001242066ee015d1166baf cni.projectcalico.org/podIP:10.221.146.103/32 cni.projectcalico.org/podIPs:10.221.146.103/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc00301faf7 0xc00301faf8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2pps7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2pps7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.080: INFO: Pod "webserver-deployment-d9f79cb5-mmkvn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mmkvn webserver-deployment-d9f79cb5- deployment-4643  be64b166-605b-4103-adf5-02785772b285 33981177582 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d69d633c1109ed4bb95eb22c215e7ff868a1c2c47f1b800506636b188cd22a93 cni.projectcalico.org/podIP:10.221.146.163/32 cni.projectcalico.org/podIPs:10.221.146.163/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc00301fd57 0xc00301fd58}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rgsg7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rgsg7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.080: INFO: Pod "webserver-deployment-d9f79cb5-pxnlg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pxnlg webserver-deployment-d9f79cb5- deployment-4643  e83d1aea-5856-4a7e-aec0-1f873b923177 33981177460 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:80cb993a8bfe647063fa6d022678b529f3ac3080b97ea841d5dd8af7036c861b cni.projectcalico.org/podIP:10.222.238.254/32 cni.projectcalico.org/podIPs:10.222.238.254/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa21e7 0xc004aa21e8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wnsxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wnsxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.080: INFO: Pod "webserver-deployment-d9f79cb5-q4424" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-q4424 webserver-deployment-d9f79cb5- deployment-4643  d1a18efe-c5d7-4a6c-ab62-8fc5091cd162 33981177523 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:fe9b7a1d265330c24e5267a849ee5ba5f504e092cd86beffd4b68b002bbf2281 cni.projectcalico.org/podIP:10.221.146.156/32 cni.projectcalico.org/podIPs:10.221.146.156/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa23e7 0xc004aa23e8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x2qb9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x2qb9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.081: INFO: Pod "webserver-deployment-d9f79cb5-tcd97" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tcd97 webserver-deployment-d9f79cb5- deployment-4643  9b83399b-1b3e-40af-9aa6-e4f13ef8e0ce 33981177032 0 2024-01-03 12:44:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:4fc6f5442b87c4ced9e955e2abd9fb79dc6cdc0b59a0ace95391349dba818a87 cni.projectcalico.org/podIP:10.221.146.152/32 cni.projectcalico.org/podIPs:10.221.146.152/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa25e7 0xc004aa25e8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87crq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87crq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:,StartTime:2024-01-03 12:44:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.081: INFO: Pod "webserver-deployment-d9f79cb5-vdtfb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vdtfb webserver-deployment-d9f79cb5- deployment-4643  f8705260-3524-4618-8a35-d2a80773e730 33981177040 0 2024-01-03 12:44:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2e1bb01fe4f1c6e37bebfc947ef98c5e01bd6867e0c3c4f8266946ff73de0b02 cni.projectcalico.org/podIP:10.222.238.251/32 cni.projectcalico.org/podIPs:10.222.238.251/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa27e7 0xc004aa27e8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bpgr7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bpgr7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.081: INFO: Pod "webserver-deployment-d9f79cb5-w2zg8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w2zg8 webserver-deployment-d9f79cb5- deployment-4643  5edc9a02-47d8-4c8c-8e0a-c74cf2e88e28 33981177072 0 2024-01-03 12:44:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1e2338a72144434ef0eaecf8f5dc7d338faae367bb9e141261f0589e03cc831c cni.projectcalico.org/podIP:10.222.238.252/32 cni.projectcalico.org/podIPs:10.222.238.252/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa2a07 0xc004aa2a08}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8r85l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8r85l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.082: INFO: Pod "webserver-deployment-d9f79cb5-wtxhg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wtxhg webserver-deployment-d9f79cb5- deployment-4643  00c53f0c-7de6-4b6a-918c-ccda2d988863 33981177532 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:96472199867111c88bace728ac89992a7c5e6ed106f403d4bd8859f235a1dcf1 cni.projectcalico.org/podIP:10.221.146.106/32 cni.projectcalico.org/podIPs:10.221.146.106/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa2c17 0xc004aa2c18}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q5wlj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q5wlj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.082: INFO: Pod "webserver-deployment-d9f79cb5-xgfhx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xgfhx webserver-deployment-d9f79cb5- deployment-4643  730aea64-4aa6-4a5c-82c3-9c71bbd2620c 33981177501 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:017f1ecfbb67cd56231e9362f34f97351605e70ae623e019f3cb9dc9b6420252 cni.projectcalico.org/podIP:10.221.146.104/32 cni.projectcalico.org/podIPs:10.221.146.104/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa2e47 0xc004aa2e48}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ljvrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ljvrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 12:44:21.083: INFO: Pod "webserver-deployment-d9f79cb5-xmkh8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xmkh8 webserver-deployment-d9f79cb5- deployment-4643  929bce8e-4fb1-4f81-a6d1-b16475d48085 33981177554 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:bc874edffa9fcd09d0022762bfb635c4b3bfa1431af6b782d669e0b4cad479ee cni.projectcalico.org/podIP:10.221.146.162/32 cni.projectcalico.org/podIPs:10.221.146.162/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa3047 0xc004aa3048}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dd8mc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dd8mc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  3 12:44:21.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4643" for this suite. 01/03/24 12:44:21.105
------------------------------
• [SLOW TEST] [8.858 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:44:12.273
    Jan  3 12:44:12.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename deployment 01/03/24 12:44:12.274
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:44:12.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:44:12.37
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan  3 12:44:12.398: INFO: Creating deployment "webserver-deployment"
    Jan  3 12:44:12.417: INFO: Waiting for observed generation 1
    Jan  3 12:44:14.452: INFO: Waiting for all required pods to come up
    Jan  3 12:44:14.483: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/03/24 12:44:14.483
    Jan  3 12:44:14.483: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-w854v" in namespace "deployment-4643" to be "running"
    Jan  3 12:44:14.483: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-8nfbr" in namespace "deployment-4643" to be "running"
    Jan  3 12:44:14.483: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9qzq7" in namespace "deployment-4643" to be "running"
    Jan  3 12:44:14.483: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fr8wx" in namespace "deployment-4643" to be "running"
    Jan  3 12:44:14.483: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jm5p9" in namespace "deployment-4643" to be "running"
    Jan  3 12:44:14.484: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-k5xtv" in namespace "deployment-4643" to be "running"
    Jan  3 12:44:14.484: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jgjvj" in namespace "deployment-4643" to be "running"
    Jan  3 12:44:14.484: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qb477" in namespace "deployment-4643" to be "running"
    Jan  3 12:44:14.484: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nkq9h" in namespace "deployment-4643" to be "running"
    Jan  3 12:44:14.501: INFO: Pod "webserver-deployment-7f5969cbc7-8nfbr": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013032ms
    Jan  3 12:44:14.503: INFO: Pod "webserver-deployment-7f5969cbc7-fr8wx": Phase="Pending", Reason="", readiness=false. Elapsed: 19.750505ms
    Jan  3 12:44:14.503: INFO: Pod "webserver-deployment-7f5969cbc7-9qzq7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.922261ms
    Jan  3 12:44:14.527: INFO: Pod "webserver-deployment-7f5969cbc7-jgjvj": Phase="Pending", Reason="", readiness=false. Elapsed: 43.037901ms
    Jan  3 12:44:14.533: INFO: Pod "webserver-deployment-7f5969cbc7-jm5p9": Phase="Pending", Reason="", readiness=false. Elapsed: 49.915474ms
    Jan  3 12:44:14.534: INFO: Pod "webserver-deployment-7f5969cbc7-nkq9h": Phase="Pending", Reason="", readiness=false. Elapsed: 50.347265ms
    Jan  3 12:44:14.534: INFO: Pod "webserver-deployment-7f5969cbc7-k5xtv": Phase="Pending", Reason="", readiness=false. Elapsed: 50.843501ms
    Jan  3 12:44:14.534: INFO: Pod "webserver-deployment-7f5969cbc7-w854v": Phase="Running", Reason="", readiness=true. Elapsed: 51.169131ms
    Jan  3 12:44:14.535: INFO: Pod "webserver-deployment-7f5969cbc7-w854v" satisfied condition "running"
    Jan  3 12:44:14.534: INFO: Pod "webserver-deployment-7f5969cbc7-qb477": Phase="Pending", Reason="", readiness=false. Elapsed: 50.751722ms
    Jan  3 12:44:16.527: INFO: Pod "webserver-deployment-7f5969cbc7-fr8wx": Phase="Running", Reason="", readiness=true. Elapsed: 2.043827305s
    Jan  3 12:44:16.527: INFO: Pod "webserver-deployment-7f5969cbc7-fr8wx" satisfied condition "running"
    Jan  3 12:44:16.530: INFO: Pod "webserver-deployment-7f5969cbc7-9qzq7": Phase="Running", Reason="", readiness=true. Elapsed: 2.046812493s
    Jan  3 12:44:16.530: INFO: Pod "webserver-deployment-7f5969cbc7-9qzq7" satisfied condition "running"
    Jan  3 12:44:16.538: INFO: Pod "webserver-deployment-7f5969cbc7-8nfbr": Phase="Running", Reason="", readiness=true. Elapsed: 2.054749186s
    Jan  3 12:44:16.538: INFO: Pod "webserver-deployment-7f5969cbc7-8nfbr" satisfied condition "running"
    Jan  3 12:44:16.544: INFO: Pod "webserver-deployment-7f5969cbc7-jgjvj": Phase="Running", Reason="", readiness=true. Elapsed: 2.060240865s
    Jan  3 12:44:16.544: INFO: Pod "webserver-deployment-7f5969cbc7-jgjvj" satisfied condition "running"
    Jan  3 12:44:16.551: INFO: Pod "webserver-deployment-7f5969cbc7-nkq9h": Phase="Running", Reason="", readiness=true. Elapsed: 2.067433792s
    Jan  3 12:44:16.551: INFO: Pod "webserver-deployment-7f5969cbc7-nkq9h" satisfied condition "running"
    Jan  3 12:44:16.554: INFO: Pod "webserver-deployment-7f5969cbc7-k5xtv": Phase="Running", Reason="", readiness=true. Elapsed: 2.07070952s
    Jan  3 12:44:16.554: INFO: Pod "webserver-deployment-7f5969cbc7-k5xtv" satisfied condition "running"
    Jan  3 12:44:16.555: INFO: Pod "webserver-deployment-7f5969cbc7-qb477": Phase="Running", Reason="", readiness=true. Elapsed: 2.070887547s
    Jan  3 12:44:16.555: INFO: Pod "webserver-deployment-7f5969cbc7-qb477" satisfied condition "running"
    Jan  3 12:44:16.555: INFO: Pod "webserver-deployment-7f5969cbc7-jm5p9": Phase="Running", Reason="", readiness=true. Elapsed: 2.07120475s
    Jan  3 12:44:16.555: INFO: Pod "webserver-deployment-7f5969cbc7-jm5p9" satisfied condition "running"
    Jan  3 12:44:16.555: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan  3 12:44:16.589: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan  3 12:44:16.633: INFO: Updating deployment webserver-deployment
    Jan  3 12:44:16.633: INFO: Waiting for observed generation 2
    Jan  3 12:44:18.677: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan  3 12:44:18.696: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan  3 12:44:18.714: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan  3 12:44:18.790: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan  3 12:44:18.790: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan  3 12:44:18.814: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan  3 12:44:18.847: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan  3 12:44:18.847: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan  3 12:44:18.893: INFO: Updating deployment webserver-deployment
    Jan  3 12:44:18.893: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan  3 12:44:18.930: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan  3 12:44:20.968: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 12:44:21.006: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-4643  ccd164a7-9712-48ce-8bfd-4d0c04bb2a18 33981177271 3 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00395cb88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-03 12:44:18 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2024-01-03 12:44:19 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan  3 12:44:21.022: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-4643  a337c5cd-a803-4508-9818-00829141903c 33981177263 3 2024-01-03 12:44:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment ccd164a7-9712-48ce-8bfd-4d0c04bb2a18 0xc0041aad37 0xc0041aad38}] [] [{kube-controller-manager Update apps/v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccd164a7-9712-48ce-8bfd-4d0c04bb2a18\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041aadd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 12:44:21.022: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan  3 12:44:21.023: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-4643  83ecf90b-98ef-4215-8624-653e5f2df47f 33981177256 3 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment ccd164a7-9712-48ce-8bfd-4d0c04bb2a18 0xc0041aaa87 0xc0041aaa88}] [] [{kube-controller-manager Update apps/v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccd164a7-9712-48ce-8bfd-4d0c04bb2a18\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041aacd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 12:44:21.071: INFO: Pod "webserver-deployment-7f5969cbc7-8nfbr" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8nfbr webserver-deployment-7f5969cbc7- deployment-4643  cbb91d03-5bdf-46b7-a4ad-6ce13a2cfc58 33981176582 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8fc9e6b5510a9934213e6213ff33402e167889a748e7c241460829cb6f9c835e cni.projectcalico.org/podIP:10.221.146.101/32 cni.projectcalico.org/podIPs:10.221.146.101/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395cf97 0xc00395cf98}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hh5gk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hh5gk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.101,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://df465db4e8dc27e5dda0bdb7b096d8a5b267b459c02189ac109dd8260bcc0aa8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.071: INFO: Pod "webserver-deployment-7f5969cbc7-9f4bs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9f4bs webserver-deployment-7f5969cbc7- deployment-4643  4f7820a4-3fdb-462f-bc01-36850e4da16b 33981177555 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d1e479210ac728af4b76562a5bf2704fd1ec7963909d64ce47bd1240984da324 cni.projectcalico.org/podIP:10.222.238.199/32 cni.projectcalico.org/podIPs:10.222.238.199/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395d197 0xc00395d198}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2h9x4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2h9x4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.071: INFO: Pod "webserver-deployment-7f5969cbc7-9qzq7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9qzq7 webserver-deployment-7f5969cbc7- deployment-4643  2f1b6df1-15ed-46b7-83c1-85d16cf7ea70 33981176654 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cf77b6465428e7fe44f7d7cf325762c9b8d03985e66af524153d3b0717cbffe9 cni.projectcalico.org/podIP:10.222.238.250/32 cni.projectcalico.org/podIPs:10.222.238.250/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395d377 0xc00395d378}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.222.238.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-plxn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-plxn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:10.222.238.250,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aa0f1514690837d184410b559e31b4721e5b5b2b83cd131571284974bf98b13b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.222.238.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.072: INFO: Pod "webserver-deployment-7f5969cbc7-c877s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c877s webserver-deployment-7f5969cbc7- deployment-4643  1f6d26ae-b674-446b-8be4-5899ce53ee18 33981177578 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cd2d0e4dfe43dee8c595737c4904e450c7c60e10c9848db1628df95e5a698e99 cni.projectcalico.org/podIP:10.221.146.110/32 cni.projectcalico.org/podIPs:10.221.146.110/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395d587 0xc00395d588}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-599m9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-599m9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.072: INFO: Pod "webserver-deployment-7f5969cbc7-cbrxg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cbrxg webserver-deployment-7f5969cbc7- deployment-4643  f3f77c7c-49ef-45bf-b8e8-186d5356d753 33981177516 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:201dc6e9fa56d340ca76eeb2e877a98d2d2ba226dc9e9413840d72edaa4f7dfa cni.projectcalico.org/podIP:10.222.238.255/32 cni.projectcalico.org/podIPs:10.222.238.255/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395d767 0xc00395d768}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jhskf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jhskf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.073: INFO: Pod "webserver-deployment-7f5969cbc7-fcmbt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fcmbt webserver-deployment-7f5969cbc7- deployment-4643  58eaefaf-a945-4914-9083-4ad27964784c 33981177238 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395d947 0xc00395d948}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7j8n8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7j8n8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.073: INFO: Pod "webserver-deployment-7f5969cbc7-gdqnc" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gdqnc webserver-deployment-7f5969cbc7- deployment-4643  a30b9597-9786-4a54-883c-64d9ba311119 33981176503 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9eaa4bbfc389ab5d65909a7b68c1d992451e1234f2cbb4035c7f9616e87177ee cni.projectcalico.org/podIP:10.222.238.248/32 cni.projectcalico.org/podIPs:10.222.238.248/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395daa7 0xc00395daa8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.222.238.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxjkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxjkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:10.222.238.248,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://23c28a1a6a2b251d247e548904723697546418343d561e99ce6f3d3fcfddb57d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.222.238.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.073: INFO: Pod "webserver-deployment-7f5969cbc7-jgjvj" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jgjvj webserver-deployment-7f5969cbc7- deployment-4643  51c8ab54-32a7-434f-9012-052888081213 33981176821 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c1178943abccd8f1d18e39f43ab348ebbf2124f306a52a177a5c6c0af5a06104 cni.projectcalico.org/podIP:10.221.146.150/32 cni.projectcalico.org/podIPs:10.221.146.150/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395dca7 0xc00395dca8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4z8rx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4z8rx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:10.221.146.150,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aac5ba7b47677fa25b1b3c14260bf821ce50910177c2a0ef875a347dea12b5fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.074: INFO: Pod "webserver-deployment-7f5969cbc7-jm5p9" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jm5p9 webserver-deployment-7f5969cbc7- deployment-4643  9654bae1-34ce-474c-950e-3990942b6be0 33981176684 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ba5436917e1dfac6bbcfea4cab9183191eafaeb28158585176ccb294c660f6cc cni.projectcalico.org/podIP:10.221.146.149/32 cni.projectcalico.org/podIPs:10.221.146.149/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00395ded7 0xc00395ded8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7qb6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7qb6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:10.221.146.149,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b6c8e6ada396ceca65cdc875b9fc70d66e601a400f4abe988f0ab8dbf6f4352e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.074: INFO: Pod "webserver-deployment-7f5969cbc7-k47xw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k47xw webserver-deployment-7f5969cbc7- deployment-4643  200d5b98-959d-41f4-be66-be16f61c7eeb 33981177465 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:73cdabcea6c69008997cde6b03a8fdd18f4875266c24557304ef9bc399dee4fa cni.projectcalico.org/podIP:10.221.146.155/32 cni.projectcalico.org/podIPs:10.221.146.155/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301e0d7 0xc00301e0d8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jcp9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jcp9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.074: INFO: Pod "webserver-deployment-7f5969cbc7-k5xtv" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k5xtv webserver-deployment-7f5969cbc7- deployment-4643  cff69f71-ce02-4d01-b82f-ccf3950eecbe 33981176661 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:aa3b54e93646bd136a1588610a0624bbeae08cb413693e578b09a8d46d255393 cni.projectcalico.org/podIP:10.222.238.249/32 cni.projectcalico.org/podIPs:10.222.238.249/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301e2b7 0xc00301e2b8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.222.238.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nw82c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nw82c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:10.222.238.249,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://18c9890716827160312789b5bccc10a320206aa171843b877b88eaf0d21a358f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.222.238.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.074: INFO: Pod "webserver-deployment-7f5969cbc7-ph4sk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ph4sk webserver-deployment-7f5969cbc7- deployment-4643  b5d5ddc2-5b77-4f4d-8731-fb80af88073f 33981177286 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301e4e7 0xc00301e4e8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jg2dj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jg2dj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.075: INFO: Pod "webserver-deployment-7f5969cbc7-pxsqp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pxsqp webserver-deployment-7f5969cbc7- deployment-4643  aae3ed13-7c4a-451e-8d4f-3bff74d0ee7d 33981177308 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301e6b7 0xc00301e6b8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8svhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8svhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:,StartTime:2024-01-03 12:44:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.075: INFO: Pod "webserver-deployment-7f5969cbc7-q8scm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q8scm webserver-deployment-7f5969cbc7- deployment-4643  b71f7219-0694-4f11-8ad0-41790d9f70c1 33981177296 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301e887 0xc00301e888}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j97sf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j97sf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.075: INFO: Pod "webserver-deployment-7f5969cbc7-qb477" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qb477 webserver-deployment-7f5969cbc7- deployment-4643  e331f895-712b-4fd7-a58e-e928fe499426 33981176832 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:460f211d21c53a4ea302e69c28905d344e8c80e28ffccf9574a0a2a694e4dcc2 cni.projectcalico.org/podIP:10.221.146.151/32 cni.projectcalico.org/podIPs:10.221.146.151/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301ea67 0xc00301ea68}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ds2w4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ds2w4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:10.221.146.151,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8c864b413de8e0dd22d4c945526295223bee40d296ff47f7c355bab7320f701a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.075: INFO: Pod "webserver-deployment-7f5969cbc7-ssbb4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ssbb4 webserver-deployment-7f5969cbc7- deployment-4643  55addda2-5808-4925-a35a-39a62f9b6093 33981177395 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301ec67 0xc00301ec68}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wj7sm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wj7sm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.075: INFO: Pod "webserver-deployment-7f5969cbc7-w854v" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-w854v webserver-deployment-7f5969cbc7- deployment-4643  6bcf2902-6ce0-476d-b08d-ffa55154a07e 33981176574 0 2024-01-03 12:44:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a5560ee24b472ec4c5dfff55f4b78de1ecf59042add19f95a3ecac5d7745c5a7 cni.projectcalico.org/podIP:10.221.146.102/32 cni.projectcalico.org/podIPs:10.221.146.102/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301ee27 0xc00301ee28}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qjll4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qjll4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.102,StartTime:2024-01-03 12:44:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 12:44:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://591dacd195e49aaade27231ac970324610b1d59a6a107fefed4bada30986b0d9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.076: INFO: Pod "webserver-deployment-7f5969cbc7-xkdx6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xkdx6 webserver-deployment-7f5969cbc7- deployment-4643  f3481d12-7fe1-4143-b778-1aae086cdd69 33981177530 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301f027 0xc00301f028}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8d4ql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8d4ql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.079: INFO: Pod "webserver-deployment-7f5969cbc7-z78mc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-z78mc webserver-deployment-7f5969cbc7- deployment-4643  5f90ea4b-4c5a-4cde-a678-7d61279a04a6 33981177254 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301f1e7 0xc00301f1e8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lpfm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lpfm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.079: INFO: Pod "webserver-deployment-7f5969cbc7-zwmcp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zwmcp webserver-deployment-7f5969cbc7- deployment-4643  802922b7-53ef-4b3a-b3b6-c0119ef89e3b 33981177458 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 83ecf90b-98ef-4215-8624-653e5f2df47f 0xc00301f3c7 0xc00301f3c8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ecf90b-98ef-4215-8624-653e5f2df47f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfk5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfk5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:,StartTime:2024-01-03 12:44:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.079: INFO: Pod "webserver-deployment-d9f79cb5-5dc4m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5dc4m webserver-deployment-d9f79cb5- deployment-4643  6f6855c6-f9ae-4653-89a6-667888130f8d 33981177251 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc00301f587 0xc00301f588}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7lqxq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7lqxq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.079: INFO: Pod "webserver-deployment-d9f79cb5-9hsx4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9hsx4 webserver-deployment-d9f79cb5- deployment-4643  2eca47d1-7760-477b-9b9d-ade98a7c57fa 33981177405 0 2024-01-03 12:44:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:43c668a10b02892921b7e1e5750e7016e0e69829117e12f199e3c8b7bfe02500 cni.projectcalico.org/podIP:10.221.146.105/32 cni.projectcalico.org/podIPs:10.221.146.105/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc00301f6f7 0xc00301f6f8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcw7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcw7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.079: INFO: Pod "webserver-deployment-d9f79cb5-clbfw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-clbfw webserver-deployment-d9f79cb5- deployment-4643  2f69e470-4ad7-4ecf-b47f-9d613c573c3d 33981177419 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f991b7982cc8d4dd1a9bbd850c800f43a0ee1e2b0e08aaa18320a381160d7c58 cni.projectcalico.org/podIP:10.222.238.253/32 cni.projectcalico.org/podIPs:10.222.238.253/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc00301f8f7 0xc00301f8f8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j5tp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j5tp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.080: INFO: Pod "webserver-deployment-d9f79cb5-lvnnz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lvnnz webserver-deployment-d9f79cb5- deployment-4643  44632ea4-be80-49a9-9661-24fab05a61a1 33981177364 0 2024-01-03 12:44:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:8342cea68f576b7e258757912a7cc0bea2f2346f0d001242066ee015d1166baf cni.projectcalico.org/podIP:10.221.146.103/32 cni.projectcalico.org/podIPs:10.221.146.103/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc00301faf7 0xc00301faf8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2pps7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2pps7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.080: INFO: Pod "webserver-deployment-d9f79cb5-mmkvn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mmkvn webserver-deployment-d9f79cb5- deployment-4643  be64b166-605b-4103-adf5-02785772b285 33981177582 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d69d633c1109ed4bb95eb22c215e7ff868a1c2c47f1b800506636b188cd22a93 cni.projectcalico.org/podIP:10.221.146.163/32 cni.projectcalico.org/podIPs:10.221.146.163/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc00301fd57 0xc00301fd58}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rgsg7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rgsg7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.080: INFO: Pod "webserver-deployment-d9f79cb5-pxnlg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pxnlg webserver-deployment-d9f79cb5- deployment-4643  e83d1aea-5856-4a7e-aec0-1f873b923177 33981177460 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:80cb993a8bfe647063fa6d022678b529f3ac3080b97ea841d5dd8af7036c861b cni.projectcalico.org/podIP:10.222.238.254/32 cni.projectcalico.org/podIPs:10.222.238.254/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa21e7 0xc004aa21e8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wnsxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wnsxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.080: INFO: Pod "webserver-deployment-d9f79cb5-q4424" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-q4424 webserver-deployment-d9f79cb5- deployment-4643  d1a18efe-c5d7-4a6c-ab62-8fc5091cd162 33981177523 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:fe9b7a1d265330c24e5267a849ee5ba5f504e092cd86beffd4b68b002bbf2281 cni.projectcalico.org/podIP:10.221.146.156/32 cni.projectcalico.org/podIPs:10.221.146.156/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa23e7 0xc004aa23e8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x2qb9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x2qb9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.081: INFO: Pod "webserver-deployment-d9f79cb5-tcd97" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tcd97 webserver-deployment-d9f79cb5- deployment-4643  9b83399b-1b3e-40af-9aa6-e4f13ef8e0ce 33981177032 0 2024-01-03 12:44:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:4fc6f5442b87c4ced9e955e2abd9fb79dc6cdc0b59a0ace95391349dba818a87 cni.projectcalico.org/podIP:10.221.146.152/32 cni.projectcalico.org/podIPs:10.221.146.152/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa25e7 0xc004aa25e8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87crq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87crq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:,StartTime:2024-01-03 12:44:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.081: INFO: Pod "webserver-deployment-d9f79cb5-vdtfb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vdtfb webserver-deployment-d9f79cb5- deployment-4643  f8705260-3524-4618-8a35-d2a80773e730 33981177040 0 2024-01-03 12:44:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2e1bb01fe4f1c6e37bebfc947ef98c5e01bd6867e0c3c4f8266946ff73de0b02 cni.projectcalico.org/podIP:10.222.238.251/32 cni.projectcalico.org/podIPs:10.222.238.251/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa27e7 0xc004aa27e8}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bpgr7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bpgr7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.081: INFO: Pod "webserver-deployment-d9f79cb5-w2zg8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w2zg8 webserver-deployment-d9f79cb5- deployment-4643  5edc9a02-47d8-4c8c-8e0a-c74cf2e88e28 33981177072 0 2024-01-03 12:44:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1e2338a72144434ef0eaecf8f5dc7d338faae367bb9e141261f0589e03cc831c cni.projectcalico.org/podIP:10.222.238.252/32 cni.projectcalico.org/podIPs:10.222.238.252/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa2a07 0xc004aa2a08}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8r85l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8r85l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-adtwo5cmi2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.162.129,PodIP:,StartTime:2024-01-03 12:44:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.082: INFO: Pod "webserver-deployment-d9f79cb5-wtxhg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wtxhg webserver-deployment-d9f79cb5- deployment-4643  00c53f0c-7de6-4b6a-918c-ccda2d988863 33981177532 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:96472199867111c88bace728ac89992a7c5e6ed106f403d4bd8859f235a1dcf1 cni.projectcalico.org/podIP:10.221.146.106/32 cni.projectcalico.org/podIPs:10.221.146.106/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa2c17 0xc004aa2c18}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q5wlj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q5wlj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.082: INFO: Pod "webserver-deployment-d9f79cb5-xgfhx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xgfhx webserver-deployment-d9f79cb5- deployment-4643  730aea64-4aa6-4a5c-82c3-9c71bbd2620c 33981177501 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:017f1ecfbb67cd56231e9362f34f97351605e70ae623e019f3cb9dc9b6420252 cni.projectcalico.org/podIP:10.221.146.104/32 cni.projectcalico.org/podIPs:10.221.146.104/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa2e47 0xc004aa2e48}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ljvrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ljvrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 12:44:21.083: INFO: Pod "webserver-deployment-d9f79cb5-xmkh8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xmkh8 webserver-deployment-d9f79cb5- deployment-4643  929bce8e-4fb1-4f81-a6d1-b16475d48085 33981177554 0 2024-01-03 12:44:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:bc874edffa9fcd09d0022762bfb635c4b3bfa1431af6b782d669e0b4cad479ee cni.projectcalico.org/podIP:10.221.146.162/32 cni.projectcalico.org/podIPs:10.221.146.162/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 a337c5cd-a803-4508-9818-00829141903c 0xc004aa3047 0xc004aa3048}] [] [{kube-controller-manager Update v1 2024-01-03 12:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a337c5cd-a803-4508-9818-00829141903c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 12:44:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2024-01-03 12:44:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dd8mc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dd8mc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-nqeu5xtrab,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 12:44:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:85.215.218.90,PodIP:,StartTime:2024-01-03 12:44:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:44:21.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4643" for this suite. 01/03/24 12:44:21.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:44:21.134
Jan  3 12:44:21.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pods 01/03/24 12:44:21.136
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:44:21.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:44:21.22
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 01/03/24 12:44:21.248
Jan  3 12:44:21.280: INFO: Waiting up to 5m0s for pod "pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c" in namespace "pods-1141" to be "running and ready"
Jan  3 12:44:21.311: INFO: Pod "pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c": Phase="Pending", Reason="", readiness=false. Elapsed: 31.381908ms
Jan  3 12:44:21.311: INFO: The phase of Pod pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:44:23.333: INFO: Pod "pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052905859s
Jan  3 12:44:23.333: INFO: The phase of Pod pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:44:25.334: INFO: Pod "pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c": Phase="Running", Reason="", readiness=true. Elapsed: 4.053996415s
Jan  3 12:44:25.334: INFO: The phase of Pod pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c is Running (Ready = true)
Jan  3 12:44:25.334: INFO: Pod "pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c" satisfied condition "running and ready"
Jan  3 12:44:25.376: INFO: Pod pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c has hostIP: 185.132.46.116
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  3 12:44:25.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1141" for this suite. 01/03/24 12:44:25.408
------------------------------
• [4.297 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:44:21.134
    Jan  3 12:44:21.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pods 01/03/24 12:44:21.136
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:44:21.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:44:21.22
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 01/03/24 12:44:21.248
    Jan  3 12:44:21.280: INFO: Waiting up to 5m0s for pod "pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c" in namespace "pods-1141" to be "running and ready"
    Jan  3 12:44:21.311: INFO: Pod "pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c": Phase="Pending", Reason="", readiness=false. Elapsed: 31.381908ms
    Jan  3 12:44:21.311: INFO: The phase of Pod pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:44:23.333: INFO: Pod "pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052905859s
    Jan  3 12:44:23.333: INFO: The phase of Pod pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:44:25.334: INFO: Pod "pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c": Phase="Running", Reason="", readiness=true. Elapsed: 4.053996415s
    Jan  3 12:44:25.334: INFO: The phase of Pod pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c is Running (Ready = true)
    Jan  3 12:44:25.334: INFO: Pod "pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c" satisfied condition "running and ready"
    Jan  3 12:44:25.376: INFO: Pod pod-hostip-7570a44c-5410-4d2e-8dfd-a0a39012ba2c has hostIP: 185.132.46.116
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:44:25.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1141" for this suite. 01/03/24 12:44:25.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:44:25.435
Jan  3 12:44:25.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:44:25.436
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:44:25.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:44:25.527
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 01/03/24 12:44:25.555
Jan  3 12:44:25.585: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931" in namespace "projected-3574" to be "Succeeded or Failed"
Jan  3 12:44:25.605: INFO: Pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931": Phase="Pending", Reason="", readiness=false. Elapsed: 20.394212ms
Jan  3 12:44:27.627: INFO: Pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042515197s
Jan  3 12:44:29.624: INFO: Pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039696616s
Jan  3 12:44:31.627: INFO: Pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931": Phase="Running", Reason="", readiness=true. Elapsed: 6.042638293s
Jan  3 12:44:33.626: INFO: Pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931": Phase="Running", Reason="", readiness=false. Elapsed: 8.041453827s
Jan  3 12:44:35.640: INFO: Pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.055474249s
STEP: Saw pod success 01/03/24 12:44:35.64
Jan  3 12:44:35.640: INFO: Pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931" satisfied condition "Succeeded or Failed"
Jan  3 12:44:35.661: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931 container client-container: <nil>
STEP: delete the pod 01/03/24 12:44:35.725
Jan  3 12:44:35.763: INFO: Waiting for pod downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931 to disappear
Jan  3 12:44:35.783: INFO: Pod downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  3 12:44:35.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3574" for this suite. 01/03/24 12:44:35.817
------------------------------
• [SLOW TEST] [10.406 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:44:25.435
    Jan  3 12:44:25.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:44:25.436
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:44:25.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:44:25.527
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 01/03/24 12:44:25.555
    Jan  3 12:44:25.585: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931" in namespace "projected-3574" to be "Succeeded or Failed"
    Jan  3 12:44:25.605: INFO: Pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931": Phase="Pending", Reason="", readiness=false. Elapsed: 20.394212ms
    Jan  3 12:44:27.627: INFO: Pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042515197s
    Jan  3 12:44:29.624: INFO: Pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039696616s
    Jan  3 12:44:31.627: INFO: Pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931": Phase="Running", Reason="", readiness=true. Elapsed: 6.042638293s
    Jan  3 12:44:33.626: INFO: Pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931": Phase="Running", Reason="", readiness=false. Elapsed: 8.041453827s
    Jan  3 12:44:35.640: INFO: Pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.055474249s
    STEP: Saw pod success 01/03/24 12:44:35.64
    Jan  3 12:44:35.640: INFO: Pod "downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931" satisfied condition "Succeeded or Failed"
    Jan  3 12:44:35.661: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931 container client-container: <nil>
    STEP: delete the pod 01/03/24 12:44:35.725
    Jan  3 12:44:35.763: INFO: Waiting for pod downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931 to disappear
    Jan  3 12:44:35.783: INFO: Pod downwardapi-volume-5811f21c-a596-45d2-aec1-cb67727e1931 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:44:35.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3574" for this suite. 01/03/24 12:44:35.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:44:35.844
Jan  3 12:44:35.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 12:44:35.846
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:44:35.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:44:35.943
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 01/03/24 12:44:35.971
Jan  3 12:44:35.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 create -f -'
Jan  3 12:44:37.285: INFO: stderr: ""
Jan  3 12:44:37.285: INFO: stdout: "pod/pause created\n"
Jan  3 12:44:37.285: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan  3 12:44:37.285: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4076" to be "running and ready"
Jan  3 12:44:37.304: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.569082ms
Jan  3 12:44:37.304: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'jb-1-26-np-64kerjapxk' to be 'Running' but was 'Pending'
Jan  3 12:44:39.325: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.039631373s
Jan  3 12:44:39.325: INFO: Pod "pause" satisfied condition "running and ready"
Jan  3 12:44:39.325: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 01/03/24 12:44:39.325
Jan  3 12:44:39.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 label pods pause testing-label=testing-label-value'
Jan  3 12:44:39.500: INFO: stderr: ""
Jan  3 12:44:39.500: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/03/24 12:44:39.5
Jan  3 12:44:39.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 get pod pause -L testing-label'
Jan  3 12:44:39.645: INFO: stderr: ""
Jan  3 12:44:39.645: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/03/24 12:44:39.645
Jan  3 12:44:39.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 label pods pause testing-label-'
Jan  3 12:44:39.810: INFO: stderr: ""
Jan  3 12:44:39.810: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/03/24 12:44:39.81
Jan  3 12:44:39.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 get pod pause -L testing-label'
Jan  3 12:44:39.956: INFO: stderr: ""
Jan  3 12:44:39.956: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 01/03/24 12:44:39.957
Jan  3 12:44:39.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 delete --grace-period=0 --force -f -'
Jan  3 12:44:40.148: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 12:44:40.148: INFO: stdout: "pod \"pause\" force deleted\n"
Jan  3 12:44:40.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 get rc,svc -l name=pause --no-headers'
Jan  3 12:44:40.322: INFO: stderr: "No resources found in kubectl-4076 namespace.\n"
Jan  3 12:44:40.322: INFO: stdout: ""
Jan  3 12:44:40.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  3 12:44:40.460: INFO: stderr: ""
Jan  3 12:44:40.460: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 12:44:40.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4076" for this suite. 01/03/24 12:44:40.496
------------------------------
• [4.676 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:44:35.844
    Jan  3 12:44:35.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 12:44:35.846
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:44:35.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:44:35.943
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 01/03/24 12:44:35.971
    Jan  3 12:44:35.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 create -f -'
    Jan  3 12:44:37.285: INFO: stderr: ""
    Jan  3 12:44:37.285: INFO: stdout: "pod/pause created\n"
    Jan  3 12:44:37.285: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan  3 12:44:37.285: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4076" to be "running and ready"
    Jan  3 12:44:37.304: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.569082ms
    Jan  3 12:44:37.304: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'jb-1-26-np-64kerjapxk' to be 'Running' but was 'Pending'
    Jan  3 12:44:39.325: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.039631373s
    Jan  3 12:44:39.325: INFO: Pod "pause" satisfied condition "running and ready"
    Jan  3 12:44:39.325: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 01/03/24 12:44:39.325
    Jan  3 12:44:39.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 label pods pause testing-label=testing-label-value'
    Jan  3 12:44:39.500: INFO: stderr: ""
    Jan  3 12:44:39.500: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/03/24 12:44:39.5
    Jan  3 12:44:39.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 get pod pause -L testing-label'
    Jan  3 12:44:39.645: INFO: stderr: ""
    Jan  3 12:44:39.645: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/03/24 12:44:39.645
    Jan  3 12:44:39.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 label pods pause testing-label-'
    Jan  3 12:44:39.810: INFO: stderr: ""
    Jan  3 12:44:39.810: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/03/24 12:44:39.81
    Jan  3 12:44:39.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 get pod pause -L testing-label'
    Jan  3 12:44:39.956: INFO: stderr: ""
    Jan  3 12:44:39.956: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 01/03/24 12:44:39.957
    Jan  3 12:44:39.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 delete --grace-period=0 --force -f -'
    Jan  3 12:44:40.148: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 12:44:40.148: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan  3 12:44:40.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 get rc,svc -l name=pause --no-headers'
    Jan  3 12:44:40.322: INFO: stderr: "No resources found in kubectl-4076 namespace.\n"
    Jan  3 12:44:40.322: INFO: stdout: ""
    Jan  3 12:44:40.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-4076 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  3 12:44:40.460: INFO: stderr: ""
    Jan  3 12:44:40.460: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:44:40.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4076" for this suite. 01/03/24 12:44:40.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:44:40.522
Jan  3 12:44:40.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename cronjob 01/03/24 12:44:40.525
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:44:40.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:44:40.612
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/03/24 12:44:40.64
STEP: Ensuring no jobs are scheduled 01/03/24 12:44:40.661
STEP: Ensuring no job exists by listing jobs explicitly 01/03/24 12:49:40.703
STEP: Removing cronjob 01/03/24 12:49:40.722
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  3 12:49:40.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-510" for this suite. 01/03/24 12:49:40.777
------------------------------
• [SLOW TEST] [300.278 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:44:40.522
    Jan  3 12:44:40.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename cronjob 01/03/24 12:44:40.525
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:44:40.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:44:40.612
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/03/24 12:44:40.64
    STEP: Ensuring no jobs are scheduled 01/03/24 12:44:40.661
    STEP: Ensuring no job exists by listing jobs explicitly 01/03/24 12:49:40.703
    STEP: Removing cronjob 01/03/24 12:49:40.722
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:49:40.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-510" for this suite. 01/03/24 12:49:40.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:49:40.803
Jan  3 12:49:40.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename dns 01/03/24 12:49:40.807
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:49:40.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:49:40.887
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/03/24 12:49:40.914
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-924.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-924.svc.cluster.local;sleep 1; done
 01/03/24 12:49:40.936
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-924.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-924.svc.cluster.local;sleep 1; done
 01/03/24 12:49:40.937
STEP: creating a pod to probe DNS 01/03/24 12:49:40.937
STEP: submitting the pod to kubernetes 01/03/24 12:49:40.938
Jan  3 12:49:40.969: INFO: Waiting up to 15m0s for pod "dns-test-09b786db-22f1-4d4f-ac1a-3344dbd11ed9" in namespace "dns-924" to be "running"
Jan  3 12:49:40.992: INFO: Pod "dns-test-09b786db-22f1-4d4f-ac1a-3344dbd11ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 23.081844ms
Jan  3 12:49:43.018: INFO: Pod "dns-test-09b786db-22f1-4d4f-ac1a-3344dbd11ed9": Phase="Running", Reason="", readiness=true. Elapsed: 2.048591163s
Jan  3 12:49:43.018: INFO: Pod "dns-test-09b786db-22f1-4d4f-ac1a-3344dbd11ed9" satisfied condition "running"
STEP: retrieving the pod 01/03/24 12:49:43.018
STEP: looking for the results for each expected name from probers 01/03/24 12:49:43.041
Jan  3 12:49:43.440: INFO: DNS probes using dns-924/dns-test-09b786db-22f1-4d4f-ac1a-3344dbd11ed9 succeeded

STEP: deleting the pod 01/03/24 12:49:43.44
STEP: deleting the test headless service 01/03/24 12:49:43.493
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  3 12:49:43.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-924" for this suite. 01/03/24 12:49:43.562
------------------------------
• [2.787 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:49:40.803
    Jan  3 12:49:40.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename dns 01/03/24 12:49:40.807
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:49:40.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:49:40.887
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/03/24 12:49:40.914
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-924.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-924.svc.cluster.local;sleep 1; done
     01/03/24 12:49:40.936
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-924.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-924.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-924.svc.cluster.local;sleep 1; done
     01/03/24 12:49:40.937
    STEP: creating a pod to probe DNS 01/03/24 12:49:40.937
    STEP: submitting the pod to kubernetes 01/03/24 12:49:40.938
    Jan  3 12:49:40.969: INFO: Waiting up to 15m0s for pod "dns-test-09b786db-22f1-4d4f-ac1a-3344dbd11ed9" in namespace "dns-924" to be "running"
    Jan  3 12:49:40.992: INFO: Pod "dns-test-09b786db-22f1-4d4f-ac1a-3344dbd11ed9": Phase="Pending", Reason="", readiness=false. Elapsed: 23.081844ms
    Jan  3 12:49:43.018: INFO: Pod "dns-test-09b786db-22f1-4d4f-ac1a-3344dbd11ed9": Phase="Running", Reason="", readiness=true. Elapsed: 2.048591163s
    Jan  3 12:49:43.018: INFO: Pod "dns-test-09b786db-22f1-4d4f-ac1a-3344dbd11ed9" satisfied condition "running"
    STEP: retrieving the pod 01/03/24 12:49:43.018
    STEP: looking for the results for each expected name from probers 01/03/24 12:49:43.041
    Jan  3 12:49:43.440: INFO: DNS probes using dns-924/dns-test-09b786db-22f1-4d4f-ac1a-3344dbd11ed9 succeeded

    STEP: deleting the pod 01/03/24 12:49:43.44
    STEP: deleting the test headless service 01/03/24 12:49:43.493
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:49:43.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-924" for this suite. 01/03/24 12:49:43.562
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:49:43.591
Jan  3 12:49:43.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 12:49:43.594
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:49:43.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:49:43.697
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 01/03/24 12:49:43.725
Jan  3 12:49:43.752: INFO: Waiting up to 5m0s for pod "pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0" in namespace "emptydir-7281" to be "Succeeded or Failed"
Jan  3 12:49:43.771: INFO: Pod "pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.690633ms
Jan  3 12:49:45.795: INFO: Pod "pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042603163s
Jan  3 12:49:47.796: INFO: Pod "pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0": Phase="Running", Reason="", readiness=false. Elapsed: 4.043070497s
Jan  3 12:49:49.795: INFO: Pod "pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041715282s
STEP: Saw pod success 01/03/24 12:49:49.795
Jan  3 12:49:49.795: INFO: Pod "pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0" satisfied condition "Succeeded or Failed"
Jan  3 12:49:49.816: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0 container test-container: <nil>
STEP: delete the pod 01/03/24 12:49:49.978
Jan  3 12:49:50.017: INFO: Waiting for pod pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0 to disappear
Jan  3 12:49:50.075: INFO: Pod pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:49:50.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7281" for this suite. 01/03/24 12:49:50.109
------------------------------
• [SLOW TEST] [6.545 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:49:43.591
    Jan  3 12:49:43.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 12:49:43.594
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:49:43.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:49:43.697
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 01/03/24 12:49:43.725
    Jan  3 12:49:43.752: INFO: Waiting up to 5m0s for pod "pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0" in namespace "emptydir-7281" to be "Succeeded or Failed"
    Jan  3 12:49:43.771: INFO: Pod "pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.690633ms
    Jan  3 12:49:45.795: INFO: Pod "pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042603163s
    Jan  3 12:49:47.796: INFO: Pod "pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0": Phase="Running", Reason="", readiness=false. Elapsed: 4.043070497s
    Jan  3 12:49:49.795: INFO: Pod "pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041715282s
    STEP: Saw pod success 01/03/24 12:49:49.795
    Jan  3 12:49:49.795: INFO: Pod "pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0" satisfied condition "Succeeded or Failed"
    Jan  3 12:49:49.816: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0 container test-container: <nil>
    STEP: delete the pod 01/03/24 12:49:49.978
    Jan  3 12:49:50.017: INFO: Waiting for pod pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0 to disappear
    Jan  3 12:49:50.075: INFO: Pod pod-a47ea129-4d4e-4086-b1b7-e7f57ad52bd0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:49:50.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7281" for this suite. 01/03/24 12:49:50.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:49:50.14
Jan  3 12:49:50.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:49:50.143
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:49:50.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:49:50.227
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 01/03/24 12:49:50.257
Jan  3 12:49:50.286: INFO: Waiting up to 5m0s for pod "downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06" in namespace "projected-6484" to be "Succeeded or Failed"
Jan  3 12:49:50.305: INFO: Pod "downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06": Phase="Pending", Reason="", readiness=false. Elapsed: 19.650466ms
Jan  3 12:49:52.327: INFO: Pod "downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041645412s
Jan  3 12:49:54.327: INFO: Pod "downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041528998s
Jan  3 12:49:56.336: INFO: Pod "downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050213059s
STEP: Saw pod success 01/03/24 12:49:56.336
Jan  3 12:49:56.336: INFO: Pod "downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06" satisfied condition "Succeeded or Failed"
Jan  3 12:49:56.356: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06 container client-container: <nil>
STEP: delete the pod 01/03/24 12:49:56.393
Jan  3 12:49:56.432: INFO: Waiting for pod downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06 to disappear
Jan  3 12:49:56.450: INFO: Pod downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  3 12:49:56.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6484" for this suite. 01/03/24 12:49:56.482
------------------------------
• [SLOW TEST] [6.370 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:49:50.14
    Jan  3 12:49:50.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:49:50.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:49:50.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:49:50.227
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 01/03/24 12:49:50.257
    Jan  3 12:49:50.286: INFO: Waiting up to 5m0s for pod "downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06" in namespace "projected-6484" to be "Succeeded or Failed"
    Jan  3 12:49:50.305: INFO: Pod "downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06": Phase="Pending", Reason="", readiness=false. Elapsed: 19.650466ms
    Jan  3 12:49:52.327: INFO: Pod "downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041645412s
    Jan  3 12:49:54.327: INFO: Pod "downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041528998s
    Jan  3 12:49:56.336: INFO: Pod "downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050213059s
    STEP: Saw pod success 01/03/24 12:49:56.336
    Jan  3 12:49:56.336: INFO: Pod "downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06" satisfied condition "Succeeded or Failed"
    Jan  3 12:49:56.356: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06 container client-container: <nil>
    STEP: delete the pod 01/03/24 12:49:56.393
    Jan  3 12:49:56.432: INFO: Waiting for pod downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06 to disappear
    Jan  3 12:49:56.450: INFO: Pod downwardapi-volume-698e23e5-8ea8-4ef8-acad-57b6d2feac06 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:49:56.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6484" for this suite. 01/03/24 12:49:56.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:49:56.514
Jan  3 12:49:56.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename var-expansion 01/03/24 12:49:56.516
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:49:56.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:49:56.605
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 01/03/24 12:49:56.634
Jan  3 12:49:56.664: INFO: Waiting up to 5m0s for pod "var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f" in namespace "var-expansion-8218" to be "Succeeded or Failed"
Jan  3 12:49:56.683: INFO: Pod "var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.747275ms
Jan  3 12:49:58.706: INFO: Pod "var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f": Phase="Running", Reason="", readiness=true. Elapsed: 2.041996508s
Jan  3 12:50:00.703: INFO: Pod "var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f": Phase="Running", Reason="", readiness=false. Elapsed: 4.038561825s
Jan  3 12:50:02.702: INFO: Pod "var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037859153s
STEP: Saw pod success 01/03/24 12:50:02.702
Jan  3 12:50:02.702: INFO: Pod "var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f" satisfied condition "Succeeded or Failed"
Jan  3 12:50:02.721: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f container dapi-container: <nil>
STEP: delete the pod 01/03/24 12:50:02.76
Jan  3 12:50:02.796: INFO: Waiting for pod var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f to disappear
Jan  3 12:50:02.813: INFO: Pod var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  3 12:50:02.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8218" for this suite. 01/03/24 12:50:02.845
------------------------------
• [SLOW TEST] [6.354 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:49:56.514
    Jan  3 12:49:56.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename var-expansion 01/03/24 12:49:56.516
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:49:56.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:49:56.605
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 01/03/24 12:49:56.634
    Jan  3 12:49:56.664: INFO: Waiting up to 5m0s for pod "var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f" in namespace "var-expansion-8218" to be "Succeeded or Failed"
    Jan  3 12:49:56.683: INFO: Pod "var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.747275ms
    Jan  3 12:49:58.706: INFO: Pod "var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f": Phase="Running", Reason="", readiness=true. Elapsed: 2.041996508s
    Jan  3 12:50:00.703: INFO: Pod "var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f": Phase="Running", Reason="", readiness=false. Elapsed: 4.038561825s
    Jan  3 12:50:02.702: INFO: Pod "var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037859153s
    STEP: Saw pod success 01/03/24 12:50:02.702
    Jan  3 12:50:02.702: INFO: Pod "var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f" satisfied condition "Succeeded or Failed"
    Jan  3 12:50:02.721: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f container dapi-container: <nil>
    STEP: delete the pod 01/03/24 12:50:02.76
    Jan  3 12:50:02.796: INFO: Waiting for pod var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f to disappear
    Jan  3 12:50:02.813: INFO: Pod var-expansion-551535c5-2fed-46ea-a0d2-c51831cc795f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:50:02.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8218" for this suite. 01/03/24 12:50:02.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:50:02.879
Jan  3 12:50:02.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename ingressclass 01/03/24 12:50:02.88
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:50:02.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:50:02.959
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/03/24 12:50:02.988
STEP: getting /apis/networking.k8s.io 01/03/24 12:50:03.016
STEP: getting /apis/networking.k8s.iov1 01/03/24 12:50:03.029
STEP: creating 01/03/24 12:50:03.042
STEP: getting 01/03/24 12:50:03.12
STEP: listing 01/03/24 12:50:03.139
STEP: watching 01/03/24 12:50:03.158
Jan  3 12:50:03.158: INFO: starting watch
STEP: patching 01/03/24 12:50:03.172
STEP: updating 01/03/24 12:50:03.201
Jan  3 12:50:03.219: INFO: waiting for watch events with expected annotations
Jan  3 12:50:03.219: INFO: saw patched and updated annotations
STEP: deleting 01/03/24 12:50:03.219
STEP: deleting a collection 01/03/24 12:50:03.281
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jan  3 12:50:03.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-4190" for this suite. 01/03/24 12:50:03.361
------------------------------
• [0.519 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:50:02.879
    Jan  3 12:50:02.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename ingressclass 01/03/24 12:50:02.88
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:50:02.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:50:02.959
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/03/24 12:50:02.988
    STEP: getting /apis/networking.k8s.io 01/03/24 12:50:03.016
    STEP: getting /apis/networking.k8s.iov1 01/03/24 12:50:03.029
    STEP: creating 01/03/24 12:50:03.042
    STEP: getting 01/03/24 12:50:03.12
    STEP: listing 01/03/24 12:50:03.139
    STEP: watching 01/03/24 12:50:03.158
    Jan  3 12:50:03.158: INFO: starting watch
    STEP: patching 01/03/24 12:50:03.172
    STEP: updating 01/03/24 12:50:03.201
    Jan  3 12:50:03.219: INFO: waiting for watch events with expected annotations
    Jan  3 12:50:03.219: INFO: saw patched and updated annotations
    STEP: deleting 01/03/24 12:50:03.219
    STEP: deleting a collection 01/03/24 12:50:03.281
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:50:03.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-4190" for this suite. 01/03/24 12:50:03.361
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:50:03.4
Jan  3 12:50:03.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename crd-webhook 01/03/24 12:50:03.403
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:50:03.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:50:03.49
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/03/24 12:50:03.518
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/03/24 12:50:03.923
STEP: Deploying the custom resource conversion webhook pod 01/03/24 12:50:03.95
STEP: Wait for the deployment to be ready 01/03/24 12:50:03.991
Jan  3 12:50:04.028: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/24 12:50:06.087
STEP: Verifying the service has paired with the endpoint 01/03/24 12:50:06.117
Jan  3 12:50:07.118: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan  3 12:50:07.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Creating a v1 custom resource 01/03/24 12:50:10.023
STEP: v2 custom resource should be converted 01/03/24 12:50:10.052
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:50:10.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4274" for this suite. 01/03/24 12:50:10.919
------------------------------
• [SLOW TEST] [7.542 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:50:03.4
    Jan  3 12:50:03.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename crd-webhook 01/03/24 12:50:03.403
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:50:03.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:50:03.49
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/03/24 12:50:03.518
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/03/24 12:50:03.923
    STEP: Deploying the custom resource conversion webhook pod 01/03/24 12:50:03.95
    STEP: Wait for the deployment to be ready 01/03/24 12:50:03.991
    Jan  3 12:50:04.028: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/24 12:50:06.087
    STEP: Verifying the service has paired with the endpoint 01/03/24 12:50:06.117
    Jan  3 12:50:07.118: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan  3 12:50:07.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Creating a v1 custom resource 01/03/24 12:50:10.023
    STEP: v2 custom resource should be converted 01/03/24 12:50:10.052
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:50:10.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4274" for this suite. 01/03/24 12:50:10.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:50:10.954
Jan  3 12:50:10.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename init-container 01/03/24 12:50:10.955
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:50:11.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:50:11.038
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 01/03/24 12:50:11.071
Jan  3 12:50:11.071: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:50:15.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2325" for this suite. 01/03/24 12:50:15.478
------------------------------
• [4.551 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:50:10.954
    Jan  3 12:50:10.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename init-container 01/03/24 12:50:10.955
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:50:11.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:50:11.038
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 01/03/24 12:50:11.071
    Jan  3 12:50:11.071: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:50:15.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2325" for this suite. 01/03/24 12:50:15.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:50:15.507
Jan  3 12:50:15.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:50:15.511
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:50:15.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:50:15.594
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-650a830e-d523-4ef5-9eb6-2ed54e6d746c 01/03/24 12:50:15.625
STEP: Creating a pod to test consume secrets 01/03/24 12:50:15.66
Jan  3 12:50:15.702: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555" in namespace "projected-6732" to be "Succeeded or Failed"
Jan  3 12:50:15.723: INFO: Pod "pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555": Phase="Pending", Reason="", readiness=false. Elapsed: 20.749865ms
Jan  3 12:50:17.755: INFO: Pod "pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555": Phase="Running", Reason="", readiness=true. Elapsed: 2.05275256s
Jan  3 12:50:19.743: INFO: Pod "pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555": Phase="Running", Reason="", readiness=false. Elapsed: 4.04049593s
Jan  3 12:50:21.744: INFO: Pod "pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041881629s
STEP: Saw pod success 01/03/24 12:50:21.744
Jan  3 12:50:21.745: INFO: Pod "pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555" satisfied condition "Succeeded or Failed"
Jan  3 12:50:21.763: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/03/24 12:50:21.801
Jan  3 12:50:21.849: INFO: Waiting for pod pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555 to disappear
Jan  3 12:50:21.869: INFO: Pod pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  3 12:50:21.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6732" for this suite. 01/03/24 12:50:21.9
------------------------------
• [SLOW TEST] [6.418 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:50:15.507
    Jan  3 12:50:15.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:50:15.511
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:50:15.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:50:15.594
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-650a830e-d523-4ef5-9eb6-2ed54e6d746c 01/03/24 12:50:15.625
    STEP: Creating a pod to test consume secrets 01/03/24 12:50:15.66
    Jan  3 12:50:15.702: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555" in namespace "projected-6732" to be "Succeeded or Failed"
    Jan  3 12:50:15.723: INFO: Pod "pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555": Phase="Pending", Reason="", readiness=false. Elapsed: 20.749865ms
    Jan  3 12:50:17.755: INFO: Pod "pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555": Phase="Running", Reason="", readiness=true. Elapsed: 2.05275256s
    Jan  3 12:50:19.743: INFO: Pod "pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555": Phase="Running", Reason="", readiness=false. Elapsed: 4.04049593s
    Jan  3 12:50:21.744: INFO: Pod "pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041881629s
    STEP: Saw pod success 01/03/24 12:50:21.744
    Jan  3 12:50:21.745: INFO: Pod "pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555" satisfied condition "Succeeded or Failed"
    Jan  3 12:50:21.763: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/03/24 12:50:21.801
    Jan  3 12:50:21.849: INFO: Waiting for pod pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555 to disappear
    Jan  3 12:50:21.869: INFO: Pod pod-projected-secrets-13e473c2-4c74-4248-a4a6-51a8a87a2555 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:50:21.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6732" for this suite. 01/03/24 12:50:21.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:50:21.928
Jan  3 12:50:21.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename secrets 01/03/24 12:50:21.93
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:50:21.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:50:22.013
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-ac3026eb-bd85-49c2-a03a-3e8516e8e742 01/03/24 12:50:22.041
STEP: Creating a pod to test consume secrets 01/03/24 12:50:22.059
Jan  3 12:50:22.087: INFO: Waiting up to 5m0s for pod "pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b" in namespace "secrets-5526" to be "Succeeded or Failed"
Jan  3 12:50:22.105: INFO: Pod "pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.391362ms
Jan  3 12:50:24.125: INFO: Pod "pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b": Phase="Running", Reason="", readiness=true. Elapsed: 2.038684091s
Jan  3 12:50:26.126: INFO: Pod "pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b": Phase="Running", Reason="", readiness=false. Elapsed: 4.039307741s
Jan  3 12:50:28.132: INFO: Pod "pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045384834s
STEP: Saw pod success 01/03/24 12:50:28.132
Jan  3 12:50:28.132: INFO: Pod "pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b" satisfied condition "Succeeded or Failed"
Jan  3 12:50:28.151: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b container secret-volume-test: <nil>
STEP: delete the pod 01/03/24 12:50:28.19
Jan  3 12:50:28.238: INFO: Waiting for pod pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b to disappear
Jan  3 12:50:28.257: INFO: Pod pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  3 12:50:28.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5526" for this suite. 01/03/24 12:50:28.289
------------------------------
• [SLOW TEST] [6.397 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:50:21.928
    Jan  3 12:50:21.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename secrets 01/03/24 12:50:21.93
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:50:21.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:50:22.013
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-ac3026eb-bd85-49c2-a03a-3e8516e8e742 01/03/24 12:50:22.041
    STEP: Creating a pod to test consume secrets 01/03/24 12:50:22.059
    Jan  3 12:50:22.087: INFO: Waiting up to 5m0s for pod "pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b" in namespace "secrets-5526" to be "Succeeded or Failed"
    Jan  3 12:50:22.105: INFO: Pod "pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.391362ms
    Jan  3 12:50:24.125: INFO: Pod "pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b": Phase="Running", Reason="", readiness=true. Elapsed: 2.038684091s
    Jan  3 12:50:26.126: INFO: Pod "pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b": Phase="Running", Reason="", readiness=false. Elapsed: 4.039307741s
    Jan  3 12:50:28.132: INFO: Pod "pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045384834s
    STEP: Saw pod success 01/03/24 12:50:28.132
    Jan  3 12:50:28.132: INFO: Pod "pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b" satisfied condition "Succeeded or Failed"
    Jan  3 12:50:28.151: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b container secret-volume-test: <nil>
    STEP: delete the pod 01/03/24 12:50:28.19
    Jan  3 12:50:28.238: INFO: Waiting for pod pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b to disappear
    Jan  3 12:50:28.257: INFO: Pod pod-secrets-1a48e275-cf4f-42a9-8162-5389929d926b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:50:28.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5526" for this suite. 01/03/24 12:50:28.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:50:28.33
Jan  3 12:50:28.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename init-container 01/03/24 12:50:28.332
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:50:28.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:50:28.435
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 01/03/24 12:50:28.464
Jan  3 12:50:28.464: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:50:33.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1894" for this suite. 01/03/24 12:50:33.566
------------------------------
• [SLOW TEST] [5.264 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:50:28.33
    Jan  3 12:50:28.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename init-container 01/03/24 12:50:28.332
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:50:28.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:50:28.435
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 01/03/24 12:50:28.464
    Jan  3 12:50:28.464: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:50:33.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1894" for this suite. 01/03/24 12:50:33.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:50:33.596
Jan  3 12:50:33.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename endpointslice 01/03/24 12:50:33.6
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:50:33.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:50:33.682
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 01/03/24 12:50:38.877
STEP: referencing matching pods with named port 01/03/24 12:50:43.917
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/03/24 12:50:48.96
STEP: recreating EndpointSlices after they've been deleted 01/03/24 12:50:54.001
Jan  3 12:50:54.091: INFO: EndpointSlice for Service endpointslice-7922/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan  3 12:51:04.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7922" for this suite. 01/03/24 12:51:04.172
------------------------------
• [SLOW TEST] [30.611 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:50:33.596
    Jan  3 12:50:33.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename endpointslice 01/03/24 12:50:33.6
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:50:33.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:50:33.682
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 01/03/24 12:50:38.877
    STEP: referencing matching pods with named port 01/03/24 12:50:43.917
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/03/24 12:50:48.96
    STEP: recreating EndpointSlices after they've been deleted 01/03/24 12:50:54.001
    Jan  3 12:50:54.091: INFO: EndpointSlice for Service endpointslice-7922/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:51:04.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7922" for this suite. 01/03/24 12:51:04.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:51:04.211
Jan  3 12:51:04.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename watch 01/03/24 12:51:04.213
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:51:04.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:51:04.297
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/03/24 12:51:04.326
STEP: modifying the configmap once 01/03/24 12:51:04.345
STEP: modifying the configmap a second time 01/03/24 12:51:04.393
STEP: deleting the configmap 01/03/24 12:51:04.428
STEP: creating a watch on configmaps from the resource version returned by the first update 01/03/24 12:51:04.449
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/03/24 12:51:04.462
Jan  3 12:51:04.462: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5716  a27bcbe5-784f-46b6-a402-85baecb62585 33981226008 0 2024-01-03 12:51:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-01-03 12:51:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 12:51:04.463: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5716  a27bcbe5-784f-46b6-a402-85baecb62585 33981226009 0 2024-01-03 12:51:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-01-03 12:51:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  3 12:51:04.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5716" for this suite. 01/03/24 12:51:04.483
------------------------------
• [0.297 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:51:04.211
    Jan  3 12:51:04.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename watch 01/03/24 12:51:04.213
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:51:04.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:51:04.297
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/03/24 12:51:04.326
    STEP: modifying the configmap once 01/03/24 12:51:04.345
    STEP: modifying the configmap a second time 01/03/24 12:51:04.393
    STEP: deleting the configmap 01/03/24 12:51:04.428
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/03/24 12:51:04.449
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/03/24 12:51:04.462
    Jan  3 12:51:04.462: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5716  a27bcbe5-784f-46b6-a402-85baecb62585 33981226008 0 2024-01-03 12:51:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-01-03 12:51:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 12:51:04.463: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5716  a27bcbe5-784f-46b6-a402-85baecb62585 33981226009 0 2024-01-03 12:51:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-01-03 12:51:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:51:04.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5716" for this suite. 01/03/24 12:51:04.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:51:04.515
Jan  3 12:51:04.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename resourcequota 01/03/24 12:51:04.517
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:51:04.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:51:04.6
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 01/03/24 12:51:04.63
STEP: Creating a ResourceQuota 01/03/24 12:51:09.653
STEP: Ensuring resource quota status is calculated 01/03/24 12:51:09.674
STEP: Creating a Pod that fits quota 01/03/24 12:51:11.695
STEP: Ensuring ResourceQuota status captures the pod usage 01/03/24 12:51:11.74
STEP: Not allowing a pod to be created that exceeds remaining quota 01/03/24 12:51:13.761
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/03/24 12:51:13.778
STEP: Ensuring a pod cannot update its resource requirements 01/03/24 12:51:13.793
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/03/24 12:51:13.813
STEP: Deleting the pod 01/03/24 12:51:15.834
STEP: Ensuring resource quota status released the pod usage 01/03/24 12:51:15.872
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  3 12:51:17.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6800" for this suite. 01/03/24 12:51:17.925
------------------------------
• [SLOW TEST] [13.436 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:51:04.515
    Jan  3 12:51:04.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename resourcequota 01/03/24 12:51:04.517
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:51:04.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:51:04.6
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 01/03/24 12:51:04.63
    STEP: Creating a ResourceQuota 01/03/24 12:51:09.653
    STEP: Ensuring resource quota status is calculated 01/03/24 12:51:09.674
    STEP: Creating a Pod that fits quota 01/03/24 12:51:11.695
    STEP: Ensuring ResourceQuota status captures the pod usage 01/03/24 12:51:11.74
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/03/24 12:51:13.761
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/03/24 12:51:13.778
    STEP: Ensuring a pod cannot update its resource requirements 01/03/24 12:51:13.793
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/03/24 12:51:13.813
    STEP: Deleting the pod 01/03/24 12:51:15.834
    STEP: Ensuring resource quota status released the pod usage 01/03/24 12:51:15.872
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:51:17.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6800" for this suite. 01/03/24 12:51:17.925
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:51:17.954
Jan  3 12:51:17.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pod-network-test 01/03/24 12:51:17.957
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:51:18.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:51:18.039
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-2534 01/03/24 12:51:18.068
STEP: creating a selector 01/03/24 12:51:18.068
STEP: Creating the service pods in kubernetes 01/03/24 12:51:18.069
Jan  3 12:51:18.069: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  3 12:51:18.192: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2534" to be "running and ready"
Jan  3 12:51:18.218: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.293509ms
Jan  3 12:51:18.218: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:51:20.240: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.047620576s
Jan  3 12:51:20.240: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 12:51:22.272: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.08032871s
Jan  3 12:51:22.272: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 12:51:24.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.04624489s
Jan  3 12:51:24.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 12:51:26.240: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.048214462s
Jan  3 12:51:26.240: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 12:51:28.239: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.046618096s
Jan  3 12:51:28.239: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 12:51:30.243: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.050865552s
Jan  3 12:51:30.243: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 12:51:32.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.046204282s
Jan  3 12:51:32.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 12:51:34.240: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.04817535s
Jan  3 12:51:34.240: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 12:51:36.242: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.049987311s
Jan  3 12:51:36.242: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 12:51:38.241: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.048587106s
Jan  3 12:51:38.241: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 12:51:40.239: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.046632999s
Jan  3 12:51:40.239: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  3 12:51:40.239: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  3 12:51:40.259: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2534" to be "running and ready"
Jan  3 12:51:40.278: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 18.413128ms
Jan  3 12:51:40.278: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  3 12:51:40.278: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  3 12:51:40.297: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2534" to be "running and ready"
Jan  3 12:51:40.318: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 20.498472ms
Jan  3 12:51:40.318: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  3 12:51:40.318: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/03/24 12:51:40.337
Jan  3 12:51:40.389: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2534" to be "running"
Jan  3 12:51:40.412: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 23.034454ms
Jan  3 12:51:42.432: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042509613s
Jan  3 12:51:44.433: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.044070811s
Jan  3 12:51:44.434: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  3 12:51:44.451: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2534" to be "running"
Jan  3 12:51:44.471: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 19.697383ms
Jan  3 12:51:44.471: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan  3 12:51:44.489: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan  3 12:51:44.490: INFO: Going to poll 10.221.146.117 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan  3 12:51:44.509: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.221.146.117:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2534 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:51:44.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:51:44.510: INFO: ExecWithOptions: Clientset creation
Jan  3 12:51:44.510: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2534/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.221.146.117%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  3 12:51:44.836: INFO: Found all 1 expected endpoints: [netserver-0]
Jan  3 12:51:44.836: INFO: Going to poll 10.222.238.203 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan  3 12:51:44.856: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.222.238.203:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2534 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:51:44.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:51:44.857: INFO: ExecWithOptions: Clientset creation
Jan  3 12:51:44.857: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2534/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.222.238.203%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  3 12:51:45.166: INFO: Found all 1 expected endpoints: [netserver-1]
Jan  3 12:51:45.166: INFO: Going to poll 10.221.146.166 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan  3 12:51:45.185: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.221.146.166:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2534 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 12:51:45.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:51:45.187: INFO: ExecWithOptions: Clientset creation
Jan  3 12:51:45.187: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2534/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.221.146.166%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  3 12:51:45.491: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan  3 12:51:45.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2534" for this suite. 01/03/24 12:51:45.523
------------------------------
• [SLOW TEST] [27.604 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:51:17.954
    Jan  3 12:51:17.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pod-network-test 01/03/24 12:51:17.957
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:51:18.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:51:18.039
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-2534 01/03/24 12:51:18.068
    STEP: creating a selector 01/03/24 12:51:18.068
    STEP: Creating the service pods in kubernetes 01/03/24 12:51:18.069
    Jan  3 12:51:18.069: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  3 12:51:18.192: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2534" to be "running and ready"
    Jan  3 12:51:18.218: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.293509ms
    Jan  3 12:51:18.218: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:51:20.240: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.047620576s
    Jan  3 12:51:20.240: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 12:51:22.272: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.08032871s
    Jan  3 12:51:22.272: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 12:51:24.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.04624489s
    Jan  3 12:51:24.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 12:51:26.240: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.048214462s
    Jan  3 12:51:26.240: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 12:51:28.239: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.046618096s
    Jan  3 12:51:28.239: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 12:51:30.243: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.050865552s
    Jan  3 12:51:30.243: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 12:51:32.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.046204282s
    Jan  3 12:51:32.238: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 12:51:34.240: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.04817535s
    Jan  3 12:51:34.240: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 12:51:36.242: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.049987311s
    Jan  3 12:51:36.242: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 12:51:38.241: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.048587106s
    Jan  3 12:51:38.241: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 12:51:40.239: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.046632999s
    Jan  3 12:51:40.239: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  3 12:51:40.239: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  3 12:51:40.259: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2534" to be "running and ready"
    Jan  3 12:51:40.278: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 18.413128ms
    Jan  3 12:51:40.278: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  3 12:51:40.278: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  3 12:51:40.297: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2534" to be "running and ready"
    Jan  3 12:51:40.318: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 20.498472ms
    Jan  3 12:51:40.318: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  3 12:51:40.318: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/03/24 12:51:40.337
    Jan  3 12:51:40.389: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2534" to be "running"
    Jan  3 12:51:40.412: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 23.034454ms
    Jan  3 12:51:42.432: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042509613s
    Jan  3 12:51:44.433: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.044070811s
    Jan  3 12:51:44.434: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  3 12:51:44.451: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2534" to be "running"
    Jan  3 12:51:44.471: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 19.697383ms
    Jan  3 12:51:44.471: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan  3 12:51:44.489: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan  3 12:51:44.490: INFO: Going to poll 10.221.146.117 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan  3 12:51:44.509: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.221.146.117:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2534 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:51:44.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:51:44.510: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:51:44.510: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2534/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.221.146.117%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  3 12:51:44.836: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan  3 12:51:44.836: INFO: Going to poll 10.222.238.203 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan  3 12:51:44.856: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.222.238.203:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2534 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:51:44.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:51:44.857: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:51:44.857: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2534/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.222.238.203%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  3 12:51:45.166: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan  3 12:51:45.166: INFO: Going to poll 10.221.146.166 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan  3 12:51:45.185: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.221.146.166:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2534 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 12:51:45.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:51:45.187: INFO: ExecWithOptions: Clientset creation
    Jan  3 12:51:45.187: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2534/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.221.146.166%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  3 12:51:45.491: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:51:45.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2534" for this suite. 01/03/24 12:51:45.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:51:45.561
Jan  3 12:51:45.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 12:51:45.563
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:51:45.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:51:45.646
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 01/03/24 12:51:45.677
Jan  3 12:51:45.709: INFO: Waiting up to 5m0s for pod "pod-a987b098-7f76-4455-ae4b-4f67baa4634d" in namespace "emptydir-6588" to be "Succeeded or Failed"
Jan  3 12:51:45.732: INFO: Pod "pod-a987b098-7f76-4455-ae4b-4f67baa4634d": Phase="Pending", Reason="", readiness=false. Elapsed: 22.982411ms
Jan  3 12:51:47.752: INFO: Pod "pod-a987b098-7f76-4455-ae4b-4f67baa4634d": Phase="Running", Reason="", readiness=true. Elapsed: 2.042665362s
Jan  3 12:51:49.751: INFO: Pod "pod-a987b098-7f76-4455-ae4b-4f67baa4634d": Phase="Running", Reason="", readiness=false. Elapsed: 4.041665914s
Jan  3 12:51:51.757: INFO: Pod "pod-a987b098-7f76-4455-ae4b-4f67baa4634d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048514815s
STEP: Saw pod success 01/03/24 12:51:51.758
Jan  3 12:51:51.758: INFO: Pod "pod-a987b098-7f76-4455-ae4b-4f67baa4634d" satisfied condition "Succeeded or Failed"
Jan  3 12:51:51.784: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-a987b098-7f76-4455-ae4b-4f67baa4634d container test-container: <nil>
STEP: delete the pod 01/03/24 12:51:51.819
Jan  3 12:51:51.862: INFO: Waiting for pod pod-a987b098-7f76-4455-ae4b-4f67baa4634d to disappear
Jan  3 12:51:51.881: INFO: Pod pod-a987b098-7f76-4455-ae4b-4f67baa4634d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 12:51:51.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6588" for this suite. 01/03/24 12:51:51.911
------------------------------
• [SLOW TEST] [6.392 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:51:45.561
    Jan  3 12:51:45.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 12:51:45.563
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:51:45.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:51:45.646
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/03/24 12:51:45.677
    Jan  3 12:51:45.709: INFO: Waiting up to 5m0s for pod "pod-a987b098-7f76-4455-ae4b-4f67baa4634d" in namespace "emptydir-6588" to be "Succeeded or Failed"
    Jan  3 12:51:45.732: INFO: Pod "pod-a987b098-7f76-4455-ae4b-4f67baa4634d": Phase="Pending", Reason="", readiness=false. Elapsed: 22.982411ms
    Jan  3 12:51:47.752: INFO: Pod "pod-a987b098-7f76-4455-ae4b-4f67baa4634d": Phase="Running", Reason="", readiness=true. Elapsed: 2.042665362s
    Jan  3 12:51:49.751: INFO: Pod "pod-a987b098-7f76-4455-ae4b-4f67baa4634d": Phase="Running", Reason="", readiness=false. Elapsed: 4.041665914s
    Jan  3 12:51:51.757: INFO: Pod "pod-a987b098-7f76-4455-ae4b-4f67baa4634d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048514815s
    STEP: Saw pod success 01/03/24 12:51:51.758
    Jan  3 12:51:51.758: INFO: Pod "pod-a987b098-7f76-4455-ae4b-4f67baa4634d" satisfied condition "Succeeded or Failed"
    Jan  3 12:51:51.784: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-a987b098-7f76-4455-ae4b-4f67baa4634d container test-container: <nil>
    STEP: delete the pod 01/03/24 12:51:51.819
    Jan  3 12:51:51.862: INFO: Waiting for pod pod-a987b098-7f76-4455-ae4b-4f67baa4634d to disappear
    Jan  3 12:51:51.881: INFO: Pod pod-a987b098-7f76-4455-ae4b-4f67baa4634d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:51:51.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6588" for this suite. 01/03/24 12:51:51.911
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:51:51.953
Jan  3 12:51:51.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename daemonsets 01/03/24 12:51:51.956
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:51:52.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:51:52.059
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
STEP: Creating simple DaemonSet "daemon-set" 01/03/24 12:51:52.188
STEP: Check that daemon pods launch on every node of the cluster. 01/03/24 12:51:52.217
Jan  3 12:51:52.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:51:52.278: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:51:53.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 12:51:53.329: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
Jan  3 12:51:54.330: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 12:51:54.330: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 01/03/24 12:51:54.349
STEP: DeleteCollection of the DaemonSets 01/03/24 12:51:54.371
STEP: Verify that ReplicaSets have been deleted 01/03/24 12:51:54.403
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
Jan  3 12:51:54.459: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33981232072"},"items":null}

Jan  3 12:51:54.480: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33981232074"},"items":[{"metadata":{"name":"daemon-set-hdn2r","generateName":"daemon-set-","namespace":"daemonsets-3519","uid":"0810cc8e-be81-44d3-8ba8-19c5d254bcc0","resourceVersion":"33981232069","creationTimestamp":"2024-01-03T12:51:52Z","deletionTimestamp":"2024-01-03T12:52:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d5fbc4e2f3f6a4b65cb683b8ec73663853555fa33c116e558aa427572717bd14","cni.projectcalico.org/podIP":"10.222.238.204/32","cni.projectcalico.org/podIPs":"10.222.238.204/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e1379133-2b8b-494e-9e34-6ed2ca3b1e91","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1379133-2b8b-494e-9e34-6ed2ca3b1e91\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.222.238.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-shf68","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-shf68","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"jb-1-26-np-adtwo5cmi2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["jb-1-26-np-adtwo5cmi2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:52Z"}],"hostIP":"85.215.162.129","podIP":"10.222.238.204","podIPs":[{"ip":"10.222.238.204"}],"startTime":"2024-01-03T12:51:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-03T12:51:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a4a82fd05bb3d54c2f71a0622cd42d63a78a454ae3b6ab3337f48c88c168b4a9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qrmrf","generateName":"daemon-set-","namespace":"daemonsets-3519","uid":"5ff3c77e-0169-4652-8313-7266d8569c25","resourceVersion":"33981232068","creationTimestamp":"2024-01-03T12:51:52Z","deletionTimestamp":"2024-01-03T12:52:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a6e4e4f75f21c6f112377f6035a7b4943d5c7327b2eeb03b0616f41b5bab41c8","cni.projectcalico.org/podIP":"10.221.146.167/32","cni.projectcalico.org/podIPs":"10.221.146.167/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e1379133-2b8b-494e-9e34-6ed2ca3b1e91","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1379133-2b8b-494e-9e34-6ed2ca3b1e91\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-khh4z","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-khh4z","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"jb-1-26-np-nqeu5xtrab","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["jb-1-26-np-nqeu5xtrab"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:54Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:54Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:52Z"}],"hostIP":"85.215.218.90","podIP":"10.221.146.167","podIPs":[{"ip":"10.221.146.167"}],"startTime":"2024-01-03T12:51:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-03T12:51:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://45e69003ff3963bd4dd6f5469b0fcb38ed80c2ca46020440d72d2d0bf3926fc9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tjjlj","generateName":"daemon-set-","namespace":"daemonsets-3519","uid":"504bd2f1-8af7-4220-953b-f4f0f8d75d09","resourceVersion":"33981232067","creationTimestamp":"2024-01-03T12:51:52Z","deletionTimestamp":"2024-01-03T12:52:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"b0f4b476e6358649e405d82f7cd9f656ff9bcda7599f900ea36dcbd5f7069a1e","cni.projectcalico.org/podIP":"10.221.146.83/32","cni.projectcalico.org/podIPs":"10.221.146.83/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e1379133-2b8b-494e-9e34-6ed2ca3b1e91","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1379133-2b8b-494e-9e34-6ed2ca3b1e91\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hxcqg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hxcqg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"jb-1-26-np-64kerjapxk","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["jb-1-26-np-64kerjapxk"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:52Z"}],"hostIP":"185.132.46.116","podIP":"10.221.146.83","podIPs":[{"ip":"10.221.146.83"}],"startTime":"2024-01-03T12:51:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-03T12:51:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e5d1a91e19cbecffd13d47b9a8e5b06441a0d0d3ac1a7e5422ddcef339239342","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:51:54.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3519" for this suite. 01/03/24 12:51:54.573
------------------------------
• [2.645 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:51:51.953
    Jan  3 12:51:51.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename daemonsets 01/03/24 12:51:51.956
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:51:52.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:51:52.059
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:834
    STEP: Creating simple DaemonSet "daemon-set" 01/03/24 12:51:52.188
    STEP: Check that daemon pods launch on every node of the cluster. 01/03/24 12:51:52.217
    Jan  3 12:51:52.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:51:52.278: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:51:53.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 12:51:53.329: INFO: Node jb-1-26-np-64kerjapxk is running 0 daemon pod, expected 1
    Jan  3 12:51:54.330: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 12:51:54.330: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 01/03/24 12:51:54.349
    STEP: DeleteCollection of the DaemonSets 01/03/24 12:51:54.371
    STEP: Verify that ReplicaSets have been deleted 01/03/24 12:51:54.403
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    Jan  3 12:51:54.459: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33981232072"},"items":null}

    Jan  3 12:51:54.480: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33981232074"},"items":[{"metadata":{"name":"daemon-set-hdn2r","generateName":"daemon-set-","namespace":"daemonsets-3519","uid":"0810cc8e-be81-44d3-8ba8-19c5d254bcc0","resourceVersion":"33981232069","creationTimestamp":"2024-01-03T12:51:52Z","deletionTimestamp":"2024-01-03T12:52:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d5fbc4e2f3f6a4b65cb683b8ec73663853555fa33c116e558aa427572717bd14","cni.projectcalico.org/podIP":"10.222.238.204/32","cni.projectcalico.org/podIPs":"10.222.238.204/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e1379133-2b8b-494e-9e34-6ed2ca3b1e91","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1379133-2b8b-494e-9e34-6ed2ca3b1e91\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.222.238.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-shf68","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-shf68","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"jb-1-26-np-adtwo5cmi2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["jb-1-26-np-adtwo5cmi2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:52Z"}],"hostIP":"85.215.162.129","podIP":"10.222.238.204","podIPs":[{"ip":"10.222.238.204"}],"startTime":"2024-01-03T12:51:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-03T12:51:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a4a82fd05bb3d54c2f71a0622cd42d63a78a454ae3b6ab3337f48c88c168b4a9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qrmrf","generateName":"daemon-set-","namespace":"daemonsets-3519","uid":"5ff3c77e-0169-4652-8313-7266d8569c25","resourceVersion":"33981232068","creationTimestamp":"2024-01-03T12:51:52Z","deletionTimestamp":"2024-01-03T12:52:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a6e4e4f75f21c6f112377f6035a7b4943d5c7327b2eeb03b0616f41b5bab41c8","cni.projectcalico.org/podIP":"10.221.146.167/32","cni.projectcalico.org/podIPs":"10.221.146.167/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e1379133-2b8b-494e-9e34-6ed2ca3b1e91","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1379133-2b8b-494e-9e34-6ed2ca3b1e91\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-khh4z","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-khh4z","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"jb-1-26-np-nqeu5xtrab","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["jb-1-26-np-nqeu5xtrab"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:54Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:54Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:52Z"}],"hostIP":"85.215.218.90","podIP":"10.221.146.167","podIPs":[{"ip":"10.221.146.167"}],"startTime":"2024-01-03T12:51:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-03T12:51:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://45e69003ff3963bd4dd6f5469b0fcb38ed80c2ca46020440d72d2d0bf3926fc9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tjjlj","generateName":"daemon-set-","namespace":"daemonsets-3519","uid":"504bd2f1-8af7-4220-953b-f4f0f8d75d09","resourceVersion":"33981232067","creationTimestamp":"2024-01-03T12:51:52Z","deletionTimestamp":"2024-01-03T12:52:24Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"b0f4b476e6358649e405d82f7cd9f656ff9bcda7599f900ea36dcbd5f7069a1e","cni.projectcalico.org/podIP":"10.221.146.83/32","cni.projectcalico.org/podIPs":"10.221.146.83/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e1379133-2b8b-494e-9e34-6ed2ca3b1e91","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1379133-2b8b-494e-9e34-6ed2ca3b1e91\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-03T12:51:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hxcqg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hxcqg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"jb-1-26-np-64kerjapxk","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["jb-1-26-np-64kerjapxk"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-03T12:51:52Z"}],"hostIP":"185.132.46.116","podIP":"10.221.146.83","podIPs":[{"ip":"10.221.146.83"}],"startTime":"2024-01-03T12:51:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-03T12:51:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e5d1a91e19cbecffd13d47b9a8e5b06441a0d0d3ac1a7e5422ddcef339239342","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:51:54.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3519" for this suite. 01/03/24 12:51:54.573
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:51:54.599
Jan  3 12:51:54.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 12:51:54.601
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:51:54.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:51:54.683
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-9bad2bd0-085e-4f56-8980-8a676c7368f7 01/03/24 12:51:54.71
STEP: Creating a pod to test consume configMaps 01/03/24 12:51:54.73
Jan  3 12:51:54.755: INFO: Waiting up to 5m0s for pod "pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801" in namespace "configmap-2240" to be "Succeeded or Failed"
Jan  3 12:51:54.774: INFO: Pod "pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801": Phase="Pending", Reason="", readiness=false. Elapsed: 18.102853ms
Jan  3 12:51:56.794: INFO: Pod "pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038473032s
Jan  3 12:51:58.795: INFO: Pod "pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039061468s
Jan  3 12:52:00.797: INFO: Pod "pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041333669s
STEP: Saw pod success 01/03/24 12:52:00.797
Jan  3 12:52:00.797: INFO: Pod "pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801" satisfied condition "Succeeded or Failed"
Jan  3 12:52:00.815: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801 container agnhost-container: <nil>
STEP: delete the pod 01/03/24 12:52:00.859
Jan  3 12:52:00.901: INFO: Waiting for pod pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801 to disappear
Jan  3 12:52:00.918: INFO: Pod pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 12:52:00.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2240" for this suite. 01/03/24 12:52:00.95
------------------------------
• [SLOW TEST] [6.378 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:51:54.599
    Jan  3 12:51:54.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 12:51:54.601
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:51:54.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:51:54.683
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-9bad2bd0-085e-4f56-8980-8a676c7368f7 01/03/24 12:51:54.71
    STEP: Creating a pod to test consume configMaps 01/03/24 12:51:54.73
    Jan  3 12:51:54.755: INFO: Waiting up to 5m0s for pod "pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801" in namespace "configmap-2240" to be "Succeeded or Failed"
    Jan  3 12:51:54.774: INFO: Pod "pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801": Phase="Pending", Reason="", readiness=false. Elapsed: 18.102853ms
    Jan  3 12:51:56.794: INFO: Pod "pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038473032s
    Jan  3 12:51:58.795: INFO: Pod "pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039061468s
    Jan  3 12:52:00.797: INFO: Pod "pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041333669s
    STEP: Saw pod success 01/03/24 12:52:00.797
    Jan  3 12:52:00.797: INFO: Pod "pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801" satisfied condition "Succeeded or Failed"
    Jan  3 12:52:00.815: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801 container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 12:52:00.859
    Jan  3 12:52:00.901: INFO: Waiting for pod pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801 to disappear
    Jan  3 12:52:00.918: INFO: Pod pod-configmaps-a3c34697-283b-4226-b9c4-1a708d0b9801 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:52:00.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2240" for this suite. 01/03/24 12:52:00.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:52:00.98
Jan  3 12:52:00.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename var-expansion 01/03/24 12:52:00.983
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:52:01.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:52:01.068
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 01/03/24 12:52:01.095
Jan  3 12:52:01.126: INFO: Waiting up to 5m0s for pod "var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d" in namespace "var-expansion-4365" to be "Succeeded or Failed"
Jan  3 12:52:01.144: INFO: Pod "var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.149449ms
Jan  3 12:52:03.169: INFO: Pod "var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.042965666s
Jan  3 12:52:05.170: INFO: Pod "var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d": Phase="Running", Reason="", readiness=false. Elapsed: 4.044069555s
Jan  3 12:52:07.168: INFO: Pod "var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042093506s
STEP: Saw pod success 01/03/24 12:52:07.168
Jan  3 12:52:07.168: INFO: Pod "var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d" satisfied condition "Succeeded or Failed"
Jan  3 12:52:07.187: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d container dapi-container: <nil>
STEP: delete the pod 01/03/24 12:52:07.225
Jan  3 12:52:07.262: INFO: Waiting for pod var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d to disappear
Jan  3 12:52:07.282: INFO: Pod var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  3 12:52:07.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4365" for this suite. 01/03/24 12:52:07.312
------------------------------
• [SLOW TEST] [6.385 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:52:00.98
    Jan  3 12:52:00.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename var-expansion 01/03/24 12:52:00.983
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:52:01.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:52:01.068
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 01/03/24 12:52:01.095
    Jan  3 12:52:01.126: INFO: Waiting up to 5m0s for pod "var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d" in namespace "var-expansion-4365" to be "Succeeded or Failed"
    Jan  3 12:52:01.144: INFO: Pod "var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.149449ms
    Jan  3 12:52:03.169: INFO: Pod "var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.042965666s
    Jan  3 12:52:05.170: INFO: Pod "var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d": Phase="Running", Reason="", readiness=false. Elapsed: 4.044069555s
    Jan  3 12:52:07.168: INFO: Pod "var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042093506s
    STEP: Saw pod success 01/03/24 12:52:07.168
    Jan  3 12:52:07.168: INFO: Pod "var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d" satisfied condition "Succeeded or Failed"
    Jan  3 12:52:07.187: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d container dapi-container: <nil>
    STEP: delete the pod 01/03/24 12:52:07.225
    Jan  3 12:52:07.262: INFO: Waiting for pod var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d to disappear
    Jan  3 12:52:07.282: INFO: Pod var-expansion-460d7eb0-a455-4f41-a6bf-5fbabe429e8d no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:52:07.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4365" for this suite. 01/03/24 12:52:07.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:52:07.369
Jan  3 12:52:07.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-probe 01/03/24 12:52:07.371
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:52:07.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:52:07.452
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475 in namespace container-probe-1801 01/03/24 12:52:07.485
Jan  3 12:52:07.512: INFO: Waiting up to 5m0s for pod "busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475" in namespace "container-probe-1801" to be "not pending"
Jan  3 12:52:07.531: INFO: Pod "busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475": Phase="Pending", Reason="", readiness=false. Elapsed: 19.030982ms
Jan  3 12:52:09.554: INFO: Pod "busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475": Phase="Running", Reason="", readiness=true. Elapsed: 2.042016118s
Jan  3 12:52:09.554: INFO: Pod "busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475" satisfied condition "not pending"
Jan  3 12:52:09.554: INFO: Started pod busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475 in namespace container-probe-1801
STEP: checking the pod's current state and verifying that restartCount is present 01/03/24 12:52:09.554
Jan  3 12:52:09.576: INFO: Initial restart count of pod busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475 is 0
Jan  3 12:53:00.199: INFO: Restart count of pod container-probe-1801/busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475 is now 1 (50.6228605s elapsed)
STEP: deleting the pod 01/03/24 12:53:00.199
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  3 12:53:00.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1801" for this suite. 01/03/24 12:53:00.27
------------------------------
• [SLOW TEST] [52.929 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:52:07.369
    Jan  3 12:52:07.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-probe 01/03/24 12:52:07.371
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:52:07.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:52:07.452
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475 in namespace container-probe-1801 01/03/24 12:52:07.485
    Jan  3 12:52:07.512: INFO: Waiting up to 5m0s for pod "busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475" in namespace "container-probe-1801" to be "not pending"
    Jan  3 12:52:07.531: INFO: Pod "busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475": Phase="Pending", Reason="", readiness=false. Elapsed: 19.030982ms
    Jan  3 12:52:09.554: INFO: Pod "busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475": Phase="Running", Reason="", readiness=true. Elapsed: 2.042016118s
    Jan  3 12:52:09.554: INFO: Pod "busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475" satisfied condition "not pending"
    Jan  3 12:52:09.554: INFO: Started pod busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475 in namespace container-probe-1801
    STEP: checking the pod's current state and verifying that restartCount is present 01/03/24 12:52:09.554
    Jan  3 12:52:09.576: INFO: Initial restart count of pod busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475 is 0
    Jan  3 12:53:00.199: INFO: Restart count of pod container-probe-1801/busybox-9985b09c-bdc3-4797-a44b-8a3ec769c475 is now 1 (50.6228605s elapsed)
    STEP: deleting the pod 01/03/24 12:53:00.199
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:53:00.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1801" for this suite. 01/03/24 12:53:00.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:53:00.3
Jan  3 12:53:00.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename replicaset 01/03/24 12:53:00.305
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:53:00.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:53:00.396
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/03/24 12:53:00.441
STEP: Verify that the required pods have come up. 01/03/24 12:53:00.462
Jan  3 12:53:00.484: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/03/24 12:53:00.485
Jan  3 12:53:00.485: INFO: Waiting up to 5m0s for pod "test-rs-nqz9r" in namespace "replicaset-2020" to be "running"
Jan  3 12:53:00.503: INFO: Pod "test-rs-nqz9r": Phase="Pending", Reason="", readiness=false. Elapsed: 17.841553ms
Jan  3 12:53:02.527: INFO: Pod "test-rs-nqz9r": Phase="Running", Reason="", readiness=true. Elapsed: 2.042143064s
Jan  3 12:53:02.527: INFO: Pod "test-rs-nqz9r" satisfied condition "running"
STEP: Getting /status 01/03/24 12:53:02.528
Jan  3 12:53:02.549: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/03/24 12:53:02.549
Jan  3 12:53:02.585: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/03/24 12:53:02.585
Jan  3 12:53:02.600: INFO: Observed &ReplicaSet event: ADDED
Jan  3 12:53:02.600: INFO: Observed &ReplicaSet event: MODIFIED
Jan  3 12:53:02.600: INFO: Observed &ReplicaSet event: MODIFIED
Jan  3 12:53:02.601: INFO: Observed &ReplicaSet event: MODIFIED
Jan  3 12:53:02.601: INFO: Found replicaset test-rs in namespace replicaset-2020 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  3 12:53:02.601: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/03/24 12:53:02.601
Jan  3 12:53:02.602: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  3 12:53:02.622: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/03/24 12:53:02.622
Jan  3 12:53:02.637: INFO: Observed &ReplicaSet event: ADDED
Jan  3 12:53:02.637: INFO: Observed &ReplicaSet event: MODIFIED
Jan  3 12:53:02.638: INFO: Observed &ReplicaSet event: MODIFIED
Jan  3 12:53:02.638: INFO: Observed &ReplicaSet event: MODIFIED
Jan  3 12:53:02.638: INFO: Observed replicaset test-rs in namespace replicaset-2020 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  3 12:53:02.638: INFO: Observed &ReplicaSet event: MODIFIED
Jan  3 12:53:02.638: INFO: Found replicaset test-rs in namespace replicaset-2020 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan  3 12:53:02.639: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  3 12:53:02.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2020" for this suite. 01/03/24 12:53:02.677
------------------------------
• [2.400 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:53:00.3
    Jan  3 12:53:00.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename replicaset 01/03/24 12:53:00.305
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:53:00.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:53:00.396
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/03/24 12:53:00.441
    STEP: Verify that the required pods have come up. 01/03/24 12:53:00.462
    Jan  3 12:53:00.484: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/03/24 12:53:00.485
    Jan  3 12:53:00.485: INFO: Waiting up to 5m0s for pod "test-rs-nqz9r" in namespace "replicaset-2020" to be "running"
    Jan  3 12:53:00.503: INFO: Pod "test-rs-nqz9r": Phase="Pending", Reason="", readiness=false. Elapsed: 17.841553ms
    Jan  3 12:53:02.527: INFO: Pod "test-rs-nqz9r": Phase="Running", Reason="", readiness=true. Elapsed: 2.042143064s
    Jan  3 12:53:02.527: INFO: Pod "test-rs-nqz9r" satisfied condition "running"
    STEP: Getting /status 01/03/24 12:53:02.528
    Jan  3 12:53:02.549: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/03/24 12:53:02.549
    Jan  3 12:53:02.585: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/03/24 12:53:02.585
    Jan  3 12:53:02.600: INFO: Observed &ReplicaSet event: ADDED
    Jan  3 12:53:02.600: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  3 12:53:02.600: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  3 12:53:02.601: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  3 12:53:02.601: INFO: Found replicaset test-rs in namespace replicaset-2020 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  3 12:53:02.601: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/03/24 12:53:02.601
    Jan  3 12:53:02.602: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  3 12:53:02.622: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/03/24 12:53:02.622
    Jan  3 12:53:02.637: INFO: Observed &ReplicaSet event: ADDED
    Jan  3 12:53:02.637: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  3 12:53:02.638: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  3 12:53:02.638: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  3 12:53:02.638: INFO: Observed replicaset test-rs in namespace replicaset-2020 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  3 12:53:02.638: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  3 12:53:02.638: INFO: Found replicaset test-rs in namespace replicaset-2020 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan  3 12:53:02.639: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:53:02.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2020" for this suite. 01/03/24 12:53:02.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:53:02.705
Jan  3 12:53:02.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 12:53:02.707
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:53:02.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:53:02.791
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/03/24 12:53:02.819
Jan  3 12:53:02.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/03/24 12:53:13.309
Jan  3 12:53:13.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:53:16.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:53:26.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6042" for this suite. 01/03/24 12:53:26.965
------------------------------
• [SLOW TEST] [24.283 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:53:02.705
    Jan  3 12:53:02.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 12:53:02.707
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:53:02.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:53:02.791
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/03/24 12:53:02.819
    Jan  3 12:53:02.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/03/24 12:53:13.309
    Jan  3 12:53:13.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:53:16.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:53:26.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6042" for this suite. 01/03/24 12:53:26.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:53:26.99
Jan  3 12:53:26.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-runtime 01/03/24 12:53:26.992
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:53:27.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:53:27.078
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 01/03/24 12:53:27.105
STEP: wait for the container to reach Succeeded 01/03/24 12:53:27.133
STEP: get the container status 01/03/24 12:53:32.253
STEP: the container should be terminated 01/03/24 12:53:32.272
STEP: the termination message should be set 01/03/24 12:53:32.272
Jan  3 12:53:32.272: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/03/24 12:53:32.272
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  3 12:53:32.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8864" for this suite. 01/03/24 12:53:32.358
------------------------------
• [SLOW TEST] [5.392 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:53:26.99
    Jan  3 12:53:26.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-runtime 01/03/24 12:53:26.992
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:53:27.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:53:27.078
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 01/03/24 12:53:27.105
    STEP: wait for the container to reach Succeeded 01/03/24 12:53:27.133
    STEP: get the container status 01/03/24 12:53:32.253
    STEP: the container should be terminated 01/03/24 12:53:32.272
    STEP: the termination message should be set 01/03/24 12:53:32.272
    Jan  3 12:53:32.272: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/03/24 12:53:32.272
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:53:32.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8864" for this suite. 01/03/24 12:53:32.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:53:32.389
Jan  3 12:53:32.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 12:53:32.391
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:53:32.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:53:32.47
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-7519 01/03/24 12:53:32.497
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7519 to expose endpoints map[] 01/03/24 12:53:32.525
Jan  3 12:53:32.578: INFO: successfully validated that service endpoint-test2 in namespace services-7519 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7519 01/03/24 12:53:32.578
Jan  3 12:53:32.635: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7519" to be "running and ready"
Jan  3 12:53:32.653: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.383625ms
Jan  3 12:53:32.653: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:53:34.674: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039349853s
Jan  3 12:53:34.674: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:53:36.677: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.042619473s
Jan  3 12:53:36.677: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  3 12:53:36.677: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7519 to expose endpoints map[pod1:[80]] 01/03/24 12:53:36.718
Jan  3 12:53:36.792: INFO: successfully validated that service endpoint-test2 in namespace services-7519 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/03/24 12:53:36.792
Jan  3 12:53:36.793: INFO: Creating new exec pod
Jan  3 12:53:36.813: INFO: Waiting up to 5m0s for pod "execpod5zd7w" in namespace "services-7519" to be "running"
Jan  3 12:53:36.833: INFO: Pod "execpod5zd7w": Phase="Pending", Reason="", readiness=false. Elapsed: 20.220102ms
Jan  3 12:53:38.866: INFO: Pod "execpod5zd7w": Phase="Running", Reason="", readiness=true. Elapsed: 2.052850636s
Jan  3 12:53:38.866: INFO: Pod "execpod5zd7w" satisfied condition "running"
Jan  3 12:53:39.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-7519 exec execpod5zd7w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  3 12:53:40.356: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  3 12:53:40.356: INFO: stdout: ""
Jan  3 12:53:40.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-7519 exec execpod5zd7w -- /bin/sh -x -c nc -v -z -w 2 10.233.60.61 80'
Jan  3 12:53:40.811: INFO: stderr: "+ nc -v -z -w 2 10.233.60.61 80\nConnection to 10.233.60.61 80 port [tcp/http] succeeded!\n"
Jan  3 12:53:40.811: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-7519 01/03/24 12:53:40.811
Jan  3 12:53:40.832: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7519" to be "running and ready"
Jan  3 12:53:40.850: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.056668ms
Jan  3 12:53:40.850: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:53:42.870: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.038229072s
Jan  3 12:53:42.870: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  3 12:53:42.870: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7519 to expose endpoints map[pod1:[80] pod2:[80]] 01/03/24 12:53:42.889
Jan  3 12:53:42.978: INFO: successfully validated that service endpoint-test2 in namespace services-7519 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/03/24 12:53:42.978
Jan  3 12:53:43.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-7519 exec execpod5zd7w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  3 12:53:44.445: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  3 12:53:44.445: INFO: stdout: ""
Jan  3 12:53:44.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-7519 exec execpod5zd7w -- /bin/sh -x -c nc -v -z -w 2 10.233.60.61 80'
Jan  3 12:53:44.920: INFO: stderr: "+ nc -v -z -w 2 10.233.60.61 80\nConnection to 10.233.60.61 80 port [tcp/http] succeeded!\n"
Jan  3 12:53:44.920: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-7519 01/03/24 12:53:44.92
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7519 to expose endpoints map[pod2:[80]] 01/03/24 12:53:44.956
Jan  3 12:53:45.026: INFO: successfully validated that service endpoint-test2 in namespace services-7519 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/03/24 12:53:45.026
Jan  3 12:53:46.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-7519 exec execpod5zd7w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  3 12:53:46.535: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  3 12:53:46.535: INFO: stdout: ""
Jan  3 12:53:46.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-7519 exec execpod5zd7w -- /bin/sh -x -c nc -v -z -w 2 10.233.60.61 80'
Jan  3 12:53:46.992: INFO: stderr: "+ nc -v -z -w 2 10.233.60.61 80\nConnection to 10.233.60.61 80 port [tcp/http] succeeded!\n"
Jan  3 12:53:46.992: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-7519 01/03/24 12:53:46.992
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7519 to expose endpoints map[] 01/03/24 12:53:47.067
Jan  3 12:53:47.112: INFO: successfully validated that service endpoint-test2 in namespace services-7519 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 12:53:47.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7519" for this suite. 01/03/24 12:53:47.185
------------------------------
• [SLOW TEST] [14.820 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:53:32.389
    Jan  3 12:53:32.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 12:53:32.391
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:53:32.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:53:32.47
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-7519 01/03/24 12:53:32.497
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7519 to expose endpoints map[] 01/03/24 12:53:32.525
    Jan  3 12:53:32.578: INFO: successfully validated that service endpoint-test2 in namespace services-7519 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7519 01/03/24 12:53:32.578
    Jan  3 12:53:32.635: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7519" to be "running and ready"
    Jan  3 12:53:32.653: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.383625ms
    Jan  3 12:53:32.653: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:53:34.674: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039349853s
    Jan  3 12:53:34.674: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:53:36.677: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.042619473s
    Jan  3 12:53:36.677: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  3 12:53:36.677: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7519 to expose endpoints map[pod1:[80]] 01/03/24 12:53:36.718
    Jan  3 12:53:36.792: INFO: successfully validated that service endpoint-test2 in namespace services-7519 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/03/24 12:53:36.792
    Jan  3 12:53:36.793: INFO: Creating new exec pod
    Jan  3 12:53:36.813: INFO: Waiting up to 5m0s for pod "execpod5zd7w" in namespace "services-7519" to be "running"
    Jan  3 12:53:36.833: INFO: Pod "execpod5zd7w": Phase="Pending", Reason="", readiness=false. Elapsed: 20.220102ms
    Jan  3 12:53:38.866: INFO: Pod "execpod5zd7w": Phase="Running", Reason="", readiness=true. Elapsed: 2.052850636s
    Jan  3 12:53:38.866: INFO: Pod "execpod5zd7w" satisfied condition "running"
    Jan  3 12:53:39.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-7519 exec execpod5zd7w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  3 12:53:40.356: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  3 12:53:40.356: INFO: stdout: ""
    Jan  3 12:53:40.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-7519 exec execpod5zd7w -- /bin/sh -x -c nc -v -z -w 2 10.233.60.61 80'
    Jan  3 12:53:40.811: INFO: stderr: "+ nc -v -z -w 2 10.233.60.61 80\nConnection to 10.233.60.61 80 port [tcp/http] succeeded!\n"
    Jan  3 12:53:40.811: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-7519 01/03/24 12:53:40.811
    Jan  3 12:53:40.832: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7519" to be "running and ready"
    Jan  3 12:53:40.850: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.056668ms
    Jan  3 12:53:40.850: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:53:42.870: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.038229072s
    Jan  3 12:53:42.870: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  3 12:53:42.870: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7519 to expose endpoints map[pod1:[80] pod2:[80]] 01/03/24 12:53:42.889
    Jan  3 12:53:42.978: INFO: successfully validated that service endpoint-test2 in namespace services-7519 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/03/24 12:53:42.978
    Jan  3 12:53:43.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-7519 exec execpod5zd7w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  3 12:53:44.445: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  3 12:53:44.445: INFO: stdout: ""
    Jan  3 12:53:44.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-7519 exec execpod5zd7w -- /bin/sh -x -c nc -v -z -w 2 10.233.60.61 80'
    Jan  3 12:53:44.920: INFO: stderr: "+ nc -v -z -w 2 10.233.60.61 80\nConnection to 10.233.60.61 80 port [tcp/http] succeeded!\n"
    Jan  3 12:53:44.920: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-7519 01/03/24 12:53:44.92
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7519 to expose endpoints map[pod2:[80]] 01/03/24 12:53:44.956
    Jan  3 12:53:45.026: INFO: successfully validated that service endpoint-test2 in namespace services-7519 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/03/24 12:53:45.026
    Jan  3 12:53:46.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-7519 exec execpod5zd7w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  3 12:53:46.535: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  3 12:53:46.535: INFO: stdout: ""
    Jan  3 12:53:46.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-7519 exec execpod5zd7w -- /bin/sh -x -c nc -v -z -w 2 10.233.60.61 80'
    Jan  3 12:53:46.992: INFO: stderr: "+ nc -v -z -w 2 10.233.60.61 80\nConnection to 10.233.60.61 80 port [tcp/http] succeeded!\n"
    Jan  3 12:53:46.992: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-7519 01/03/24 12:53:46.992
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7519 to expose endpoints map[] 01/03/24 12:53:47.067
    Jan  3 12:53:47.112: INFO: successfully validated that service endpoint-test2 in namespace services-7519 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:53:47.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7519" for this suite. 01/03/24 12:53:47.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:53:47.212
Jan  3 12:53:47.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename job 01/03/24 12:53:47.215
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:53:47.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:53:47.291
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 01/03/24 12:53:47.318
STEP: Ensuring active pods == parallelism 01/03/24 12:53:47.338
STEP: delete a job 01/03/24 12:53:49.358
STEP: deleting Job.batch foo in namespace job-1480, will wait for the garbage collector to delete the pods 01/03/24 12:53:49.359
Jan  3 12:53:49.446: INFO: Deleting Job.batch foo took: 20.049629ms
Jan  3 12:53:49.546: INFO: Terminating Job.batch foo pods took: 100.37263ms
STEP: Ensuring job was deleted 01/03/24 12:54:23.147
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  3 12:54:23.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1480" for this suite. 01/03/24 12:54:23.198
------------------------------
• [SLOW TEST] [36.010 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:53:47.212
    Jan  3 12:53:47.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename job 01/03/24 12:53:47.215
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:53:47.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:53:47.291
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 01/03/24 12:53:47.318
    STEP: Ensuring active pods == parallelism 01/03/24 12:53:47.338
    STEP: delete a job 01/03/24 12:53:49.358
    STEP: deleting Job.batch foo in namespace job-1480, will wait for the garbage collector to delete the pods 01/03/24 12:53:49.359
    Jan  3 12:53:49.446: INFO: Deleting Job.batch foo took: 20.049629ms
    Jan  3 12:53:49.546: INFO: Terminating Job.batch foo pods took: 100.37263ms
    STEP: Ensuring job was deleted 01/03/24 12:54:23.147
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:54:23.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1480" for this suite. 01/03/24 12:54:23.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:54:23.23
Jan  3 12:54:23.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename csistoragecapacity 01/03/24 12:54:23.232
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:54:23.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:54:23.314
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/03/24 12:54:23.342
STEP: getting /apis/storage.k8s.io 01/03/24 12:54:23.37
STEP: getting /apis/storage.k8s.io/v1 01/03/24 12:54:23.383
STEP: creating 01/03/24 12:54:23.396
STEP: watching 01/03/24 12:54:23.46
Jan  3 12:54:23.460: INFO: starting watch
STEP: getting 01/03/24 12:54:23.505
STEP: listing in namespace 01/03/24 12:54:23.523
STEP: listing across namespaces 01/03/24 12:54:23.54
STEP: patching 01/03/24 12:54:23.563
STEP: updating 01/03/24 12:54:23.583
Jan  3 12:54:23.601: INFO: waiting for watch events with expected annotations in namespace
Jan  3 12:54:23.601: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/03/24 12:54:23.601
STEP: deleting a collection 01/03/24 12:54:23.689
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jan  3 12:54:23.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-5218" for this suite. 01/03/24 12:54:23.764
------------------------------
• [0.560 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:54:23.23
    Jan  3 12:54:23.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename csistoragecapacity 01/03/24 12:54:23.232
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:54:23.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:54:23.314
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/03/24 12:54:23.342
    STEP: getting /apis/storage.k8s.io 01/03/24 12:54:23.37
    STEP: getting /apis/storage.k8s.io/v1 01/03/24 12:54:23.383
    STEP: creating 01/03/24 12:54:23.396
    STEP: watching 01/03/24 12:54:23.46
    Jan  3 12:54:23.460: INFO: starting watch
    STEP: getting 01/03/24 12:54:23.505
    STEP: listing in namespace 01/03/24 12:54:23.523
    STEP: listing across namespaces 01/03/24 12:54:23.54
    STEP: patching 01/03/24 12:54:23.563
    STEP: updating 01/03/24 12:54:23.583
    Jan  3 12:54:23.601: INFO: waiting for watch events with expected annotations in namespace
    Jan  3 12:54:23.601: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/03/24 12:54:23.601
    STEP: deleting a collection 01/03/24 12:54:23.689
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:54:23.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-5218" for this suite. 01/03/24 12:54:23.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:54:23.796
Jan  3 12:54:23.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 12:54:23.798
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:54:23.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:54:23.881
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 01/03/24 12:54:23.908
Jan  3 12:54:23.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 create -f -'
Jan  3 12:54:24.893: INFO: stderr: ""
Jan  3 12:54:24.893: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/03/24 12:54:24.893
Jan  3 12:54:24.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 12:54:25.038: INFO: stderr: ""
Jan  3 12:54:25.038: INFO: stdout: "update-demo-nautilus-4ss2g update-demo-nautilus-fjsh2 "
Jan  3 12:54:25.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods update-demo-nautilus-4ss2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 12:54:25.176: INFO: stderr: ""
Jan  3 12:54:25.176: INFO: stdout: ""
Jan  3 12:54:25.176: INFO: update-demo-nautilus-4ss2g is created but not running
Jan  3 12:54:30.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 12:54:30.329: INFO: stderr: ""
Jan  3 12:54:30.329: INFO: stdout: "update-demo-nautilus-4ss2g update-demo-nautilus-fjsh2 "
Jan  3 12:54:30.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods update-demo-nautilus-4ss2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 12:54:30.486: INFO: stderr: ""
Jan  3 12:54:30.486: INFO: stdout: "true"
Jan  3 12:54:30.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods update-demo-nautilus-4ss2g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 12:54:30.615: INFO: stderr: ""
Jan  3 12:54:30.615: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  3 12:54:30.615: INFO: validating pod update-demo-nautilus-4ss2g
Jan  3 12:54:30.724: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 12:54:30.724: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 12:54:30.724: INFO: update-demo-nautilus-4ss2g is verified up and running
Jan  3 12:54:30.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods update-demo-nautilus-fjsh2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 12:54:30.881: INFO: stderr: ""
Jan  3 12:54:30.881: INFO: stdout: "true"
Jan  3 12:54:30.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods update-demo-nautilus-fjsh2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 12:54:31.027: INFO: stderr: ""
Jan  3 12:54:31.027: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  3 12:54:31.027: INFO: validating pod update-demo-nautilus-fjsh2
Jan  3 12:54:31.127: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 12:54:31.127: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 12:54:31.127: INFO: update-demo-nautilus-fjsh2 is verified up and running
STEP: using delete to clean up resources 01/03/24 12:54:31.127
Jan  3 12:54:31.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 delete --grace-period=0 --force -f -'
Jan  3 12:54:31.295: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 12:54:31.295: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan  3 12:54:31.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get rc,svc -l name=update-demo --no-headers'
Jan  3 12:54:31.451: INFO: stderr: "No resources found in kubectl-6295 namespace.\n"
Jan  3 12:54:31.451: INFO: stdout: ""
Jan  3 12:54:31.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  3 12:54:31.602: INFO: stderr: ""
Jan  3 12:54:31.602: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 12:54:31.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6295" for this suite. 01/03/24 12:54:31.632
------------------------------
• [SLOW TEST] [7.864 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:54:23.796
    Jan  3 12:54:23.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 12:54:23.798
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:54:23.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:54:23.881
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 01/03/24 12:54:23.908
    Jan  3 12:54:23.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 create -f -'
    Jan  3 12:54:24.893: INFO: stderr: ""
    Jan  3 12:54:24.893: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/03/24 12:54:24.893
    Jan  3 12:54:24.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 12:54:25.038: INFO: stderr: ""
    Jan  3 12:54:25.038: INFO: stdout: "update-demo-nautilus-4ss2g update-demo-nautilus-fjsh2 "
    Jan  3 12:54:25.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods update-demo-nautilus-4ss2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 12:54:25.176: INFO: stderr: ""
    Jan  3 12:54:25.176: INFO: stdout: ""
    Jan  3 12:54:25.176: INFO: update-demo-nautilus-4ss2g is created but not running
    Jan  3 12:54:30.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 12:54:30.329: INFO: stderr: ""
    Jan  3 12:54:30.329: INFO: stdout: "update-demo-nautilus-4ss2g update-demo-nautilus-fjsh2 "
    Jan  3 12:54:30.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods update-demo-nautilus-4ss2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 12:54:30.486: INFO: stderr: ""
    Jan  3 12:54:30.486: INFO: stdout: "true"
    Jan  3 12:54:30.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods update-demo-nautilus-4ss2g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 12:54:30.615: INFO: stderr: ""
    Jan  3 12:54:30.615: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  3 12:54:30.615: INFO: validating pod update-demo-nautilus-4ss2g
    Jan  3 12:54:30.724: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 12:54:30.724: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 12:54:30.724: INFO: update-demo-nautilus-4ss2g is verified up and running
    Jan  3 12:54:30.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods update-demo-nautilus-fjsh2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 12:54:30.881: INFO: stderr: ""
    Jan  3 12:54:30.881: INFO: stdout: "true"
    Jan  3 12:54:30.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods update-demo-nautilus-fjsh2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 12:54:31.027: INFO: stderr: ""
    Jan  3 12:54:31.027: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  3 12:54:31.027: INFO: validating pod update-demo-nautilus-fjsh2
    Jan  3 12:54:31.127: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 12:54:31.127: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 12:54:31.127: INFO: update-demo-nautilus-fjsh2 is verified up and running
    STEP: using delete to clean up resources 01/03/24 12:54:31.127
    Jan  3 12:54:31.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 delete --grace-period=0 --force -f -'
    Jan  3 12:54:31.295: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 12:54:31.295: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan  3 12:54:31.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get rc,svc -l name=update-demo --no-headers'
    Jan  3 12:54:31.451: INFO: stderr: "No resources found in kubectl-6295 namespace.\n"
    Jan  3 12:54:31.451: INFO: stdout: ""
    Jan  3 12:54:31.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-6295 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  3 12:54:31.602: INFO: stderr: ""
    Jan  3 12:54:31.602: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:54:31.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6295" for this suite. 01/03/24 12:54:31.632
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:54:31.66
Jan  3 12:54:31.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pods 01/03/24 12:54:31.663
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:54:31.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:54:31.739
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jan  3 12:54:31.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: creating the pod 01/03/24 12:54:31.766
STEP: submitting the pod to kubernetes 01/03/24 12:54:31.767
Jan  3 12:54:31.792: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363" in namespace "pods-5554" to be "running and ready"
Jan  3 12:54:31.809: INFO: Pod "pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363": Phase="Pending", Reason="", readiness=false. Elapsed: 17.574153ms
Jan  3 12:54:31.809: INFO: The phase of Pod pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:54:33.833: INFO: Pod "pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041024036s
Jan  3 12:54:33.833: INFO: The phase of Pod pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 12:54:35.831: INFO: Pod "pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363": Phase="Running", Reason="", readiness=true. Elapsed: 4.038952896s
Jan  3 12:54:35.831: INFO: The phase of Pod pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363 is Running (Ready = true)
Jan  3 12:54:35.831: INFO: Pod "pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  3 12:54:36.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5554" for this suite. 01/03/24 12:54:36.116
------------------------------
• [4.482 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:54:31.66
    Jan  3 12:54:31.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pods 01/03/24 12:54:31.663
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:54:31.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:54:31.739
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jan  3 12:54:31.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: creating the pod 01/03/24 12:54:31.766
    STEP: submitting the pod to kubernetes 01/03/24 12:54:31.767
    Jan  3 12:54:31.792: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363" in namespace "pods-5554" to be "running and ready"
    Jan  3 12:54:31.809: INFO: Pod "pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363": Phase="Pending", Reason="", readiness=false. Elapsed: 17.574153ms
    Jan  3 12:54:31.809: INFO: The phase of Pod pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:54:33.833: INFO: Pod "pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041024036s
    Jan  3 12:54:33.833: INFO: The phase of Pod pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 12:54:35.831: INFO: Pod "pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363": Phase="Running", Reason="", readiness=true. Elapsed: 4.038952896s
    Jan  3 12:54:35.831: INFO: The phase of Pod pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363 is Running (Ready = true)
    Jan  3 12:54:35.831: INFO: Pod "pod-exec-websocket-0f0165c7-5559-4c12-bdf8-c955a53d9363" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:54:36.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5554" for this suite. 01/03/24 12:54:36.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:54:36.145
Jan  3 12:54:36.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename podtemplate 01/03/24 12:54:36.148
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:54:36.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:54:36.225
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/03/24 12:54:36.252
STEP: Replace a pod template 01/03/24 12:54:36.273
Jan  3 12:54:36.310: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan  3 12:54:36.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9180" for this suite. 01/03/24 12:54:36.331
------------------------------
• [0.209 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:54:36.145
    Jan  3 12:54:36.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename podtemplate 01/03/24 12:54:36.148
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:54:36.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:54:36.225
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/03/24 12:54:36.252
    STEP: Replace a pod template 01/03/24 12:54:36.273
    Jan  3 12:54:36.310: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:54:36.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9180" for this suite. 01/03/24 12:54:36.331
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:54:36.355
Jan  3 12:54:36.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 12:54:36.357
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:54:36.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:54:36.434
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/03/24 12:54:36.463
Jan  3 12:54:36.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 12:54:39.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:54:50.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5123" for this suite. 01/03/24 12:54:50.245
------------------------------
• [SLOW TEST] [13.920 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:54:36.355
    Jan  3 12:54:36.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 12:54:36.357
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:54:36.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:54:36.434
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/03/24 12:54:36.463
    Jan  3 12:54:36.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 12:54:39.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:54:50.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5123" for this suite. 01/03/24 12:54:50.245
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:54:50.276
Jan  3 12:54:50.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:54:50.279
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:54:50.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:54:50.362
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 01/03/24 12:54:50.389
Jan  3 12:54:50.423: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08" in namespace "projected-8199" to be "Succeeded or Failed"
Jan  3 12:54:50.441: INFO: Pod "downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08": Phase="Pending", Reason="", readiness=false. Elapsed: 18.1785ms
Jan  3 12:54:52.462: INFO: Pod "downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08": Phase="Running", Reason="", readiness=true. Elapsed: 2.038613094s
Jan  3 12:54:54.465: INFO: Pod "downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08": Phase="Running", Reason="", readiness=false. Elapsed: 4.042509709s
Jan  3 12:54:56.474: INFO: Pod "downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051469022s
STEP: Saw pod success 01/03/24 12:54:56.474
Jan  3 12:54:56.475: INFO: Pod "downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08" satisfied condition "Succeeded or Failed"
Jan  3 12:54:56.509: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08 container client-container: <nil>
STEP: delete the pod 01/03/24 12:54:56.663
Jan  3 12:54:56.706: INFO: Waiting for pod downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08 to disappear
Jan  3 12:54:56.723: INFO: Pod downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  3 12:54:56.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8199" for this suite. 01/03/24 12:54:56.753
------------------------------
• [SLOW TEST] [6.514 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:54:50.276
    Jan  3 12:54:50.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:54:50.279
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:54:50.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:54:50.362
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 01/03/24 12:54:50.389
    Jan  3 12:54:50.423: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08" in namespace "projected-8199" to be "Succeeded or Failed"
    Jan  3 12:54:50.441: INFO: Pod "downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08": Phase="Pending", Reason="", readiness=false. Elapsed: 18.1785ms
    Jan  3 12:54:52.462: INFO: Pod "downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08": Phase="Running", Reason="", readiness=true. Elapsed: 2.038613094s
    Jan  3 12:54:54.465: INFO: Pod "downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08": Phase="Running", Reason="", readiness=false. Elapsed: 4.042509709s
    Jan  3 12:54:56.474: INFO: Pod "downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051469022s
    STEP: Saw pod success 01/03/24 12:54:56.474
    Jan  3 12:54:56.475: INFO: Pod "downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08" satisfied condition "Succeeded or Failed"
    Jan  3 12:54:56.509: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08 container client-container: <nil>
    STEP: delete the pod 01/03/24 12:54:56.663
    Jan  3 12:54:56.706: INFO: Waiting for pod downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08 to disappear
    Jan  3 12:54:56.723: INFO: Pod downwardapi-volume-aaa757db-a661-4eb3-a1c6-06c6e6f81d08 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:54:56.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8199" for this suite. 01/03/24 12:54:56.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:54:56.793
Jan  3 12:54:56.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pods 01/03/24 12:54:56.795
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:54:56.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:54:56.873
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 01/03/24 12:54:56.901
Jan  3 12:54:56.930: INFO: Waiting up to 5m0s for pod "pod-29hsm" in namespace "pods-841" to be "running"
Jan  3 12:54:56.949: INFO: Pod "pod-29hsm": Phase="Pending", Reason="", readiness=false. Elapsed: 18.926598ms
Jan  3 12:54:58.970: INFO: Pod "pod-29hsm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039976501s
Jan  3 12:55:00.970: INFO: Pod "pod-29hsm": Phase="Running", Reason="", readiness=true. Elapsed: 4.039755771s
Jan  3 12:55:00.970: INFO: Pod "pod-29hsm" satisfied condition "running"
STEP: patching /status 01/03/24 12:55:00.97
Jan  3 12:55:00.992: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  3 12:55:00.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-841" for this suite. 01/03/24 12:55:01.023
------------------------------
• [4.256 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:54:56.793
    Jan  3 12:54:56.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pods 01/03/24 12:54:56.795
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:54:56.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:54:56.873
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 01/03/24 12:54:56.901
    Jan  3 12:54:56.930: INFO: Waiting up to 5m0s for pod "pod-29hsm" in namespace "pods-841" to be "running"
    Jan  3 12:54:56.949: INFO: Pod "pod-29hsm": Phase="Pending", Reason="", readiness=false. Elapsed: 18.926598ms
    Jan  3 12:54:58.970: INFO: Pod "pod-29hsm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039976501s
    Jan  3 12:55:00.970: INFO: Pod "pod-29hsm": Phase="Running", Reason="", readiness=true. Elapsed: 4.039755771s
    Jan  3 12:55:00.970: INFO: Pod "pod-29hsm" satisfied condition "running"
    STEP: patching /status 01/03/24 12:55:00.97
    Jan  3 12:55:00.992: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:55:00.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-841" for this suite. 01/03/24 12:55:01.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:55:01.057
Jan  3 12:55:01.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename var-expansion 01/03/24 12:55:01.059
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:55:01.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:55:01.138
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 01/03/24 12:55:01.164
Jan  3 12:55:01.200: INFO: Waiting up to 5m0s for pod "var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e" in namespace "var-expansion-6428" to be "Succeeded or Failed"
Jan  3 12:55:01.222: INFO: Pod "var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.982454ms
Jan  3 12:55:03.241: INFO: Pod "var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.040355228s
Jan  3 12:55:05.242: INFO: Pod "var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e": Phase="Running", Reason="", readiness=false. Elapsed: 4.041439784s
Jan  3 12:55:07.248: INFO: Pod "var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047360326s
STEP: Saw pod success 01/03/24 12:55:07.248
Jan  3 12:55:07.248: INFO: Pod "var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e" satisfied condition "Succeeded or Failed"
Jan  3 12:55:07.288: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e container dapi-container: <nil>
STEP: delete the pod 01/03/24 12:55:07.339
Jan  3 12:55:07.377: INFO: Waiting for pod var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e to disappear
Jan  3 12:55:07.395: INFO: Pod var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  3 12:55:07.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6428" for this suite. 01/03/24 12:55:07.425
------------------------------
• [SLOW TEST] [6.394 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:55:01.057
    Jan  3 12:55:01.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename var-expansion 01/03/24 12:55:01.059
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:55:01.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:55:01.138
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 01/03/24 12:55:01.164
    Jan  3 12:55:01.200: INFO: Waiting up to 5m0s for pod "var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e" in namespace "var-expansion-6428" to be "Succeeded or Failed"
    Jan  3 12:55:01.222: INFO: Pod "var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.982454ms
    Jan  3 12:55:03.241: INFO: Pod "var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.040355228s
    Jan  3 12:55:05.242: INFO: Pod "var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e": Phase="Running", Reason="", readiness=false. Elapsed: 4.041439784s
    Jan  3 12:55:07.248: INFO: Pod "var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047360326s
    STEP: Saw pod success 01/03/24 12:55:07.248
    Jan  3 12:55:07.248: INFO: Pod "var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e" satisfied condition "Succeeded or Failed"
    Jan  3 12:55:07.288: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e container dapi-container: <nil>
    STEP: delete the pod 01/03/24 12:55:07.339
    Jan  3 12:55:07.377: INFO: Waiting for pod var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e to disappear
    Jan  3 12:55:07.395: INFO: Pod var-expansion-85aea7e6-0f78-4aaa-b4ed-1604cb6fbd4e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:55:07.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6428" for this suite. 01/03/24 12:55:07.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:55:07.456
Jan  3 12:55:07.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 12:55:07.458
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:55:07.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:55:07.546
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 01/03/24 12:55:07.573
Jan  3 12:55:07.604: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3" in namespace "projected-3439" to be "Succeeded or Failed"
Jan  3 12:55:07.626: INFO: Pod "downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3": Phase="Pending", Reason="", readiness=false. Elapsed: 21.833383ms
Jan  3 12:55:09.646: INFO: Pod "downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.0415611s
Jan  3 12:55:11.645: INFO: Pod "downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3": Phase="Running", Reason="", readiness=false. Elapsed: 4.041222708s
Jan  3 12:55:13.646: INFO: Pod "downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042201745s
STEP: Saw pod success 01/03/24 12:55:13.646
Jan  3 12:55:13.647: INFO: Pod "downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3" satisfied condition "Succeeded or Failed"
Jan  3 12:55:13.665: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3 container client-container: <nil>
STEP: delete the pod 01/03/24 12:55:13.704
Jan  3 12:55:13.737: INFO: Waiting for pod downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3 to disappear
Jan  3 12:55:13.755: INFO: Pod downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  3 12:55:13.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3439" for this suite. 01/03/24 12:55:13.784
------------------------------
• [SLOW TEST] [6.351 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:55:07.456
    Jan  3 12:55:07.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 12:55:07.458
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:55:07.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:55:07.546
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 01/03/24 12:55:07.573
    Jan  3 12:55:07.604: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3" in namespace "projected-3439" to be "Succeeded or Failed"
    Jan  3 12:55:07.626: INFO: Pod "downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3": Phase="Pending", Reason="", readiness=false. Elapsed: 21.833383ms
    Jan  3 12:55:09.646: INFO: Pod "downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.0415611s
    Jan  3 12:55:11.645: INFO: Pod "downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3": Phase="Running", Reason="", readiness=false. Elapsed: 4.041222708s
    Jan  3 12:55:13.646: INFO: Pod "downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042201745s
    STEP: Saw pod success 01/03/24 12:55:13.646
    Jan  3 12:55:13.647: INFO: Pod "downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3" satisfied condition "Succeeded or Failed"
    Jan  3 12:55:13.665: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3 container client-container: <nil>
    STEP: delete the pod 01/03/24 12:55:13.704
    Jan  3 12:55:13.737: INFO: Waiting for pod downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3 to disappear
    Jan  3 12:55:13.755: INFO: Pod downwardapi-volume-ece0c8e9-bf29-409a-ab8f-298c1208d1e3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:55:13.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3439" for this suite. 01/03/24 12:55:13.784
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:55:13.809
Jan  3 12:55:13.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 12:55:13.812
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:55:13.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:55:13.893
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 01/03/24 12:55:13.927
Jan  3 12:55:13.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: rename a version 01/03/24 12:55:20.411
STEP: check the new version name is served 01/03/24 12:55:20.488
STEP: check the old version name is removed 01/03/24 12:55:22.453
STEP: check the other version is not changed 01/03/24 12:55:23.76
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:55:29.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9408" for this suite. 01/03/24 12:55:29.105
------------------------------
• [SLOW TEST] [15.320 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:55:13.809
    Jan  3 12:55:13.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 12:55:13.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:55:13.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:55:13.893
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 01/03/24 12:55:13.927
    Jan  3 12:55:13.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: rename a version 01/03/24 12:55:20.411
    STEP: check the new version name is served 01/03/24 12:55:20.488
    STEP: check the old version name is removed 01/03/24 12:55:22.453
    STEP: check the other version is not changed 01/03/24 12:55:23.76
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:55:29.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9408" for this suite. 01/03/24 12:55:29.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:55:29.13
Jan  3 12:55:29.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 12:55:29.133
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:55:29.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:55:29.219
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 01/03/24 12:55:29.245
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 12:55:29.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3098" for this suite. 01/03/24 12:55:29.288
------------------------------
• [0.182 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:55:29.13
    Jan  3 12:55:29.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 12:55:29.133
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:55:29.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:55:29.219
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 01/03/24 12:55:29.245
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:55:29.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3098" for this suite. 01/03/24 12:55:29.288
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:55:29.313
Jan  3 12:55:29.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-runtime 01/03/24 12:55:29.316
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:55:29.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:55:29.407
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 01/03/24 12:55:29.435
STEP: wait for the container to reach Succeeded 01/03/24 12:55:29.462
STEP: get the container status 01/03/24 12:55:33.572
STEP: the container should be terminated 01/03/24 12:55:33.591
STEP: the termination message should be set 01/03/24 12:55:33.591
Jan  3 12:55:33.591: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/03/24 12:55:33.591
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  3 12:55:33.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9674" for this suite. 01/03/24 12:55:33.685
------------------------------
• [4.395 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:55:29.313
    Jan  3 12:55:29.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-runtime 01/03/24 12:55:29.316
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:55:29.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:55:29.407
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 01/03/24 12:55:29.435
    STEP: wait for the container to reach Succeeded 01/03/24 12:55:29.462
    STEP: get the container status 01/03/24 12:55:33.572
    STEP: the container should be terminated 01/03/24 12:55:33.591
    STEP: the termination message should be set 01/03/24 12:55:33.591
    Jan  3 12:55:33.591: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/03/24 12:55:33.591
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:55:33.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9674" for this suite. 01/03/24 12:55:33.685
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:55:33.711
Jan  3 12:55:33.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename disruption 01/03/24 12:55:33.714
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:55:33.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:55:33.835
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 01/03/24 12:55:33.885
STEP: Waiting for all pods to be running 01/03/24 12:55:33.993
Jan  3 12:55:34.012: INFO: running pods: 0 < 3
Jan  3 12:55:36.034: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  3 12:55:38.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7869" for this suite. 01/03/24 12:55:38.087
------------------------------
• [4.401 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:55:33.711
    Jan  3 12:55:33.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename disruption 01/03/24 12:55:33.714
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:55:33.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:55:33.835
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 01/03/24 12:55:33.885
    STEP: Waiting for all pods to be running 01/03/24 12:55:33.993
    Jan  3 12:55:34.012: INFO: running pods: 0 < 3
    Jan  3 12:55:36.034: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:55:38.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7869" for this suite. 01/03/24 12:55:38.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:55:38.116
Jan  3 12:55:38.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename sched-preemption 01/03/24 12:55:38.118
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:55:38.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:55:38.197
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan  3 12:55:38.282: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  3 12:56:38.511: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 01/03/24 12:56:38.535
Jan  3 12:56:38.609: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan  3 12:56:38.633: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan  3 12:56:38.697: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan  3 12:56:38.719: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan  3 12:56:38.776: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan  3 12:56:38.798: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/03/24 12:56:38.798
Jan  3 12:56:38.798: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9439" to be "running"
Jan  3 12:56:38.821: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 23.049374ms
Jan  3 12:56:40.842: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.044000018s
Jan  3 12:56:40.842: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan  3 12:56:40.842: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9439" to be "running"
Jan  3 12:56:40.861: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.946798ms
Jan  3 12:56:40.861: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 12:56:40.861: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9439" to be "running"
Jan  3 12:56:40.899: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 37.818689ms
Jan  3 12:56:42.941: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.079783826s
Jan  3 12:56:42.941: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 12:56:42.941: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9439" to be "running"
Jan  3 12:56:42.962: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.951141ms
Jan  3 12:56:42.962: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 12:56:42.962: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9439" to be "running"
Jan  3 12:56:42.980: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.879942ms
Jan  3 12:56:42.980: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 12:56:42.980: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9439" to be "running"
Jan  3 12:56:42.998: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.174115ms
Jan  3 12:56:42.998: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/03/24 12:56:42.999
Jan  3 12:56:43.021: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9439" to be "running"
Jan  3 12:56:43.044: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 23.215319ms
Jan  3 12:56:45.065: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044146572s
Jan  3 12:56:47.063: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041535138s
Jan  3 12:56:49.083: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.061768309s
Jan  3 12:56:49.083: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 12:56:49.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9439" for this suite. 01/03/24 12:56:49.409
------------------------------
• [SLOW TEST] [71.320 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:55:38.116
    Jan  3 12:55:38.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename sched-preemption 01/03/24 12:55:38.118
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:55:38.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:55:38.197
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan  3 12:55:38.282: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  3 12:56:38.511: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 01/03/24 12:56:38.535
    Jan  3 12:56:38.609: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan  3 12:56:38.633: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan  3 12:56:38.697: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan  3 12:56:38.719: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan  3 12:56:38.776: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan  3 12:56:38.798: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/03/24 12:56:38.798
    Jan  3 12:56:38.798: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9439" to be "running"
    Jan  3 12:56:38.821: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 23.049374ms
    Jan  3 12:56:40.842: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.044000018s
    Jan  3 12:56:40.842: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan  3 12:56:40.842: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9439" to be "running"
    Jan  3 12:56:40.861: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.946798ms
    Jan  3 12:56:40.861: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 12:56:40.861: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9439" to be "running"
    Jan  3 12:56:40.899: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 37.818689ms
    Jan  3 12:56:42.941: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.079783826s
    Jan  3 12:56:42.941: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 12:56:42.941: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9439" to be "running"
    Jan  3 12:56:42.962: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.951141ms
    Jan  3 12:56:42.962: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 12:56:42.962: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9439" to be "running"
    Jan  3 12:56:42.980: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.879942ms
    Jan  3 12:56:42.980: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 12:56:42.980: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9439" to be "running"
    Jan  3 12:56:42.998: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.174115ms
    Jan  3 12:56:42.998: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/03/24 12:56:42.999
    Jan  3 12:56:43.021: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9439" to be "running"
    Jan  3 12:56:43.044: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 23.215319ms
    Jan  3 12:56:45.065: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044146572s
    Jan  3 12:56:47.063: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041535138s
    Jan  3 12:56:49.083: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.061768309s
    Jan  3 12:56:49.083: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 12:56:49.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9439" for this suite. 01/03/24 12:56:49.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 12:56:49.437
Jan  3 12:56:49.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-probe 01/03/24 12:56:49.44
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:56:49.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:56:49.538
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-189ee1c3-3a28-435f-b05d-164a21230e02 in namespace container-probe-3741 01/03/24 12:56:49.565
Jan  3 12:56:49.594: INFO: Waiting up to 5m0s for pod "liveness-189ee1c3-3a28-435f-b05d-164a21230e02" in namespace "container-probe-3741" to be "not pending"
Jan  3 12:56:49.618: INFO: Pod "liveness-189ee1c3-3a28-435f-b05d-164a21230e02": Phase="Pending", Reason="", readiness=false. Elapsed: 23.3173ms
Jan  3 12:56:51.641: INFO: Pod "liveness-189ee1c3-3a28-435f-b05d-164a21230e02": Phase="Running", Reason="", readiness=true. Elapsed: 2.046353096s
Jan  3 12:56:51.641: INFO: Pod "liveness-189ee1c3-3a28-435f-b05d-164a21230e02" satisfied condition "not pending"
Jan  3 12:56:51.641: INFO: Started pod liveness-189ee1c3-3a28-435f-b05d-164a21230e02 in namespace container-probe-3741
STEP: checking the pod's current state and verifying that restartCount is present 01/03/24 12:56:51.641
Jan  3 12:56:51.661: INFO: Initial restart count of pod liveness-189ee1c3-3a28-435f-b05d-164a21230e02 is 0
STEP: deleting the pod 01/03/24 13:00:52.41
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  3 13:00:52.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3741" for this suite. 01/03/24 13:00:52.483
------------------------------
• [SLOW TEST] [243.083 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 12:56:49.437
    Jan  3 12:56:49.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-probe 01/03/24 12:56:49.44
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 12:56:49.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 12:56:49.538
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-189ee1c3-3a28-435f-b05d-164a21230e02 in namespace container-probe-3741 01/03/24 12:56:49.565
    Jan  3 12:56:49.594: INFO: Waiting up to 5m0s for pod "liveness-189ee1c3-3a28-435f-b05d-164a21230e02" in namespace "container-probe-3741" to be "not pending"
    Jan  3 12:56:49.618: INFO: Pod "liveness-189ee1c3-3a28-435f-b05d-164a21230e02": Phase="Pending", Reason="", readiness=false. Elapsed: 23.3173ms
    Jan  3 12:56:51.641: INFO: Pod "liveness-189ee1c3-3a28-435f-b05d-164a21230e02": Phase="Running", Reason="", readiness=true. Elapsed: 2.046353096s
    Jan  3 12:56:51.641: INFO: Pod "liveness-189ee1c3-3a28-435f-b05d-164a21230e02" satisfied condition "not pending"
    Jan  3 12:56:51.641: INFO: Started pod liveness-189ee1c3-3a28-435f-b05d-164a21230e02 in namespace container-probe-3741
    STEP: checking the pod's current state and verifying that restartCount is present 01/03/24 12:56:51.641
    Jan  3 12:56:51.661: INFO: Initial restart count of pod liveness-189ee1c3-3a28-435f-b05d-164a21230e02 is 0
    STEP: deleting the pod 01/03/24 13:00:52.41
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:00:52.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3741" for this suite. 01/03/24 13:00:52.483
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:00:52.521
Jan  3 13:00:52.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 13:00:52.524
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:00:52.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:00:52.626
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 01/03/24 13:00:52.659
Jan  3 13:00:52.659: INFO: namespace kubectl-5183
Jan  3 13:00:52.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-5183 create -f -'
Jan  3 13:00:53.781: INFO: stderr: ""
Jan  3 13:00:53.781: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/03/24 13:00:53.781
Jan  3 13:00:54.804: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 13:00:54.804: INFO: Found 0 / 1
Jan  3 13:00:55.802: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 13:00:55.802: INFO: Found 1 / 1
Jan  3 13:00:55.803: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan  3 13:00:55.824: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 13:00:55.825: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  3 13:00:55.825: INFO: wait on agnhost-primary startup in kubectl-5183 
Jan  3 13:00:55.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-5183 logs agnhost-primary-zdrt5 agnhost-primary'
Jan  3 13:00:56.136: INFO: stderr: ""
Jan  3 13:00:56.136: INFO: stdout: "Paused\n"
STEP: exposing RC 01/03/24 13:00:56.136
Jan  3 13:00:56.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-5183 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan  3 13:00:56.309: INFO: stderr: ""
Jan  3 13:00:56.309: INFO: stdout: "service/rm2 exposed\n"
Jan  3 13:00:56.326: INFO: Service rm2 in namespace kubectl-5183 found.
STEP: exposing service 01/03/24 13:00:58.36
Jan  3 13:00:58.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-5183 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan  3 13:00:58.550: INFO: stderr: ""
Jan  3 13:00:58.550: INFO: stdout: "service/rm3 exposed\n"
Jan  3 13:00:58.566: INFO: Service rm3 in namespace kubectl-5183 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 13:01:00.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5183" for this suite. 01/03/24 13:01:00.636
------------------------------
• [SLOW TEST] [8.152 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:00:52.521
    Jan  3 13:00:52.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 13:00:52.524
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:00:52.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:00:52.626
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 01/03/24 13:00:52.659
    Jan  3 13:00:52.659: INFO: namespace kubectl-5183
    Jan  3 13:00:52.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-5183 create -f -'
    Jan  3 13:00:53.781: INFO: stderr: ""
    Jan  3 13:00:53.781: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/03/24 13:00:53.781
    Jan  3 13:00:54.804: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 13:00:54.804: INFO: Found 0 / 1
    Jan  3 13:00:55.802: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 13:00:55.802: INFO: Found 1 / 1
    Jan  3 13:00:55.803: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan  3 13:00:55.824: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 13:00:55.825: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  3 13:00:55.825: INFO: wait on agnhost-primary startup in kubectl-5183 
    Jan  3 13:00:55.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-5183 logs agnhost-primary-zdrt5 agnhost-primary'
    Jan  3 13:00:56.136: INFO: stderr: ""
    Jan  3 13:00:56.136: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/03/24 13:00:56.136
    Jan  3 13:00:56.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-5183 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan  3 13:00:56.309: INFO: stderr: ""
    Jan  3 13:00:56.309: INFO: stdout: "service/rm2 exposed\n"
    Jan  3 13:00:56.326: INFO: Service rm2 in namespace kubectl-5183 found.
    STEP: exposing service 01/03/24 13:00:58.36
    Jan  3 13:00:58.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-5183 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan  3 13:00:58.550: INFO: stderr: ""
    Jan  3 13:00:58.550: INFO: stdout: "service/rm3 exposed\n"
    Jan  3 13:00:58.566: INFO: Service rm3 in namespace kubectl-5183 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:01:00.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5183" for this suite. 01/03/24 13:01:00.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:01:00.675
Jan  3 13:01:00.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename statefulset 01/03/24 13:01:00.677
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:01:00.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:01:00.758
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2846 01/03/24 13:01:00.786
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-2846 01/03/24 13:01:00.807
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2846 01/03/24 13:01:00.827
Jan  3 13:01:00.845: INFO: Found 0 stateful pods, waiting for 1
Jan  3 13:01:10.878: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/03/24 13:01:10.878
Jan  3 13:01:10.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-2846 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 13:01:11.360: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 13:01:11.360: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 13:01:11.360: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 13:01:11.382: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan  3 13:01:21.406: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 13:01:21.406: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 13:01:21.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999439s
Jan  3 13:01:22.504: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.978506088s
Jan  3 13:01:23.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.958976125s
Jan  3 13:01:24.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.938630546s
Jan  3 13:01:25.565: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.917757026s
Jan  3 13:01:26.585: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.897809851s
Jan  3 13:01:27.604: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.877961245s
Jan  3 13:01:28.626: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.857605936s
Jan  3 13:01:29.647: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.835505707s
Jan  3 13:01:30.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 815.430202ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2846 01/03/24 13:01:31.678
Jan  3 13:01:31.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-2846 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 13:01:32.154: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 13:01:32.154: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 13:01:32.154: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 13:01:32.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-2846 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 13:01:32.677: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan  3 13:01:32.677: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 13:01:32.677: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 13:01:32.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-2846 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 13:01:33.181: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan  3 13:01:33.181: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 13:01:33.181: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 13:01:33.200: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 13:01:33.200: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 13:01:33.200: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/03/24 13:01:33.2
Jan  3 13:01:33.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-2846 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 13:01:33.695: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 13:01:33.695: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 13:01:33.695: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 13:01:33.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-2846 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 13:01:34.142: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 13:01:34.142: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 13:01:34.142: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 13:01:34.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-2846 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 13:01:34.626: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 13:01:34.626: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 13:01:34.626: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 13:01:34.626: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 13:01:34.644: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan  3 13:01:44.687: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 13:01:44.687: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 13:01:44.687: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 13:01:44.745: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Jan  3 13:01:44.745: INFO: ss-0  jb-1-26-np-64kerjapxk  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:00 +0000 UTC  }]
Jan  3 13:01:44.745: INFO: ss-1  jb-1-26-np-nqeu5xtrab  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  }]
Jan  3 13:01:44.745: INFO: ss-2  jb-1-26-np-adtwo5cmi2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  }]
Jan  3 13:01:44.745: INFO: 
Jan  3 13:01:44.745: INFO: StatefulSet ss has not reached scale 0, at 3
Jan  3 13:01:45.767: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
Jan  3 13:01:45.767: INFO: ss-0  jb-1-26-np-64kerjapxk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:00 +0000 UTC  }]
Jan  3 13:01:45.767: INFO: ss-1  jb-1-26-np-nqeu5xtrab  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  }]
Jan  3 13:01:45.767: INFO: ss-2  jb-1-26-np-adtwo5cmi2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  }]
Jan  3 13:01:45.767: INFO: 
Jan  3 13:01:45.767: INFO: StatefulSet ss has not reached scale 0, at 3
Jan  3 13:01:46.786: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.959235698s
Jan  3 13:01:47.808: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.939172072s
Jan  3 13:01:48.848: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.918694679s
Jan  3 13:01:49.869: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.878260995s
Jan  3 13:01:50.888: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.857614352s
Jan  3 13:01:51.908: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.838026267s
Jan  3 13:01:52.927: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.818195437s
Jan  3 13:01:53.946: INFO: Verifying statefulset ss doesn't scale past 0 for another 799.443803ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2846 01/03/24 13:01:54.946
Jan  3 13:01:54.969: INFO: Scaling statefulset ss to 0
Jan  3 13:01:55.030: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  3 13:01:55.049: INFO: Deleting all statefulset in ns statefulset-2846
Jan  3 13:01:55.074: INFO: Scaling statefulset ss to 0
Jan  3 13:01:55.135: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 13:01:55.154: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  3 13:01:55.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2846" for this suite. 01/03/24 13:01:55.244
------------------------------
• [SLOW TEST] [54.591 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:01:00.675
    Jan  3 13:01:00.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename statefulset 01/03/24 13:01:00.677
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:01:00.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:01:00.758
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2846 01/03/24 13:01:00.786
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-2846 01/03/24 13:01:00.807
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2846 01/03/24 13:01:00.827
    Jan  3 13:01:00.845: INFO: Found 0 stateful pods, waiting for 1
    Jan  3 13:01:10.878: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/03/24 13:01:10.878
    Jan  3 13:01:10.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-2846 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 13:01:11.360: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 13:01:11.360: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 13:01:11.360: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 13:01:11.382: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan  3 13:01:21.406: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 13:01:21.406: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 13:01:21.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999439s
    Jan  3 13:01:22.504: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.978506088s
    Jan  3 13:01:23.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.958976125s
    Jan  3 13:01:24.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.938630546s
    Jan  3 13:01:25.565: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.917757026s
    Jan  3 13:01:26.585: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.897809851s
    Jan  3 13:01:27.604: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.877961245s
    Jan  3 13:01:28.626: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.857605936s
    Jan  3 13:01:29.647: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.835505707s
    Jan  3 13:01:30.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 815.430202ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2846 01/03/24 13:01:31.678
    Jan  3 13:01:31.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-2846 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 13:01:32.154: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  3 13:01:32.154: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 13:01:32.154: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 13:01:32.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-2846 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 13:01:32.677: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan  3 13:01:32.677: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 13:01:32.677: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 13:01:32.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-2846 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 13:01:33.181: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan  3 13:01:33.181: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 13:01:33.181: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 13:01:33.200: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 13:01:33.200: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 13:01:33.200: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/03/24 13:01:33.2
    Jan  3 13:01:33.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-2846 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 13:01:33.695: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 13:01:33.695: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 13:01:33.695: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 13:01:33.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-2846 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 13:01:34.142: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 13:01:34.142: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 13:01:34.142: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 13:01:34.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=statefulset-2846 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 13:01:34.626: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 13:01:34.626: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 13:01:34.626: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 13:01:34.626: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 13:01:34.644: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan  3 13:01:44.687: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 13:01:44.687: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 13:01:44.687: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 13:01:44.745: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
    Jan  3 13:01:44.745: INFO: ss-0  jb-1-26-np-64kerjapxk  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:00 +0000 UTC  }]
    Jan  3 13:01:44.745: INFO: ss-1  jb-1-26-np-nqeu5xtrab  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  }]
    Jan  3 13:01:44.745: INFO: ss-2  jb-1-26-np-adtwo5cmi2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  }]
    Jan  3 13:01:44.745: INFO: 
    Jan  3 13:01:44.745: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan  3 13:01:45.767: INFO: POD   NODE                   PHASE    GRACE  CONDITIONS
    Jan  3 13:01:45.767: INFO: ss-0  jb-1-26-np-64kerjapxk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:00 +0000 UTC  }]
    Jan  3 13:01:45.767: INFO: ss-1  jb-1-26-np-nqeu5xtrab  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  }]
    Jan  3 13:01:45.767: INFO: ss-2  jb-1-26-np-adtwo5cmi2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:01:21 +0000 UTC  }]
    Jan  3 13:01:45.767: INFO: 
    Jan  3 13:01:45.767: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan  3 13:01:46.786: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.959235698s
    Jan  3 13:01:47.808: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.939172072s
    Jan  3 13:01:48.848: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.918694679s
    Jan  3 13:01:49.869: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.878260995s
    Jan  3 13:01:50.888: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.857614352s
    Jan  3 13:01:51.908: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.838026267s
    Jan  3 13:01:52.927: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.818195437s
    Jan  3 13:01:53.946: INFO: Verifying statefulset ss doesn't scale past 0 for another 799.443803ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2846 01/03/24 13:01:54.946
    Jan  3 13:01:54.969: INFO: Scaling statefulset ss to 0
    Jan  3 13:01:55.030: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  3 13:01:55.049: INFO: Deleting all statefulset in ns statefulset-2846
    Jan  3 13:01:55.074: INFO: Scaling statefulset ss to 0
    Jan  3 13:01:55.135: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 13:01:55.154: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:01:55.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2846" for this suite. 01/03/24 13:01:55.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:01:55.279
Jan  3 13:01:55.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 13:01:55.28
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:01:55.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:01:55.364
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-43a3bd61-929a-4dd8-8699-bfa6f625039c 01/03/24 13:01:55.393
STEP: Creating a pod to test consume configMaps 01/03/24 13:01:55.414
Jan  3 13:01:55.442: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00" in namespace "projected-5179" to be "Succeeded or Failed"
Jan  3 13:01:55.459: INFO: Pod "pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00": Phase="Pending", Reason="", readiness=false. Elapsed: 17.891303ms
Jan  3 13:01:57.478: INFO: Pod "pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00": Phase="Running", Reason="", readiness=true. Elapsed: 2.036401241s
Jan  3 13:01:59.480: INFO: Pod "pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00": Phase="Running", Reason="", readiness=false. Elapsed: 4.038593302s
Jan  3 13:02:01.479: INFO: Pod "pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037770089s
STEP: Saw pod success 01/03/24 13:02:01.479
Jan  3 13:02:01.480: INFO: Pod "pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00" satisfied condition "Succeeded or Failed"
Jan  3 13:02:01.498: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00 container agnhost-container: <nil>
STEP: delete the pod 01/03/24 13:02:01.65
Jan  3 13:02:01.694: INFO: Waiting for pod pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00 to disappear
Jan  3 13:02:01.712: INFO: Pod pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  3 13:02:01.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5179" for this suite. 01/03/24 13:02:01.744
------------------------------
• [SLOW TEST] [6.496 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:01:55.279
    Jan  3 13:01:55.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 13:01:55.28
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:01:55.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:01:55.364
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-43a3bd61-929a-4dd8-8699-bfa6f625039c 01/03/24 13:01:55.393
    STEP: Creating a pod to test consume configMaps 01/03/24 13:01:55.414
    Jan  3 13:01:55.442: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00" in namespace "projected-5179" to be "Succeeded or Failed"
    Jan  3 13:01:55.459: INFO: Pod "pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00": Phase="Pending", Reason="", readiness=false. Elapsed: 17.891303ms
    Jan  3 13:01:57.478: INFO: Pod "pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00": Phase="Running", Reason="", readiness=true. Elapsed: 2.036401241s
    Jan  3 13:01:59.480: INFO: Pod "pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00": Phase="Running", Reason="", readiness=false. Elapsed: 4.038593302s
    Jan  3 13:02:01.479: INFO: Pod "pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037770089s
    STEP: Saw pod success 01/03/24 13:02:01.479
    Jan  3 13:02:01.480: INFO: Pod "pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00" satisfied condition "Succeeded or Failed"
    Jan  3 13:02:01.498: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00 container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 13:02:01.65
    Jan  3 13:02:01.694: INFO: Waiting for pod pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00 to disappear
    Jan  3 13:02:01.712: INFO: Pod pod-projected-configmaps-bd6f0a1f-a0cc-462f-928b-5984af9b1c00 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:02:01.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5179" for this suite. 01/03/24 13:02:01.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:02:01.78
Jan  3 13:02:01.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 13:02:01.782
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:01.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:01.862
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 01/03/24 13:02:01.89
STEP: fetching the ConfigMap 01/03/24 13:02:01.908
STEP: patching the ConfigMap 01/03/24 13:02:01.926
STEP: listing all ConfigMaps in all namespaces with a label selector 01/03/24 13:02:01.946
STEP: deleting the ConfigMap by collection with a label selector 01/03/24 13:02:01.966
STEP: listing all ConfigMaps in test namespace 01/03/24 13:02:01.998
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 13:02:02.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8816" for this suite. 01/03/24 13:02:02.034
------------------------------
• [0.279 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:02:01.78
    Jan  3 13:02:01.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 13:02:01.782
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:01.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:01.862
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 01/03/24 13:02:01.89
    STEP: fetching the ConfigMap 01/03/24 13:02:01.908
    STEP: patching the ConfigMap 01/03/24 13:02:01.926
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/03/24 13:02:01.946
    STEP: deleting the ConfigMap by collection with a label selector 01/03/24 13:02:01.966
    STEP: listing all ConfigMaps in test namespace 01/03/24 13:02:01.998
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:02:02.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8816" for this suite. 01/03/24 13:02:02.034
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:02:02.063
Jan  3 13:02:02.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename events 01/03/24 13:02:02.065
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:02.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:02.142
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/03/24 13:02:02.171
STEP: listing events in all namespaces 01/03/24 13:02:02.193
STEP: listing events in test namespace 01/03/24 13:02:02.214
STEP: listing events with field selection filtering on source 01/03/24 13:02:02.238
STEP: listing events with field selection filtering on reportingController 01/03/24 13:02:02.255
STEP: getting the test event 01/03/24 13:02:02.273
STEP: patching the test event 01/03/24 13:02:02.29
STEP: getting the test event 01/03/24 13:02:02.34
STEP: updating the test event 01/03/24 13:02:02.359
STEP: getting the test event 01/03/24 13:02:02.382
STEP: deleting the test event 01/03/24 13:02:02.4
STEP: listing events in all namespaces 01/03/24 13:02:02.427
STEP: listing events in test namespace 01/03/24 13:02:02.446
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan  3 13:02:02.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8944" for this suite. 01/03/24 13:02:02.486
------------------------------
• [0.447 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:02:02.063
    Jan  3 13:02:02.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename events 01/03/24 13:02:02.065
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:02.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:02.142
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/03/24 13:02:02.171
    STEP: listing events in all namespaces 01/03/24 13:02:02.193
    STEP: listing events in test namespace 01/03/24 13:02:02.214
    STEP: listing events with field selection filtering on source 01/03/24 13:02:02.238
    STEP: listing events with field selection filtering on reportingController 01/03/24 13:02:02.255
    STEP: getting the test event 01/03/24 13:02:02.273
    STEP: patching the test event 01/03/24 13:02:02.29
    STEP: getting the test event 01/03/24 13:02:02.34
    STEP: updating the test event 01/03/24 13:02:02.359
    STEP: getting the test event 01/03/24 13:02:02.382
    STEP: deleting the test event 01/03/24 13:02:02.4
    STEP: listing events in all namespaces 01/03/24 13:02:02.427
    STEP: listing events in test namespace 01/03/24 13:02:02.446
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:02:02.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8944" for this suite. 01/03/24 13:02:02.486
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:02:02.51
Jan  3 13:02:02.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 13:02:02.513
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:02.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:02.607
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 01/03/24 13:02:02.636
Jan  3 13:02:02.683: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52" in namespace "projected-321" to be "Succeeded or Failed"
Jan  3 13:02:02.701: INFO: Pod "downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52": Phase="Pending", Reason="", readiness=false. Elapsed: 17.446881ms
Jan  3 13:02:04.719: INFO: Pod "downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52": Phase="Running", Reason="", readiness=true. Elapsed: 2.036369342s
Jan  3 13:02:06.724: INFO: Pod "downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52": Phase="Running", Reason="", readiness=false. Elapsed: 4.040688377s
Jan  3 13:02:08.723: INFO: Pod "downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040027924s
STEP: Saw pod success 01/03/24 13:02:08.723
Jan  3 13:02:08.723: INFO: Pod "downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52" satisfied condition "Succeeded or Failed"
Jan  3 13:02:08.743: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52 container client-container: <nil>
STEP: delete the pod 01/03/24 13:02:08.781
Jan  3 13:02:08.830: INFO: Waiting for pod downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52 to disappear
Jan  3 13:02:08.850: INFO: Pod downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  3 13:02:08.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-321" for this suite. 01/03/24 13:02:08.882
------------------------------
• [SLOW TEST] [6.395 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:02:02.51
    Jan  3 13:02:02.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 13:02:02.513
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:02.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:02.607
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 01/03/24 13:02:02.636
    Jan  3 13:02:02.683: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52" in namespace "projected-321" to be "Succeeded or Failed"
    Jan  3 13:02:02.701: INFO: Pod "downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52": Phase="Pending", Reason="", readiness=false. Elapsed: 17.446881ms
    Jan  3 13:02:04.719: INFO: Pod "downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52": Phase="Running", Reason="", readiness=true. Elapsed: 2.036369342s
    Jan  3 13:02:06.724: INFO: Pod "downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52": Phase="Running", Reason="", readiness=false. Elapsed: 4.040688377s
    Jan  3 13:02:08.723: INFO: Pod "downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040027924s
    STEP: Saw pod success 01/03/24 13:02:08.723
    Jan  3 13:02:08.723: INFO: Pod "downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52" satisfied condition "Succeeded or Failed"
    Jan  3 13:02:08.743: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52 container client-container: <nil>
    STEP: delete the pod 01/03/24 13:02:08.781
    Jan  3 13:02:08.830: INFO: Waiting for pod downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52 to disappear
    Jan  3 13:02:08.850: INFO: Pod downwardapi-volume-6ff3a67b-f8da-414c-b88b-2c0bfbd93d52 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:02:08.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-321" for this suite. 01/03/24 13:02:08.882
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:02:08.908
Jan  3 13:02:08.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename statefulset 01/03/24 13:02:08.91
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:08.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:08.993
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-820 01/03/24 13:02:09.02
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Jan  3 13:02:09.086: INFO: Found 0 stateful pods, waiting for 1
Jan  3 13:02:19.106: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/03/24 13:02:19.146
W0103 13:02:19.179121      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  3 13:02:19.219: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 13:02:19.219: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Jan  3 13:02:29.255: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 13:02:29.255: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/03/24 13:02:29.31
STEP: Delete all of the StatefulSets 01/03/24 13:02:29.329
STEP: Verify that StatefulSets have been deleted 01/03/24 13:02:29.36
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  3 13:02:29.387: INFO: Deleting all statefulset in ns statefulset-820
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  3 13:02:29.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-820" for this suite. 01/03/24 13:02:29.476
------------------------------
• [SLOW TEST] [20.597 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:02:08.908
    Jan  3 13:02:08.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename statefulset 01/03/24 13:02:08.91
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:08.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:08.993
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-820 01/03/24 13:02:09.02
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Jan  3 13:02:09.086: INFO: Found 0 stateful pods, waiting for 1
    Jan  3 13:02:19.106: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/03/24 13:02:19.146
    W0103 13:02:19.179121      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  3 13:02:19.219: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 13:02:19.219: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Jan  3 13:02:29.255: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 13:02:29.255: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/03/24 13:02:29.31
    STEP: Delete all of the StatefulSets 01/03/24 13:02:29.329
    STEP: Verify that StatefulSets have been deleted 01/03/24 13:02:29.36
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  3 13:02:29.387: INFO: Deleting all statefulset in ns statefulset-820
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:02:29.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-820" for this suite. 01/03/24 13:02:29.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:02:29.506
Jan  3 13:02:29.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 13:02:29.508
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:29.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:29.586
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 13:02:29.659
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:02:30.161
STEP: Deploying the webhook pod 01/03/24 13:02:30.186
STEP: Wait for the deployment to be ready 01/03/24 13:02:30.229
Jan  3 13:02:30.262: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/03/24 13:02:32.318
STEP: Verifying the service has paired with the endpoint 01/03/24 13:02:32.347
Jan  3 13:02:33.348: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/03/24 13:02:33.366
STEP: create a configmap that should be updated by the webhook 01/03/24 13:02:33.498
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:02:33.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1878" for this suite. 01/03/24 13:02:33.79
STEP: Destroying namespace "webhook-1878-markers" for this suite. 01/03/24 13:02:33.818
------------------------------
• [4.338 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:02:29.506
    Jan  3 13:02:29.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 13:02:29.508
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:29.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:29.586
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 13:02:29.659
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:02:30.161
    STEP: Deploying the webhook pod 01/03/24 13:02:30.186
    STEP: Wait for the deployment to be ready 01/03/24 13:02:30.229
    Jan  3 13:02:30.262: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/03/24 13:02:32.318
    STEP: Verifying the service has paired with the endpoint 01/03/24 13:02:32.347
    Jan  3 13:02:33.348: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/03/24 13:02:33.366
    STEP: create a configmap that should be updated by the webhook 01/03/24 13:02:33.498
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:02:33.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1878" for this suite. 01/03/24 13:02:33.79
    STEP: Destroying namespace "webhook-1878-markers" for this suite. 01/03/24 13:02:33.818
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:02:33.848
Jan  3 13:02:33.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename disruption 01/03/24 13:02:33.85
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:33.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:33.929
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:02:33.957
Jan  3 13:02:33.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename disruption-2 01/03/24 13:02:33.959
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:34.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:34.045
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 01/03/24 13:02:34.098
STEP: Waiting for the pdb to be processed 01/03/24 13:02:34.132
STEP: Waiting for the pdb to be processed 01/03/24 13:02:34.171
STEP: listing a collection of PDBs across all namespaces 01/03/24 13:02:34.197
STEP: listing a collection of PDBs in namespace disruption-7885 01/03/24 13:02:34.216
STEP: deleting a collection of PDBs 01/03/24 13:02:34.232
STEP: Waiting for the PDB collection to be deleted 01/03/24 13:02:34.264
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jan  3 13:02:34.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  3 13:02:34.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-2820" for this suite. 01/03/24 13:02:34.324
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7885" for this suite. 01/03/24 13:02:34.349
------------------------------
• [0.525 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:02:33.848
    Jan  3 13:02:33.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename disruption 01/03/24 13:02:33.85
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:33.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:33.929
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:02:33.957
    Jan  3 13:02:33.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename disruption-2 01/03/24 13:02:33.959
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:34.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:34.045
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 01/03/24 13:02:34.098
    STEP: Waiting for the pdb to be processed 01/03/24 13:02:34.132
    STEP: Waiting for the pdb to be processed 01/03/24 13:02:34.171
    STEP: listing a collection of PDBs across all namespaces 01/03/24 13:02:34.197
    STEP: listing a collection of PDBs in namespace disruption-7885 01/03/24 13:02:34.216
    STEP: deleting a collection of PDBs 01/03/24 13:02:34.232
    STEP: Waiting for the PDB collection to be deleted 01/03/24 13:02:34.264
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:02:34.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:02:34.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-2820" for this suite. 01/03/24 13:02:34.324
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7885" for this suite. 01/03/24 13:02:34.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:02:34.376
Jan  3 13:02:34.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 13:02:34.379
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:34.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:34.457
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-4060/configmap-test-38463842-0b9c-4299-9549-03263b3ecad3 01/03/24 13:02:34.485
STEP: Creating a pod to test consume configMaps 01/03/24 13:02:34.505
Jan  3 13:02:34.533: INFO: Waiting up to 5m0s for pod "pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be" in namespace "configmap-4060" to be "Succeeded or Failed"
Jan  3 13:02:34.551: INFO: Pod "pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be": Phase="Pending", Reason="", readiness=false. Elapsed: 17.581233ms
Jan  3 13:02:36.570: INFO: Pod "pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036512773s
Jan  3 13:02:38.575: INFO: Pod "pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04198763s
Jan  3 13:02:40.574: INFO: Pod "pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041062969s
STEP: Saw pod success 01/03/24 13:02:40.574
Jan  3 13:02:40.575: INFO: Pod "pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be" satisfied condition "Succeeded or Failed"
Jan  3 13:02:40.593: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be container env-test: <nil>
STEP: delete the pod 01/03/24 13:02:40.632
Jan  3 13:02:40.678: INFO: Waiting for pod pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be to disappear
Jan  3 13:02:40.699: INFO: Pod pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 13:02:40.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4060" for this suite. 01/03/24 13:02:40.731
------------------------------
• [SLOW TEST] [6.379 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:02:34.376
    Jan  3 13:02:34.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 13:02:34.379
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:34.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:34.457
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-4060/configmap-test-38463842-0b9c-4299-9549-03263b3ecad3 01/03/24 13:02:34.485
    STEP: Creating a pod to test consume configMaps 01/03/24 13:02:34.505
    Jan  3 13:02:34.533: INFO: Waiting up to 5m0s for pod "pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be" in namespace "configmap-4060" to be "Succeeded or Failed"
    Jan  3 13:02:34.551: INFO: Pod "pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be": Phase="Pending", Reason="", readiness=false. Elapsed: 17.581233ms
    Jan  3 13:02:36.570: INFO: Pod "pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036512773s
    Jan  3 13:02:38.575: INFO: Pod "pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04198763s
    Jan  3 13:02:40.574: INFO: Pod "pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041062969s
    STEP: Saw pod success 01/03/24 13:02:40.574
    Jan  3 13:02:40.575: INFO: Pod "pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be" satisfied condition "Succeeded or Failed"
    Jan  3 13:02:40.593: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be container env-test: <nil>
    STEP: delete the pod 01/03/24 13:02:40.632
    Jan  3 13:02:40.678: INFO: Waiting for pod pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be to disappear
    Jan  3 13:02:40.699: INFO: Pod pod-configmaps-db8a5a6e-ae82-4bce-b97e-4fc529f439be no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:02:40.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4060" for this suite. 01/03/24 13:02:40.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:02:40.758
Jan  3 13:02:40.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename gc 01/03/24 13:02:40.761
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:40.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:40.844
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/03/24 13:02:40.891
STEP: create the rc2 01/03/24 13:02:40.912
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/03/24 13:02:45.957
STEP: delete the rc simpletest-rc-to-be-deleted 01/03/24 13:02:47.133
STEP: wait for the rc to be deleted 01/03/24 13:02:47.158
Jan  3 13:02:52.215: INFO: 69 pods remaining
Jan  3 13:02:52.215: INFO: 69 pods has nil DeletionTimestamp
Jan  3 13:02:52.215: INFO: 
STEP: Gathering metrics 01/03/24 13:02:57.212
W0103 13:02:57.239544      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  3 13:02:57.242: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan  3 13:02:57.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-22r78" in namespace "gc-4741"
Jan  3 13:02:57.279: INFO: Deleting pod "simpletest-rc-to-be-deleted-28wm8" in namespace "gc-4741"
Jan  3 13:02:57.317: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d7q7" in namespace "gc-4741"
Jan  3 13:02:57.353: INFO: Deleting pod "simpletest-rc-to-be-deleted-2ghmb" in namespace "gc-4741"
Jan  3 13:02:57.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xzs6" in namespace "gc-4741"
Jan  3 13:02:57.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-42ft5" in namespace "gc-4741"
Jan  3 13:02:57.477: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ctvl" in namespace "gc-4741"
Jan  3 13:02:57.513: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nk6h" in namespace "gc-4741"
Jan  3 13:02:57.556: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dn24" in namespace "gc-4741"
Jan  3 13:02:57.589: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hxnk" in namespace "gc-4741"
Jan  3 13:02:57.624: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pgs7" in namespace "gc-4741"
Jan  3 13:02:57.664: INFO: Deleting pod "simpletest-rc-to-be-deleted-5stf7" in namespace "gc-4741"
Jan  3 13:02:57.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wvnx" in namespace "gc-4741"
Jan  3 13:02:57.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wxzf" in namespace "gc-4741"
Jan  3 13:02:57.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xrpv" in namespace "gc-4741"
Jan  3 13:02:57.811: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zv4r" in namespace "gc-4741"
Jan  3 13:02:57.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dn64" in namespace "gc-4741"
Jan  3 13:02:57.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-7j5wx" in namespace "gc-4741"
Jan  3 13:02:57.918: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vdmn" in namespace "gc-4741"
Jan  3 13:02:57.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-88tjb" in namespace "gc-4741"
Jan  3 13:02:58.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ng4r" in namespace "gc-4741"
Jan  3 13:02:58.051: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x9wv" in namespace "gc-4741"
Jan  3 13:02:58.085: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xzbp" in namespace "gc-4741"
Jan  3 13:02:58.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-922l5" in namespace "gc-4741"
Jan  3 13:02:58.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-9br24" in namespace "gc-4741"
Jan  3 13:02:58.208: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f296" in namespace "gc-4741"
Jan  3 13:02:58.246: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcl5j" in namespace "gc-4741"
Jan  3 13:02:58.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgqrt" in namespace "gc-4741"
Jan  3 13:02:58.332: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjg8d" in namespace "gc-4741"
Jan  3 13:02:58.371: INFO: Deleting pod "simpletest-rc-to-be-deleted-ch9f7" in namespace "gc-4741"
Jan  3 13:02:58.409: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7g6v" in namespace "gc-4741"
Jan  3 13:02:58.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9hdb" in namespace "gc-4741"
Jan  3 13:02:58.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-df5n2" in namespace "gc-4741"
Jan  3 13:02:58.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfx8c" in namespace "gc-4741"
Jan  3 13:02:58.561: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnxzm" in namespace "gc-4741"
Jan  3 13:02:58.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqxrh" in namespace "gc-4741"
Jan  3 13:02:58.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-dz9rs" in namespace "gc-4741"
Jan  3 13:02:58.664: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhsfj" in namespace "gc-4741"
Jan  3 13:02:58.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-flfjt" in namespace "gc-4741"
Jan  3 13:02:58.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-fp657" in namespace "gc-4741"
Jan  3 13:02:58.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-gc7n4" in namespace "gc-4741"
Jan  3 13:02:58.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-gpbz9" in namespace "gc-4741"
Jan  3 13:02:58.828: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtdlf" in namespace "gc-4741"
Jan  3 13:02:58.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxcpf" in namespace "gc-4741"
Jan  3 13:02:58.900: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9zc2" in namespace "gc-4741"
Jan  3 13:02:58.937: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcx9c" in namespace "gc-4741"
Jan  3 13:02:58.976: INFO: Deleting pod "simpletest-rc-to-be-deleted-hk6cg" in namespace "gc-4741"
Jan  3 13:02:59.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-hrnvs" in namespace "gc-4741"
Jan  3 13:02:59.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9zt9" in namespace "gc-4741"
Jan  3 13:02:59.078: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnsvg" in namespace "gc-4741"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  3 13:02:59.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4741" for this suite. 01/03/24 13:02:59.138
------------------------------
• [SLOW TEST] [18.405 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:02:40.758
    Jan  3 13:02:40.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename gc 01/03/24 13:02:40.761
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:40.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:40.844
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/03/24 13:02:40.891
    STEP: create the rc2 01/03/24 13:02:40.912
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/03/24 13:02:45.957
    STEP: delete the rc simpletest-rc-to-be-deleted 01/03/24 13:02:47.133
    STEP: wait for the rc to be deleted 01/03/24 13:02:47.158
    Jan  3 13:02:52.215: INFO: 69 pods remaining
    Jan  3 13:02:52.215: INFO: 69 pods has nil DeletionTimestamp
    Jan  3 13:02:52.215: INFO: 
    STEP: Gathering metrics 01/03/24 13:02:57.212
    W0103 13:02:57.239544      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  3 13:02:57.242: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan  3 13:02:57.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-22r78" in namespace "gc-4741"
    Jan  3 13:02:57.279: INFO: Deleting pod "simpletest-rc-to-be-deleted-28wm8" in namespace "gc-4741"
    Jan  3 13:02:57.317: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d7q7" in namespace "gc-4741"
    Jan  3 13:02:57.353: INFO: Deleting pod "simpletest-rc-to-be-deleted-2ghmb" in namespace "gc-4741"
    Jan  3 13:02:57.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xzs6" in namespace "gc-4741"
    Jan  3 13:02:57.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-42ft5" in namespace "gc-4741"
    Jan  3 13:02:57.477: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ctvl" in namespace "gc-4741"
    Jan  3 13:02:57.513: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nk6h" in namespace "gc-4741"
    Jan  3 13:02:57.556: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dn24" in namespace "gc-4741"
    Jan  3 13:02:57.589: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hxnk" in namespace "gc-4741"
    Jan  3 13:02:57.624: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pgs7" in namespace "gc-4741"
    Jan  3 13:02:57.664: INFO: Deleting pod "simpletest-rc-to-be-deleted-5stf7" in namespace "gc-4741"
    Jan  3 13:02:57.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wvnx" in namespace "gc-4741"
    Jan  3 13:02:57.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wxzf" in namespace "gc-4741"
    Jan  3 13:02:57.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xrpv" in namespace "gc-4741"
    Jan  3 13:02:57.811: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zv4r" in namespace "gc-4741"
    Jan  3 13:02:57.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dn64" in namespace "gc-4741"
    Jan  3 13:02:57.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-7j5wx" in namespace "gc-4741"
    Jan  3 13:02:57.918: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vdmn" in namespace "gc-4741"
    Jan  3 13:02:57.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-88tjb" in namespace "gc-4741"
    Jan  3 13:02:58.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ng4r" in namespace "gc-4741"
    Jan  3 13:02:58.051: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x9wv" in namespace "gc-4741"
    Jan  3 13:02:58.085: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xzbp" in namespace "gc-4741"
    Jan  3 13:02:58.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-922l5" in namespace "gc-4741"
    Jan  3 13:02:58.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-9br24" in namespace "gc-4741"
    Jan  3 13:02:58.208: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f296" in namespace "gc-4741"
    Jan  3 13:02:58.246: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcl5j" in namespace "gc-4741"
    Jan  3 13:02:58.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgqrt" in namespace "gc-4741"
    Jan  3 13:02:58.332: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjg8d" in namespace "gc-4741"
    Jan  3 13:02:58.371: INFO: Deleting pod "simpletest-rc-to-be-deleted-ch9f7" in namespace "gc-4741"
    Jan  3 13:02:58.409: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7g6v" in namespace "gc-4741"
    Jan  3 13:02:58.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9hdb" in namespace "gc-4741"
    Jan  3 13:02:58.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-df5n2" in namespace "gc-4741"
    Jan  3 13:02:58.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfx8c" in namespace "gc-4741"
    Jan  3 13:02:58.561: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnxzm" in namespace "gc-4741"
    Jan  3 13:02:58.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqxrh" in namespace "gc-4741"
    Jan  3 13:02:58.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-dz9rs" in namespace "gc-4741"
    Jan  3 13:02:58.664: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhsfj" in namespace "gc-4741"
    Jan  3 13:02:58.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-flfjt" in namespace "gc-4741"
    Jan  3 13:02:58.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-fp657" in namespace "gc-4741"
    Jan  3 13:02:58.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-gc7n4" in namespace "gc-4741"
    Jan  3 13:02:58.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-gpbz9" in namespace "gc-4741"
    Jan  3 13:02:58.828: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtdlf" in namespace "gc-4741"
    Jan  3 13:02:58.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxcpf" in namespace "gc-4741"
    Jan  3 13:02:58.900: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9zc2" in namespace "gc-4741"
    Jan  3 13:02:58.937: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcx9c" in namespace "gc-4741"
    Jan  3 13:02:58.976: INFO: Deleting pod "simpletest-rc-to-be-deleted-hk6cg" in namespace "gc-4741"
    Jan  3 13:02:59.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-hrnvs" in namespace "gc-4741"
    Jan  3 13:02:59.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9zt9" in namespace "gc-4741"
    Jan  3 13:02:59.078: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnsvg" in namespace "gc-4741"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:02:59.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4741" for this suite. 01/03/24 13:02:59.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:02:59.168
Jan  3 13:02:59.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename namespaces 01/03/24 13:02:59.169
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:59.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:59.246
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-8qjfw" 01/03/24 13:02:59.273
Jan  3 13:02:59.344: INFO: Namespace "e2e-ns-8qjfw-7495" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-8qjfw-7495" 01/03/24 13:02:59.344
Jan  3 13:02:59.380: INFO: Namespace "e2e-ns-8qjfw-7495" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-8qjfw-7495" 01/03/24 13:02:59.38
Jan  3 13:02:59.419: INFO: Namespace "e2e-ns-8qjfw-7495" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:02:59.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-134" for this suite. 01/03/24 13:02:59.441
STEP: Destroying namespace "e2e-ns-8qjfw-7495" for this suite. 01/03/24 13:02:59.463
------------------------------
• [0.319 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:02:59.168
    Jan  3 13:02:59.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename namespaces 01/03/24 13:02:59.169
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:59.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:59.246
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-8qjfw" 01/03/24 13:02:59.273
    Jan  3 13:02:59.344: INFO: Namespace "e2e-ns-8qjfw-7495" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-8qjfw-7495" 01/03/24 13:02:59.344
    Jan  3 13:02:59.380: INFO: Namespace "e2e-ns-8qjfw-7495" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-8qjfw-7495" 01/03/24 13:02:59.38
    Jan  3 13:02:59.419: INFO: Namespace "e2e-ns-8qjfw-7495" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:02:59.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-134" for this suite. 01/03/24 13:02:59.441
    STEP: Destroying namespace "e2e-ns-8qjfw-7495" for this suite. 01/03/24 13:02:59.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:02:59.487
Jan  3 13:02:59.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename runtimeclass 01/03/24 13:02:59.489
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:59.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:59.565
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  3 13:02:59.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9528" for this suite. 01/03/24 13:02:59.639
------------------------------
• [0.178 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:02:59.487
    Jan  3 13:02:59.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename runtimeclass 01/03/24 13:02:59.489
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:59.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:59.565
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:02:59.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9528" for this suite. 01/03/24 13:02:59.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:02:59.668
Jan  3 13:02:59.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename job 01/03/24 13:02:59.67
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:59.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:59.76
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 01/03/24 13:02:59.787
STEP: Ensuring active pods == parallelism 01/03/24 13:02:59.808
STEP: Orphaning one of the Job's Pods 01/03/24 13:03:09.83
Jan  3 13:03:10.405: INFO: Successfully updated pod "adopt-release-d7xh4"
STEP: Checking that the Job readopts the Pod 01/03/24 13:03:10.405
Jan  3 13:03:10.405: INFO: Waiting up to 15m0s for pod "adopt-release-d7xh4" in namespace "job-6259" to be "adopted"
Jan  3 13:03:10.431: INFO: Pod "adopt-release-d7xh4": Phase="Running", Reason="", readiness=true. Elapsed: 25.593217ms
Jan  3 13:03:12.453: INFO: Pod "adopt-release-d7xh4": Phase="Running", Reason="", readiness=true. Elapsed: 2.047236515s
Jan  3 13:03:12.453: INFO: Pod "adopt-release-d7xh4" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/03/24 13:03:12.453
Jan  3 13:03:13.007: INFO: Successfully updated pod "adopt-release-d7xh4"
STEP: Checking that the Job releases the Pod 01/03/24 13:03:13.008
Jan  3 13:03:13.008: INFO: Waiting up to 15m0s for pod "adopt-release-d7xh4" in namespace "job-6259" to be "released"
Jan  3 13:03:13.029: INFO: Pod "adopt-release-d7xh4": Phase="Running", Reason="", readiness=true. Elapsed: 21.526978ms
Jan  3 13:03:15.052: INFO: Pod "adopt-release-d7xh4": Phase="Running", Reason="", readiness=true. Elapsed: 2.044519203s
Jan  3 13:03:15.052: INFO: Pod "adopt-release-d7xh4" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  3 13:03:15.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6259" for this suite. 01/03/24 13:03:15.085
------------------------------
• [SLOW TEST] [15.444 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:02:59.668
    Jan  3 13:02:59.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename job 01/03/24 13:02:59.67
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:02:59.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:02:59.76
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 01/03/24 13:02:59.787
    STEP: Ensuring active pods == parallelism 01/03/24 13:02:59.808
    STEP: Orphaning one of the Job's Pods 01/03/24 13:03:09.83
    Jan  3 13:03:10.405: INFO: Successfully updated pod "adopt-release-d7xh4"
    STEP: Checking that the Job readopts the Pod 01/03/24 13:03:10.405
    Jan  3 13:03:10.405: INFO: Waiting up to 15m0s for pod "adopt-release-d7xh4" in namespace "job-6259" to be "adopted"
    Jan  3 13:03:10.431: INFO: Pod "adopt-release-d7xh4": Phase="Running", Reason="", readiness=true. Elapsed: 25.593217ms
    Jan  3 13:03:12.453: INFO: Pod "adopt-release-d7xh4": Phase="Running", Reason="", readiness=true. Elapsed: 2.047236515s
    Jan  3 13:03:12.453: INFO: Pod "adopt-release-d7xh4" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/03/24 13:03:12.453
    Jan  3 13:03:13.007: INFO: Successfully updated pod "adopt-release-d7xh4"
    STEP: Checking that the Job releases the Pod 01/03/24 13:03:13.008
    Jan  3 13:03:13.008: INFO: Waiting up to 15m0s for pod "adopt-release-d7xh4" in namespace "job-6259" to be "released"
    Jan  3 13:03:13.029: INFO: Pod "adopt-release-d7xh4": Phase="Running", Reason="", readiness=true. Elapsed: 21.526978ms
    Jan  3 13:03:15.052: INFO: Pod "adopt-release-d7xh4": Phase="Running", Reason="", readiness=true. Elapsed: 2.044519203s
    Jan  3 13:03:15.052: INFO: Pod "adopt-release-d7xh4" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:03:15.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6259" for this suite. 01/03/24 13:03:15.085
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:03:15.115
Jan  3 13:03:15.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename statefulset 01/03/24 13:03:15.117
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:03:15.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:03:15.196
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1391 01/03/24 13:03:15.224
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 01/03/24 13:03:15.266
Jan  3 13:03:15.305: INFO: Found 0 stateful pods, waiting for 3
Jan  3 13:03:25.331: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 13:03:25.331: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 13:03:25.331: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/03/24 13:03:25.387
Jan  3 13:03:25.436: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/03/24 13:03:25.436
STEP: Not applying an update when the partition is greater than the number of replicas 01/03/24 13:03:35.519
STEP: Performing a canary update 01/03/24 13:03:35.52
Jan  3 13:03:35.576: INFO: Updating stateful set ss2
Jan  3 13:03:35.625: INFO: Waiting for Pod statefulset-1391/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 01/03/24 13:03:45.674
Jan  3 13:03:45.771: INFO: Found 2 stateful pods, waiting for 3
Jan  3 13:03:55.795: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 13:03:55.795: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 13:03:55.795: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/03/24 13:03:55.835
Jan  3 13:03:55.883: INFO: Updating stateful set ss2
Jan  3 13:03:55.920: INFO: Waiting for Pod statefulset-1391/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jan  3 13:04:06.016: INFO: Updating stateful set ss2
Jan  3 13:04:06.058: INFO: Waiting for StatefulSet statefulset-1391/ss2 to complete update
Jan  3 13:04:06.058: INFO: Waiting for Pod statefulset-1391/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  3 13:04:16.115: INFO: Deleting all statefulset in ns statefulset-1391
Jan  3 13:04:16.132: INFO: Scaling statefulset ss2 to 0
Jan  3 13:04:26.224: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 13:04:26.244: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  3 13:04:26.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1391" for this suite. 01/03/24 13:04:26.336
------------------------------
• [SLOW TEST] [71.246 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:03:15.115
    Jan  3 13:03:15.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename statefulset 01/03/24 13:03:15.117
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:03:15.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:03:15.196
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1391 01/03/24 13:03:15.224
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 01/03/24 13:03:15.266
    Jan  3 13:03:15.305: INFO: Found 0 stateful pods, waiting for 3
    Jan  3 13:03:25.331: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 13:03:25.331: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 13:03:25.331: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/03/24 13:03:25.387
    Jan  3 13:03:25.436: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/03/24 13:03:25.436
    STEP: Not applying an update when the partition is greater than the number of replicas 01/03/24 13:03:35.519
    STEP: Performing a canary update 01/03/24 13:03:35.52
    Jan  3 13:03:35.576: INFO: Updating stateful set ss2
    Jan  3 13:03:35.625: INFO: Waiting for Pod statefulset-1391/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 01/03/24 13:03:45.674
    Jan  3 13:03:45.771: INFO: Found 2 stateful pods, waiting for 3
    Jan  3 13:03:55.795: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 13:03:55.795: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 13:03:55.795: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/03/24 13:03:55.835
    Jan  3 13:03:55.883: INFO: Updating stateful set ss2
    Jan  3 13:03:55.920: INFO: Waiting for Pod statefulset-1391/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jan  3 13:04:06.016: INFO: Updating stateful set ss2
    Jan  3 13:04:06.058: INFO: Waiting for StatefulSet statefulset-1391/ss2 to complete update
    Jan  3 13:04:06.058: INFO: Waiting for Pod statefulset-1391/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  3 13:04:16.115: INFO: Deleting all statefulset in ns statefulset-1391
    Jan  3 13:04:16.132: INFO: Scaling statefulset ss2 to 0
    Jan  3 13:04:26.224: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 13:04:26.244: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:04:26.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1391" for this suite. 01/03/24 13:04:26.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:04:26.364
Jan  3 13:04:26.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename resourcequota 01/03/24 13:04:26.367
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:04:26.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:04:26.446
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-v8fgf" 01/03/24 13:04:26.492
Jan  3 13:04:26.538: INFO: Resource quota "e2e-rq-status-v8fgf" reports spec: hard cpu limit of 500m
Jan  3 13:04:26.538: INFO: Resource quota "e2e-rq-status-v8fgf" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-v8fgf" /status 01/03/24 13:04:26.538
STEP: Confirm /status for "e2e-rq-status-v8fgf" resourceQuota via watch 01/03/24 13:04:26.579
Jan  3 13:04:26.593: INFO: observed resourceQuota "e2e-rq-status-v8fgf" in namespace "resourcequota-8807" with hard status: v1.ResourceList(nil)
Jan  3 13:04:26.593: INFO: Found resourceQuota "e2e-rq-status-v8fgf" in namespace "resourcequota-8807" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan  3 13:04:26.593: INFO: ResourceQuota "e2e-rq-status-v8fgf" /status was updated
STEP: Patching hard spec values for cpu & memory 01/03/24 13:04:26.612
Jan  3 13:04:26.633: INFO: Resource quota "e2e-rq-status-v8fgf" reports spec: hard cpu limit of 1
Jan  3 13:04:26.633: INFO: Resource quota "e2e-rq-status-v8fgf" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-v8fgf" /status 01/03/24 13:04:26.633
STEP: Confirm /status for "e2e-rq-status-v8fgf" resourceQuota via watch 01/03/24 13:04:26.653
Jan  3 13:04:26.666: INFO: observed resourceQuota "e2e-rq-status-v8fgf" in namespace "resourcequota-8807" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan  3 13:04:26.667: INFO: Found resourceQuota "e2e-rq-status-v8fgf" in namespace "resourcequota-8807" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jan  3 13:04:26.667: INFO: ResourceQuota "e2e-rq-status-v8fgf" /status was patched
STEP: Get "e2e-rq-status-v8fgf" /status 01/03/24 13:04:26.668
Jan  3 13:04:26.687: INFO: Resourcequota "e2e-rq-status-v8fgf" reports status: hard cpu of 1
Jan  3 13:04:26.687: INFO: Resourcequota "e2e-rq-status-v8fgf" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-v8fgf" /status before checking Spec is unchanged 01/03/24 13:04:26.706
Jan  3 13:04:26.726: INFO: Resourcequota "e2e-rq-status-v8fgf" reports status: hard cpu of 2
Jan  3 13:04:26.726: INFO: Resourcequota "e2e-rq-status-v8fgf" reports status: hard memory of 2Gi
Jan  3 13:04:26.739: INFO: Found resourceQuota "e2e-rq-status-v8fgf" in namespace "resourcequota-8807" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jan  3 13:09:21.779: INFO: ResourceQuota "e2e-rq-status-v8fgf" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  3 13:09:21.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8807" for this suite. 01/03/24 13:09:21.812
------------------------------
• [SLOW TEST] [295.476 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:04:26.364
    Jan  3 13:04:26.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename resourcequota 01/03/24 13:04:26.367
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:04:26.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:04:26.446
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-v8fgf" 01/03/24 13:04:26.492
    Jan  3 13:04:26.538: INFO: Resource quota "e2e-rq-status-v8fgf" reports spec: hard cpu limit of 500m
    Jan  3 13:04:26.538: INFO: Resource quota "e2e-rq-status-v8fgf" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-v8fgf" /status 01/03/24 13:04:26.538
    STEP: Confirm /status for "e2e-rq-status-v8fgf" resourceQuota via watch 01/03/24 13:04:26.579
    Jan  3 13:04:26.593: INFO: observed resourceQuota "e2e-rq-status-v8fgf" in namespace "resourcequota-8807" with hard status: v1.ResourceList(nil)
    Jan  3 13:04:26.593: INFO: Found resourceQuota "e2e-rq-status-v8fgf" in namespace "resourcequota-8807" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan  3 13:04:26.593: INFO: ResourceQuota "e2e-rq-status-v8fgf" /status was updated
    STEP: Patching hard spec values for cpu & memory 01/03/24 13:04:26.612
    Jan  3 13:04:26.633: INFO: Resource quota "e2e-rq-status-v8fgf" reports spec: hard cpu limit of 1
    Jan  3 13:04:26.633: INFO: Resource quota "e2e-rq-status-v8fgf" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-v8fgf" /status 01/03/24 13:04:26.633
    STEP: Confirm /status for "e2e-rq-status-v8fgf" resourceQuota via watch 01/03/24 13:04:26.653
    Jan  3 13:04:26.666: INFO: observed resourceQuota "e2e-rq-status-v8fgf" in namespace "resourcequota-8807" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan  3 13:04:26.667: INFO: Found resourceQuota "e2e-rq-status-v8fgf" in namespace "resourcequota-8807" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jan  3 13:04:26.667: INFO: ResourceQuota "e2e-rq-status-v8fgf" /status was patched
    STEP: Get "e2e-rq-status-v8fgf" /status 01/03/24 13:04:26.668
    Jan  3 13:04:26.687: INFO: Resourcequota "e2e-rq-status-v8fgf" reports status: hard cpu of 1
    Jan  3 13:04:26.687: INFO: Resourcequota "e2e-rq-status-v8fgf" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-v8fgf" /status before checking Spec is unchanged 01/03/24 13:04:26.706
    Jan  3 13:04:26.726: INFO: Resourcequota "e2e-rq-status-v8fgf" reports status: hard cpu of 2
    Jan  3 13:04:26.726: INFO: Resourcequota "e2e-rq-status-v8fgf" reports status: hard memory of 2Gi
    Jan  3 13:04:26.739: INFO: Found resourceQuota "e2e-rq-status-v8fgf" in namespace "resourcequota-8807" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jan  3 13:09:21.779: INFO: ResourceQuota "e2e-rq-status-v8fgf" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:09:21.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8807" for this suite. 01/03/24 13:09:21.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:09:21.842
Jan  3 13:09:21.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename security-context-test 01/03/24 13:09:21.845
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:09:21.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:09:21.943
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Jan  3 13:09:22.005: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a2685b54-df69-4ce5-971a-bf1204541553" in namespace "security-context-test-1939" to be "Succeeded or Failed"
Jan  3 13:09:22.022: INFO: Pod "busybox-user-65534-a2685b54-df69-4ce5-971a-bf1204541553": Phase="Pending", Reason="", readiness=false. Elapsed: 17.563365ms
Jan  3 13:09:24.042: INFO: Pod "busybox-user-65534-a2685b54-df69-4ce5-971a-bf1204541553": Phase="Running", Reason="", readiness=true. Elapsed: 2.036674605s
Jan  3 13:09:26.045: INFO: Pod "busybox-user-65534-a2685b54-df69-4ce5-971a-bf1204541553": Phase="Running", Reason="", readiness=false. Elapsed: 4.039842964s
Jan  3 13:09:28.050: INFO: Pod "busybox-user-65534-a2685b54-df69-4ce5-971a-bf1204541553": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045531902s
Jan  3 13:09:28.050: INFO: Pod "busybox-user-65534-a2685b54-df69-4ce5-971a-bf1204541553" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  3 13:09:28.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1939" for this suite. 01/03/24 13:09:28.082
------------------------------
• [SLOW TEST] [6.266 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:09:21.842
    Jan  3 13:09:21.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename security-context-test 01/03/24 13:09:21.845
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:09:21.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:09:21.943
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Jan  3 13:09:22.005: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a2685b54-df69-4ce5-971a-bf1204541553" in namespace "security-context-test-1939" to be "Succeeded or Failed"
    Jan  3 13:09:22.022: INFO: Pod "busybox-user-65534-a2685b54-df69-4ce5-971a-bf1204541553": Phase="Pending", Reason="", readiness=false. Elapsed: 17.563365ms
    Jan  3 13:09:24.042: INFO: Pod "busybox-user-65534-a2685b54-df69-4ce5-971a-bf1204541553": Phase="Running", Reason="", readiness=true. Elapsed: 2.036674605s
    Jan  3 13:09:26.045: INFO: Pod "busybox-user-65534-a2685b54-df69-4ce5-971a-bf1204541553": Phase="Running", Reason="", readiness=false. Elapsed: 4.039842964s
    Jan  3 13:09:28.050: INFO: Pod "busybox-user-65534-a2685b54-df69-4ce5-971a-bf1204541553": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045531902s
    Jan  3 13:09:28.050: INFO: Pod "busybox-user-65534-a2685b54-df69-4ce5-971a-bf1204541553" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:09:28.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1939" for this suite. 01/03/24 13:09:28.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:09:28.111
Jan  3 13:09:28.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pods 01/03/24 13:09:28.114
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:09:28.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:09:28.191
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 01/03/24 13:09:28.22
Jan  3 13:09:28.246: INFO: created test-pod-1
Jan  3 13:09:28.267: INFO: created test-pod-2
Jan  3 13:09:28.287: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/03/24 13:09:28.287
Jan  3 13:09:28.288: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2251' to be running and ready
Jan  3 13:09:28.364: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  3 13:09:28.364: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  3 13:09:28.364: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  3 13:09:28.364: INFO: 0 / 3 pods in namespace 'pods-2251' are running and ready (0 seconds elapsed)
Jan  3 13:09:28.364: INFO: expected 0 pod replicas in namespace 'pods-2251', 0 are Running and Ready.
Jan  3 13:09:28.364: INFO: POD         NODE                   PHASE    GRACE  CONDITIONS
Jan  3 13:09:28.364: INFO: test-pod-1  jb-1-26-np-64kerjapxk  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  }]
Jan  3 13:09:28.364: INFO: test-pod-2  jb-1-26-np-64kerjapxk  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  }]
Jan  3 13:09:28.364: INFO: test-pod-3  jb-1-26-np-64kerjapxk  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  }]
Jan  3 13:09:28.364: INFO: 
Jan  3 13:09:30.467: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  3 13:09:30.467: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  3 13:09:30.467: INFO: 1 / 3 pods in namespace 'pods-2251' are running and ready (2 seconds elapsed)
Jan  3 13:09:30.467: INFO: expected 0 pod replicas in namespace 'pods-2251', 0 are Running and Ready.
Jan  3 13:09:30.467: INFO: POD         NODE                   PHASE    GRACE  CONDITIONS
Jan  3 13:09:30.467: INFO: test-pod-2  jb-1-26-np-64kerjapxk  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  }]
Jan  3 13:09:30.468: INFO: test-pod-3  jb-1-26-np-64kerjapxk  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  }]
Jan  3 13:09:30.468: INFO: 
Jan  3 13:09:32.428: INFO: 3 / 3 pods in namespace 'pods-2251' are running and ready (4 seconds elapsed)
Jan  3 13:09:32.428: INFO: expected 0 pod replicas in namespace 'pods-2251', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/03/24 13:09:32.498
Jan  3 13:09:32.519: INFO: Pod quantity 3 is different from expected quantity 0
Jan  3 13:09:33.540: INFO: Pod quantity 3 is different from expected quantity 0
Jan  3 13:09:34.539: INFO: Pod quantity 3 is different from expected quantity 0
Jan  3 13:09:35.538: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  3 13:09:36.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2251" for this suite. 01/03/24 13:09:36.568
------------------------------
• [SLOW TEST] [8.481 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:09:28.111
    Jan  3 13:09:28.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pods 01/03/24 13:09:28.114
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:09:28.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:09:28.191
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 01/03/24 13:09:28.22
    Jan  3 13:09:28.246: INFO: created test-pod-1
    Jan  3 13:09:28.267: INFO: created test-pod-2
    Jan  3 13:09:28.287: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/03/24 13:09:28.287
    Jan  3 13:09:28.288: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2251' to be running and ready
    Jan  3 13:09:28.364: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  3 13:09:28.364: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  3 13:09:28.364: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  3 13:09:28.364: INFO: 0 / 3 pods in namespace 'pods-2251' are running and ready (0 seconds elapsed)
    Jan  3 13:09:28.364: INFO: expected 0 pod replicas in namespace 'pods-2251', 0 are Running and Ready.
    Jan  3 13:09:28.364: INFO: POD         NODE                   PHASE    GRACE  CONDITIONS
    Jan  3 13:09:28.364: INFO: test-pod-1  jb-1-26-np-64kerjapxk  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  }]
    Jan  3 13:09:28.364: INFO: test-pod-2  jb-1-26-np-64kerjapxk  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  }]
    Jan  3 13:09:28.364: INFO: test-pod-3  jb-1-26-np-64kerjapxk  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  }]
    Jan  3 13:09:28.364: INFO: 
    Jan  3 13:09:30.467: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  3 13:09:30.467: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  3 13:09:30.467: INFO: 1 / 3 pods in namespace 'pods-2251' are running and ready (2 seconds elapsed)
    Jan  3 13:09:30.467: INFO: expected 0 pod replicas in namespace 'pods-2251', 0 are Running and Ready.
    Jan  3 13:09:30.467: INFO: POD         NODE                   PHASE    GRACE  CONDITIONS
    Jan  3 13:09:30.467: INFO: test-pod-2  jb-1-26-np-64kerjapxk  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  }]
    Jan  3 13:09:30.468: INFO: test-pod-3  jb-1-26-np-64kerjapxk  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-03 13:09:28 +0000 UTC  }]
    Jan  3 13:09:30.468: INFO: 
    Jan  3 13:09:32.428: INFO: 3 / 3 pods in namespace 'pods-2251' are running and ready (4 seconds elapsed)
    Jan  3 13:09:32.428: INFO: expected 0 pod replicas in namespace 'pods-2251', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/03/24 13:09:32.498
    Jan  3 13:09:32.519: INFO: Pod quantity 3 is different from expected quantity 0
    Jan  3 13:09:33.540: INFO: Pod quantity 3 is different from expected quantity 0
    Jan  3 13:09:34.539: INFO: Pod quantity 3 is different from expected quantity 0
    Jan  3 13:09:35.538: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:09:36.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2251" for this suite. 01/03/24 13:09:36.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:09:36.597
Jan  3 13:09:36.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 13:09:36.599
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:09:36.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:09:36.685
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 13:09:36.764
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:09:37.379
STEP: Deploying the webhook pod 01/03/24 13:09:37.404
STEP: Wait for the deployment to be ready 01/03/24 13:09:37.445
Jan  3 13:09:37.481: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 13:09:39.536: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 13, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/03/24 13:09:41.555
STEP: Verifying the service has paired with the endpoint 01/03/24 13:09:41.587
Jan  3 13:09:42.588: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 01/03/24 13:09:42.607
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/03/24 13:09:42.621
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/03/24 13:09:42.622
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/03/24 13:09:42.622
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/03/24 13:09:42.638
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/03/24 13:09:42.638
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/03/24 13:09:42.652
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:09:42.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1820" for this suite. 01/03/24 13:09:42.805
STEP: Destroying namespace "webhook-1820-markers" for this suite. 01/03/24 13:09:42.833
------------------------------
• [SLOW TEST] [6.260 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:09:36.597
    Jan  3 13:09:36.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 13:09:36.599
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:09:36.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:09:36.685
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 13:09:36.764
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:09:37.379
    STEP: Deploying the webhook pod 01/03/24 13:09:37.404
    STEP: Wait for the deployment to be ready 01/03/24 13:09:37.445
    Jan  3 13:09:37.481: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan  3 13:09:39.536: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 13, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/03/24 13:09:41.555
    STEP: Verifying the service has paired with the endpoint 01/03/24 13:09:41.587
    Jan  3 13:09:42.588: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 01/03/24 13:09:42.607
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/03/24 13:09:42.621
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/03/24 13:09:42.622
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/03/24 13:09:42.622
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/03/24 13:09:42.638
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/03/24 13:09:42.638
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/03/24 13:09:42.652
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:09:42.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1820" for this suite. 01/03/24 13:09:42.805
    STEP: Destroying namespace "webhook-1820-markers" for this suite. 01/03/24 13:09:42.833
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:09:42.858
Jan  3 13:09:42.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename sched-preemption 01/03/24 13:09:42.861
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:09:42.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:09:42.945
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan  3 13:09:43.028: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  3 13:10:43.180: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 01/03/24 13:10:43.2
Jan  3 13:10:43.262: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan  3 13:10:43.283: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan  3 13:10:43.333: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan  3 13:10:43.354: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan  3 13:10:43.403: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan  3 13:10:43.437: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/03/24 13:10:43.437
Jan  3 13:10:43.438: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2491" to be "running"
Jan  3 13:10:43.458: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 20.237438ms
Jan  3 13:10:45.487: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048828895s
Jan  3 13:10:47.480: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.041931959s
Jan  3 13:10:47.480: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan  3 13:10:47.480: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2491" to be "running"
Jan  3 13:10:47.499: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.549977ms
Jan  3 13:10:47.499: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 13:10:47.499: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2491" to be "running"
Jan  3 13:10:47.533: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 34.427836ms
Jan  3 13:10:47.533: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 13:10:47.533: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2491" to be "running"
Jan  3 13:10:47.553: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.997422ms
Jan  3 13:10:47.553: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 13:10:47.553: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2491" to be "running"
Jan  3 13:10:47.597: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 44.029446ms
Jan  3 13:10:47.597: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 13:10:47.598: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2491" to be "running"
Jan  3 13:10:47.620: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 22.866673ms
Jan  3 13:10:47.620: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/03/24 13:10:47.62
Jan  3 13:10:47.676: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan  3 13:10:47.694: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.725318ms
Jan  3 13:10:49.729: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05370216s
Jan  3 13:10:51.714: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038230115s
Jan  3 13:10:53.713: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.037663177s
Jan  3 13:10:53.713: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:10:53.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-2491" for this suite. 01/03/24 13:10:54.096
------------------------------
• [SLOW TEST] [71.261 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:09:42.858
    Jan  3 13:09:42.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename sched-preemption 01/03/24 13:09:42.861
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:09:42.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:09:42.945
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan  3 13:09:43.028: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  3 13:10:43.180: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 01/03/24 13:10:43.2
    Jan  3 13:10:43.262: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan  3 13:10:43.283: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan  3 13:10:43.333: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan  3 13:10:43.354: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan  3 13:10:43.403: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan  3 13:10:43.437: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/03/24 13:10:43.437
    Jan  3 13:10:43.438: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2491" to be "running"
    Jan  3 13:10:43.458: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 20.237438ms
    Jan  3 13:10:45.487: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048828895s
    Jan  3 13:10:47.480: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.041931959s
    Jan  3 13:10:47.480: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan  3 13:10:47.480: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2491" to be "running"
    Jan  3 13:10:47.499: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.549977ms
    Jan  3 13:10:47.499: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 13:10:47.499: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2491" to be "running"
    Jan  3 13:10:47.533: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 34.427836ms
    Jan  3 13:10:47.533: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 13:10:47.533: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2491" to be "running"
    Jan  3 13:10:47.553: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.997422ms
    Jan  3 13:10:47.553: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 13:10:47.553: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2491" to be "running"
    Jan  3 13:10:47.597: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 44.029446ms
    Jan  3 13:10:47.597: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 13:10:47.598: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2491" to be "running"
    Jan  3 13:10:47.620: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 22.866673ms
    Jan  3 13:10:47.620: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/03/24 13:10:47.62
    Jan  3 13:10:47.676: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan  3 13:10:47.694: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.725318ms
    Jan  3 13:10:49.729: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05370216s
    Jan  3 13:10:51.714: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038230115s
    Jan  3 13:10:53.713: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.037663177s
    Jan  3 13:10:53.713: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:10:53.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-2491" for this suite. 01/03/24 13:10:54.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:10:54.122
Jan  3 13:10:54.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename secrets 01/03/24 13:10:54.125
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:10:54.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:10:54.204
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-1823/secret-test-5d20123e-4c11-4f6d-90af-daa4c3a156fa 01/03/24 13:10:54.231
STEP: Creating a pod to test consume secrets 01/03/24 13:10:54.25
Jan  3 13:10:54.277: INFO: Waiting up to 5m0s for pod "pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe" in namespace "secrets-1823" to be "Succeeded or Failed"
Jan  3 13:10:54.295: INFO: Pod "pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe": Phase="Pending", Reason="", readiness=false. Elapsed: 17.86408ms
Jan  3 13:10:56.317: INFO: Pod "pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039776528s
Jan  3 13:10:58.315: INFO: Pod "pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe": Phase="Running", Reason="", readiness=false. Elapsed: 4.038047305s
Jan  3 13:11:00.316: INFO: Pod "pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038725107s
STEP: Saw pod success 01/03/24 13:11:00.316
Jan  3 13:11:00.316: INFO: Pod "pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe" satisfied condition "Succeeded or Failed"
Jan  3 13:11:00.334: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe container env-test: <nil>
STEP: delete the pod 01/03/24 13:11:00.506
Jan  3 13:11:00.543: INFO: Waiting for pod pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe to disappear
Jan  3 13:11:00.559: INFO: Pod pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  3 13:11:00.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1823" for this suite. 01/03/24 13:11:00.592
------------------------------
• [SLOW TEST] [6.497 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:10:54.122
    Jan  3 13:10:54.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename secrets 01/03/24 13:10:54.125
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:10:54.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:10:54.204
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-1823/secret-test-5d20123e-4c11-4f6d-90af-daa4c3a156fa 01/03/24 13:10:54.231
    STEP: Creating a pod to test consume secrets 01/03/24 13:10:54.25
    Jan  3 13:10:54.277: INFO: Waiting up to 5m0s for pod "pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe" in namespace "secrets-1823" to be "Succeeded or Failed"
    Jan  3 13:10:54.295: INFO: Pod "pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe": Phase="Pending", Reason="", readiness=false. Elapsed: 17.86408ms
    Jan  3 13:10:56.317: INFO: Pod "pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039776528s
    Jan  3 13:10:58.315: INFO: Pod "pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe": Phase="Running", Reason="", readiness=false. Elapsed: 4.038047305s
    Jan  3 13:11:00.316: INFO: Pod "pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038725107s
    STEP: Saw pod success 01/03/24 13:11:00.316
    Jan  3 13:11:00.316: INFO: Pod "pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe" satisfied condition "Succeeded or Failed"
    Jan  3 13:11:00.334: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe container env-test: <nil>
    STEP: delete the pod 01/03/24 13:11:00.506
    Jan  3 13:11:00.543: INFO: Waiting for pod pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe to disappear
    Jan  3 13:11:00.559: INFO: Pod pod-configmaps-bbe83ac6-5e4d-4eed-b49d-9993b19c51fe no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:11:00.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1823" for this suite. 01/03/24 13:11:00.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:11:00.626
Jan  3 13:11:00.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubelet-test 01/03/24 13:11:00.628
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:11:00.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:11:00.727
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan  3 13:11:00.790: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs9f64132b-5a95-4d5d-8c35-ad926342187a" in namespace "kubelet-test-7837" to be "running and ready"
Jan  3 13:11:00.819: INFO: Pod "busybox-readonly-fs9f64132b-5a95-4d5d-8c35-ad926342187a": Phase="Pending", Reason="", readiness=false. Elapsed: 29.568969ms
Jan  3 13:11:00.820: INFO: The phase of Pod busybox-readonly-fs9f64132b-5a95-4d5d-8c35-ad926342187a is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:11:02.839: INFO: Pod "busybox-readonly-fs9f64132b-5a95-4d5d-8c35-ad926342187a": Phase="Running", Reason="", readiness=true. Elapsed: 2.04875426s
Jan  3 13:11:02.839: INFO: The phase of Pod busybox-readonly-fs9f64132b-5a95-4d5d-8c35-ad926342187a is Running (Ready = true)
Jan  3 13:11:02.839: INFO: Pod "busybox-readonly-fs9f64132b-5a95-4d5d-8c35-ad926342187a" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  3 13:11:02.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7837" for this suite. 01/03/24 13:11:02.928
------------------------------
• [2.329 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:11:00.626
    Jan  3 13:11:00.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubelet-test 01/03/24 13:11:00.628
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:11:00.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:11:00.727
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan  3 13:11:00.790: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs9f64132b-5a95-4d5d-8c35-ad926342187a" in namespace "kubelet-test-7837" to be "running and ready"
    Jan  3 13:11:00.819: INFO: Pod "busybox-readonly-fs9f64132b-5a95-4d5d-8c35-ad926342187a": Phase="Pending", Reason="", readiness=false. Elapsed: 29.568969ms
    Jan  3 13:11:00.820: INFO: The phase of Pod busybox-readonly-fs9f64132b-5a95-4d5d-8c35-ad926342187a is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:11:02.839: INFO: Pod "busybox-readonly-fs9f64132b-5a95-4d5d-8c35-ad926342187a": Phase="Running", Reason="", readiness=true. Elapsed: 2.04875426s
    Jan  3 13:11:02.839: INFO: The phase of Pod busybox-readonly-fs9f64132b-5a95-4d5d-8c35-ad926342187a is Running (Ready = true)
    Jan  3 13:11:02.839: INFO: Pod "busybox-readonly-fs9f64132b-5a95-4d5d-8c35-ad926342187a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:11:02.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7837" for this suite. 01/03/24 13:11:02.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:11:02.96
Jan  3 13:11:02.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubelet-test 01/03/24 13:11:02.962
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:11:03.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:11:03.042
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan  3 13:11:03.100: INFO: Waiting up to 5m0s for pod "busybox-scheduling-a5fd9ff6-9c7c-4a42-80f5-bae12ab024bc" in namespace "kubelet-test-8747" to be "running and ready"
Jan  3 13:11:03.118: INFO: Pod "busybox-scheduling-a5fd9ff6-9c7c-4a42-80f5-bae12ab024bc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.029531ms
Jan  3 13:11:03.118: INFO: The phase of Pod busybox-scheduling-a5fd9ff6-9c7c-4a42-80f5-bae12ab024bc is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:11:05.139: INFO: Pod "busybox-scheduling-a5fd9ff6-9c7c-4a42-80f5-bae12ab024bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.038676051s
Jan  3 13:11:05.139: INFO: The phase of Pod busybox-scheduling-a5fd9ff6-9c7c-4a42-80f5-bae12ab024bc is Running (Ready = true)
Jan  3 13:11:05.139: INFO: Pod "busybox-scheduling-a5fd9ff6-9c7c-4a42-80f5-bae12ab024bc" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  3 13:11:05.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8747" for this suite. 01/03/24 13:11:05.228
------------------------------
• [2.295 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:11:02.96
    Jan  3 13:11:02.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubelet-test 01/03/24 13:11:02.962
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:11:03.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:11:03.042
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan  3 13:11:03.100: INFO: Waiting up to 5m0s for pod "busybox-scheduling-a5fd9ff6-9c7c-4a42-80f5-bae12ab024bc" in namespace "kubelet-test-8747" to be "running and ready"
    Jan  3 13:11:03.118: INFO: Pod "busybox-scheduling-a5fd9ff6-9c7c-4a42-80f5-bae12ab024bc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.029531ms
    Jan  3 13:11:03.118: INFO: The phase of Pod busybox-scheduling-a5fd9ff6-9c7c-4a42-80f5-bae12ab024bc is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:11:05.139: INFO: Pod "busybox-scheduling-a5fd9ff6-9c7c-4a42-80f5-bae12ab024bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.038676051s
    Jan  3 13:11:05.139: INFO: The phase of Pod busybox-scheduling-a5fd9ff6-9c7c-4a42-80f5-bae12ab024bc is Running (Ready = true)
    Jan  3 13:11:05.139: INFO: Pod "busybox-scheduling-a5fd9ff6-9c7c-4a42-80f5-bae12ab024bc" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:11:05.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8747" for this suite. 01/03/24 13:11:05.228
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:11:05.256
Jan  3 13:11:05.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 13:11:05.259
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:11:05.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:11:05.34
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9242 01/03/24 13:11:05.367
STEP: changing the ExternalName service to type=ClusterIP 01/03/24 13:11:05.386
STEP: creating replication controller externalname-service in namespace services-9242 01/03/24 13:11:05.438
I0103 13:11:05.460884      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9242, replica count: 2
I0103 13:11:08.512437      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 13:11:08.512: INFO: Creating new exec pod
Jan  3 13:11:08.535: INFO: Waiting up to 5m0s for pod "execpodmczj9" in namespace "services-9242" to be "running"
Jan  3 13:11:08.553: INFO: Pod "execpodmczj9": Phase="Pending", Reason="", readiness=false. Elapsed: 17.993313ms
Jan  3 13:11:10.573: INFO: Pod "execpodmczj9": Phase="Running", Reason="", readiness=true. Elapsed: 2.037133865s
Jan  3 13:11:10.573: INFO: Pod "execpodmczj9" satisfied condition "running"
Jan  3 13:11:11.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9242 exec execpodmczj9 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan  3 13:11:12.087: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  3 13:11:12.087: INFO: stdout: ""
Jan  3 13:11:12.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9242 exec execpodmczj9 -- /bin/sh -x -c nc -v -z -w 2 10.233.40.188 80'
Jan  3 13:11:12.555: INFO: stderr: "+ nc -v -z -w 2 10.233.40.188 80\nConnection to 10.233.40.188 80 port [tcp/http] succeeded!\n"
Jan  3 13:11:12.555: INFO: stdout: ""
Jan  3 13:11:12.555: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 13:11:12.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9242" for this suite. 01/03/24 13:11:12.64
------------------------------
• [SLOW TEST] [7.408 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:11:05.256
    Jan  3 13:11:05.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 13:11:05.259
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:11:05.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:11:05.34
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9242 01/03/24 13:11:05.367
    STEP: changing the ExternalName service to type=ClusterIP 01/03/24 13:11:05.386
    STEP: creating replication controller externalname-service in namespace services-9242 01/03/24 13:11:05.438
    I0103 13:11:05.460884      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9242, replica count: 2
    I0103 13:11:08.512437      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 13:11:08.512: INFO: Creating new exec pod
    Jan  3 13:11:08.535: INFO: Waiting up to 5m0s for pod "execpodmczj9" in namespace "services-9242" to be "running"
    Jan  3 13:11:08.553: INFO: Pod "execpodmczj9": Phase="Pending", Reason="", readiness=false. Elapsed: 17.993313ms
    Jan  3 13:11:10.573: INFO: Pod "execpodmczj9": Phase="Running", Reason="", readiness=true. Elapsed: 2.037133865s
    Jan  3 13:11:10.573: INFO: Pod "execpodmczj9" satisfied condition "running"
    Jan  3 13:11:11.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9242 exec execpodmczj9 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan  3 13:11:12.087: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan  3 13:11:12.087: INFO: stdout: ""
    Jan  3 13:11:12.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-9242 exec execpodmczj9 -- /bin/sh -x -c nc -v -z -w 2 10.233.40.188 80'
    Jan  3 13:11:12.555: INFO: stderr: "+ nc -v -z -w 2 10.233.40.188 80\nConnection to 10.233.40.188 80 port [tcp/http] succeeded!\n"
    Jan  3 13:11:12.555: INFO: stdout: ""
    Jan  3 13:11:12.555: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:11:12.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9242" for this suite. 01/03/24 13:11:12.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:11:12.666
Jan  3 13:11:12.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename statefulset 01/03/24 13:11:12.669
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:11:12.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:11:12.744
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4497 01/03/24 13:11:12.771
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 01/03/24 13:11:12.789
STEP: Creating pod with conflicting port in namespace statefulset-4497 01/03/24 13:11:12.809
STEP: Waiting until pod test-pod will start running in namespace statefulset-4497 01/03/24 13:11:12.835
Jan  3 13:11:12.835: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4497" to be "running"
Jan  3 13:11:12.856: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.501167ms
Jan  3 13:11:14.882: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.04646052s
Jan  3 13:11:14.882: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-4497 01/03/24 13:11:14.882
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4497 01/03/24 13:11:14.902
Jan  3 13:11:14.934: INFO: Observed stateful pod in namespace: statefulset-4497, name: ss-0, uid: 6871a2da-d5ba-4580-b146-357a7a1e9e1a, status phase: Pending. Waiting for statefulset controller to delete.
Jan  3 13:11:14.973: INFO: Observed stateful pod in namespace: statefulset-4497, name: ss-0, uid: 6871a2da-d5ba-4580-b146-357a7a1e9e1a, status phase: Failed. Waiting for statefulset controller to delete.
Jan  3 13:11:15.008: INFO: Observed stateful pod in namespace: statefulset-4497, name: ss-0, uid: 6871a2da-d5ba-4580-b146-357a7a1e9e1a, status phase: Failed. Waiting for statefulset controller to delete.
Jan  3 13:11:15.011: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4497
STEP: Removing pod with conflicting port in namespace statefulset-4497 01/03/24 13:11:15.011
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4497 and will be in running state 01/03/24 13:11:15.043
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  3 13:11:17.078: INFO: Deleting all statefulset in ns statefulset-4497
Jan  3 13:11:17.096: INFO: Scaling statefulset ss to 0
Jan  3 13:11:27.175: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 13:11:27.195: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  3 13:11:27.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4497" for this suite. 01/03/24 13:11:27.288
------------------------------
• [SLOW TEST] [14.649 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:11:12.666
    Jan  3 13:11:12.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename statefulset 01/03/24 13:11:12.669
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:11:12.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:11:12.744
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4497 01/03/24 13:11:12.771
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 01/03/24 13:11:12.789
    STEP: Creating pod with conflicting port in namespace statefulset-4497 01/03/24 13:11:12.809
    STEP: Waiting until pod test-pod will start running in namespace statefulset-4497 01/03/24 13:11:12.835
    Jan  3 13:11:12.835: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4497" to be "running"
    Jan  3 13:11:12.856: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.501167ms
    Jan  3 13:11:14.882: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.04646052s
    Jan  3 13:11:14.882: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-4497 01/03/24 13:11:14.882
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4497 01/03/24 13:11:14.902
    Jan  3 13:11:14.934: INFO: Observed stateful pod in namespace: statefulset-4497, name: ss-0, uid: 6871a2da-d5ba-4580-b146-357a7a1e9e1a, status phase: Pending. Waiting for statefulset controller to delete.
    Jan  3 13:11:14.973: INFO: Observed stateful pod in namespace: statefulset-4497, name: ss-0, uid: 6871a2da-d5ba-4580-b146-357a7a1e9e1a, status phase: Failed. Waiting for statefulset controller to delete.
    Jan  3 13:11:15.008: INFO: Observed stateful pod in namespace: statefulset-4497, name: ss-0, uid: 6871a2da-d5ba-4580-b146-357a7a1e9e1a, status phase: Failed. Waiting for statefulset controller to delete.
    Jan  3 13:11:15.011: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4497
    STEP: Removing pod with conflicting port in namespace statefulset-4497 01/03/24 13:11:15.011
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4497 and will be in running state 01/03/24 13:11:15.043
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  3 13:11:17.078: INFO: Deleting all statefulset in ns statefulset-4497
    Jan  3 13:11:17.096: INFO: Scaling statefulset ss to 0
    Jan  3 13:11:27.175: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 13:11:27.195: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:11:27.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4497" for this suite. 01/03/24 13:11:27.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:11:27.327
Jan  3 13:11:27.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename cronjob 01/03/24 13:11:27.328
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:11:27.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:11:27.408
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/03/24 13:11:27.435
STEP: Ensuring a job is scheduled 01/03/24 13:11:27.457
STEP: Ensuring exactly one is scheduled 01/03/24 13:12:01.481
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/03/24 13:12:01.499
STEP: Ensuring no more jobs are scheduled 01/03/24 13:12:01.517
STEP: Removing cronjob 01/03/24 13:17:01.554
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  3 13:17:01.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1221" for this suite. 01/03/24 13:17:01.608
------------------------------
• [SLOW TEST] [334.321 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:11:27.327
    Jan  3 13:11:27.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename cronjob 01/03/24 13:11:27.328
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:11:27.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:11:27.408
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/03/24 13:11:27.435
    STEP: Ensuring a job is scheduled 01/03/24 13:11:27.457
    STEP: Ensuring exactly one is scheduled 01/03/24 13:12:01.481
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/03/24 13:12:01.499
    STEP: Ensuring no more jobs are scheduled 01/03/24 13:12:01.517
    STEP: Removing cronjob 01/03/24 13:17:01.554
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:17:01.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1221" for this suite. 01/03/24 13:17:01.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:17:01.652
Jan  3 13:17:01.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename resourcequota 01/03/24 13:17:01.655
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:01.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:01.735
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 01/03/24 13:17:01.763
STEP: Ensuring ResourceQuota status is calculated 01/03/24 13:17:01.782
STEP: Creating a ResourceQuota with not terminating scope 01/03/24 13:17:03.8
STEP: Ensuring ResourceQuota status is calculated 01/03/24 13:17:03.819
STEP: Creating a long running pod 01/03/24 13:17:05.838
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/03/24 13:17:05.875
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/03/24 13:17:07.897
STEP: Deleting the pod 01/03/24 13:17:09.916
STEP: Ensuring resource quota status released the pod usage 01/03/24 13:17:09.955
STEP: Creating a terminating pod 01/03/24 13:17:11.976
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/03/24 13:17:12.003
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/03/24 13:17:14.025
STEP: Deleting the pod 01/03/24 13:17:16.045
STEP: Ensuring resource quota status released the pod usage 01/03/24 13:17:16.096
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  3 13:17:18.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5002" for this suite. 01/03/24 13:17:18.151
------------------------------
• [SLOW TEST] [16.537 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:17:01.652
    Jan  3 13:17:01.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename resourcequota 01/03/24 13:17:01.655
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:01.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:01.735
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 01/03/24 13:17:01.763
    STEP: Ensuring ResourceQuota status is calculated 01/03/24 13:17:01.782
    STEP: Creating a ResourceQuota with not terminating scope 01/03/24 13:17:03.8
    STEP: Ensuring ResourceQuota status is calculated 01/03/24 13:17:03.819
    STEP: Creating a long running pod 01/03/24 13:17:05.838
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/03/24 13:17:05.875
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/03/24 13:17:07.897
    STEP: Deleting the pod 01/03/24 13:17:09.916
    STEP: Ensuring resource quota status released the pod usage 01/03/24 13:17:09.955
    STEP: Creating a terminating pod 01/03/24 13:17:11.976
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/03/24 13:17:12.003
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/03/24 13:17:14.025
    STEP: Deleting the pod 01/03/24 13:17:16.045
    STEP: Ensuring resource quota status released the pod usage 01/03/24 13:17:16.096
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:17:18.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5002" for this suite. 01/03/24 13:17:18.151
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:17:18.19
Jan  3 13:17:18.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename svcaccounts 01/03/24 13:17:18.192
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:18.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:18.274
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 01/03/24 13:17:18.302
STEP: watching for the ServiceAccount to be added 01/03/24 13:17:18.342
STEP: patching the ServiceAccount 01/03/24 13:17:18.356
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/03/24 13:17:18.376
STEP: deleting the ServiceAccount 01/03/24 13:17:18.394
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  3 13:17:18.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6357" for this suite. 01/03/24 13:17:18.456
------------------------------
• [0.292 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:17:18.19
    Jan  3 13:17:18.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename svcaccounts 01/03/24 13:17:18.192
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:18.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:18.274
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 01/03/24 13:17:18.302
    STEP: watching for the ServiceAccount to be added 01/03/24 13:17:18.342
    STEP: patching the ServiceAccount 01/03/24 13:17:18.356
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/03/24 13:17:18.376
    STEP: deleting the ServiceAccount 01/03/24 13:17:18.394
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:17:18.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6357" for this suite. 01/03/24 13:17:18.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:17:18.485
Jan  3 13:17:18.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename proxy 01/03/24 13:17:18.488
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:18.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:18.569
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan  3 13:17:18.597: INFO: Creating pod...
Jan  3 13:17:18.622: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8152" to be "running"
Jan  3 13:17:18.640: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 17.489055ms
Jan  3 13:17:20.658: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.036086089s
Jan  3 13:17:20.658: INFO: Pod "agnhost" satisfied condition "running"
Jan  3 13:17:20.658: INFO: Creating service...
Jan  3 13:17:20.690: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/pods/agnhost/proxy?method=DELETE
Jan  3 13:17:20.787: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  3 13:17:20.787: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/pods/agnhost/proxy?method=OPTIONS
Jan  3 13:17:20.819: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  3 13:17:20.819: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/pods/agnhost/proxy?method=PATCH
Jan  3 13:17:20.850: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  3 13:17:20.850: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/pods/agnhost/proxy?method=POST
Jan  3 13:17:20.884: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  3 13:17:20.884: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/pods/agnhost/proxy?method=PUT
Jan  3 13:17:20.916: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  3 13:17:20.916: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/services/e2e-proxy-test-service/proxy?method=DELETE
Jan  3 13:17:20.953: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  3 13:17:20.953: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan  3 13:17:20.995: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  3 13:17:20.995: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/services/e2e-proxy-test-service/proxy?method=PATCH
Jan  3 13:17:21.032: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  3 13:17:21.032: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/services/e2e-proxy-test-service/proxy?method=POST
Jan  3 13:17:21.066: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  3 13:17:21.067: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/services/e2e-proxy-test-service/proxy?method=PUT
Jan  3 13:17:21.100: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  3 13:17:21.100: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/pods/agnhost/proxy?method=GET
Jan  3 13:17:21.118: INFO: http.Client request:GET StatusCode:301
Jan  3 13:17:21.118: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/services/e2e-proxy-test-service/proxy?method=GET
Jan  3 13:17:21.139: INFO: http.Client request:GET StatusCode:301
Jan  3 13:17:21.140: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/pods/agnhost/proxy?method=HEAD
Jan  3 13:17:21.157: INFO: http.Client request:HEAD StatusCode:301
Jan  3 13:17:21.157: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/services/e2e-proxy-test-service/proxy?method=HEAD
Jan  3 13:17:21.182: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan  3 13:17:21.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8152" for this suite. 01/03/24 13:17:21.215
------------------------------
• [2.756 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:17:18.485
    Jan  3 13:17:18.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename proxy 01/03/24 13:17:18.488
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:18.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:18.569
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan  3 13:17:18.597: INFO: Creating pod...
    Jan  3 13:17:18.622: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8152" to be "running"
    Jan  3 13:17:18.640: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 17.489055ms
    Jan  3 13:17:20.658: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.036086089s
    Jan  3 13:17:20.658: INFO: Pod "agnhost" satisfied condition "running"
    Jan  3 13:17:20.658: INFO: Creating service...
    Jan  3 13:17:20.690: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/pods/agnhost/proxy?method=DELETE
    Jan  3 13:17:20.787: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  3 13:17:20.787: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/pods/agnhost/proxy?method=OPTIONS
    Jan  3 13:17:20.819: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  3 13:17:20.819: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/pods/agnhost/proxy?method=PATCH
    Jan  3 13:17:20.850: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  3 13:17:20.850: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/pods/agnhost/proxy?method=POST
    Jan  3 13:17:20.884: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  3 13:17:20.884: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/pods/agnhost/proxy?method=PUT
    Jan  3 13:17:20.916: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  3 13:17:20.916: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan  3 13:17:20.953: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  3 13:17:20.953: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan  3 13:17:20.995: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  3 13:17:20.995: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan  3 13:17:21.032: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  3 13:17:21.032: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/services/e2e-proxy-test-service/proxy?method=POST
    Jan  3 13:17:21.066: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  3 13:17:21.067: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/services/e2e-proxy-test-service/proxy?method=PUT
    Jan  3 13:17:21.100: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  3 13:17:21.100: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/pods/agnhost/proxy?method=GET
    Jan  3 13:17:21.118: INFO: http.Client request:GET StatusCode:301
    Jan  3 13:17:21.118: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/services/e2e-proxy-test-service/proxy?method=GET
    Jan  3 13:17:21.139: INFO: http.Client request:GET StatusCode:301
    Jan  3 13:17:21.140: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/pods/agnhost/proxy?method=HEAD
    Jan  3 13:17:21.157: INFO: http.Client request:HEAD StatusCode:301
    Jan  3 13:17:21.157: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8152/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan  3 13:17:21.182: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:17:21.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8152" for this suite. 01/03/24 13:17:21.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:17:21.244
Jan  3 13:17:21.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename prestop 01/03/24 13:17:21.246
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:21.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:21.325
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-4046 01/03/24 13:17:21.353
STEP: Waiting for pods to come up. 01/03/24 13:17:21.381
Jan  3 13:17:21.381: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4046" to be "running"
Jan  3 13:17:21.399: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 17.392931ms
Jan  3 13:17:23.423: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041882282s
Jan  3 13:17:25.419: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.038120106s
Jan  3 13:17:25.419: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-4046 01/03/24 13:17:25.438
Jan  3 13:17:25.458: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4046" to be "running"
Jan  3 13:17:25.476: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 17.321769ms
Jan  3 13:17:27.494: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035780493s
Jan  3 13:17:29.498: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.039252695s
Jan  3 13:17:29.498: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/03/24 13:17:29.498
Jan  3 13:17:34.613: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/03/24 13:17:34.614
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jan  3 13:17:34.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-4046" for this suite. 01/03/24 13:17:34.681
------------------------------
• [SLOW TEST] [13.478 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:17:21.244
    Jan  3 13:17:21.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename prestop 01/03/24 13:17:21.246
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:21.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:21.325
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-4046 01/03/24 13:17:21.353
    STEP: Waiting for pods to come up. 01/03/24 13:17:21.381
    Jan  3 13:17:21.381: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4046" to be "running"
    Jan  3 13:17:21.399: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 17.392931ms
    Jan  3 13:17:23.423: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041882282s
    Jan  3 13:17:25.419: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.038120106s
    Jan  3 13:17:25.419: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-4046 01/03/24 13:17:25.438
    Jan  3 13:17:25.458: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4046" to be "running"
    Jan  3 13:17:25.476: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 17.321769ms
    Jan  3 13:17:27.494: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035780493s
    Jan  3 13:17:29.498: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.039252695s
    Jan  3 13:17:29.498: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/03/24 13:17:29.498
    Jan  3 13:17:34.613: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/03/24 13:17:34.614
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:17:34.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-4046" for this suite. 01/03/24 13:17:34.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:17:34.724
Jan  3 13:17:34.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 13:17:34.726
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:34.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:34.812
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/03/24 13:17:34.84
Jan  3 13:17:34.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-690 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan  3 13:17:35.002: INFO: stderr: ""
Jan  3 13:17:35.002: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/03/24 13:17:35.002
STEP: verifying the pod e2e-test-httpd-pod was created 01/03/24 13:17:40.054
Jan  3 13:17:40.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-690 get pod e2e-test-httpd-pod -o json'
Jan  3 13:17:40.231: INFO: stderr: ""
Jan  3 13:17:40.231: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"c81ef7e90979fbd979c2170e731b24e76892d1a4269111f434835e78a0d22b85\",\n            \"cni.projectcalico.org/podIP\": \"10.221.146.103/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.221.146.103/32\"\n        },\n        \"creationTimestamp\": \"2024-01-03T13:17:34Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-690\",\n        \"resourceVersion\": \"33981433887\",\n        \"uid\": \"d8f52d89-f89c-4242-a9d5-7893ecc551fa\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-mxnnj\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"jb-1-26-np-64kerjapxk\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-mxnnj\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-03T13:17:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-03T13:17:36Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-03T13:17:36Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-03T13:17:34Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://fe94780fb21c691c122412488f945e98d9ed60515ad54ffccd4981f1cd6a6e1b\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2024-01-03T13:17:36Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"185.132.46.116\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.221.146.103\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.221.146.103\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2024-01-03T13:17:35Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/03/24 13:17:40.231
Jan  3 13:17:40.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-690 replace -f -'
Jan  3 13:17:41.230: INFO: stderr: ""
Jan  3 13:17:41.230: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/03/24 13:17:41.23
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jan  3 13:17:41.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-690 delete pods e2e-test-httpd-pod'
Jan  3 13:17:43.604: INFO: stderr: ""
Jan  3 13:17:43.604: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 13:17:43.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-690" for this suite. 01/03/24 13:17:43.635
------------------------------
• [SLOW TEST] [8.937 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:17:34.724
    Jan  3 13:17:34.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 13:17:34.726
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:34.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:34.812
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/03/24 13:17:34.84
    Jan  3 13:17:34.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-690 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan  3 13:17:35.002: INFO: stderr: ""
    Jan  3 13:17:35.002: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/03/24 13:17:35.002
    STEP: verifying the pod e2e-test-httpd-pod was created 01/03/24 13:17:40.054
    Jan  3 13:17:40.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-690 get pod e2e-test-httpd-pod -o json'
    Jan  3 13:17:40.231: INFO: stderr: ""
    Jan  3 13:17:40.231: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"c81ef7e90979fbd979c2170e731b24e76892d1a4269111f434835e78a0d22b85\",\n            \"cni.projectcalico.org/podIP\": \"10.221.146.103/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.221.146.103/32\"\n        },\n        \"creationTimestamp\": \"2024-01-03T13:17:34Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-690\",\n        \"resourceVersion\": \"33981433887\",\n        \"uid\": \"d8f52d89-f89c-4242-a9d5-7893ecc551fa\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-mxnnj\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"jb-1-26-np-64kerjapxk\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-mxnnj\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-03T13:17:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-03T13:17:36Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-03T13:17:36Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-03T13:17:34Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://fe94780fb21c691c122412488f945e98d9ed60515ad54ffccd4981f1cd6a6e1b\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2024-01-03T13:17:36Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"185.132.46.116\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.221.146.103\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.221.146.103\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2024-01-03T13:17:35Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/03/24 13:17:40.231
    Jan  3 13:17:40.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-690 replace -f -'
    Jan  3 13:17:41.230: INFO: stderr: ""
    Jan  3 13:17:41.230: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/03/24 13:17:41.23
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jan  3 13:17:41.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-690 delete pods e2e-test-httpd-pod'
    Jan  3 13:17:43.604: INFO: stderr: ""
    Jan  3 13:17:43.604: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:17:43.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-690" for this suite. 01/03/24 13:17:43.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:17:43.662
Jan  3 13:17:43.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename endpointslicemirroring 01/03/24 13:17:43.664
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:43.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:43.744
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/03/24 13:17:43.799
Jan  3 13:17:43.835: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/03/24 13:17:45.857
STEP: mirroring deletion of a custom Endpoint 01/03/24 13:17:45.894
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jan  3 13:17:45.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-5601" for this suite. 01/03/24 13:17:45.968
------------------------------
• [2.327 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:17:43.662
    Jan  3 13:17:43.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename endpointslicemirroring 01/03/24 13:17:43.664
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:43.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:43.744
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/03/24 13:17:43.799
    Jan  3 13:17:43.835: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/03/24 13:17:45.857
    STEP: mirroring deletion of a custom Endpoint 01/03/24 13:17:45.894
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:17:45.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-5601" for this suite. 01/03/24 13:17:45.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:17:45.994
Jan  3 13:17:45.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 13:17:45.997
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:46.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:46.09
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 01/03/24 13:17:46.119
Jan  3 13:17:46.149: INFO: Waiting up to 5m0s for pod "downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984" in namespace "downward-api-3457" to be "Succeeded or Failed"
Jan  3 13:17:46.170: INFO: Pod "downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984": Phase="Pending", Reason="", readiness=false. Elapsed: 21.402593ms
Jan  3 13:17:48.192: INFO: Pod "downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984": Phase="Running", Reason="", readiness=true. Elapsed: 2.04344548s
Jan  3 13:17:50.191: INFO: Pod "downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984": Phase="Running", Reason="", readiness=false. Elapsed: 4.04242825s
Jan  3 13:17:52.198: INFO: Pod "downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04920664s
STEP: Saw pod success 01/03/24 13:17:52.198
Jan  3 13:17:52.198: INFO: Pod "downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984" satisfied condition "Succeeded or Failed"
Jan  3 13:17:52.221: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984 container dapi-container: <nil>
STEP: delete the pod 01/03/24 13:17:52.377
Jan  3 13:17:52.417: INFO: Waiting for pod downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984 to disappear
Jan  3 13:17:52.435: INFO: Pod downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  3 13:17:52.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3457" for this suite. 01/03/24 13:17:52.466
------------------------------
• [SLOW TEST] [6.496 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:17:45.994
    Jan  3 13:17:45.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 13:17:45.997
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:46.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:46.09
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 01/03/24 13:17:46.119
    Jan  3 13:17:46.149: INFO: Waiting up to 5m0s for pod "downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984" in namespace "downward-api-3457" to be "Succeeded or Failed"
    Jan  3 13:17:46.170: INFO: Pod "downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984": Phase="Pending", Reason="", readiness=false. Elapsed: 21.402593ms
    Jan  3 13:17:48.192: INFO: Pod "downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984": Phase="Running", Reason="", readiness=true. Elapsed: 2.04344548s
    Jan  3 13:17:50.191: INFO: Pod "downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984": Phase="Running", Reason="", readiness=false. Elapsed: 4.04242825s
    Jan  3 13:17:52.198: INFO: Pod "downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04920664s
    STEP: Saw pod success 01/03/24 13:17:52.198
    Jan  3 13:17:52.198: INFO: Pod "downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984" satisfied condition "Succeeded or Failed"
    Jan  3 13:17:52.221: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984 container dapi-container: <nil>
    STEP: delete the pod 01/03/24 13:17:52.377
    Jan  3 13:17:52.417: INFO: Waiting for pod downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984 to disappear
    Jan  3 13:17:52.435: INFO: Pod downward-api-c35bf808-d803-4f0a-a68d-2919e7d67984 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:17:52.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3457" for this suite. 01/03/24 13:17:52.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:17:52.494
Jan  3 13:17:52.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename dns 01/03/24 13:17:52.496
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:52.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:52.578
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/03/24 13:17:52.606
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/03/24 13:17:52.606
STEP: creating a pod to probe DNS 01/03/24 13:17:52.606
STEP: submitting the pod to kubernetes 01/03/24 13:17:52.607
Jan  3 13:17:52.634: INFO: Waiting up to 15m0s for pod "dns-test-0f0bb7e4-4164-4152-9ed3-bc6846d6a724" in namespace "dns-3598" to be "running"
Jan  3 13:17:52.654: INFO: Pod "dns-test-0f0bb7e4-4164-4152-9ed3-bc6846d6a724": Phase="Pending", Reason="", readiness=false. Elapsed: 20.63156ms
Jan  3 13:17:54.676: INFO: Pod "dns-test-0f0bb7e4-4164-4152-9ed3-bc6846d6a724": Phase="Running", Reason="", readiness=true. Elapsed: 2.042709134s
Jan  3 13:17:54.676: INFO: Pod "dns-test-0f0bb7e4-4164-4152-9ed3-bc6846d6a724" satisfied condition "running"
STEP: retrieving the pod 01/03/24 13:17:54.676
STEP: looking for the results for each expected name from probers 01/03/24 13:17:54.695
Jan  3 13:17:54.888: INFO: DNS probes using dns-3598/dns-test-0f0bb7e4-4164-4152-9ed3-bc6846d6a724 succeeded

STEP: deleting the pod 01/03/24 13:17:54.888
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  3 13:17:54.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3598" for this suite. 01/03/24 13:17:54.953
------------------------------
• [2.488 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:17:52.494
    Jan  3 13:17:52.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename dns 01/03/24 13:17:52.496
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:52.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:52.578
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/03/24 13:17:52.606
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/03/24 13:17:52.606
    STEP: creating a pod to probe DNS 01/03/24 13:17:52.606
    STEP: submitting the pod to kubernetes 01/03/24 13:17:52.607
    Jan  3 13:17:52.634: INFO: Waiting up to 15m0s for pod "dns-test-0f0bb7e4-4164-4152-9ed3-bc6846d6a724" in namespace "dns-3598" to be "running"
    Jan  3 13:17:52.654: INFO: Pod "dns-test-0f0bb7e4-4164-4152-9ed3-bc6846d6a724": Phase="Pending", Reason="", readiness=false. Elapsed: 20.63156ms
    Jan  3 13:17:54.676: INFO: Pod "dns-test-0f0bb7e4-4164-4152-9ed3-bc6846d6a724": Phase="Running", Reason="", readiness=true. Elapsed: 2.042709134s
    Jan  3 13:17:54.676: INFO: Pod "dns-test-0f0bb7e4-4164-4152-9ed3-bc6846d6a724" satisfied condition "running"
    STEP: retrieving the pod 01/03/24 13:17:54.676
    STEP: looking for the results for each expected name from probers 01/03/24 13:17:54.695
    Jan  3 13:17:54.888: INFO: DNS probes using dns-3598/dns-test-0f0bb7e4-4164-4152-9ed3-bc6846d6a724 succeeded

    STEP: deleting the pod 01/03/24 13:17:54.888
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:17:54.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3598" for this suite. 01/03/24 13:17:54.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:17:54.984
Jan  3 13:17:54.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename hostport 01/03/24 13:17:54.986
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:55.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:55.065
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/03/24 13:17:55.112
Jan  3 13:17:55.137: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-821" to be "running and ready"
Jan  3 13:17:55.164: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 27.133521ms
Jan  3 13:17:55.164: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:17:57.185: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048205205s
Jan  3 13:17:57.185: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:17:59.183: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.046290445s
Jan  3 13:17:59.183: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  3 13:17:59.183: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 85.215.218.90 on the node which pod1 resides and expect scheduled 01/03/24 13:17:59.183
Jan  3 13:17:59.204: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-821" to be "running and ready"
Jan  3 13:17:59.222: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.997685ms
Jan  3 13:17:59.222: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:18:01.243: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038573404s
Jan  3 13:18:01.243: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:18:03.259: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.054686577s
Jan  3 13:18:03.259: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  3 13:18:03.259: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 85.215.218.90 but use UDP protocol on the node which pod2 resides 01/03/24 13:18:03.259
Jan  3 13:18:03.279: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-821" to be "running and ready"
Jan  3 13:18:03.297: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.763964ms
Jan  3 13:18:03.297: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:18:05.317: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037253246s
Jan  3 13:18:05.317: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:18:07.316: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.036804915s
Jan  3 13:18:07.316: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan  3 13:18:07.316: INFO: Pod "pod3" satisfied condition "running and ready"
Jan  3 13:18:07.337: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-821" to be "running and ready"
Jan  3 13:18:07.355: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 17.728976ms
Jan  3 13:18:07.355: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:18:09.375: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.03860173s
Jan  3 13:18:09.375: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan  3 13:18:09.375: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/03/24 13:18:09.393
Jan  3 13:18:09.393: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 85.215.218.90 http://127.0.0.1:54323/hostname] Namespace:hostport-821 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 13:18:09.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 13:18:09.395: INFO: ExecWithOptions: Clientset creation
Jan  3 13:18:09.395: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-821/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+85.215.218.90+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 85.215.218.90, port: 54323 01/03/24 13:18:10.137
Jan  3 13:18:10.137: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://85.215.218.90:54323/hostname] Namespace:hostport-821 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 13:18:10.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 13:18:10.139: INFO: ExecWithOptions: Clientset creation
Jan  3 13:18:10.140: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-821/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F85.215.218.90%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 85.215.218.90, port: 54323 UDP 01/03/24 13:18:10.461
Jan  3 13:18:10.461: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 85.215.218.90 54323] Namespace:hostport-821 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 13:18:10.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 13:18:10.462: INFO: ExecWithOptions: Clientset creation
Jan  3 13:18:10.462: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-821/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+85.215.218.90+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jan  3 13:18:15.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-821" for this suite. 01/03/24 13:18:15.847
------------------------------
• [SLOW TEST] [20.891 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:17:54.984
    Jan  3 13:17:54.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename hostport 01/03/24 13:17:54.986
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:17:55.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:17:55.065
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/03/24 13:17:55.112
    Jan  3 13:17:55.137: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-821" to be "running and ready"
    Jan  3 13:17:55.164: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 27.133521ms
    Jan  3 13:17:55.164: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:17:57.185: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048205205s
    Jan  3 13:17:57.185: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:17:59.183: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.046290445s
    Jan  3 13:17:59.183: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  3 13:17:59.183: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 85.215.218.90 on the node which pod1 resides and expect scheduled 01/03/24 13:17:59.183
    Jan  3 13:17:59.204: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-821" to be "running and ready"
    Jan  3 13:17:59.222: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.997685ms
    Jan  3 13:17:59.222: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:18:01.243: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038573404s
    Jan  3 13:18:01.243: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:18:03.259: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.054686577s
    Jan  3 13:18:03.259: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  3 13:18:03.259: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 85.215.218.90 but use UDP protocol on the node which pod2 resides 01/03/24 13:18:03.259
    Jan  3 13:18:03.279: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-821" to be "running and ready"
    Jan  3 13:18:03.297: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.763964ms
    Jan  3 13:18:03.297: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:18:05.317: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037253246s
    Jan  3 13:18:05.317: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:18:07.316: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.036804915s
    Jan  3 13:18:07.316: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan  3 13:18:07.316: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan  3 13:18:07.337: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-821" to be "running and ready"
    Jan  3 13:18:07.355: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 17.728976ms
    Jan  3 13:18:07.355: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:18:09.375: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.03860173s
    Jan  3 13:18:09.375: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan  3 13:18:09.375: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/03/24 13:18:09.393
    Jan  3 13:18:09.393: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 85.215.218.90 http://127.0.0.1:54323/hostname] Namespace:hostport-821 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 13:18:09.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 13:18:09.395: INFO: ExecWithOptions: Clientset creation
    Jan  3 13:18:09.395: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-821/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+85.215.218.90+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 85.215.218.90, port: 54323 01/03/24 13:18:10.137
    Jan  3 13:18:10.137: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://85.215.218.90:54323/hostname] Namespace:hostport-821 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 13:18:10.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 13:18:10.139: INFO: ExecWithOptions: Clientset creation
    Jan  3 13:18:10.140: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-821/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F85.215.218.90%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 85.215.218.90, port: 54323 UDP 01/03/24 13:18:10.461
    Jan  3 13:18:10.461: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 85.215.218.90 54323] Namespace:hostport-821 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 13:18:10.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 13:18:10.462: INFO: ExecWithOptions: Clientset creation
    Jan  3 13:18:10.462: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-821/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+85.215.218.90+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:18:15.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-821" for this suite. 01/03/24 13:18:15.847
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:18:15.876
Jan  3 13:18:15.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename security-context-test 01/03/24 13:18:15.879
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:15.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:15.98
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Jan  3 13:18:16.042: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-a280fff0-2e6c-4e36-8e3e-bad829601ca7" in namespace "security-context-test-6869" to be "Succeeded or Failed"
Jan  3 13:18:16.074: INFO: Pod "busybox-privileged-false-a280fff0-2e6c-4e36-8e3e-bad829601ca7": Phase="Pending", Reason="", readiness=false. Elapsed: 31.598218ms
Jan  3 13:18:18.093: INFO: Pod "busybox-privileged-false-a280fff0-2e6c-4e36-8e3e-bad829601ca7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051130251s
Jan  3 13:18:20.102: INFO: Pod "busybox-privileged-false-a280fff0-2e6c-4e36-8e3e-bad829601ca7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05953085s
Jan  3 13:18:20.102: INFO: Pod "busybox-privileged-false-a280fff0-2e6c-4e36-8e3e-bad829601ca7" satisfied condition "Succeeded or Failed"
Jan  3 13:18:20.167: INFO: Got logs for pod "busybox-privileged-false-a280fff0-2e6c-4e36-8e3e-bad829601ca7": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  3 13:18:20.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6869" for this suite. 01/03/24 13:18:20.197
------------------------------
• [4.348 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:18:15.876
    Jan  3 13:18:15.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename security-context-test 01/03/24 13:18:15.879
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:15.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:15.98
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Jan  3 13:18:16.042: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-a280fff0-2e6c-4e36-8e3e-bad829601ca7" in namespace "security-context-test-6869" to be "Succeeded or Failed"
    Jan  3 13:18:16.074: INFO: Pod "busybox-privileged-false-a280fff0-2e6c-4e36-8e3e-bad829601ca7": Phase="Pending", Reason="", readiness=false. Elapsed: 31.598218ms
    Jan  3 13:18:18.093: INFO: Pod "busybox-privileged-false-a280fff0-2e6c-4e36-8e3e-bad829601ca7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051130251s
    Jan  3 13:18:20.102: INFO: Pod "busybox-privileged-false-a280fff0-2e6c-4e36-8e3e-bad829601ca7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05953085s
    Jan  3 13:18:20.102: INFO: Pod "busybox-privileged-false-a280fff0-2e6c-4e36-8e3e-bad829601ca7" satisfied condition "Succeeded or Failed"
    Jan  3 13:18:20.167: INFO: Got logs for pod "busybox-privileged-false-a280fff0-2e6c-4e36-8e3e-bad829601ca7": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:18:20.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6869" for this suite. 01/03/24 13:18:20.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:18:20.226
Jan  3 13:18:20.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename containers 01/03/24 13:18:20.229
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:20.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:20.311
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 01/03/24 13:18:20.338
Jan  3 13:18:20.364: INFO: Waiting up to 5m0s for pod "client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f" in namespace "containers-9691" to be "Succeeded or Failed"
Jan  3 13:18:20.381: INFO: Pod "client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.293805ms
Jan  3 13:18:22.413: INFO: Pod "client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f": Phase="Running", Reason="", readiness=true. Elapsed: 2.049521578s
Jan  3 13:18:24.401: INFO: Pod "client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f": Phase="Running", Reason="", readiness=false. Elapsed: 4.037464417s
Jan  3 13:18:26.403: INFO: Pod "client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039782222s
STEP: Saw pod success 01/03/24 13:18:26.404
Jan  3 13:18:26.404: INFO: Pod "client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f" satisfied condition "Succeeded or Failed"
Jan  3 13:18:26.422: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f container agnhost-container: <nil>
STEP: delete the pod 01/03/24 13:18:26.461
Jan  3 13:18:26.511: INFO: Waiting for pod client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f to disappear
Jan  3 13:18:26.530: INFO: Pod client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan  3 13:18:26.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9691" for this suite. 01/03/24 13:18:26.561
------------------------------
• [SLOW TEST] [6.362 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:18:20.226
    Jan  3 13:18:20.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename containers 01/03/24 13:18:20.229
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:20.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:20.311
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 01/03/24 13:18:20.338
    Jan  3 13:18:20.364: INFO: Waiting up to 5m0s for pod "client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f" in namespace "containers-9691" to be "Succeeded or Failed"
    Jan  3 13:18:20.381: INFO: Pod "client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.293805ms
    Jan  3 13:18:22.413: INFO: Pod "client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f": Phase="Running", Reason="", readiness=true. Elapsed: 2.049521578s
    Jan  3 13:18:24.401: INFO: Pod "client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f": Phase="Running", Reason="", readiness=false. Elapsed: 4.037464417s
    Jan  3 13:18:26.403: INFO: Pod "client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039782222s
    STEP: Saw pod success 01/03/24 13:18:26.404
    Jan  3 13:18:26.404: INFO: Pod "client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f" satisfied condition "Succeeded or Failed"
    Jan  3 13:18:26.422: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 13:18:26.461
    Jan  3 13:18:26.511: INFO: Waiting for pod client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f to disappear
    Jan  3 13:18:26.530: INFO: Pod client-containers-1ec6054b-e266-41c6-a66a-7b5286eb108f no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:18:26.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9691" for this suite. 01/03/24 13:18:26.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:18:26.597
Jan  3 13:18:26.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 13:18:26.599
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:26.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:26.695
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-76424b76-c27e-4799-af29-419cfe2df69d 01/03/24 13:18:26.723
STEP: Creating a pod to test consume configMaps 01/03/24 13:18:26.741
Jan  3 13:18:26.767: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930" in namespace "projected-3581" to be "Succeeded or Failed"
Jan  3 13:18:26.793: INFO: Pod "pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930": Phase="Pending", Reason="", readiness=false. Elapsed: 26.236364ms
Jan  3 13:18:28.813: INFO: Pod "pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046296879s
Jan  3 13:18:30.814: INFO: Pod "pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046894453s
STEP: Saw pod success 01/03/24 13:18:30.814
Jan  3 13:18:30.814: INFO: Pod "pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930" satisfied condition "Succeeded or Failed"
Jan  3 13:18:30.833: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930 container agnhost-container: <nil>
STEP: delete the pod 01/03/24 13:18:30.882
Jan  3 13:18:30.919: INFO: Waiting for pod pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930 to disappear
Jan  3 13:18:30.936: INFO: Pod pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  3 13:18:30.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3581" for this suite. 01/03/24 13:18:30.968
------------------------------
• [4.412 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:18:26.597
    Jan  3 13:18:26.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 13:18:26.599
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:26.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:26.695
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-76424b76-c27e-4799-af29-419cfe2df69d 01/03/24 13:18:26.723
    STEP: Creating a pod to test consume configMaps 01/03/24 13:18:26.741
    Jan  3 13:18:26.767: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930" in namespace "projected-3581" to be "Succeeded or Failed"
    Jan  3 13:18:26.793: INFO: Pod "pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930": Phase="Pending", Reason="", readiness=false. Elapsed: 26.236364ms
    Jan  3 13:18:28.813: INFO: Pod "pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046296879s
    Jan  3 13:18:30.814: INFO: Pod "pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046894453s
    STEP: Saw pod success 01/03/24 13:18:30.814
    Jan  3 13:18:30.814: INFO: Pod "pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930" satisfied condition "Succeeded or Failed"
    Jan  3 13:18:30.833: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930 container agnhost-container: <nil>
    STEP: delete the pod 01/03/24 13:18:30.882
    Jan  3 13:18:30.919: INFO: Waiting for pod pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930 to disappear
    Jan  3 13:18:30.936: INFO: Pod pod-projected-configmaps-5721d1f3-143a-4492-8aa3-c78530c45930 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:18:30.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3581" for this suite. 01/03/24 13:18:30.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:18:31.014
Jan  3 13:18:31.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 13:18:31.016
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:31.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:31.095
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-6726 01/03/24 13:18:31.123
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6726 to expose endpoints map[] 01/03/24 13:18:31.15
Jan  3 13:18:31.200: INFO: successfully validated that service multi-endpoint-test in namespace services-6726 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6726 01/03/24 13:18:31.2
Jan  3 13:18:31.226: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6726" to be "running and ready"
Jan  3 13:18:31.246: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.0544ms
Jan  3 13:18:31.246: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:18:33.266: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.039417788s
Jan  3 13:18:33.266: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  3 13:18:33.266: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6726 to expose endpoints map[pod1:[100]] 01/03/24 13:18:33.283
Jan  3 13:18:33.353: INFO: successfully validated that service multi-endpoint-test in namespace services-6726 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-6726 01/03/24 13:18:33.353
Jan  3 13:18:33.374: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6726" to be "running and ready"
Jan  3 13:18:33.392: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.997561ms
Jan  3 13:18:33.392: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:18:35.414: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.040628356s
Jan  3 13:18:35.414: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  3 13:18:35.414: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6726 to expose endpoints map[pod1:[100] pod2:[101]] 01/03/24 13:18:35.44
Jan  3 13:18:35.541: INFO: successfully validated that service multi-endpoint-test in namespace services-6726 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/03/24 13:18:35.541
Jan  3 13:18:35.541: INFO: Creating new exec pod
Jan  3 13:18:35.563: INFO: Waiting up to 5m0s for pod "execpoddk5zj" in namespace "services-6726" to be "running"
Jan  3 13:18:35.582: INFO: Pod "execpoddk5zj": Phase="Pending", Reason="", readiness=false. Elapsed: 19.908706ms
Jan  3 13:18:37.604: INFO: Pod "execpoddk5zj": Phase="Running", Reason="", readiness=true. Elapsed: 2.041217981s
Jan  3 13:18:37.604: INFO: Pod "execpoddk5zj" satisfied condition "running"
Jan  3 13:18:38.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-6726 exec execpoddk5zj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jan  3 13:18:39.136: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan  3 13:18:39.136: INFO: stdout: ""
Jan  3 13:18:39.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-6726 exec execpoddk5zj -- /bin/sh -x -c nc -v -z -w 2 10.233.49.242 80'
Jan  3 13:18:39.565: INFO: stderr: "+ nc -v -z -w 2 10.233.49.242 80\nConnection to 10.233.49.242 80 port [tcp/http] succeeded!\n"
Jan  3 13:18:39.565: INFO: stdout: ""
Jan  3 13:18:39.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-6726 exec execpoddk5zj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jan  3 13:18:40.004: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan  3 13:18:40.004: INFO: stdout: ""
Jan  3 13:18:40.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-6726 exec execpoddk5zj -- /bin/sh -x -c nc -v -z -w 2 10.233.49.242 81'
Jan  3 13:18:40.460: INFO: stderr: "+ nc -v -z -w 2 10.233.49.242 81\nConnection to 10.233.49.242 81 port [tcp/*] succeeded!\n"
Jan  3 13:18:40.461: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-6726 01/03/24 13:18:40.461
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6726 to expose endpoints map[pod2:[101]] 01/03/24 13:18:40.505
Jan  3 13:18:40.574: INFO: successfully validated that service multi-endpoint-test in namespace services-6726 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-6726 01/03/24 13:18:40.574
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6726 to expose endpoints map[] 01/03/24 13:18:40.614
Jan  3 13:18:40.673: INFO: successfully validated that service multi-endpoint-test in namespace services-6726 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 13:18:40.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6726" for this suite. 01/03/24 13:18:40.741
------------------------------
• [SLOW TEST] [9.751 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:18:31.014
    Jan  3 13:18:31.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 13:18:31.016
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:31.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:31.095
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-6726 01/03/24 13:18:31.123
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6726 to expose endpoints map[] 01/03/24 13:18:31.15
    Jan  3 13:18:31.200: INFO: successfully validated that service multi-endpoint-test in namespace services-6726 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-6726 01/03/24 13:18:31.2
    Jan  3 13:18:31.226: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6726" to be "running and ready"
    Jan  3 13:18:31.246: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.0544ms
    Jan  3 13:18:31.246: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:18:33.266: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.039417788s
    Jan  3 13:18:33.266: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  3 13:18:33.266: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6726 to expose endpoints map[pod1:[100]] 01/03/24 13:18:33.283
    Jan  3 13:18:33.353: INFO: successfully validated that service multi-endpoint-test in namespace services-6726 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-6726 01/03/24 13:18:33.353
    Jan  3 13:18:33.374: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6726" to be "running and ready"
    Jan  3 13:18:33.392: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.997561ms
    Jan  3 13:18:33.392: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:18:35.414: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.040628356s
    Jan  3 13:18:35.414: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  3 13:18:35.414: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6726 to expose endpoints map[pod1:[100] pod2:[101]] 01/03/24 13:18:35.44
    Jan  3 13:18:35.541: INFO: successfully validated that service multi-endpoint-test in namespace services-6726 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/03/24 13:18:35.541
    Jan  3 13:18:35.541: INFO: Creating new exec pod
    Jan  3 13:18:35.563: INFO: Waiting up to 5m0s for pod "execpoddk5zj" in namespace "services-6726" to be "running"
    Jan  3 13:18:35.582: INFO: Pod "execpoddk5zj": Phase="Pending", Reason="", readiness=false. Elapsed: 19.908706ms
    Jan  3 13:18:37.604: INFO: Pod "execpoddk5zj": Phase="Running", Reason="", readiness=true. Elapsed: 2.041217981s
    Jan  3 13:18:37.604: INFO: Pod "execpoddk5zj" satisfied condition "running"
    Jan  3 13:18:38.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-6726 exec execpoddk5zj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jan  3 13:18:39.136: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan  3 13:18:39.136: INFO: stdout: ""
    Jan  3 13:18:39.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-6726 exec execpoddk5zj -- /bin/sh -x -c nc -v -z -w 2 10.233.49.242 80'
    Jan  3 13:18:39.565: INFO: stderr: "+ nc -v -z -w 2 10.233.49.242 80\nConnection to 10.233.49.242 80 port [tcp/http] succeeded!\n"
    Jan  3 13:18:39.565: INFO: stdout: ""
    Jan  3 13:18:39.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-6726 exec execpoddk5zj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jan  3 13:18:40.004: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan  3 13:18:40.004: INFO: stdout: ""
    Jan  3 13:18:40.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-6726 exec execpoddk5zj -- /bin/sh -x -c nc -v -z -w 2 10.233.49.242 81'
    Jan  3 13:18:40.460: INFO: stderr: "+ nc -v -z -w 2 10.233.49.242 81\nConnection to 10.233.49.242 81 port [tcp/*] succeeded!\n"
    Jan  3 13:18:40.461: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-6726 01/03/24 13:18:40.461
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6726 to expose endpoints map[pod2:[101]] 01/03/24 13:18:40.505
    Jan  3 13:18:40.574: INFO: successfully validated that service multi-endpoint-test in namespace services-6726 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-6726 01/03/24 13:18:40.574
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6726 to expose endpoints map[] 01/03/24 13:18:40.614
    Jan  3 13:18:40.673: INFO: successfully validated that service multi-endpoint-test in namespace services-6726 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:18:40.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6726" for this suite. 01/03/24 13:18:40.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:18:40.767
Jan  3 13:18:40.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename svcaccounts 01/03/24 13:18:40.768
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:40.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:40.851
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Jan  3 13:18:40.946: INFO: created pod pod-service-account-defaultsa
Jan  3 13:18:40.946: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan  3 13:18:40.972: INFO: created pod pod-service-account-mountsa
Jan  3 13:18:40.972: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan  3 13:18:40.998: INFO: created pod pod-service-account-nomountsa
Jan  3 13:18:40.998: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan  3 13:18:41.020: INFO: created pod pod-service-account-defaultsa-mountspec
Jan  3 13:18:41.020: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan  3 13:18:41.041: INFO: created pod pod-service-account-mountsa-mountspec
Jan  3 13:18:41.041: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan  3 13:18:41.063: INFO: created pod pod-service-account-nomountsa-mountspec
Jan  3 13:18:41.063: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan  3 13:18:41.082: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan  3 13:18:41.082: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan  3 13:18:41.105: INFO: created pod pod-service-account-mountsa-nomountspec
Jan  3 13:18:41.105: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan  3 13:18:41.126: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan  3 13:18:41.126: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  3 13:18:41.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5806" for this suite. 01/03/24 13:18:41.163
------------------------------
• [0.421 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:18:40.767
    Jan  3 13:18:40.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename svcaccounts 01/03/24 13:18:40.768
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:40.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:40.851
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Jan  3 13:18:40.946: INFO: created pod pod-service-account-defaultsa
    Jan  3 13:18:40.946: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan  3 13:18:40.972: INFO: created pod pod-service-account-mountsa
    Jan  3 13:18:40.972: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan  3 13:18:40.998: INFO: created pod pod-service-account-nomountsa
    Jan  3 13:18:40.998: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan  3 13:18:41.020: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan  3 13:18:41.020: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan  3 13:18:41.041: INFO: created pod pod-service-account-mountsa-mountspec
    Jan  3 13:18:41.041: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan  3 13:18:41.063: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan  3 13:18:41.063: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan  3 13:18:41.082: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan  3 13:18:41.082: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan  3 13:18:41.105: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan  3 13:18:41.105: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan  3 13:18:41.126: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan  3 13:18:41.126: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:18:41.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5806" for this suite. 01/03/24 13:18:41.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:18:41.19
Jan  3 13:18:41.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 13:18:41.191
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:41.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:41.272
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-0c0412d1-5102-4301-ab62-0cb8b3d47fe2 01/03/24 13:18:41.298
STEP: Creating a pod to test consume secrets 01/03/24 13:18:41.315
Jan  3 13:18:41.342: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c" in namespace "projected-1784" to be "Succeeded or Failed"
Jan  3 13:18:41.364: INFO: Pod "pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.936549ms
Jan  3 13:18:43.383: INFO: Pod "pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041477343s
Jan  3 13:18:45.385: INFO: Pod "pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c": Phase="Running", Reason="", readiness=false. Elapsed: 4.04315393s
Jan  3 13:18:47.387: INFO: Pod "pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044938429s
STEP: Saw pod success 01/03/24 13:18:47.387
Jan  3 13:18:47.387: INFO: Pod "pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c" satisfied condition "Succeeded or Failed"
Jan  3 13:18:47.408: INFO: Trying to get logs from node jb-1-26-np-adtwo5cmi2 pod pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c container projected-secret-volume-test: <nil>
STEP: delete the pod 01/03/24 13:18:47.6
Jan  3 13:18:47.636: INFO: Waiting for pod pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c to disappear
Jan  3 13:18:47.654: INFO: Pod pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  3 13:18:47.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1784" for this suite. 01/03/24 13:18:47.686
------------------------------
• [SLOW TEST] [6.519 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:18:41.19
    Jan  3 13:18:41.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 13:18:41.191
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:41.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:41.272
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-0c0412d1-5102-4301-ab62-0cb8b3d47fe2 01/03/24 13:18:41.298
    STEP: Creating a pod to test consume secrets 01/03/24 13:18:41.315
    Jan  3 13:18:41.342: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c" in namespace "projected-1784" to be "Succeeded or Failed"
    Jan  3 13:18:41.364: INFO: Pod "pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.936549ms
    Jan  3 13:18:43.383: INFO: Pod "pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041477343s
    Jan  3 13:18:45.385: INFO: Pod "pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c": Phase="Running", Reason="", readiness=false. Elapsed: 4.04315393s
    Jan  3 13:18:47.387: INFO: Pod "pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044938429s
    STEP: Saw pod success 01/03/24 13:18:47.387
    Jan  3 13:18:47.387: INFO: Pod "pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c" satisfied condition "Succeeded or Failed"
    Jan  3 13:18:47.408: INFO: Trying to get logs from node jb-1-26-np-adtwo5cmi2 pod pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/03/24 13:18:47.6
    Jan  3 13:18:47.636: INFO: Waiting for pod pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c to disappear
    Jan  3 13:18:47.654: INFO: Pod pod-projected-secrets-e5788914-abbc-4371-b0c4-ad63bda7dd1c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:18:47.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1784" for this suite. 01/03/24 13:18:47.686
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:18:47.71
Jan  3 13:18:47.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename events 01/03/24 13:18:47.712
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:47.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:47.798
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/03/24 13:18:47.825
STEP: listing all events in all namespaces 01/03/24 13:18:47.844
STEP: patching the test event 01/03/24 13:18:47.862
STEP: fetching the test event 01/03/24 13:18:47.884
STEP: updating the test event 01/03/24 13:18:47.904
STEP: getting the test event 01/03/24 13:18:47.942
STEP: deleting the test event 01/03/24 13:18:47.96
STEP: listing all events in all namespaces 01/03/24 13:18:47.985
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan  3 13:18:48.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7008" for this suite. 01/03/24 13:18:48.023
------------------------------
• [0.337 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:18:47.71
    Jan  3 13:18:47.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename events 01/03/24 13:18:47.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:47.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:47.798
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/03/24 13:18:47.825
    STEP: listing all events in all namespaces 01/03/24 13:18:47.844
    STEP: patching the test event 01/03/24 13:18:47.862
    STEP: fetching the test event 01/03/24 13:18:47.884
    STEP: updating the test event 01/03/24 13:18:47.904
    STEP: getting the test event 01/03/24 13:18:47.942
    STEP: deleting the test event 01/03/24 13:18:47.96
    STEP: listing all events in all namespaces 01/03/24 13:18:47.985
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:18:48.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7008" for this suite. 01/03/24 13:18:48.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:18:48.05
Jan  3 13:18:48.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 13:18:48.053
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:48.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:48.134
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-df2eb3cb-f388-4e2e-88a2-b8e1cb46eca0 01/03/24 13:18:48.183
STEP: Creating the pod 01/03/24 13:18:48.201
Jan  3 13:18:48.227: INFO: Waiting up to 5m0s for pod "pod-configmaps-f0c41abb-0926-4c62-b20f-cf64eeb90429" in namespace "configmap-843" to be "running"
Jan  3 13:18:48.246: INFO: Pod "pod-configmaps-f0c41abb-0926-4c62-b20f-cf64eeb90429": Phase="Pending", Reason="", readiness=false. Elapsed: 19.493157ms
Jan  3 13:18:50.266: INFO: Pod "pod-configmaps-f0c41abb-0926-4c62-b20f-cf64eeb90429": Phase="Running", Reason="", readiness=true. Elapsed: 2.039518949s
Jan  3 13:18:50.266: INFO: Pod "pod-configmaps-f0c41abb-0926-4c62-b20f-cf64eeb90429" satisfied condition "running"
STEP: Waiting for pod with text data 01/03/24 13:18:50.266
STEP: Waiting for pod with binary data 01/03/24 13:18:50.307
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 13:18:50.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-843" for this suite. 01/03/24 13:18:50.382
------------------------------
• [2.358 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:18:48.05
    Jan  3 13:18:48.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 13:18:48.053
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:48.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:48.134
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-df2eb3cb-f388-4e2e-88a2-b8e1cb46eca0 01/03/24 13:18:48.183
    STEP: Creating the pod 01/03/24 13:18:48.201
    Jan  3 13:18:48.227: INFO: Waiting up to 5m0s for pod "pod-configmaps-f0c41abb-0926-4c62-b20f-cf64eeb90429" in namespace "configmap-843" to be "running"
    Jan  3 13:18:48.246: INFO: Pod "pod-configmaps-f0c41abb-0926-4c62-b20f-cf64eeb90429": Phase="Pending", Reason="", readiness=false. Elapsed: 19.493157ms
    Jan  3 13:18:50.266: INFO: Pod "pod-configmaps-f0c41abb-0926-4c62-b20f-cf64eeb90429": Phase="Running", Reason="", readiness=true. Elapsed: 2.039518949s
    Jan  3 13:18:50.266: INFO: Pod "pod-configmaps-f0c41abb-0926-4c62-b20f-cf64eeb90429" satisfied condition "running"
    STEP: Waiting for pod with text data 01/03/24 13:18:50.266
    STEP: Waiting for pod with binary data 01/03/24 13:18:50.307
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:18:50.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-843" for this suite. 01/03/24 13:18:50.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:18:50.415
Jan  3 13:18:50.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 13:18:50.417
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:50.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:50.511
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 01/03/24 13:18:50.538
Jan  3 13:18:50.566: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a" in namespace "projected-5410" to be "Succeeded or Failed"
Jan  3 13:18:50.585: INFO: Pod "downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.708771ms
Jan  3 13:18:52.606: INFO: Pod "downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a": Phase="Running", Reason="", readiness=true. Elapsed: 2.039783748s
Jan  3 13:18:54.604: INFO: Pod "downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a": Phase="Running", Reason="", readiness=false. Elapsed: 4.038127s
Jan  3 13:18:56.608: INFO: Pod "downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041977767s
STEP: Saw pod success 01/03/24 13:18:56.608
Jan  3 13:18:56.609: INFO: Pod "downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a" satisfied condition "Succeeded or Failed"
Jan  3 13:18:56.627: INFO: Trying to get logs from node jb-1-26-np-adtwo5cmi2 pod downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a container client-container: <nil>
STEP: delete the pod 01/03/24 13:18:56.67
Jan  3 13:18:56.710: INFO: Waiting for pod downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a to disappear
Jan  3 13:18:56.728: INFO: Pod downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  3 13:18:56.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5410" for this suite. 01/03/24 13:18:56.761
------------------------------
• [SLOW TEST] [6.377 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:18:50.415
    Jan  3 13:18:50.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 13:18:50.417
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:50.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:50.511
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 01/03/24 13:18:50.538
    Jan  3 13:18:50.566: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a" in namespace "projected-5410" to be "Succeeded or Failed"
    Jan  3 13:18:50.585: INFO: Pod "downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.708771ms
    Jan  3 13:18:52.606: INFO: Pod "downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a": Phase="Running", Reason="", readiness=true. Elapsed: 2.039783748s
    Jan  3 13:18:54.604: INFO: Pod "downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a": Phase="Running", Reason="", readiness=false. Elapsed: 4.038127s
    Jan  3 13:18:56.608: INFO: Pod "downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041977767s
    STEP: Saw pod success 01/03/24 13:18:56.608
    Jan  3 13:18:56.609: INFO: Pod "downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a" satisfied condition "Succeeded or Failed"
    Jan  3 13:18:56.627: INFO: Trying to get logs from node jb-1-26-np-adtwo5cmi2 pod downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a container client-container: <nil>
    STEP: delete the pod 01/03/24 13:18:56.67
    Jan  3 13:18:56.710: INFO: Waiting for pod downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a to disappear
    Jan  3 13:18:56.728: INFO: Pod downwardapi-volume-5053dbda-5fd1-4970-9559-08bcd415759a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:18:56.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5410" for this suite. 01/03/24 13:18:56.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:18:56.798
Jan  3 13:18:56.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 13:18:56.799
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:56.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:56.883
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 13:18:56.964
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:18:57.303
STEP: Deploying the webhook pod 01/03/24 13:18:57.328
STEP: Wait for the deployment to be ready 01/03/24 13:18:57.365
Jan  3 13:18:57.404: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/24 13:18:59.458
STEP: Verifying the service has paired with the endpoint 01/03/24 13:18:59.486
Jan  3 13:19:00.487: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/03/24 13:19:00.507
STEP: Registering slow webhook via the AdmissionRegistration API 01/03/24 13:19:00.508
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/03/24 13:19:00.631
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/03/24 13:19:01.668
STEP: Registering slow webhook via the AdmissionRegistration API 01/03/24 13:19:01.668
STEP: Having no error when timeout is longer than webhook latency 01/03/24 13:19:02.781
STEP: Registering slow webhook via the AdmissionRegistration API 01/03/24 13:19:02.782
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/03/24 13:19:07.992
STEP: Registering slow webhook via the AdmissionRegistration API 01/03/24 13:19:07.993
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:19:13.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7797" for this suite. 01/03/24 13:19:13.262
STEP: Destroying namespace "webhook-7797-markers" for this suite. 01/03/24 13:19:13.286
------------------------------
• [SLOW TEST] [16.510 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:18:56.798
    Jan  3 13:18:56.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 13:18:56.799
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:18:56.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:18:56.883
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 13:18:56.964
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:18:57.303
    STEP: Deploying the webhook pod 01/03/24 13:18:57.328
    STEP: Wait for the deployment to be ready 01/03/24 13:18:57.365
    Jan  3 13:18:57.404: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/24 13:18:59.458
    STEP: Verifying the service has paired with the endpoint 01/03/24 13:18:59.486
    Jan  3 13:19:00.487: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/03/24 13:19:00.507
    STEP: Registering slow webhook via the AdmissionRegistration API 01/03/24 13:19:00.508
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/03/24 13:19:00.631
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/03/24 13:19:01.668
    STEP: Registering slow webhook via the AdmissionRegistration API 01/03/24 13:19:01.668
    STEP: Having no error when timeout is longer than webhook latency 01/03/24 13:19:02.781
    STEP: Registering slow webhook via the AdmissionRegistration API 01/03/24 13:19:02.782
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/03/24 13:19:07.992
    STEP: Registering slow webhook via the AdmissionRegistration API 01/03/24 13:19:07.993
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:19:13.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7797" for this suite. 01/03/24 13:19:13.262
    STEP: Destroying namespace "webhook-7797-markers" for this suite. 01/03/24 13:19:13.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:19:13.311
Jan  3 13:19:13.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/24 13:19:13.312
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:19:13.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:19:13.392
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/03/24 13:19:13.439
Jan  3 13:19:13.470: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4371" to be "running and ready"
Jan  3 13:19:13.489: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 18.923045ms
Jan  3 13:19:13.489: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:19:15.507: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037326032s
Jan  3 13:19:15.507: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:19:17.509: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.03877353s
Jan  3 13:19:17.509: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  3 13:19:17.509: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 01/03/24 13:19:17.527
Jan  3 13:19:17.551: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4371" to be "running and ready"
Jan  3 13:19:17.570: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 19.250682ms
Jan  3 13:19:17.570: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:19:19.594: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.042788684s
Jan  3 13:19:19.594: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan  3 13:19:19.594: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/03/24 13:19:19.612
Jan  3 13:19:19.639: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  3 13:19:19.660: INFO: Pod pod-with-prestop-http-hook still exists
Jan  3 13:19:21.661: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  3 13:19:21.683: INFO: Pod pod-with-prestop-http-hook still exists
Jan  3 13:19:23.661: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  3 13:19:23.684: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/03/24 13:19:23.684
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan  3 13:19:23.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4371" for this suite. 01/03/24 13:19:23.761
------------------------------
• [SLOW TEST] [10.475 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:19:13.311
    Jan  3 13:19:13.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/24 13:19:13.312
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:19:13.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:19:13.392
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/03/24 13:19:13.439
    Jan  3 13:19:13.470: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4371" to be "running and ready"
    Jan  3 13:19:13.489: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 18.923045ms
    Jan  3 13:19:13.489: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:19:15.507: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037326032s
    Jan  3 13:19:15.507: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:19:17.509: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.03877353s
    Jan  3 13:19:17.509: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  3 13:19:17.509: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 01/03/24 13:19:17.527
    Jan  3 13:19:17.551: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4371" to be "running and ready"
    Jan  3 13:19:17.570: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 19.250682ms
    Jan  3 13:19:17.570: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:19:19.594: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.042788684s
    Jan  3 13:19:19.594: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan  3 13:19:19.594: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/03/24 13:19:19.612
    Jan  3 13:19:19.639: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  3 13:19:19.660: INFO: Pod pod-with-prestop-http-hook still exists
    Jan  3 13:19:21.661: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  3 13:19:21.683: INFO: Pod pod-with-prestop-http-hook still exists
    Jan  3 13:19:23.661: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  3 13:19:23.684: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/03/24 13:19:23.684
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:19:23.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4371" for this suite. 01/03/24 13:19:23.761
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:19:23.789
Jan  3 13:19:23.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename namespaces 01/03/24 13:19:23.792
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:19:23.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:19:23.871
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 01/03/24 13:19:23.898
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:19:23.965
STEP: Creating a pod in the namespace 01/03/24 13:19:23.992
STEP: Waiting for the pod to have running status 01/03/24 13:19:24.02
Jan  3 13:19:24.020: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-348" to be "running"
Jan  3 13:19:24.040: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.022706ms
Jan  3 13:19:26.064: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.044241372s
Jan  3 13:19:26.064: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/03/24 13:19:26.064
STEP: Waiting for the namespace to be removed. 01/03/24 13:19:26.091
STEP: Recreating the namespace 01/03/24 13:19:38.111
STEP: Verifying there are no pods in the namespace 01/03/24 13:19:38.165
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:19:38.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9270" for this suite. 01/03/24 13:19:38.221
STEP: Destroying namespace "nsdeletetest-348" for this suite. 01/03/24 13:19:38.245
Jan  3 13:19:38.264: INFO: Namespace nsdeletetest-348 was already deleted
STEP: Destroying namespace "nsdeletetest-3572" for this suite. 01/03/24 13:19:38.264
------------------------------
• [SLOW TEST] [14.503 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:19:23.789
    Jan  3 13:19:23.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename namespaces 01/03/24 13:19:23.792
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:19:23.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:19:23.871
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 01/03/24 13:19:23.898
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:19:23.965
    STEP: Creating a pod in the namespace 01/03/24 13:19:23.992
    STEP: Waiting for the pod to have running status 01/03/24 13:19:24.02
    Jan  3 13:19:24.020: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-348" to be "running"
    Jan  3 13:19:24.040: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.022706ms
    Jan  3 13:19:26.064: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.044241372s
    Jan  3 13:19:26.064: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/03/24 13:19:26.064
    STEP: Waiting for the namespace to be removed. 01/03/24 13:19:26.091
    STEP: Recreating the namespace 01/03/24 13:19:38.111
    STEP: Verifying there are no pods in the namespace 01/03/24 13:19:38.165
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:19:38.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9270" for this suite. 01/03/24 13:19:38.221
    STEP: Destroying namespace "nsdeletetest-348" for this suite. 01/03/24 13:19:38.245
    Jan  3 13:19:38.264: INFO: Namespace nsdeletetest-348 was already deleted
    STEP: Destroying namespace "nsdeletetest-3572" for this suite. 01/03/24 13:19:38.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:19:38.295
Jan  3 13:19:38.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 13:19:38.297
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:19:38.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:19:38.376
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-bc0d27ef-e305-435a-b774-12a90487da3f 01/03/24 13:19:38.405
STEP: Creating a pod to test consume configMaps 01/03/24 13:19:38.442
Jan  3 13:19:38.470: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371" in namespace "projected-9098" to be "Succeeded or Failed"
Jan  3 13:19:38.490: INFO: Pod "pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371": Phase="Pending", Reason="", readiness=false. Elapsed: 20.079512ms
Jan  3 13:19:40.523: INFO: Pod "pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371": Phase="Running", Reason="", readiness=true. Elapsed: 2.052515873s
Jan  3 13:19:42.512: INFO: Pod "pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371": Phase="Running", Reason="", readiness=false. Elapsed: 4.04223435s
Jan  3 13:19:44.511: INFO: Pod "pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040939164s
STEP: Saw pod success 01/03/24 13:19:44.511
Jan  3 13:19:44.511: INFO: Pod "pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371" satisfied condition "Succeeded or Failed"
Jan  3 13:19:44.536: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/03/24 13:19:44.6
Jan  3 13:19:44.635: INFO: Waiting for pod pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371 to disappear
Jan  3 13:19:44.653: INFO: Pod pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  3 13:19:44.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9098" for this suite. 01/03/24 13:19:44.684
------------------------------
• [SLOW TEST] [6.415 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:19:38.295
    Jan  3 13:19:38.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 13:19:38.297
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:19:38.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:19:38.376
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-bc0d27ef-e305-435a-b774-12a90487da3f 01/03/24 13:19:38.405
    STEP: Creating a pod to test consume configMaps 01/03/24 13:19:38.442
    Jan  3 13:19:38.470: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371" in namespace "projected-9098" to be "Succeeded or Failed"
    Jan  3 13:19:38.490: INFO: Pod "pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371": Phase="Pending", Reason="", readiness=false. Elapsed: 20.079512ms
    Jan  3 13:19:40.523: INFO: Pod "pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371": Phase="Running", Reason="", readiness=true. Elapsed: 2.052515873s
    Jan  3 13:19:42.512: INFO: Pod "pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371": Phase="Running", Reason="", readiness=false. Elapsed: 4.04223435s
    Jan  3 13:19:44.511: INFO: Pod "pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040939164s
    STEP: Saw pod success 01/03/24 13:19:44.511
    Jan  3 13:19:44.511: INFO: Pod "pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371" satisfied condition "Succeeded or Failed"
    Jan  3 13:19:44.536: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/03/24 13:19:44.6
    Jan  3 13:19:44.635: INFO: Waiting for pod pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371 to disappear
    Jan  3 13:19:44.653: INFO: Pod pod-projected-configmaps-2afc3d1a-3ba9-43a8-90b7-0223fb13e371 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:19:44.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9098" for this suite. 01/03/24 13:19:44.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:19:44.712
Jan  3 13:19:44.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-runtime 01/03/24 13:19:44.715
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:19:44.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:19:44.799
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/03/24 13:19:44.89
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/03/24 13:20:03.308
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/03/24 13:20:03.328
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/03/24 13:20:03.362
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/03/24 13:20:03.363
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/03/24 13:20:03.435
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/03/24 13:20:07.549
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/03/24 13:20:09.609
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/03/24 13:20:09.645
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/03/24 13:20:09.646
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/03/24 13:20:09.728
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/03/24 13:20:10.767
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/03/24 13:20:14.867
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/03/24 13:20:14.905
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/03/24 13:20:14.905
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  3 13:20:15.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6023" for this suite. 01/03/24 13:20:15.08
------------------------------
• [SLOW TEST] [30.394 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:19:44.712
    Jan  3 13:19:44.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-runtime 01/03/24 13:19:44.715
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:19:44.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:19:44.799
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/03/24 13:19:44.89
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/03/24 13:20:03.308
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/03/24 13:20:03.328
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/03/24 13:20:03.362
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/03/24 13:20:03.363
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/03/24 13:20:03.435
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/03/24 13:20:07.549
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/03/24 13:20:09.609
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/03/24 13:20:09.645
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/03/24 13:20:09.646
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/03/24 13:20:09.728
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/03/24 13:20:10.767
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/03/24 13:20:14.867
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/03/24 13:20:14.905
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/03/24 13:20:14.905
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:20:15.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6023" for this suite. 01/03/24 13:20:15.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:20:15.107
Jan  3 13:20:15.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 13:20:15.109
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:15.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:15.191
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/03/24 13:20:15.22
Jan  3 13:20:15.246: INFO: Waiting up to 5m0s for pod "pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7" in namespace "emptydir-503" to be "Succeeded or Failed"
Jan  3 13:20:15.266: INFO: Pod "pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.721928ms
Jan  3 13:20:17.290: INFO: Pod "pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042912618s
Jan  3 13:20:19.286: INFO: Pod "pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038883728s
STEP: Saw pod success 01/03/24 13:20:19.286
Jan  3 13:20:19.286: INFO: Pod "pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7" satisfied condition "Succeeded or Failed"
Jan  3 13:20:19.303: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7 container test-container: <nil>
STEP: delete the pod 01/03/24 13:20:19.341
Jan  3 13:20:19.384: INFO: Waiting for pod pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7 to disappear
Jan  3 13:20:19.403: INFO: Pod pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 13:20:19.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-503" for this suite. 01/03/24 13:20:19.434
------------------------------
• [4.354 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:20:15.107
    Jan  3 13:20:15.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 13:20:15.109
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:15.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:15.191
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/03/24 13:20:15.22
    Jan  3 13:20:15.246: INFO: Waiting up to 5m0s for pod "pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7" in namespace "emptydir-503" to be "Succeeded or Failed"
    Jan  3 13:20:15.266: INFO: Pod "pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.721928ms
    Jan  3 13:20:17.290: INFO: Pod "pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042912618s
    Jan  3 13:20:19.286: INFO: Pod "pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038883728s
    STEP: Saw pod success 01/03/24 13:20:19.286
    Jan  3 13:20:19.286: INFO: Pod "pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7" satisfied condition "Succeeded or Failed"
    Jan  3 13:20:19.303: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7 container test-container: <nil>
    STEP: delete the pod 01/03/24 13:20:19.341
    Jan  3 13:20:19.384: INFO: Waiting for pod pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7 to disappear
    Jan  3 13:20:19.403: INFO: Pod pod-835a5a43-2d03-48a4-a762-6ab3fe8742f7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:20:19.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-503" for this suite. 01/03/24 13:20:19.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:20:19.466
Jan  3 13:20:19.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 13:20:19.469
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:19.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:19.549
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 13:20:19.62
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:20:19.928
STEP: Deploying the webhook pod 01/03/24 13:20:19.955
STEP: Wait for the deployment to be ready 01/03/24 13:20:19.997
Jan  3 13:20:20.076: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/24 13:20:22.133
STEP: Verifying the service has paired with the endpoint 01/03/24 13:20:22.19
Jan  3 13:20:23.192: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 01/03/24 13:20:23.209
STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/24 13:20:23.355
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/03/24 13:20:23.469
STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/24 13:20:23.517
STEP: Patching a validating webhook configuration's rules to include the create operation 01/03/24 13:20:23.557
STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/24 13:20:23.584
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:20:23.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4455" for this suite. 01/03/24 13:20:23.778
STEP: Destroying namespace "webhook-4455-markers" for this suite. 01/03/24 13:20:23.803
------------------------------
• [4.361 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:20:19.466
    Jan  3 13:20:19.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 13:20:19.469
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:19.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:19.549
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 13:20:19.62
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:20:19.928
    STEP: Deploying the webhook pod 01/03/24 13:20:19.955
    STEP: Wait for the deployment to be ready 01/03/24 13:20:19.997
    Jan  3 13:20:20.076: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/24 13:20:22.133
    STEP: Verifying the service has paired with the endpoint 01/03/24 13:20:22.19
    Jan  3 13:20:23.192: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 01/03/24 13:20:23.209
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/24 13:20:23.355
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/03/24 13:20:23.469
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/24 13:20:23.517
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/03/24 13:20:23.557
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/24 13:20:23.584
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:20:23.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4455" for this suite. 01/03/24 13:20:23.778
    STEP: Destroying namespace "webhook-4455-markers" for this suite. 01/03/24 13:20:23.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:20:23.829
Jan  3 13:20:23.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename svcaccounts 01/03/24 13:20:23.832
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:23.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:23.912
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Jan  3 13:20:23.983: INFO: Waiting up to 5m0s for pod "pod-service-account-ff020cde-bff8-4920-903a-027fec577635" in namespace "svcaccounts-6841" to be "running"
Jan  3 13:20:24.001: INFO: Pod "pod-service-account-ff020cde-bff8-4920-903a-027fec577635": Phase="Pending", Reason="", readiness=false. Elapsed: 17.139066ms
Jan  3 13:20:26.024: INFO: Pod "pod-service-account-ff020cde-bff8-4920-903a-027fec577635": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040208459s
Jan  3 13:20:28.022: INFO: Pod "pod-service-account-ff020cde-bff8-4920-903a-027fec577635": Phase="Running", Reason="", readiness=true. Elapsed: 4.038360959s
Jan  3 13:20:28.022: INFO: Pod "pod-service-account-ff020cde-bff8-4920-903a-027fec577635" satisfied condition "running"
STEP: reading a file in the container 01/03/24 13:20:28.022
Jan  3 13:20:28.023: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6841 pod-service-account-ff020cde-bff8-4920-903a-027fec577635 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/03/24 13:20:28.481
Jan  3 13:20:28.482: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6841 pod-service-account-ff020cde-bff8-4920-903a-027fec577635 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/03/24 13:20:28.926
Jan  3 13:20:28.926: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6841 pod-service-account-ff020cde-bff8-4920-903a-027fec577635 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan  3 13:20:29.414: INFO: Got root ca configmap in namespace "svcaccounts-6841"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  3 13:20:29.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6841" for this suite. 01/03/24 13:20:29.466
------------------------------
• [SLOW TEST] [5.662 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:20:23.829
    Jan  3 13:20:23.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename svcaccounts 01/03/24 13:20:23.832
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:23.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:23.912
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Jan  3 13:20:23.983: INFO: Waiting up to 5m0s for pod "pod-service-account-ff020cde-bff8-4920-903a-027fec577635" in namespace "svcaccounts-6841" to be "running"
    Jan  3 13:20:24.001: INFO: Pod "pod-service-account-ff020cde-bff8-4920-903a-027fec577635": Phase="Pending", Reason="", readiness=false. Elapsed: 17.139066ms
    Jan  3 13:20:26.024: INFO: Pod "pod-service-account-ff020cde-bff8-4920-903a-027fec577635": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040208459s
    Jan  3 13:20:28.022: INFO: Pod "pod-service-account-ff020cde-bff8-4920-903a-027fec577635": Phase="Running", Reason="", readiness=true. Elapsed: 4.038360959s
    Jan  3 13:20:28.022: INFO: Pod "pod-service-account-ff020cde-bff8-4920-903a-027fec577635" satisfied condition "running"
    STEP: reading a file in the container 01/03/24 13:20:28.022
    Jan  3 13:20:28.023: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6841 pod-service-account-ff020cde-bff8-4920-903a-027fec577635 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/03/24 13:20:28.481
    Jan  3 13:20:28.482: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6841 pod-service-account-ff020cde-bff8-4920-903a-027fec577635 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/03/24 13:20:28.926
    Jan  3 13:20:28.926: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6841 pod-service-account-ff020cde-bff8-4920-903a-027fec577635 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan  3 13:20:29.414: INFO: Got root ca configmap in namespace "svcaccounts-6841"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:20:29.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6841" for this suite. 01/03/24 13:20:29.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:20:29.495
Jan  3 13:20:29.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename custom-resource-definition 01/03/24 13:20:29.497
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:29.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:29.592
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan  3 13:20:29.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:20:33.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8106" for this suite. 01/03/24 13:20:33.081
------------------------------
• [3.623 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:20:29.495
    Jan  3 13:20:29.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename custom-resource-definition 01/03/24 13:20:29.497
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:29.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:29.592
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan  3 13:20:29.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:20:33.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8106" for this suite. 01/03/24 13:20:33.081
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:20:33.12
Jan  3 13:20:33.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 13:20:33.122
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:33.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:33.21
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 13:20:33.286
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:20:33.584
STEP: Deploying the webhook pod 01/03/24 13:20:33.609
STEP: Wait for the deployment to be ready 01/03/24 13:20:33.647
Jan  3 13:20:33.681: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/24 13:20:35.76
STEP: Verifying the service has paired with the endpoint 01/03/24 13:20:35.786
Jan  3 13:20:36.787: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 01/03/24 13:20:37.02
STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/24 13:20:37.256
STEP: Deleting the collection of validation webhooks 01/03/24 13:20:37.431
STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/24 13:20:37.554
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:20:37.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1653" for this suite. 01/03/24 13:20:37.763
STEP: Destroying namespace "webhook-1653-markers" for this suite. 01/03/24 13:20:37.788
------------------------------
• [4.694 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:20:33.12
    Jan  3 13:20:33.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 13:20:33.122
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:33.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:33.21
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 13:20:33.286
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:20:33.584
    STEP: Deploying the webhook pod 01/03/24 13:20:33.609
    STEP: Wait for the deployment to be ready 01/03/24 13:20:33.647
    Jan  3 13:20:33.681: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/24 13:20:35.76
    STEP: Verifying the service has paired with the endpoint 01/03/24 13:20:35.786
    Jan  3 13:20:36.787: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 01/03/24 13:20:37.02
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/24 13:20:37.256
    STEP: Deleting the collection of validation webhooks 01/03/24 13:20:37.431
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/24 13:20:37.554
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:20:37.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1653" for this suite. 01/03/24 13:20:37.763
    STEP: Destroying namespace "webhook-1653-markers" for this suite. 01/03/24 13:20:37.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:20:37.82
Jan  3 13:20:37.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename resourcequota 01/03/24 13:20:37.823
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:37.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:37.903
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 01/03/24 13:20:37.931
STEP: Creating a ResourceQuota 01/03/24 13:20:42.954
STEP: Ensuring resource quota status is calculated 01/03/24 13:20:42.973
STEP: Creating a Service 01/03/24 13:20:44.996
STEP: Creating a NodePort Service 01/03/24 13:20:45.034
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/03/24 13:20:45.086
STEP: Ensuring resource quota status captures service creation 01/03/24 13:20:45.134
STEP: Deleting Services 01/03/24 13:20:47.155
STEP: Ensuring resource quota status released usage 01/03/24 13:20:47.235
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  3 13:20:49.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-19" for this suite. 01/03/24 13:20:49.285
------------------------------
• [SLOW TEST] [11.491 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:20:37.82
    Jan  3 13:20:37.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename resourcequota 01/03/24 13:20:37.823
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:37.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:37.903
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 01/03/24 13:20:37.931
    STEP: Creating a ResourceQuota 01/03/24 13:20:42.954
    STEP: Ensuring resource quota status is calculated 01/03/24 13:20:42.973
    STEP: Creating a Service 01/03/24 13:20:44.996
    STEP: Creating a NodePort Service 01/03/24 13:20:45.034
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/03/24 13:20:45.086
    STEP: Ensuring resource quota status captures service creation 01/03/24 13:20:45.134
    STEP: Deleting Services 01/03/24 13:20:47.155
    STEP: Ensuring resource quota status released usage 01/03/24 13:20:47.235
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:20:49.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-19" for this suite. 01/03/24 13:20:49.285
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:20:49.315
Jan  3 13:20:49.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 13:20:49.317
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:49.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:49.399
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 13:20:49.473
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:20:50.008
STEP: Deploying the webhook pod 01/03/24 13:20:50.07
STEP: Wait for the deployment to be ready 01/03/24 13:20:50.112
Jan  3 13:20:50.147: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/24 13:20:52.201
STEP: Verifying the service has paired with the endpoint 01/03/24 13:20:52.23
Jan  3 13:20:53.231: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 01/03/24 13:20:53.254
STEP: Creating a custom resource definition that should be denied by the webhook 01/03/24 13:20:53.383
Jan  3 13:20:53.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:20:53.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7223" for this suite. 01/03/24 13:20:53.662
STEP: Destroying namespace "webhook-7223-markers" for this suite. 01/03/24 13:20:53.687
------------------------------
• [4.418 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:20:49.315
    Jan  3 13:20:49.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 13:20:49.317
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:49.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:49.399
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 13:20:49.473
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:20:50.008
    STEP: Deploying the webhook pod 01/03/24 13:20:50.07
    STEP: Wait for the deployment to be ready 01/03/24 13:20:50.112
    Jan  3 13:20:50.147: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/24 13:20:52.201
    STEP: Verifying the service has paired with the endpoint 01/03/24 13:20:52.23
    Jan  3 13:20:53.231: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/03/24 13:20:53.254
    STEP: Creating a custom resource definition that should be denied by the webhook 01/03/24 13:20:53.383
    Jan  3 13:20:53.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:20:53.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7223" for this suite. 01/03/24 13:20:53.662
    STEP: Destroying namespace "webhook-7223-markers" for this suite. 01/03/24 13:20:53.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:20:53.738
Jan  3 13:20:53.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename podtemplate 01/03/24 13:20:53.74
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:53.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:53.832
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/03/24 13:20:53.86
Jan  3 13:20:53.890: INFO: created test-podtemplate-1
Jan  3 13:20:53.914: INFO: created test-podtemplate-2
Jan  3 13:20:53.934: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/03/24 13:20:53.934
STEP: delete collection of pod templates 01/03/24 13:20:53.954
Jan  3 13:20:53.954: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/03/24 13:20:53.999
Jan  3 13:20:53.999: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan  3 13:20:54.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9893" for this suite. 01/03/24 13:20:54.043
------------------------------
• [0.332 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:20:53.738
    Jan  3 13:20:53.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename podtemplate 01/03/24 13:20:53.74
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:53.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:53.832
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/03/24 13:20:53.86
    Jan  3 13:20:53.890: INFO: created test-podtemplate-1
    Jan  3 13:20:53.914: INFO: created test-podtemplate-2
    Jan  3 13:20:53.934: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/03/24 13:20:53.934
    STEP: delete collection of pod templates 01/03/24 13:20:53.954
    Jan  3 13:20:53.954: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/03/24 13:20:53.999
    Jan  3 13:20:53.999: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:20:54.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9893" for this suite. 01/03/24 13:20:54.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:20:54.074
Jan  3 13:20:54.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename resourcequota 01/03/24 13:20:54.076
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:54.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:54.177
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 01/03/24 13:20:54.209
STEP: Creating a ResourceQuota 01/03/24 13:20:59.23
STEP: Ensuring resource quota status is calculated 01/03/24 13:20:59.249
STEP: Creating a ReplicaSet 01/03/24 13:21:01.271
STEP: Ensuring resource quota status captures replicaset creation 01/03/24 13:21:01.299
STEP: Deleting a ReplicaSet 01/03/24 13:21:03.321
STEP: Ensuring resource quota status released usage 01/03/24 13:21:03.344
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  3 13:21:05.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6119" for this suite. 01/03/24 13:21:05.397
------------------------------
• [SLOW TEST] [11.348 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:20:54.074
    Jan  3 13:20:54.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename resourcequota 01/03/24 13:20:54.076
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:20:54.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:20:54.177
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 01/03/24 13:20:54.209
    STEP: Creating a ResourceQuota 01/03/24 13:20:59.23
    STEP: Ensuring resource quota status is calculated 01/03/24 13:20:59.249
    STEP: Creating a ReplicaSet 01/03/24 13:21:01.271
    STEP: Ensuring resource quota status captures replicaset creation 01/03/24 13:21:01.299
    STEP: Deleting a ReplicaSet 01/03/24 13:21:03.321
    STEP: Ensuring resource quota status released usage 01/03/24 13:21:03.344
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:21:05.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6119" for this suite. 01/03/24 13:21:05.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:21:05.429
Jan  3 13:21:05.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename kubectl 01/03/24 13:21:05.431
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:05.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:05.525
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jan  3 13:21:05.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-2417 create -f -'
Jan  3 13:21:07.079: INFO: stderr: ""
Jan  3 13:21:07.079: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan  3 13:21:07.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-2417 create -f -'
Jan  3 13:21:07.654: INFO: stderr: ""
Jan  3 13:21:07.654: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/03/24 13:21:07.654
Jan  3 13:21:08.677: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 13:21:08.677: INFO: Found 0 / 1
Jan  3 13:21:09.674: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 13:21:09.674: INFO: Found 1 / 1
Jan  3 13:21:09.674: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan  3 13:21:09.693: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 13:21:09.693: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  3 13:21:09.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-2417 describe pod agnhost-primary-5qvgp'
Jan  3 13:21:09.900: INFO: stderr: ""
Jan  3 13:21:09.900: INFO: stdout: "Name:             agnhost-primary-5qvgp\nNamespace:        kubectl-2417\nPriority:         0\nService Account:  default\nNode:             jb-1-26-np-64kerjapxk/185.132.46.116\nStart Time:       Wed, 03 Jan 2024 13:21:07 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 8997ea56694011dcf4b365f61d4f8f33dcbf1708e8eacf7979736b6f93f88409\n                  cni.projectcalico.org/podIP: 10.221.146.79/32\n                  cni.projectcalico.org/podIPs: 10.221.146.79/32\nStatus:           Running\nIP:               10.221.146.79\nIPs:\n  IP:           10.221.146.79\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://8480451aa81b539a65bfde15dbf77716e2acef60efe85931af6309950625a12f\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 03 Jan 2024 13:21:08 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wpstx (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-wpstx:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-2417/agnhost-primary-5qvgp to jb-1-26-np-64kerjapxk\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jan  3 13:21:09.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-2417 describe rc agnhost-primary'
Jan  3 13:21:10.141: INFO: stderr: ""
Jan  3 13:21:10.141: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2417\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-5qvgp\n"
Jan  3 13:21:10.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-2417 describe service agnhost-primary'
Jan  3 13:21:10.331: INFO: stderr: ""
Jan  3 13:21:10.331: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2417\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.20.59\nIPs:               10.233.20.59\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.221.146.79:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan  3 13:21:10.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-2417 describe node jb-1-26-np-64kerjapxk'
Jan  3 13:21:10.639: INFO: stderr: ""
Jan  3 13:21:10.639: INFO: stdout: "Name:               jb-1-26-np-64kerjapxk\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    cloud.ionos.com/datacenter-id=f18abd83-0966-47b5-8bef-22a67f6dc7bb\n                    cloud.ionos.com/node-id=fc8d9657-7552-4f0e-8299-0562c9a3eb8e\n                    cloud.ionos.com/nodepool-id=31985d16-42c7-4dc2-9675-59654edb3c53\n                    cloud.ionos.com/nodepool-name=jb-1.26-np\n                    enterprise.cloud.ionos.com/datacenter-id=f18abd83-0966-47b5-8bef-22a67f6dc7bb\n                    enterprise.cloud.ionos.com/node-id=fc8d9657-7552-4f0e-8299-0562c9a3eb8e\n                    failure-domain.beta.kubernetes.io/region=de-txl\n                    failure-domain.beta.kubernetes.io/zone=AUTO\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=jb-1-26-np-64kerjapxk\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    topology.kubernetes.io/region=de-txl\n                    topology.kubernetes.io/zone=AUTO\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cloud.ionos.com\":\"fc8d9657-7552-4f0e-8299-0562c9a3eb8e\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 185.132.46.116/32\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.221.146.64\n                    projectcalico.org/IPv4WireguardInterfaceAddr: 10.221.146.67\n                    projectcalico.org/WireguardPublicKey: mBsOaXIUf8SvkSXp3q0hL06xCIzy4zS96f/pmrtXM0M=\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 03 Jan 2024 11:26:50 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  jb-1-26-np-64kerjapxk\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 03 Jan 2024 13:21:04 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 03 Jan 2024 11:32:39 +0000   Wed, 03 Jan 2024 11:32:39 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 03 Jan 2024 13:21:09 +0000   Wed, 03 Jan 2024 11:26:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 03 Jan 2024 13:21:09 +0000   Wed, 03 Jan 2024 11:26:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 03 Jan 2024 13:21:09 +0000   Wed, 03 Jan 2024 11:26:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 03 Jan 2024 13:21:09 +0000   Wed, 03 Jan 2024 11:29:40 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  ExternalIP:  185.132.46.116\n  InternalIP:  185.132.46.116\n  Hostname:    jb-1-26-np-64kerjapxk\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    10082632Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               1974556Ki\n  pods:                 110\nAllocatable:\n  cpu:                  4\n  ephemeral-storage:    9292153636\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               823580Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 a1fe4d147e2d481d82bbf78036fe4a0b\n  System UUID:                fc8d9657-7552-4f0e-8299-0562c9a3eb8e\n  Boot ID:                    b1a479b6-18e2-49a0-a961-9852d8d9c53f\n  Kernel Version:             5.15.0-91-generic\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.2\n  Kubelet Version:            v1.26.9\n  Kube-Proxy Version:         v1.26.9\nProviderID:                   ionos://fc8d9657-7552-4f0e-8299-0562c9a3eb8e\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-r98wj                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         114m\n  kube-system                 csi-ionoscloud-t8qkm                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         114m\n  kube-system                 konnectivity-agent-gnnsp                                   16m (0%)      0 (0%)      32Mi (3%)        32Mi (3%)      114m\n  kube-system                 kube-proxy-z7q4m                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         114m\n  kube-system                 nginx-proxy-jb-1-26-np-64kerjapxk                          25m (0%)      0 (0%)      32M (3%)         0 (0%)         114m\n  kubectl-2417                agnhost-primary-5qvgp                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-4hltf    0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests      Limits\n  --------             --------      ------\n  cpu                  291m (7%)     0 (0%)\n  memory               64018Ki (7%)  32Mi (3%)\n  ephemeral-storage    0 (0%)        0 (0%)\n  hugepages-1Gi        0 (0%)        0 (0%)\n  hugepages-2Mi        0 (0%)        0 (0%)\n  example.com/fakecpu  0             0\nEvents:                <none>\n"
Jan  3 13:21:10.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-2417 describe namespace kubectl-2417'
Jan  3 13:21:10.847: INFO: stderr: ""
Jan  3 13:21:10.847: INFO: stdout: "Name:         kubectl-2417\nLabels:       e2e-framework=kubectl\n              e2e-run=574f13e1-2ed7-447d-a3ec-af70b4fe9007\n              kubernetes.io/metadata.name=kubectl-2417\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  3 13:21:10.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2417" for this suite. 01/03/24 13:21:10.879
------------------------------
• [SLOW TEST] [5.478 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:21:05.429
    Jan  3 13:21:05.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename kubectl 01/03/24 13:21:05.431
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:05.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:05.525
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jan  3 13:21:05.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-2417 create -f -'
    Jan  3 13:21:07.079: INFO: stderr: ""
    Jan  3 13:21:07.079: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan  3 13:21:07.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-2417 create -f -'
    Jan  3 13:21:07.654: INFO: stderr: ""
    Jan  3 13:21:07.654: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/03/24 13:21:07.654
    Jan  3 13:21:08.677: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 13:21:08.677: INFO: Found 0 / 1
    Jan  3 13:21:09.674: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 13:21:09.674: INFO: Found 1 / 1
    Jan  3 13:21:09.674: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan  3 13:21:09.693: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 13:21:09.693: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  3 13:21:09.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-2417 describe pod agnhost-primary-5qvgp'
    Jan  3 13:21:09.900: INFO: stderr: ""
    Jan  3 13:21:09.900: INFO: stdout: "Name:             agnhost-primary-5qvgp\nNamespace:        kubectl-2417\nPriority:         0\nService Account:  default\nNode:             jb-1-26-np-64kerjapxk/185.132.46.116\nStart Time:       Wed, 03 Jan 2024 13:21:07 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 8997ea56694011dcf4b365f61d4f8f33dcbf1708e8eacf7979736b6f93f88409\n                  cni.projectcalico.org/podIP: 10.221.146.79/32\n                  cni.projectcalico.org/podIPs: 10.221.146.79/32\nStatus:           Running\nIP:               10.221.146.79\nIPs:\n  IP:           10.221.146.79\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://8480451aa81b539a65bfde15dbf77716e2acef60efe85931af6309950625a12f\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 03 Jan 2024 13:21:08 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wpstx (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-wpstx:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-2417/agnhost-primary-5qvgp to jb-1-26-np-64kerjapxk\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Jan  3 13:21:09.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-2417 describe rc agnhost-primary'
    Jan  3 13:21:10.141: INFO: stderr: ""
    Jan  3 13:21:10.141: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2417\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-5qvgp\n"
    Jan  3 13:21:10.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-2417 describe service agnhost-primary'
    Jan  3 13:21:10.331: INFO: stderr: ""
    Jan  3 13:21:10.331: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2417\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.20.59\nIPs:               10.233.20.59\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.221.146.79:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan  3 13:21:10.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-2417 describe node jb-1-26-np-64kerjapxk'
    Jan  3 13:21:10.639: INFO: stderr: ""
    Jan  3 13:21:10.639: INFO: stdout: "Name:               jb-1-26-np-64kerjapxk\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    cloud.ionos.com/datacenter-id=f18abd83-0966-47b5-8bef-22a67f6dc7bb\n                    cloud.ionos.com/node-id=fc8d9657-7552-4f0e-8299-0562c9a3eb8e\n                    cloud.ionos.com/nodepool-id=31985d16-42c7-4dc2-9675-59654edb3c53\n                    cloud.ionos.com/nodepool-name=jb-1.26-np\n                    enterprise.cloud.ionos.com/datacenter-id=f18abd83-0966-47b5-8bef-22a67f6dc7bb\n                    enterprise.cloud.ionos.com/node-id=fc8d9657-7552-4f0e-8299-0562c9a3eb8e\n                    failure-domain.beta.kubernetes.io/region=de-txl\n                    failure-domain.beta.kubernetes.io/zone=AUTO\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=jb-1-26-np-64kerjapxk\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    topology.kubernetes.io/region=de-txl\n                    topology.kubernetes.io/zone=AUTO\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cloud.ionos.com\":\"fc8d9657-7552-4f0e-8299-0562c9a3eb8e\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 185.132.46.116/32\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.221.146.64\n                    projectcalico.org/IPv4WireguardInterfaceAddr: 10.221.146.67\n                    projectcalico.org/WireguardPublicKey: mBsOaXIUf8SvkSXp3q0hL06xCIzy4zS96f/pmrtXM0M=\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 03 Jan 2024 11:26:50 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  jb-1-26-np-64kerjapxk\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 03 Jan 2024 13:21:04 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 03 Jan 2024 11:32:39 +0000   Wed, 03 Jan 2024 11:32:39 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 03 Jan 2024 13:21:09 +0000   Wed, 03 Jan 2024 11:26:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 03 Jan 2024 13:21:09 +0000   Wed, 03 Jan 2024 11:26:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 03 Jan 2024 13:21:09 +0000   Wed, 03 Jan 2024 11:26:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 03 Jan 2024 13:21:09 +0000   Wed, 03 Jan 2024 11:29:40 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  ExternalIP:  185.132.46.116\n  InternalIP:  185.132.46.116\n  Hostname:    jb-1-26-np-64kerjapxk\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    10082632Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               1974556Ki\n  pods:                 110\nAllocatable:\n  cpu:                  4\n  ephemeral-storage:    9292153636\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               823580Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 a1fe4d147e2d481d82bbf78036fe4a0b\n  System UUID:                fc8d9657-7552-4f0e-8299-0562c9a3eb8e\n  Boot ID:                    b1a479b6-18e2-49a0-a961-9852d8d9c53f\n  Kernel Version:             5.15.0-91-generic\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.2\n  Kubelet Version:            v1.26.9\n  Kube-Proxy Version:         v1.26.9\nProviderID:                   ionos://fc8d9657-7552-4f0e-8299-0562c9a3eb8e\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-r98wj                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         114m\n  kube-system                 csi-ionoscloud-t8qkm                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         114m\n  kube-system                 konnectivity-agent-gnnsp                                   16m (0%)      0 (0%)      32Mi (3%)        32Mi (3%)      114m\n  kube-system                 kube-proxy-z7q4m                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         114m\n  kube-system                 nginx-proxy-jb-1-26-np-64kerjapxk                          25m (0%)      0 (0%)      32M (3%)         0 (0%)         114m\n  kubectl-2417                agnhost-primary-5qvgp                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-4hltf    0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests      Limits\n  --------             --------      ------\n  cpu                  291m (7%)     0 (0%)\n  memory               64018Ki (7%)  32Mi (3%)\n  ephemeral-storage    0 (0%)        0 (0%)\n  hugepages-1Gi        0 (0%)        0 (0%)\n  hugepages-2Mi        0 (0%)        0 (0%)\n  example.com/fakecpu  0             0\nEvents:                <none>\n"
    Jan  3 13:21:10.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=kubectl-2417 describe namespace kubectl-2417'
    Jan  3 13:21:10.847: INFO: stderr: ""
    Jan  3 13:21:10.847: INFO: stdout: "Name:         kubectl-2417\nLabels:       e2e-framework=kubectl\n              e2e-run=574f13e1-2ed7-447d-a3ec-af70b4fe9007\n              kubernetes.io/metadata.name=kubectl-2417\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:21:10.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2417" for this suite. 01/03/24 13:21:10.879
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:21:10.907
Jan  3 13:21:10.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename resourcequota 01/03/24 13:21:10.91
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:10.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:10.991
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 01/03/24 13:21:11.018
STEP: Getting a ResourceQuota 01/03/24 13:21:11.039
STEP: Updating a ResourceQuota 01/03/24 13:21:11.057
STEP: Verifying a ResourceQuota was modified 01/03/24 13:21:11.076
STEP: Deleting a ResourceQuota 01/03/24 13:21:11.093
STEP: Verifying the deleted ResourceQuota 01/03/24 13:21:11.117
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  3 13:21:11.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4901" for this suite. 01/03/24 13:21:11.155
------------------------------
• [0.275 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:21:10.907
    Jan  3 13:21:10.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename resourcequota 01/03/24 13:21:10.91
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:10.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:10.991
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 01/03/24 13:21:11.018
    STEP: Getting a ResourceQuota 01/03/24 13:21:11.039
    STEP: Updating a ResourceQuota 01/03/24 13:21:11.057
    STEP: Verifying a ResourceQuota was modified 01/03/24 13:21:11.076
    STEP: Deleting a ResourceQuota 01/03/24 13:21:11.093
    STEP: Verifying the deleted ResourceQuota 01/03/24 13:21:11.117
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:21:11.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4901" for this suite. 01/03/24 13:21:11.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:21:11.184
Jan  3 13:21:11.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename security-context-test 01/03/24 13:21:11.187
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:11.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:11.281
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Jan  3 13:21:11.333: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-47849b03-03bd-4c8e-ae0d-2d29c6b7f91e" in namespace "security-context-test-7413" to be "Succeeded or Failed"
Jan  3 13:21:11.352: INFO: Pod "busybox-readonly-false-47849b03-03bd-4c8e-ae0d-2d29c6b7f91e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.242028ms
Jan  3 13:21:13.374: INFO: Pod "busybox-readonly-false-47849b03-03bd-4c8e-ae0d-2d29c6b7f91e": Phase="Running", Reason="", readiness=true. Elapsed: 2.040060823s
Jan  3 13:21:15.374: INFO: Pod "busybox-readonly-false-47849b03-03bd-4c8e-ae0d-2d29c6b7f91e": Phase="Running", Reason="", readiness=false. Elapsed: 4.04082659s
Jan  3 13:21:17.371: INFO: Pod "busybox-readonly-false-47849b03-03bd-4c8e-ae0d-2d29c6b7f91e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037653932s
Jan  3 13:21:17.371: INFO: Pod "busybox-readonly-false-47849b03-03bd-4c8e-ae0d-2d29c6b7f91e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  3 13:21:17.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7413" for this suite. 01/03/24 13:21:17.403
------------------------------
• [SLOW TEST] [6.242 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:21:11.184
    Jan  3 13:21:11.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename security-context-test 01/03/24 13:21:11.187
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:11.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:11.281
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Jan  3 13:21:11.333: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-47849b03-03bd-4c8e-ae0d-2d29c6b7f91e" in namespace "security-context-test-7413" to be "Succeeded or Failed"
    Jan  3 13:21:11.352: INFO: Pod "busybox-readonly-false-47849b03-03bd-4c8e-ae0d-2d29c6b7f91e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.242028ms
    Jan  3 13:21:13.374: INFO: Pod "busybox-readonly-false-47849b03-03bd-4c8e-ae0d-2d29c6b7f91e": Phase="Running", Reason="", readiness=true. Elapsed: 2.040060823s
    Jan  3 13:21:15.374: INFO: Pod "busybox-readonly-false-47849b03-03bd-4c8e-ae0d-2d29c6b7f91e": Phase="Running", Reason="", readiness=false. Elapsed: 4.04082659s
    Jan  3 13:21:17.371: INFO: Pod "busybox-readonly-false-47849b03-03bd-4c8e-ae0d-2d29c6b7f91e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037653932s
    Jan  3 13:21:17.371: INFO: Pod "busybox-readonly-false-47849b03-03bd-4c8e-ae0d-2d29c6b7f91e" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:21:17.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7413" for this suite. 01/03/24 13:21:17.403
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:21:17.427
Jan  3 13:21:17.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename replication-controller 01/03/24 13:21:17.428
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:17.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:17.503
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711 01/03/24 13:21:17.53
Jan  3 13:21:17.572: INFO: Pod name my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711: Found 1 pods out of 1
Jan  3 13:21:17.572: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711" are running
Jan  3 13:21:17.572: INFO: Waiting up to 5m0s for pod "my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711-msjzs" in namespace "replication-controller-6475" to be "running"
Jan  3 13:21:17.591: INFO: Pod "my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711-msjzs": Phase="Pending", Reason="", readiness=false. Elapsed: 18.565688ms
Jan  3 13:21:19.612: INFO: Pod "my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711-msjzs": Phase="Running", Reason="", readiness=true. Elapsed: 2.039520188s
Jan  3 13:21:19.612: INFO: Pod "my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711-msjzs" satisfied condition "running"
Jan  3 13:21:19.612: INFO: Pod "my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711-msjzs" is running (conditions: [])
Jan  3 13:21:19.612: INFO: Trying to dial the pod
Jan  3 13:21:24.755: INFO: Controller my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711: Got expected result from replica 1 [my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711-msjzs]: "my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711-msjzs", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  3 13:21:24.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6475" for this suite. 01/03/24 13:21:24.788
------------------------------
• [SLOW TEST] [7.386 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:21:17.427
    Jan  3 13:21:17.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename replication-controller 01/03/24 13:21:17.428
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:17.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:17.503
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711 01/03/24 13:21:17.53
    Jan  3 13:21:17.572: INFO: Pod name my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711: Found 1 pods out of 1
    Jan  3 13:21:17.572: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711" are running
    Jan  3 13:21:17.572: INFO: Waiting up to 5m0s for pod "my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711-msjzs" in namespace "replication-controller-6475" to be "running"
    Jan  3 13:21:17.591: INFO: Pod "my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711-msjzs": Phase="Pending", Reason="", readiness=false. Elapsed: 18.565688ms
    Jan  3 13:21:19.612: INFO: Pod "my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711-msjzs": Phase="Running", Reason="", readiness=true. Elapsed: 2.039520188s
    Jan  3 13:21:19.612: INFO: Pod "my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711-msjzs" satisfied condition "running"
    Jan  3 13:21:19.612: INFO: Pod "my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711-msjzs" is running (conditions: [])
    Jan  3 13:21:19.612: INFO: Trying to dial the pod
    Jan  3 13:21:24.755: INFO: Controller my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711: Got expected result from replica 1 [my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711-msjzs]: "my-hostname-basic-1512aa02-465d-4a3e-8eb4-eb260f76e711-msjzs", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:21:24.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6475" for this suite. 01/03/24 13:21:24.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:21:24.818
Jan  3 13:21:24.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 13:21:24.82
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:24.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:24.898
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 13:21:24.972
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:21:25.344
STEP: Deploying the webhook pod 01/03/24 13:21:25.368
STEP: Wait for the deployment to be ready 01/03/24 13:21:25.413
Jan  3 13:21:25.448: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/24 13:21:27.501
STEP: Verifying the service has paired with the endpoint 01/03/24 13:21:27.531
Jan  3 13:21:28.532: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 01/03/24 13:21:28.749
STEP: Creating a configMap that should be mutated 01/03/24 13:21:28.903
STEP: Deleting the collection of validation webhooks 01/03/24 13:21:29.19
STEP: Creating a configMap that should not be mutated 01/03/24 13:21:29.298
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:21:29.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4283" for this suite. 01/03/24 13:21:29.47
STEP: Destroying namespace "webhook-4283-markers" for this suite. 01/03/24 13:21:29.494
------------------------------
• [4.698 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:21:24.818
    Jan  3 13:21:24.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 13:21:24.82
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:24.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:24.898
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 13:21:24.972
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:21:25.344
    STEP: Deploying the webhook pod 01/03/24 13:21:25.368
    STEP: Wait for the deployment to be ready 01/03/24 13:21:25.413
    Jan  3 13:21:25.448: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/24 13:21:27.501
    STEP: Verifying the service has paired with the endpoint 01/03/24 13:21:27.531
    Jan  3 13:21:28.532: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 01/03/24 13:21:28.749
    STEP: Creating a configMap that should be mutated 01/03/24 13:21:28.903
    STEP: Deleting the collection of validation webhooks 01/03/24 13:21:29.19
    STEP: Creating a configMap that should not be mutated 01/03/24 13:21:29.298
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:21:29.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4283" for this suite. 01/03/24 13:21:29.47
    STEP: Destroying namespace "webhook-4283-markers" for this suite. 01/03/24 13:21:29.494
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:21:29.517
Jan  3 13:21:29.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename job 01/03/24 13:21:29.52
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:29.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:29.598
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 01/03/24 13:21:29.625
STEP: Ensure pods equal to parallelism count is attached to the job 01/03/24 13:21:29.647
STEP: patching /status 01/03/24 13:21:33.667
STEP: updating /status 01/03/24 13:21:33.691
STEP: get /status 01/03/24 13:21:33.73
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  3 13:21:33.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4114" for this suite. 01/03/24 13:21:33.78
------------------------------
• [4.287 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:21:29.517
    Jan  3 13:21:29.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename job 01/03/24 13:21:29.52
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:29.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:29.598
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 01/03/24 13:21:29.625
    STEP: Ensure pods equal to parallelism count is attached to the job 01/03/24 13:21:29.647
    STEP: patching /status 01/03/24 13:21:33.667
    STEP: updating /status 01/03/24 13:21:33.691
    STEP: get /status 01/03/24 13:21:33.73
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:21:33.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4114" for this suite. 01/03/24 13:21:33.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:21:33.81
Jan  3 13:21:33.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/24 13:21:33.812
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:33.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:33.893
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/03/24 13:21:33.939
Jan  3 13:21:33.965: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3578" to be "running and ready"
Jan  3 13:21:33.984: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 18.585186ms
Jan  3 13:21:33.984: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:21:36.009: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043358101s
Jan  3 13:21:36.009: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:21:38.005: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.039214188s
Jan  3 13:21:38.005: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  3 13:21:38.005: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 01/03/24 13:21:38.023
Jan  3 13:21:38.047: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3578" to be "running and ready"
Jan  3 13:21:38.067: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 19.231842ms
Jan  3 13:21:38.067: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:21:40.101: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.053595303s
Jan  3 13:21:40.101: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan  3 13:21:40.101: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/03/24 13:21:40.121
Jan  3 13:21:40.158: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  3 13:21:40.177: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  3 13:21:42.178: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  3 13:21:42.211: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  3 13:21:44.178: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  3 13:21:44.197: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/03/24 13:21:44.197
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan  3 13:21:44.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-3578" for this suite. 01/03/24 13:21:44.393
------------------------------
• [SLOW TEST] [10.609 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:21:33.81
    Jan  3 13:21:33.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/24 13:21:33.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:33.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:33.893
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/03/24 13:21:33.939
    Jan  3 13:21:33.965: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3578" to be "running and ready"
    Jan  3 13:21:33.984: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 18.585186ms
    Jan  3 13:21:33.984: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:21:36.009: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043358101s
    Jan  3 13:21:36.009: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:21:38.005: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.039214188s
    Jan  3 13:21:38.005: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  3 13:21:38.005: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 01/03/24 13:21:38.023
    Jan  3 13:21:38.047: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3578" to be "running and ready"
    Jan  3 13:21:38.067: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 19.231842ms
    Jan  3 13:21:38.067: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:21:40.101: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.053595303s
    Jan  3 13:21:40.101: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan  3 13:21:40.101: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/03/24 13:21:40.121
    Jan  3 13:21:40.158: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  3 13:21:40.177: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan  3 13:21:42.178: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  3 13:21:42.211: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan  3 13:21:44.178: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  3 13:21:44.197: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/03/24 13:21:44.197
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:21:44.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-3578" for this suite. 01/03/24 13:21:44.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:21:44.424
Jan  3 13:21:44.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-runtime 01/03/24 13:21:44.427
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:44.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:44.507
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 01/03/24 13:21:44.535
STEP: wait for the container to reach Succeeded 01/03/24 13:21:44.561
STEP: get the container status 01/03/24 13:21:48.661
STEP: the container should be terminated 01/03/24 13:21:48.679
STEP: the termination message should be set 01/03/24 13:21:48.679
Jan  3 13:21:48.680: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/03/24 13:21:48.68
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  3 13:21:48.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2664" for this suite. 01/03/24 13:21:48.767
------------------------------
• [4.369 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:21:44.424
    Jan  3 13:21:44.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-runtime 01/03/24 13:21:44.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:44.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:44.507
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 01/03/24 13:21:44.535
    STEP: wait for the container to reach Succeeded 01/03/24 13:21:44.561
    STEP: get the container status 01/03/24 13:21:48.661
    STEP: the container should be terminated 01/03/24 13:21:48.679
    STEP: the termination message should be set 01/03/24 13:21:48.679
    Jan  3 13:21:48.680: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/03/24 13:21:48.68
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:21:48.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2664" for this suite. 01/03/24 13:21:48.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:21:48.799
Jan  3 13:21:48.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename watch 01/03/24 13:21:48.801
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:48.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:48.878
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/03/24 13:21:48.905
STEP: creating a new configmap 01/03/24 13:21:48.918
STEP: modifying the configmap once 01/03/24 13:21:48.936
STEP: closing the watch once it receives two notifications 01/03/24 13:21:48.973
Jan  3 13:21:48.973: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4934  c97d3247-2ba8-4022-8a3a-e79cb0527c66 33981466786 0 2024-01-03 13:21:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-03 13:21:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 13:21:48.974: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4934  c97d3247-2ba8-4022-8a3a-e79cb0527c66 33981466793 0 2024-01-03 13:21:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-03 13:21:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/03/24 13:21:48.974
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/03/24 13:21:49.011
STEP: deleting the configmap 01/03/24 13:21:49.024
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/03/24 13:21:49.045
Jan  3 13:21:49.045: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4934  c97d3247-2ba8-4022-8a3a-e79cb0527c66 33981466798 0 2024-01-03 13:21:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-03 13:21:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 13:21:49.045: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4934  c97d3247-2ba8-4022-8a3a-e79cb0527c66 33981466803 0 2024-01-03 13:21:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-03 13:21:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  3 13:21:49.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4934" for this suite. 01/03/24 13:21:49.065
------------------------------
• [0.291 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:21:48.799
    Jan  3 13:21:48.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename watch 01/03/24 13:21:48.801
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:48.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:48.878
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/03/24 13:21:48.905
    STEP: creating a new configmap 01/03/24 13:21:48.918
    STEP: modifying the configmap once 01/03/24 13:21:48.936
    STEP: closing the watch once it receives two notifications 01/03/24 13:21:48.973
    Jan  3 13:21:48.973: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4934  c97d3247-2ba8-4022-8a3a-e79cb0527c66 33981466786 0 2024-01-03 13:21:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-03 13:21:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 13:21:48.974: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4934  c97d3247-2ba8-4022-8a3a-e79cb0527c66 33981466793 0 2024-01-03 13:21:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-03 13:21:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/03/24 13:21:48.974
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/03/24 13:21:49.011
    STEP: deleting the configmap 01/03/24 13:21:49.024
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/03/24 13:21:49.045
    Jan  3 13:21:49.045: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4934  c97d3247-2ba8-4022-8a3a-e79cb0527c66 33981466798 0 2024-01-03 13:21:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-03 13:21:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 13:21:49.045: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4934  c97d3247-2ba8-4022-8a3a-e79cb0527c66 33981466803 0 2024-01-03 13:21:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-03 13:21:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:21:49.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4934" for this suite. 01/03/24 13:21:49.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:21:49.095
Jan  3 13:21:49.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename crd-watch 01/03/24 13:21:49.096
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:49.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:49.176
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan  3 13:21:49.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Creating first CR  01/03/24 13:21:54.886
Jan  3 13:21:54.910: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-03T13:21:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-03T13:21:54Z]] name:name1 resourceVersion:33981467538 uid:279fe40e-b0c8-4a1f-8ff0-b98fcf1b1248] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/03/24 13:22:04.911
Jan  3 13:22:04.936: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-03T13:22:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-03T13:22:04Z]] name:name2 resourceVersion:33981468754 uid:eacebb66-b84a-40aa-b45a-22e6a6cd5760] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/03/24 13:22:14.936
Jan  3 13:22:14.974: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-03T13:21:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-03T13:22:14Z]] name:name1 resourceVersion:33981469971 uid:279fe40e-b0c8-4a1f-8ff0-b98fcf1b1248] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/03/24 13:22:24.975
Jan  3 13:22:25.000: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-03T13:22:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-03T13:22:24Z]] name:name2 resourceVersion:33981471147 uid:eacebb66-b84a-40aa-b45a-22e6a6cd5760] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/03/24 13:22:35.001
Jan  3 13:22:35.033: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-03T13:21:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-03T13:22:14Z]] name:name1 resourceVersion:33981472316 uid:279fe40e-b0c8-4a1f-8ff0-b98fcf1b1248] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/03/24 13:22:45.034
Jan  3 13:22:45.061: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-03T13:22:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-03T13:22:24Z]] name:name2 resourceVersion:33981473783 uid:eacebb66-b84a-40aa-b45a-22e6a6cd5760] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:22:55.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-6779" for this suite. 01/03/24 13:22:55.652
------------------------------
• [SLOW TEST] [66.589 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:21:49.095
    Jan  3 13:21:49.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename crd-watch 01/03/24 13:21:49.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:21:49.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:21:49.176
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan  3 13:21:49.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Creating first CR  01/03/24 13:21:54.886
    Jan  3 13:21:54.910: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-03T13:21:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-03T13:21:54Z]] name:name1 resourceVersion:33981467538 uid:279fe40e-b0c8-4a1f-8ff0-b98fcf1b1248] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/03/24 13:22:04.911
    Jan  3 13:22:04.936: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-03T13:22:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-03T13:22:04Z]] name:name2 resourceVersion:33981468754 uid:eacebb66-b84a-40aa-b45a-22e6a6cd5760] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/03/24 13:22:14.936
    Jan  3 13:22:14.974: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-03T13:21:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-03T13:22:14Z]] name:name1 resourceVersion:33981469971 uid:279fe40e-b0c8-4a1f-8ff0-b98fcf1b1248] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/03/24 13:22:24.975
    Jan  3 13:22:25.000: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-03T13:22:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-03T13:22:24Z]] name:name2 resourceVersion:33981471147 uid:eacebb66-b84a-40aa-b45a-22e6a6cd5760] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/03/24 13:22:35.001
    Jan  3 13:22:35.033: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-03T13:21:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-03T13:22:14Z]] name:name1 resourceVersion:33981472316 uid:279fe40e-b0c8-4a1f-8ff0-b98fcf1b1248] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/03/24 13:22:45.034
    Jan  3 13:22:45.061: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-03T13:22:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-03T13:22:24Z]] name:name2 resourceVersion:33981473783 uid:eacebb66-b84a-40aa-b45a-22e6a6cd5760] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:22:55.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-6779" for this suite. 01/03/24 13:22:55.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:22:55.691
Jan  3 13:22:55.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 13:22:55.693
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:22:55.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:22:55.774
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 13:22:55.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9532" for this suite. 01/03/24 13:22:55.841
------------------------------
• [0.177 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:22:55.691
    Jan  3 13:22:55.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 13:22:55.693
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:22:55.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:22:55.774
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:22:55.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9532" for this suite. 01/03/24 13:22:55.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:22:55.869
Jan  3 13:22:55.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename ingress 01/03/24 13:22:55.872
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:22:55.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:22:55.971
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/03/24 13:22:56.005
STEP: getting /apis/networking.k8s.io 01/03/24 13:22:56.032
STEP: getting /apis/networking.k8s.iov1 01/03/24 13:22:56.046
STEP: creating 01/03/24 13:22:56.06
STEP: getting 01/03/24 13:22:56.116
STEP: listing 01/03/24 13:22:56.133
STEP: watching 01/03/24 13:22:56.149
Jan  3 13:22:56.149: INFO: starting watch
STEP: cluster-wide listing 01/03/24 13:22:56.162
STEP: cluster-wide watching 01/03/24 13:22:56.179
Jan  3 13:22:56.180: INFO: starting watch
STEP: patching 01/03/24 13:22:56.193
STEP: updating 01/03/24 13:22:56.212
Jan  3 13:22:56.249: INFO: waiting for watch events with expected annotations
Jan  3 13:22:56.249: INFO: saw patched and updated annotations
STEP: patching /status 01/03/24 13:22:56.249
STEP: updating /status 01/03/24 13:22:56.27
STEP: get /status 01/03/24 13:22:56.309
STEP: deleting 01/03/24 13:22:56.326
STEP: deleting a collection 01/03/24 13:22:56.386
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jan  3 13:22:56.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-7217" for this suite. 01/03/24 13:22:56.469
------------------------------
• [0.625 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:22:55.869
    Jan  3 13:22:55.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename ingress 01/03/24 13:22:55.872
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:22:55.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:22:55.971
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/03/24 13:22:56.005
    STEP: getting /apis/networking.k8s.io 01/03/24 13:22:56.032
    STEP: getting /apis/networking.k8s.iov1 01/03/24 13:22:56.046
    STEP: creating 01/03/24 13:22:56.06
    STEP: getting 01/03/24 13:22:56.116
    STEP: listing 01/03/24 13:22:56.133
    STEP: watching 01/03/24 13:22:56.149
    Jan  3 13:22:56.149: INFO: starting watch
    STEP: cluster-wide listing 01/03/24 13:22:56.162
    STEP: cluster-wide watching 01/03/24 13:22:56.179
    Jan  3 13:22:56.180: INFO: starting watch
    STEP: patching 01/03/24 13:22:56.193
    STEP: updating 01/03/24 13:22:56.212
    Jan  3 13:22:56.249: INFO: waiting for watch events with expected annotations
    Jan  3 13:22:56.249: INFO: saw patched and updated annotations
    STEP: patching /status 01/03/24 13:22:56.249
    STEP: updating /status 01/03/24 13:22:56.27
    STEP: get /status 01/03/24 13:22:56.309
    STEP: deleting 01/03/24 13:22:56.326
    STEP: deleting a collection 01/03/24 13:22:56.386
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:22:56.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-7217" for this suite. 01/03/24 13:22:56.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:22:56.497
Jan  3 13:22:56.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename webhook 01/03/24 13:22:56.5
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:22:56.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:22:56.58
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/03/24 13:22:56.651
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:22:56.972
STEP: Deploying the webhook pod 01/03/24 13:22:56.997
STEP: Wait for the deployment to be ready 01/03/24 13:22:57.038
Jan  3 13:22:57.076: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 13:22:59.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 13, 22, 57, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 22, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 22, 57, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 22, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/03/24 13:23:01.147
STEP: Verifying the service has paired with the endpoint 01/03/24 13:23:01.176
Jan  3 13:23:02.176: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jan  3 13:23:02.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4501-crds.webhook.example.com via the AdmissionRegistration API 01/03/24 13:23:02.757
STEP: Creating a custom resource while v1 is storage version 01/03/24 13:23:02.882
STEP: Patching Custom Resource Definition to set v2 as storage 01/03/24 13:23:05.143
STEP: Patching the custom resource while v2 is storage version 01/03/24 13:23:05.193
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:23:05.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3469" for this suite. 01/03/24 13:23:06.099
STEP: Destroying namespace "webhook-3469-markers" for this suite. 01/03/24 13:23:06.125
------------------------------
• [SLOW TEST] [9.661 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:22:56.497
    Jan  3 13:22:56.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename webhook 01/03/24 13:22:56.5
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:22:56.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:22:56.58
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/03/24 13:22:56.651
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/24 13:22:56.972
    STEP: Deploying the webhook pod 01/03/24 13:22:56.997
    STEP: Wait for the deployment to be ready 01/03/24 13:22:57.038
    Jan  3 13:22:57.076: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan  3 13:22:59.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 13, 22, 57, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 22, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 22, 57, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 22, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/03/24 13:23:01.147
    STEP: Verifying the service has paired with the endpoint 01/03/24 13:23:01.176
    Jan  3 13:23:02.176: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jan  3 13:23:02.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4501-crds.webhook.example.com via the AdmissionRegistration API 01/03/24 13:23:02.757
    STEP: Creating a custom resource while v1 is storage version 01/03/24 13:23:02.882
    STEP: Patching Custom Resource Definition to set v2 as storage 01/03/24 13:23:05.143
    STEP: Patching the custom resource while v2 is storage version 01/03/24 13:23:05.193
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:23:05.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3469" for this suite. 01/03/24 13:23:06.099
    STEP: Destroying namespace "webhook-3469-markers" for this suite. 01/03/24 13:23:06.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:23:06.161
Jan  3 13:23:06.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename deployment 01/03/24 13:23:06.162
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:23:06.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:23:06.243
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan  3 13:23:06.272: INFO: Creating deployment "test-recreate-deployment"
Jan  3 13:23:06.295: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan  3 13:23:06.333: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan  3 13:23:06.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-795566c5cb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:23:08.369: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:23:10.370: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan  3 13:23:10.407: INFO: Updating deployment test-recreate-deployment
Jan  3 13:23:10.407: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 13:23:10.540: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7908  f596ccd1-9fd0-43dc-944b-d3b985347516 33981477346 2 2024-01-03 13:23:06 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051e2598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-03 13:23:10 +0000 UTC,LastTransitionTime:2024-01-03 13:23:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2024-01-03 13:23:10 +0000 UTC,LastTransitionTime:2024-01-03 13:23:06 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan  3 13:23:10.558: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-7908  f3caa182-e60e-4fb9-bead-452d1761c7ca 33981477342 1 2024-01-03 13:23:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f596ccd1-9fd0-43dc-944b-d3b985347516 0xc0051e2ac0 0xc0051e2ac1}] [] [{kube-controller-manager Update apps/v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f596ccd1-9fd0-43dc-944b-d3b985347516\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051e2b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 13:23:10.558: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan  3 13:23:10.559: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-7908  c7aaf2b9-7dd8-47c0-9677-28554c74e098 33981477333 2 2024-01-03 13:23:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f596ccd1-9fd0-43dc-944b-d3b985347516 0xc0051e2987 0xc0051e2988}] [] [{kube-controller-manager Update apps/v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f596ccd1-9fd0-43dc-944b-d3b985347516\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051e2a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 13:23:10.577: INFO: Pod "test-recreate-deployment-cff6dc657-kmmlq" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-kmmlq test-recreate-deployment-cff6dc657- deployment-7908  b50ca97e-ad3d-428d-a937-103cfd75ca1b 33981477348 0 2024-01-03 13:23:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 f3caa182-e60e-4fb9-bead-452d1761c7ca 0xc0051e30a0 0xc0051e30a1}] [] [{kube-controller-manager Update v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3caa182-e60e-4fb9-bead-452d1761c7ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rtn26,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rtn26,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:23:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:23:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:23:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:23:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 13:23:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  3 13:23:10.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7908" for this suite. 01/03/24 13:23:10.608
------------------------------
• [4.472 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:23:06.161
    Jan  3 13:23:06.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename deployment 01/03/24 13:23:06.162
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:23:06.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:23:06.243
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan  3 13:23:06.272: INFO: Creating deployment "test-recreate-deployment"
    Jan  3 13:23:06.295: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan  3 13:23:06.333: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan  3 13:23:06.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-795566c5cb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jan  3 13:23:08.369: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 23, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 13:23:10.370: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan  3 13:23:10.407: INFO: Updating deployment test-recreate-deployment
    Jan  3 13:23:10.407: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 13:23:10.540: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-7908  f596ccd1-9fd0-43dc-944b-d3b985347516 33981477346 2 2024-01-03 13:23:06 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051e2598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-03 13:23:10 +0000 UTC,LastTransitionTime:2024-01-03 13:23:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2024-01-03 13:23:10 +0000 UTC,LastTransitionTime:2024-01-03 13:23:06 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan  3 13:23:10.558: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-7908  f3caa182-e60e-4fb9-bead-452d1761c7ca 33981477342 1 2024-01-03 13:23:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f596ccd1-9fd0-43dc-944b-d3b985347516 0xc0051e2ac0 0xc0051e2ac1}] [] [{kube-controller-manager Update apps/v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f596ccd1-9fd0-43dc-944b-d3b985347516\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051e2b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 13:23:10.558: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan  3 13:23:10.559: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-7908  c7aaf2b9-7dd8-47c0-9677-28554c74e098 33981477333 2 2024-01-03 13:23:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f596ccd1-9fd0-43dc-944b-d3b985347516 0xc0051e2987 0xc0051e2988}] [] [{kube-controller-manager Update apps/v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f596ccd1-9fd0-43dc-944b-d3b985347516\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051e2a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 13:23:10.577: INFO: Pod "test-recreate-deployment-cff6dc657-kmmlq" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-kmmlq test-recreate-deployment-cff6dc657- deployment-7908  b50ca97e-ad3d-428d-a937-103cfd75ca1b 33981477348 0 2024-01-03 13:23:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 f3caa182-e60e-4fb9-bead-452d1761c7ca 0xc0051e30a0 0xc0051e30a1}] [] [{kube-controller-manager Update v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3caa182-e60e-4fb9-bead-452d1761c7ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-03 13:23:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rtn26,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rtn26,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:23:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:23:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:23:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:23:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:,StartTime:2024-01-03 13:23:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:23:10.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7908" for this suite. 01/03/24 13:23:10.608
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:23:10.634
Jan  3 13:23:10.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename services 01/03/24 13:23:10.637
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:23:10.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:23:10.728
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-3764 01/03/24 13:23:10.755
STEP: creating service affinity-clusterip-transition in namespace services-3764 01/03/24 13:23:10.755
STEP: creating replication controller affinity-clusterip-transition in namespace services-3764 01/03/24 13:23:10.796
I0103 13:23:10.816057      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3764, replica count: 3
I0103 13:23:13.867269      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 13:23:13.903: INFO: Creating new exec pod
Jan  3 13:23:13.924: INFO: Waiting up to 5m0s for pod "execpod-affinity7dm92" in namespace "services-3764" to be "running"
Jan  3 13:23:13.942: INFO: Pod "execpod-affinity7dm92": Phase="Pending", Reason="", readiness=false. Elapsed: 17.916062ms
Jan  3 13:23:15.967: INFO: Pod "execpod-affinity7dm92": Phase="Running", Reason="", readiness=true. Elapsed: 2.043127611s
Jan  3 13:23:15.968: INFO: Pod "execpod-affinity7dm92" satisfied condition "running"
Jan  3 13:23:16.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3764 exec execpod-affinity7dm92 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan  3 13:23:17.417: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan  3 13:23:17.417: INFO: stdout: ""
Jan  3 13:23:17.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3764 exec execpod-affinity7dm92 -- /bin/sh -x -c nc -v -z -w 2 10.233.57.112 80'
Jan  3 13:23:17.875: INFO: stderr: "+ nc -v -z -w 2 10.233.57.112 80\nConnection to 10.233.57.112 80 port [tcp/http] succeeded!\n"
Jan  3 13:23:17.875: INFO: stdout: ""
Jan  3 13:23:17.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3764 exec execpod-affinity7dm92 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.57.112:80/ ; done'
Jan  3 13:23:18.461: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n"
Jan  3 13:23:18.461: INFO: stdout: "\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-m9fbt\naffinity-clusterip-transition-5npjl\naffinity-clusterip-transition-5npjl\naffinity-clusterip-transition-5npjl\naffinity-clusterip-transition-5npjl\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-5npjl\naffinity-clusterip-transition-5npjl\naffinity-clusterip-transition-m9fbt\naffinity-clusterip-transition-5npjl\naffinity-clusterip-transition-m9fbt\naffinity-clusterip-transition-m9fbt\naffinity-clusterip-transition-m9fbt\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-5npjl"
Jan  3 13:23:18.461: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:18.461: INFO: Received response from host: affinity-clusterip-transition-m9fbt
Jan  3 13:23:18.461: INFO: Received response from host: affinity-clusterip-transition-5npjl
Jan  3 13:23:18.461: INFO: Received response from host: affinity-clusterip-transition-5npjl
Jan  3 13:23:18.461: INFO: Received response from host: affinity-clusterip-transition-5npjl
Jan  3 13:23:18.461: INFO: Received response from host: affinity-clusterip-transition-5npjl
Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-5npjl
Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-5npjl
Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-m9fbt
Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-5npjl
Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-m9fbt
Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-m9fbt
Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-m9fbt
Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-5npjl
Jan  3 13:23:18.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3764 exec execpod-affinity7dm92 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.57.112:80/ ; done'
Jan  3 13:23:19.004: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n"
Jan  3 13:23:19.004: INFO: stdout: "\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7"
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
Jan  3 13:23:19.004: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3764, will wait for the garbage collector to delete the pods 01/03/24 13:23:19.042
Jan  3 13:23:19.134: INFO: Deleting ReplicationController affinity-clusterip-transition took: 23.12741ms
Jan  3 13:23:19.235: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.80718ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  3 13:23:22.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3764" for this suite. 01/03/24 13:23:22.306
------------------------------
• [SLOW TEST] [11.697 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:23:10.634
    Jan  3 13:23:10.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename services 01/03/24 13:23:10.637
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:23:10.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:23:10.728
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-3764 01/03/24 13:23:10.755
    STEP: creating service affinity-clusterip-transition in namespace services-3764 01/03/24 13:23:10.755
    STEP: creating replication controller affinity-clusterip-transition in namespace services-3764 01/03/24 13:23:10.796
    I0103 13:23:10.816057      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3764, replica count: 3
    I0103 13:23:13.867269      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 13:23:13.903: INFO: Creating new exec pod
    Jan  3 13:23:13.924: INFO: Waiting up to 5m0s for pod "execpod-affinity7dm92" in namespace "services-3764" to be "running"
    Jan  3 13:23:13.942: INFO: Pod "execpod-affinity7dm92": Phase="Pending", Reason="", readiness=false. Elapsed: 17.916062ms
    Jan  3 13:23:15.967: INFO: Pod "execpod-affinity7dm92": Phase="Running", Reason="", readiness=true. Elapsed: 2.043127611s
    Jan  3 13:23:15.968: INFO: Pod "execpod-affinity7dm92" satisfied condition "running"
    Jan  3 13:23:16.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3764 exec execpod-affinity7dm92 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan  3 13:23:17.417: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan  3 13:23:17.417: INFO: stdout: ""
    Jan  3 13:23:17.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3764 exec execpod-affinity7dm92 -- /bin/sh -x -c nc -v -z -w 2 10.233.57.112 80'
    Jan  3 13:23:17.875: INFO: stderr: "+ nc -v -z -w 2 10.233.57.112 80\nConnection to 10.233.57.112 80 port [tcp/http] succeeded!\n"
    Jan  3 13:23:17.875: INFO: stdout: ""
    Jan  3 13:23:17.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3764 exec execpod-affinity7dm92 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.57.112:80/ ; done'
    Jan  3 13:23:18.461: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n"
    Jan  3 13:23:18.461: INFO: stdout: "\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-m9fbt\naffinity-clusterip-transition-5npjl\naffinity-clusterip-transition-5npjl\naffinity-clusterip-transition-5npjl\naffinity-clusterip-transition-5npjl\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-5npjl\naffinity-clusterip-transition-5npjl\naffinity-clusterip-transition-m9fbt\naffinity-clusterip-transition-5npjl\naffinity-clusterip-transition-m9fbt\naffinity-clusterip-transition-m9fbt\naffinity-clusterip-transition-m9fbt\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-5npjl"
    Jan  3 13:23:18.461: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:18.461: INFO: Received response from host: affinity-clusterip-transition-m9fbt
    Jan  3 13:23:18.461: INFO: Received response from host: affinity-clusterip-transition-5npjl
    Jan  3 13:23:18.461: INFO: Received response from host: affinity-clusterip-transition-5npjl
    Jan  3 13:23:18.461: INFO: Received response from host: affinity-clusterip-transition-5npjl
    Jan  3 13:23:18.461: INFO: Received response from host: affinity-clusterip-transition-5npjl
    Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-5npjl
    Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-5npjl
    Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-m9fbt
    Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-5npjl
    Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-m9fbt
    Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-m9fbt
    Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-m9fbt
    Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:18.462: INFO: Received response from host: affinity-clusterip-transition-5npjl
    Jan  3 13:23:18.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3764843863 --namespace=services-3764 exec execpod-affinity7dm92 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.57.112:80/ ; done'
    Jan  3 13:23:19.004: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.57.112:80/\n"
    Jan  3 13:23:19.004: INFO: stdout: "\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7\naffinity-clusterip-transition-r9dv7"
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Received response from host: affinity-clusterip-transition-r9dv7
    Jan  3 13:23:19.004: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3764, will wait for the garbage collector to delete the pods 01/03/24 13:23:19.042
    Jan  3 13:23:19.134: INFO: Deleting ReplicationController affinity-clusterip-transition took: 23.12741ms
    Jan  3 13:23:19.235: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.80718ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:23:22.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3764" for this suite. 01/03/24 13:23:22.306
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:23:22.332
Jan  3 13:23:22.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 13:23:22.335
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:23:22.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:23:22.413
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 01/03/24 13:23:22.441
Jan  3 13:23:22.471: INFO: Waiting up to 5m0s for pod "pod-5bacf270-c567-41d9-a67e-52df65ad2edf" in namespace "emptydir-1488" to be "Succeeded or Failed"
Jan  3 13:23:22.505: INFO: Pod "pod-5bacf270-c567-41d9-a67e-52df65ad2edf": Phase="Pending", Reason="", readiness=false. Elapsed: 33.899873ms
Jan  3 13:23:24.526: INFO: Pod "pod-5bacf270-c567-41d9-a67e-52df65ad2edf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055655747s
Jan  3 13:23:26.531: INFO: Pod "pod-5bacf270-c567-41d9-a67e-52df65ad2edf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060560531s
STEP: Saw pod success 01/03/24 13:23:26.531
Jan  3 13:23:26.532: INFO: Pod "pod-5bacf270-c567-41d9-a67e-52df65ad2edf" satisfied condition "Succeeded or Failed"
Jan  3 13:23:26.553: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-5bacf270-c567-41d9-a67e-52df65ad2edf container test-container: <nil>
STEP: delete the pod 01/03/24 13:23:26.728
Jan  3 13:23:26.769: INFO: Waiting for pod pod-5bacf270-c567-41d9-a67e-52df65ad2edf to disappear
Jan  3 13:23:26.788: INFO: Pod pod-5bacf270-c567-41d9-a67e-52df65ad2edf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 13:23:26.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1488" for this suite. 01/03/24 13:23:26.819
------------------------------
• [4.516 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:23:22.332
    Jan  3 13:23:22.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 13:23:22.335
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:23:22.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:23:22.413
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/03/24 13:23:22.441
    Jan  3 13:23:22.471: INFO: Waiting up to 5m0s for pod "pod-5bacf270-c567-41d9-a67e-52df65ad2edf" in namespace "emptydir-1488" to be "Succeeded or Failed"
    Jan  3 13:23:22.505: INFO: Pod "pod-5bacf270-c567-41d9-a67e-52df65ad2edf": Phase="Pending", Reason="", readiness=false. Elapsed: 33.899873ms
    Jan  3 13:23:24.526: INFO: Pod "pod-5bacf270-c567-41d9-a67e-52df65ad2edf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055655747s
    Jan  3 13:23:26.531: INFO: Pod "pod-5bacf270-c567-41d9-a67e-52df65ad2edf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060560531s
    STEP: Saw pod success 01/03/24 13:23:26.531
    Jan  3 13:23:26.532: INFO: Pod "pod-5bacf270-c567-41d9-a67e-52df65ad2edf" satisfied condition "Succeeded or Failed"
    Jan  3 13:23:26.553: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-5bacf270-c567-41d9-a67e-52df65ad2edf container test-container: <nil>
    STEP: delete the pod 01/03/24 13:23:26.728
    Jan  3 13:23:26.769: INFO: Waiting for pod pod-5bacf270-c567-41d9-a67e-52df65ad2edf to disappear
    Jan  3 13:23:26.788: INFO: Pod pod-5bacf270-c567-41d9-a67e-52df65ad2edf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:23:26.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1488" for this suite. 01/03/24 13:23:26.819
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:23:26.849
Jan  3 13:23:26.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename configmap 01/03/24 13:23:26.852
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:23:26.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:23:26.95
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-38707c52-1d88-4988-8b79-5f146727ba81 01/03/24 13:23:26.996
STEP: Creating configMap with name cm-test-opt-upd-b2b6a79f-8df0-48e8-b975-5ce39306bdba 01/03/24 13:23:27.019
STEP: Creating the pod 01/03/24 13:23:27.037
Jan  3 13:23:27.064: INFO: Waiting up to 5m0s for pod "pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4" in namespace "configmap-245" to be "running and ready"
Jan  3 13:23:27.082: INFO: Pod "pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.308332ms
Jan  3 13:23:27.082: INFO: The phase of Pod pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:23:29.101: INFO: Pod "pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037063843s
Jan  3 13:23:29.102: INFO: The phase of Pod pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:23:31.101: INFO: Pod "pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4": Phase="Running", Reason="", readiness=true. Elapsed: 4.036540415s
Jan  3 13:23:31.101: INFO: The phase of Pod pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4 is Running (Ready = true)
Jan  3 13:23:31.101: INFO: Pod "pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-38707c52-1d88-4988-8b79-5f146727ba81 01/03/24 13:23:31.253
STEP: Updating configmap cm-test-opt-upd-b2b6a79f-8df0-48e8-b975-5ce39306bdba 01/03/24 13:23:31.278
STEP: Creating configMap with name cm-test-opt-create-9300f777-5f4e-4b52-8c12-2e122ae2885b 01/03/24 13:23:31.296
STEP: waiting to observe update in volume 01/03/24 13:23:31.317
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  3 13:24:57.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-245" for this suite. 01/03/24 13:24:57.354
------------------------------
• [SLOW TEST] [90.542 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:23:26.849
    Jan  3 13:23:26.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename configmap 01/03/24 13:23:26.852
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:23:26.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:23:26.95
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-38707c52-1d88-4988-8b79-5f146727ba81 01/03/24 13:23:26.996
    STEP: Creating configMap with name cm-test-opt-upd-b2b6a79f-8df0-48e8-b975-5ce39306bdba 01/03/24 13:23:27.019
    STEP: Creating the pod 01/03/24 13:23:27.037
    Jan  3 13:23:27.064: INFO: Waiting up to 5m0s for pod "pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4" in namespace "configmap-245" to be "running and ready"
    Jan  3 13:23:27.082: INFO: Pod "pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.308332ms
    Jan  3 13:23:27.082: INFO: The phase of Pod pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:23:29.101: INFO: Pod "pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037063843s
    Jan  3 13:23:29.102: INFO: The phase of Pod pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:23:31.101: INFO: Pod "pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4": Phase="Running", Reason="", readiness=true. Elapsed: 4.036540415s
    Jan  3 13:23:31.101: INFO: The phase of Pod pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4 is Running (Ready = true)
    Jan  3 13:23:31.101: INFO: Pod "pod-configmaps-992ca9a2-a93f-461d-be1c-3bbfc997f9f4" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-38707c52-1d88-4988-8b79-5f146727ba81 01/03/24 13:23:31.253
    STEP: Updating configmap cm-test-opt-upd-b2b6a79f-8df0-48e8-b975-5ce39306bdba 01/03/24 13:23:31.278
    STEP: Creating configMap with name cm-test-opt-create-9300f777-5f4e-4b52-8c12-2e122ae2885b 01/03/24 13:23:31.296
    STEP: waiting to observe update in volume 01/03/24 13:23:31.317
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:24:57.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-245" for this suite. 01/03/24 13:24:57.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:24:57.395
Jan  3 13:24:57.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename downward-api 01/03/24 13:24:57.397
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:24:57.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:24:57.486
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 01/03/24 13:24:57.514
Jan  3 13:24:57.544: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e" in namespace "downward-api-5199" to be "Succeeded or Failed"
Jan  3 13:24:57.563: INFO: Pod "downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.344527ms
Jan  3 13:24:59.588: INFO: Pod "downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04418905s
Jan  3 13:25:01.584: INFO: Pod "downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040392402s
STEP: Saw pod success 01/03/24 13:25:01.584
Jan  3 13:25:01.584: INFO: Pod "downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e" satisfied condition "Succeeded or Failed"
Jan  3 13:25:01.603: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e container client-container: <nil>
STEP: delete the pod 01/03/24 13:25:01.641
Jan  3 13:25:01.681: INFO: Waiting for pod downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e to disappear
Jan  3 13:25:01.701: INFO: Pod downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  3 13:25:01.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5199" for this suite. 01/03/24 13:25:01.732
------------------------------
• [4.361 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:24:57.395
    Jan  3 13:24:57.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename downward-api 01/03/24 13:24:57.397
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:24:57.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:24:57.486
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 01/03/24 13:24:57.514
    Jan  3 13:24:57.544: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e" in namespace "downward-api-5199" to be "Succeeded or Failed"
    Jan  3 13:24:57.563: INFO: Pod "downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.344527ms
    Jan  3 13:24:59.588: INFO: Pod "downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04418905s
    Jan  3 13:25:01.584: INFO: Pod "downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040392402s
    STEP: Saw pod success 01/03/24 13:25:01.584
    Jan  3 13:25:01.584: INFO: Pod "downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e" satisfied condition "Succeeded or Failed"
    Jan  3 13:25:01.603: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e container client-container: <nil>
    STEP: delete the pod 01/03/24 13:25:01.641
    Jan  3 13:25:01.681: INFO: Waiting for pod downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e to disappear
    Jan  3 13:25:01.701: INFO: Pod downwardapi-volume-7c362c7f-ec4d-4d1f-89e7-0044c51ceb7e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:25:01.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5199" for this suite. 01/03/24 13:25:01.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:25:01.759
Jan  3 13:25:01.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename deployment 01/03/24 13:25:01.761
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:25:01.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:25:01.842
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan  3 13:25:01.915: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan  3 13:25:06.948: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/03/24 13:25:06.948
Jan  3 13:25:06.949: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan  3 13:25:08.967: INFO: Creating deployment "test-rollover-deployment"
Jan  3 13:25:09.005: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan  3 13:25:11.046: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan  3 13:25:11.085: INFO: Ensure that both replica sets have 1 created replica
Jan  3 13:25:11.135: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan  3 13:25:11.173: INFO: Updating deployment test-rollover-deployment
Jan  3 13:25:11.173: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan  3 13:25:13.213: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan  3 13:25:13.253: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan  3 13:25:13.288: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 13:25:13.288: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:25:15.324: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 13:25:15.324: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:25:17.326: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 13:25:17.326: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:25:19.326: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 13:25:19.326: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:25:21.327: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 13:25:21.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 13:25:23.352: INFO: 
Jan  3 13:25:23.352: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 13:25:23.403: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3759  381fd1b8-756b-46b3-9719-d78183d03ffa 33981494294 2 2024-01-03 13:25:08 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-03 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:25:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00301fc38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-03 13:25:09 +0000 UTC,LastTransitionTime:2024-01-03 13:25:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2024-01-03 13:25:22 +0000 UTC,LastTransitionTime:2024-01-03 13:25:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  3 13:25:23.420: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-3759  deb4f5ad-e329-46dd-9448-f0ed9f521dfd 33981494274 2 2024-01-03 13:25:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 381fd1b8-756b-46b3-9719-d78183d03ffa 0xc000f9aba7 0xc000f9aba8}] [] [{kube-controller-manager Update apps/v1 2024-01-03 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"381fd1b8-756b-46b3-9719-d78183d03ffa\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:25:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000f9ad38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  3 13:25:23.420: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan  3 13:25:23.421: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3759  06f46b01-27a8-4ecd-915f-b2bba3950c99 33981494293 2 2024-01-03 13:25:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 381fd1b8-756b-46b3-9719-d78183d03ffa 0xc000f9a887 0xc000f9a888}] [] [{e2e.test Update apps/v1 2024-01-03 13:25:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:25:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"381fd1b8-756b-46b3-9719-d78183d03ffa\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:25:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000f9a9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 13:25:23.421: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-3759  1c33b26f-babb-4617-bacf-ce90d6907ecf 33981492833 2 2024-01-03 13:25:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 381fd1b8-756b-46b3-9719-d78183d03ffa 0xc000f9ada7 0xc000f9ada8}] [] [{kube-controller-manager Update apps/v1 2024-01-03 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"381fd1b8-756b-46b3-9719-d78183d03ffa\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:25:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000f9af28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 13:25:23.440: INFO: Pod "test-rollover-deployment-6c6df9974f-cjsnq" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-cjsnq test-rollover-deployment-6c6df9974f- deployment-3759  f7f4f094-3845-4789-8169-36533518cff2 33981493029 0 2024-01-03 13:25:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:640d8679e0186741564476b47bca26b2ddfe54f0ba0f37ef84fd40f348dc0534 cni.projectcalico.org/podIP:10.221.146.80/32 cni.projectcalico.org/podIPs:10.221.146.80/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f deb4f5ad-e329-46dd-9448-f0ed9f521dfd 0xc000f9b867 0xc000f9b868}] [] [{kube-controller-manager Update v1 2024-01-03 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"deb4f5ad-e329-46dd-9448-f0ed9f521dfd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 13:25:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 13:25:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwkml,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwkml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:25:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:25:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:25:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:25:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.80,StartTime:2024-01-03 13:25:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 13:25:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://17ff46d00ee2fd0796618a62564245420ec9fd5737cc7fbd958cb8276048b0c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  3 13:25:23.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3759" for this suite. 01/03/24 13:25:23.471
------------------------------
• [SLOW TEST] [21.749 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:25:01.759
    Jan  3 13:25:01.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename deployment 01/03/24 13:25:01.761
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:25:01.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:25:01.842
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan  3 13:25:01.915: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan  3 13:25:06.948: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/03/24 13:25:06.948
    Jan  3 13:25:06.949: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan  3 13:25:08.967: INFO: Creating deployment "test-rollover-deployment"
    Jan  3 13:25:09.005: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan  3 13:25:11.046: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan  3 13:25:11.085: INFO: Ensure that both replica sets have 1 created replica
    Jan  3 13:25:11.135: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan  3 13:25:11.173: INFO: Updating deployment test-rollover-deployment
    Jan  3 13:25:11.173: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan  3 13:25:13.213: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan  3 13:25:13.253: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan  3 13:25:13.288: INFO: all replica sets need to contain the pod-template-hash label
    Jan  3 13:25:13.288: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 13:25:15.324: INFO: all replica sets need to contain the pod-template-hash label
    Jan  3 13:25:15.324: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 13:25:17.326: INFO: all replica sets need to contain the pod-template-hash label
    Jan  3 13:25:17.326: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 13:25:19.326: INFO: all replica sets need to contain the pod-template-hash label
    Jan  3 13:25:19.326: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 13:25:21.327: INFO: all replica sets need to contain the pod-template-hash label
    Jan  3 13:25:21.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 3, 13, 25, 12, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 3, 13, 25, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 13:25:23.352: INFO: 
    Jan  3 13:25:23.352: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 13:25:23.403: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-3759  381fd1b8-756b-46b3-9719-d78183d03ffa 33981494294 2 2024-01-03 13:25:08 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-03 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:25:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00301fc38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-03 13:25:09 +0000 UTC,LastTransitionTime:2024-01-03 13:25:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2024-01-03 13:25:22 +0000 UTC,LastTransitionTime:2024-01-03 13:25:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  3 13:25:23.420: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-3759  deb4f5ad-e329-46dd-9448-f0ed9f521dfd 33981494274 2 2024-01-03 13:25:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 381fd1b8-756b-46b3-9719-d78183d03ffa 0xc000f9aba7 0xc000f9aba8}] [] [{kube-controller-manager Update apps/v1 2024-01-03 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"381fd1b8-756b-46b3-9719-d78183d03ffa\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:25:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000f9ad38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 13:25:23.420: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan  3 13:25:23.421: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3759  06f46b01-27a8-4ecd-915f-b2bba3950c99 33981494293 2 2024-01-03 13:25:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 381fd1b8-756b-46b3-9719-d78183d03ffa 0xc000f9a887 0xc000f9a888}] [] [{e2e.test Update apps/v1 2024-01-03 13:25:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:25:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"381fd1b8-756b-46b3-9719-d78183d03ffa\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:25:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000f9a9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 13:25:23.421: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-3759  1c33b26f-babb-4617-bacf-ce90d6907ecf 33981492833 2 2024-01-03 13:25:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 381fd1b8-756b-46b3-9719-d78183d03ffa 0xc000f9ada7 0xc000f9ada8}] [] [{kube-controller-manager Update apps/v1 2024-01-03 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"381fd1b8-756b-46b3-9719-d78183d03ffa\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-03 13:25:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000f9af28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 13:25:23.440: INFO: Pod "test-rollover-deployment-6c6df9974f-cjsnq" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-cjsnq test-rollover-deployment-6c6df9974f- deployment-3759  f7f4f094-3845-4789-8169-36533518cff2 33981493029 0 2024-01-03 13:25:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:640d8679e0186741564476b47bca26b2ddfe54f0ba0f37ef84fd40f348dc0534 cni.projectcalico.org/podIP:10.221.146.80/32 cni.projectcalico.org/podIPs:10.221.146.80/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f deb4f5ad-e329-46dd-9448-f0ed9f521dfd 0xc000f9b867 0xc000f9b868}] [] [{kube-controller-manager Update v1 2024-01-03 13:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"deb4f5ad-e329-46dd-9448-f0ed9f521dfd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2024-01-03 13:25:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2024-01-03 13:25:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.221.146.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwkml,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwkml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:jb-1-26-np-64kerjapxk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:25:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:25:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:25:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-03 13:25:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:185.132.46.116,PodIP:10.221.146.80,StartTime:2024-01-03 13:25:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-03 13:25:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://17ff46d00ee2fd0796618a62564245420ec9fd5737cc7fbd958cb8276048b0c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.221.146.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:25:23.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3759" for this suite. 01/03/24 13:25:23.471
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:25:23.513
Jan  3 13:25:23.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename secrets 01/03/24 13:25:23.514
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:25:23.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:25:23.596
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-f9334253-1b1a-4709-a7f2-080062789f37 01/03/24 13:25:23.623
STEP: Creating a pod to test consume secrets 01/03/24 13:25:23.644
Jan  3 13:25:23.670: INFO: Waiting up to 5m0s for pod "pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9" in namespace "secrets-6673" to be "Succeeded or Failed"
Jan  3 13:25:23.689: INFO: Pod "pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.159257ms
Jan  3 13:25:25.709: INFO: Pod "pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9": Phase="Running", Reason="", readiness=true. Elapsed: 2.038347089s
Jan  3 13:25:27.716: INFO: Pod "pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9": Phase="Running", Reason="", readiness=false. Elapsed: 4.045610426s
Jan  3 13:25:29.709: INFO: Pod "pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038386546s
STEP: Saw pod success 01/03/24 13:25:29.709
Jan  3 13:25:29.709: INFO: Pod "pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9" satisfied condition "Succeeded or Failed"
Jan  3 13:25:29.728: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9 container secret-volume-test: <nil>
STEP: delete the pod 01/03/24 13:25:29.773
Jan  3 13:25:29.813: INFO: Waiting for pod pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9 to disappear
Jan  3 13:25:29.832: INFO: Pod pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  3 13:25:29.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6673" for this suite. 01/03/24 13:25:29.864
------------------------------
• [SLOW TEST] [6.386 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:25:23.513
    Jan  3 13:25:23.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename secrets 01/03/24 13:25:23.514
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:25:23.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:25:23.596
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-f9334253-1b1a-4709-a7f2-080062789f37 01/03/24 13:25:23.623
    STEP: Creating a pod to test consume secrets 01/03/24 13:25:23.644
    Jan  3 13:25:23.670: INFO: Waiting up to 5m0s for pod "pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9" in namespace "secrets-6673" to be "Succeeded or Failed"
    Jan  3 13:25:23.689: INFO: Pod "pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.159257ms
    Jan  3 13:25:25.709: INFO: Pod "pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9": Phase="Running", Reason="", readiness=true. Elapsed: 2.038347089s
    Jan  3 13:25:27.716: INFO: Pod "pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9": Phase="Running", Reason="", readiness=false. Elapsed: 4.045610426s
    Jan  3 13:25:29.709: INFO: Pod "pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038386546s
    STEP: Saw pod success 01/03/24 13:25:29.709
    Jan  3 13:25:29.709: INFO: Pod "pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9" satisfied condition "Succeeded or Failed"
    Jan  3 13:25:29.728: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9 container secret-volume-test: <nil>
    STEP: delete the pod 01/03/24 13:25:29.773
    Jan  3 13:25:29.813: INFO: Waiting for pod pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9 to disappear
    Jan  3 13:25:29.832: INFO: Pod pod-secrets-d0dff15c-ade1-472e-bd40-1b852dc454f9 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:25:29.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6673" for this suite. 01/03/24 13:25:29.864
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:25:29.899
Jan  3 13:25:29.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename dns 01/03/24 13:25:29.902
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:25:29.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:25:29.984
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/03/24 13:25:30.01
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3732.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local; sleep 1; done
 01/03/24 13:25:30.055
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3732.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local; sleep 1; done
 01/03/24 13:25:30.055
STEP: creating a pod to probe DNS 01/03/24 13:25:30.056
STEP: submitting the pod to kubernetes 01/03/24 13:25:30.056
Jan  3 13:25:30.119: INFO: Waiting up to 15m0s for pod "dns-test-467f734c-6cac-447d-ae89-6cae753f438b" in namespace "dns-3732" to be "running"
Jan  3 13:25:30.137: INFO: Pod "dns-test-467f734c-6cac-447d-ae89-6cae753f438b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.646847ms
Jan  3 13:25:32.161: INFO: Pod "dns-test-467f734c-6cac-447d-ae89-6cae753f438b": Phase="Running", Reason="", readiness=true. Elapsed: 2.041821281s
Jan  3 13:25:32.161: INFO: Pod "dns-test-467f734c-6cac-447d-ae89-6cae753f438b" satisfied condition "running"
STEP: retrieving the pod 01/03/24 13:25:32.161
STEP: looking for the results for each expected name from probers 01/03/24 13:25:32.181
Jan  3 13:25:32.314: INFO: DNS probes using dns-test-467f734c-6cac-447d-ae89-6cae753f438b succeeded

STEP: deleting the pod 01/03/24 13:25:32.314
STEP: changing the externalName to bar.example.com 01/03/24 13:25:32.357
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3732.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local; sleep 1; done
 01/03/24 13:25:32.394
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3732.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local; sleep 1; done
 01/03/24 13:25:32.395
STEP: creating a second pod to probe DNS 01/03/24 13:25:32.395
STEP: submitting the pod to kubernetes 01/03/24 13:25:32.395
Jan  3 13:25:32.421: INFO: Waiting up to 15m0s for pod "dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343" in namespace "dns-3732" to be "running"
Jan  3 13:25:32.439: INFO: Pod "dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343": Phase="Pending", Reason="", readiness=false. Elapsed: 17.959338ms
Jan  3 13:25:34.463: INFO: Pod "dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343": Phase="Running", Reason="", readiness=true. Elapsed: 2.041441147s
Jan  3 13:25:34.463: INFO: Pod "dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343" satisfied condition "running"
STEP: retrieving the pod 01/03/24 13:25:34.463
STEP: looking for the results for each expected name from probers 01/03/24 13:25:34.484
Jan  3 13:25:34.583: INFO: File wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 13:25:34.615: INFO: File jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 13:25:34.615: INFO: Lookups using dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 failed for: [wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local]

Jan  3 13:25:39.651: INFO: File wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 13:25:39.694: INFO: File jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 13:25:39.694: INFO: Lookups using dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 failed for: [wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local]

Jan  3 13:25:44.650: INFO: File wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 13:25:44.681: INFO: File jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 13:25:44.681: INFO: Lookups using dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 failed for: [wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local]

Jan  3 13:25:49.649: INFO: File wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 13:25:49.684: INFO: File jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 13:25:49.684: INFO: Lookups using dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 failed for: [wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local]

Jan  3 13:25:54.653: INFO: File wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 13:25:54.688: INFO: File jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 13:25:54.688: INFO: Lookups using dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 failed for: [wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local]

Jan  3 13:25:59.650: INFO: File wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 13:25:59.683: INFO: File jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 13:25:59.683: INFO: Lookups using dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 failed for: [wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local]

Jan  3 13:26:04.686: INFO: DNS probes using dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 succeeded

STEP: deleting the pod 01/03/24 13:26:04.686
STEP: changing the service to type=ClusterIP 01/03/24 13:26:04.725
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3732.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local; sleep 1; done
 01/03/24 13:26:04.794
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3732.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local; sleep 1; done
 01/03/24 13:26:04.794
STEP: creating a third pod to probe DNS 01/03/24 13:26:04.796
STEP: submitting the pod to kubernetes 01/03/24 13:26:04.813
Jan  3 13:26:04.839: INFO: Waiting up to 15m0s for pod "dns-test-2522e59c-074b-44e6-8e89-34709b723716" in namespace "dns-3732" to be "running"
Jan  3 13:26:04.856: INFO: Pod "dns-test-2522e59c-074b-44e6-8e89-34709b723716": Phase="Pending", Reason="", readiness=false. Elapsed: 16.986193ms
Jan  3 13:26:06.876: INFO: Pod "dns-test-2522e59c-074b-44e6-8e89-34709b723716": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037262888s
Jan  3 13:26:08.876: INFO: Pod "dns-test-2522e59c-074b-44e6-8e89-34709b723716": Phase="Running", Reason="", readiness=true. Elapsed: 4.036869046s
Jan  3 13:26:08.876: INFO: Pod "dns-test-2522e59c-074b-44e6-8e89-34709b723716" satisfied condition "running"
STEP: retrieving the pod 01/03/24 13:26:08.876
STEP: looking for the results for each expected name from probers 01/03/24 13:26:08.895
Jan  3 13:26:09.040: INFO: DNS probes using dns-test-2522e59c-074b-44e6-8e89-34709b723716 succeeded

STEP: deleting the pod 01/03/24 13:26:09.04
STEP: deleting the test externalName service 01/03/24 13:26:09.083
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  3 13:26:09.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3732" for this suite. 01/03/24 13:26:09.154
------------------------------
• [SLOW TEST] [39.279 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:25:29.899
    Jan  3 13:25:29.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename dns 01/03/24 13:25:29.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:25:29.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:25:29.984
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/03/24 13:25:30.01
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3732.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local; sleep 1; done
     01/03/24 13:25:30.055
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3732.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local; sleep 1; done
     01/03/24 13:25:30.055
    STEP: creating a pod to probe DNS 01/03/24 13:25:30.056
    STEP: submitting the pod to kubernetes 01/03/24 13:25:30.056
    Jan  3 13:25:30.119: INFO: Waiting up to 15m0s for pod "dns-test-467f734c-6cac-447d-ae89-6cae753f438b" in namespace "dns-3732" to be "running"
    Jan  3 13:25:30.137: INFO: Pod "dns-test-467f734c-6cac-447d-ae89-6cae753f438b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.646847ms
    Jan  3 13:25:32.161: INFO: Pod "dns-test-467f734c-6cac-447d-ae89-6cae753f438b": Phase="Running", Reason="", readiness=true. Elapsed: 2.041821281s
    Jan  3 13:25:32.161: INFO: Pod "dns-test-467f734c-6cac-447d-ae89-6cae753f438b" satisfied condition "running"
    STEP: retrieving the pod 01/03/24 13:25:32.161
    STEP: looking for the results for each expected name from probers 01/03/24 13:25:32.181
    Jan  3 13:25:32.314: INFO: DNS probes using dns-test-467f734c-6cac-447d-ae89-6cae753f438b succeeded

    STEP: deleting the pod 01/03/24 13:25:32.314
    STEP: changing the externalName to bar.example.com 01/03/24 13:25:32.357
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3732.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local; sleep 1; done
     01/03/24 13:25:32.394
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3732.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local; sleep 1; done
     01/03/24 13:25:32.395
    STEP: creating a second pod to probe DNS 01/03/24 13:25:32.395
    STEP: submitting the pod to kubernetes 01/03/24 13:25:32.395
    Jan  3 13:25:32.421: INFO: Waiting up to 15m0s for pod "dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343" in namespace "dns-3732" to be "running"
    Jan  3 13:25:32.439: INFO: Pod "dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343": Phase="Pending", Reason="", readiness=false. Elapsed: 17.959338ms
    Jan  3 13:25:34.463: INFO: Pod "dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343": Phase="Running", Reason="", readiness=true. Elapsed: 2.041441147s
    Jan  3 13:25:34.463: INFO: Pod "dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343" satisfied condition "running"
    STEP: retrieving the pod 01/03/24 13:25:34.463
    STEP: looking for the results for each expected name from probers 01/03/24 13:25:34.484
    Jan  3 13:25:34.583: INFO: File wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 13:25:34.615: INFO: File jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 13:25:34.615: INFO: Lookups using dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 failed for: [wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local]

    Jan  3 13:25:39.651: INFO: File wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 13:25:39.694: INFO: File jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 13:25:39.694: INFO: Lookups using dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 failed for: [wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local]

    Jan  3 13:25:44.650: INFO: File wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 13:25:44.681: INFO: File jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 13:25:44.681: INFO: Lookups using dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 failed for: [wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local]

    Jan  3 13:25:49.649: INFO: File wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 13:25:49.684: INFO: File jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 13:25:49.684: INFO: Lookups using dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 failed for: [wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local]

    Jan  3 13:25:54.653: INFO: File wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 13:25:54.688: INFO: File jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 13:25:54.688: INFO: Lookups using dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 failed for: [wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local]

    Jan  3 13:25:59.650: INFO: File wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 13:25:59.683: INFO: File jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local from pod  dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 13:25:59.683: INFO: Lookups using dns-3732/dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 failed for: [wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local]

    Jan  3 13:26:04.686: INFO: DNS probes using dns-test-5b813b74-dc8e-443c-8f6d-c8143e18b343 succeeded

    STEP: deleting the pod 01/03/24 13:26:04.686
    STEP: changing the service to type=ClusterIP 01/03/24 13:26:04.725
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3732.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3732.svc.cluster.local; sleep 1; done
     01/03/24 13:26:04.794
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3732.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3732.svc.cluster.local; sleep 1; done
     01/03/24 13:26:04.794
    STEP: creating a third pod to probe DNS 01/03/24 13:26:04.796
    STEP: submitting the pod to kubernetes 01/03/24 13:26:04.813
    Jan  3 13:26:04.839: INFO: Waiting up to 15m0s for pod "dns-test-2522e59c-074b-44e6-8e89-34709b723716" in namespace "dns-3732" to be "running"
    Jan  3 13:26:04.856: INFO: Pod "dns-test-2522e59c-074b-44e6-8e89-34709b723716": Phase="Pending", Reason="", readiness=false. Elapsed: 16.986193ms
    Jan  3 13:26:06.876: INFO: Pod "dns-test-2522e59c-074b-44e6-8e89-34709b723716": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037262888s
    Jan  3 13:26:08.876: INFO: Pod "dns-test-2522e59c-074b-44e6-8e89-34709b723716": Phase="Running", Reason="", readiness=true. Elapsed: 4.036869046s
    Jan  3 13:26:08.876: INFO: Pod "dns-test-2522e59c-074b-44e6-8e89-34709b723716" satisfied condition "running"
    STEP: retrieving the pod 01/03/24 13:26:08.876
    STEP: looking for the results for each expected name from probers 01/03/24 13:26:08.895
    Jan  3 13:26:09.040: INFO: DNS probes using dns-test-2522e59c-074b-44e6-8e89-34709b723716 succeeded

    STEP: deleting the pod 01/03/24 13:26:09.04
    STEP: deleting the test externalName service 01/03/24 13:26:09.083
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:26:09.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3732" for this suite. 01/03/24 13:26:09.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:26:09.182
Jan  3 13:26:09.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename sched-pred 01/03/24 13:26:09.185
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:26:09.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:26:09.27
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan  3 13:26:09.297: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  3 13:26:09.337: INFO: Waiting for terminating namespaces to be deleted...
Jan  3 13:26:09.357: INFO: 
Logging pods the apiserver thinks is on node jb-1-26-np-64kerjapxk before test
Jan  3 13:26:09.388: INFO: calico-node-r98wj from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.388: INFO: 	Container calico-node ready: true, restart count 2
Jan  3 13:26:09.388: INFO: csi-ionoscloud-t8qkm from kube-system started at 2024-01-03 11:27:28 +0000 UTC (2 container statuses recorded)
Jan  3 13:26:09.388: INFO: 	Container csi-ionoscloud ready: true, restart count 0
Jan  3 13:26:09.388: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  3 13:26:09.388: INFO: konnectivity-agent-gnnsp from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.388: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  3 13:26:09.388: INFO: kube-proxy-z7q4m from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.388: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 13:26:09.388: INFO: nginx-proxy-jb-1-26-np-64kerjapxk from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.388: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan  3 13:26:09.388: INFO: sonobuoy from sonobuoy started at 2024-01-03 11:39:58 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.388: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  3 13:26:09.388: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-4hltf from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 13:26:09.388: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 13:26:09.388: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 13:26:09.388: INFO: 
Logging pods the apiserver thinks is on node jb-1-26-np-adtwo5cmi2 before test
Jan  3 13:26:09.416: INFO: calico-kube-controllers-54564bcfb4-kjb7m from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.416: INFO: 	Container calico-kube-controllers ready: true, restart count 2
Jan  3 13:26:09.416: INFO: calico-node-j4nfd from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.416: INFO: 	Container calico-node ready: true, restart count 1
Jan  3 13:26:09.416: INFO: calico-typha-8555b568f6-pqf6q from kube-system started at 2024-01-03 12:14:40 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.416: INFO: 	Container calico-typha ready: true, restart count 0
Jan  3 13:26:09.416: INFO: csi-ionoscloud-4z7q8 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (2 container statuses recorded)
Jan  3 13:26:09.416: INFO: 	Container csi-ionoscloud ready: true, restart count 0
Jan  3 13:26:09.416: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  3 13:26:09.416: INFO: konnectivity-agent-srxbt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.416: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  3 13:26:09.416: INFO: kube-proxy-ml4kt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.416: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 13:26:09.416: INFO: nginx-proxy-jb-1-26-np-adtwo5cmi2 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.416: INFO: 	Container nginx-proxy ready: true, restart count 0
Jan  3 13:26:09.416: INFO: snapshot-validation-webhook-684799cdd5-x6hm4 from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.416: INFO: 	Container snapshot-validation ready: true, restart count 0
Jan  3 13:26:09.416: INFO: sonobuoy-e2e-job-61ed32d11fea4649 from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 13:26:09.416: INFO: 	Container e2e ready: true, restart count 0
Jan  3 13:26:09.416: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 13:26:09.416: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-cb8zx from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 13:26:09.416: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 13:26:09.416: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 13:26:09.416: INFO: 
Logging pods the apiserver thinks is on node jb-1-26-np-nqeu5xtrab before test
Jan  3 13:26:09.443: INFO: calico-node-t5pwv from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.443: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 13:26:09.443: INFO: calico-typha-8555b568f6-bdlz4 from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.443: INFO: 	Container calico-typha ready: true, restart count 0
Jan  3 13:26:09.443: INFO: coredns-89967fcdc-9jbpx from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.443: INFO: 	Container coredns ready: true, restart count 0
Jan  3 13:26:09.443: INFO: coredns-89967fcdc-zjgbq from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.443: INFO: 	Container coredns ready: true, restart count 0
Jan  3 13:26:09.443: INFO: csi-ionoscloud-kv8wz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (2 container statuses recorded)
Jan  3 13:26:09.443: INFO: 	Container csi-ionoscloud ready: true, restart count 0
Jan  3 13:26:09.443: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan  3 13:26:09.443: INFO: ionos-policy-validator-64d9954f65-dpd5v from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.443: INFO: 	Container ionos-policy-validator ready: true, restart count 0
Jan  3 13:26:09.443: INFO: konnectivity-agent-v6hrz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.443: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  3 13:26:09.443: INFO: kube-proxy-hqxkb from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.443: INFO: 	Container kube-proxy ready: true, restart count 1
Jan  3 13:26:09.443: INFO: nginx-proxy-jb-1-26-np-nqeu5xtrab from kube-system started at 2024-01-03 11:30:18 +0000 UTC (1 container statuses recorded)
Jan  3 13:26:09.443: INFO: 	Container nginx-proxy ready: true, restart count 1
Jan  3 13:26:09.443: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-j7ffw from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
Jan  3 13:26:09.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 13:26:09.443: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node jb-1-26-np-64kerjapxk 01/03/24 13:26:09.539
STEP: verifying the node has the label node jb-1-26-np-adtwo5cmi2 01/03/24 13:26:09.58
STEP: verifying the node has the label node jb-1-26-np-nqeu5xtrab 01/03/24 13:26:09.621
Jan  3 13:26:09.667: INFO: Pod calico-kube-controllers-54564bcfb4-kjb7m requesting resource cpu=0m on Node jb-1-26-np-adtwo5cmi2
Jan  3 13:26:09.667: INFO: Pod calico-node-j4nfd requesting resource cpu=250m on Node jb-1-26-np-adtwo5cmi2
Jan  3 13:26:09.667: INFO: Pod calico-node-r98wj requesting resource cpu=250m on Node jb-1-26-np-64kerjapxk
Jan  3 13:26:09.667: INFO: Pod calico-node-t5pwv requesting resource cpu=250m on Node jb-1-26-np-nqeu5xtrab
Jan  3 13:26:09.667: INFO: Pod calico-typha-8555b568f6-bdlz4 requesting resource cpu=0m on Node jb-1-26-np-nqeu5xtrab
Jan  3 13:26:09.668: INFO: Pod calico-typha-8555b568f6-pqf6q requesting resource cpu=0m on Node jb-1-26-np-adtwo5cmi2
Jan  3 13:26:09.668: INFO: Pod coredns-89967fcdc-9jbpx requesting resource cpu=100m on Node jb-1-26-np-nqeu5xtrab
Jan  3 13:26:09.668: INFO: Pod coredns-89967fcdc-zjgbq requesting resource cpu=100m on Node jb-1-26-np-nqeu5xtrab
Jan  3 13:26:09.668: INFO: Pod csi-ionoscloud-4z7q8 requesting resource cpu=0m on Node jb-1-26-np-adtwo5cmi2
Jan  3 13:26:09.668: INFO: Pod csi-ionoscloud-kv8wz requesting resource cpu=0m on Node jb-1-26-np-nqeu5xtrab
Jan  3 13:26:09.668: INFO: Pod csi-ionoscloud-t8qkm requesting resource cpu=0m on Node jb-1-26-np-64kerjapxk
Jan  3 13:26:09.668: INFO: Pod ionos-policy-validator-64d9954f65-dpd5v requesting resource cpu=5m on Node jb-1-26-np-nqeu5xtrab
Jan  3 13:26:09.668: INFO: Pod konnectivity-agent-gnnsp requesting resource cpu=16m on Node jb-1-26-np-64kerjapxk
Jan  3 13:26:09.668: INFO: Pod konnectivity-agent-srxbt requesting resource cpu=16m on Node jb-1-26-np-adtwo5cmi2
Jan  3 13:26:09.668: INFO: Pod konnectivity-agent-v6hrz requesting resource cpu=16m on Node jb-1-26-np-nqeu5xtrab
Jan  3 13:26:09.668: INFO: Pod kube-proxy-hqxkb requesting resource cpu=0m on Node jb-1-26-np-nqeu5xtrab
Jan  3 13:26:09.668: INFO: Pod kube-proxy-ml4kt requesting resource cpu=0m on Node jb-1-26-np-adtwo5cmi2
Jan  3 13:26:09.668: INFO: Pod kube-proxy-z7q4m requesting resource cpu=0m on Node jb-1-26-np-64kerjapxk
Jan  3 13:26:09.668: INFO: Pod nginx-proxy-jb-1-26-np-64kerjapxk requesting resource cpu=25m on Node jb-1-26-np-64kerjapxk
Jan  3 13:26:09.668: INFO: Pod nginx-proxy-jb-1-26-np-adtwo5cmi2 requesting resource cpu=25m on Node jb-1-26-np-adtwo5cmi2
Jan  3 13:26:09.668: INFO: Pod nginx-proxy-jb-1-26-np-nqeu5xtrab requesting resource cpu=25m on Node jb-1-26-np-nqeu5xtrab
Jan  3 13:26:09.669: INFO: Pod snapshot-validation-webhook-684799cdd5-x6hm4 requesting resource cpu=0m on Node jb-1-26-np-adtwo5cmi2
Jan  3 13:26:09.669: INFO: Pod sonobuoy requesting resource cpu=0m on Node jb-1-26-np-64kerjapxk
Jan  3 13:26:09.669: INFO: Pod sonobuoy-e2e-job-61ed32d11fea4649 requesting resource cpu=0m on Node jb-1-26-np-adtwo5cmi2
Jan  3 13:26:09.669: INFO: Pod sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-4hltf requesting resource cpu=0m on Node jb-1-26-np-64kerjapxk
Jan  3 13:26:09.669: INFO: Pod sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-cb8zx requesting resource cpu=0m on Node jb-1-26-np-adtwo5cmi2
Jan  3 13:26:09.669: INFO: Pod sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-j7ffw requesting resource cpu=0m on Node jb-1-26-np-nqeu5xtrab
STEP: Starting Pods to consume most of the cluster CPU. 01/03/24 13:26:09.669
Jan  3 13:26:09.669: INFO: Creating a pod which consumes cpu=2596m on Node jb-1-26-np-64kerjapxk
Jan  3 13:26:09.698: INFO: Creating a pod which consumes cpu=2596m on Node jb-1-26-np-adtwo5cmi2
Jan  3 13:26:09.721: INFO: Creating a pod which consumes cpu=2452m on Node jb-1-26-np-nqeu5xtrab
Jan  3 13:26:09.749: INFO: Waiting up to 5m0s for pod "filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214" in namespace "sched-pred-1325" to be "running"
Jan  3 13:26:09.772: INFO: Pod "filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214": Phase="Pending", Reason="", readiness=false. Elapsed: 22.653974ms
Jan  3 13:26:11.792: INFO: Pod "filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042846186s
Jan  3 13:26:13.793: INFO: Pod "filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214": Phase="Running", Reason="", readiness=true. Elapsed: 4.04395182s
Jan  3 13:26:13.793: INFO: Pod "filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214" satisfied condition "running"
Jan  3 13:26:13.793: INFO: Waiting up to 5m0s for pod "filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2" in namespace "sched-pred-1325" to be "running"
Jan  3 13:26:13.812: INFO: Pod "filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2": Phase="Running", Reason="", readiness=true. Elapsed: 18.491011ms
Jan  3 13:26:13.812: INFO: Pod "filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2" satisfied condition "running"
Jan  3 13:26:13.812: INFO: Waiting up to 5m0s for pod "filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e" in namespace "sched-pred-1325" to be "running"
Jan  3 13:26:13.832: INFO: Pod "filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e": Phase="Running", Reason="", readiness=true. Elapsed: 20.036223ms
Jan  3 13:26:13.832: INFO: Pod "filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/03/24 13:26:13.832
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214.17a6d93e16d02f6b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1325/filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214 to jb-1-26-np-64kerjapxk] 01/03/24 13:26:13.853
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214.17a6d93e7f5b90a4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/03/24 13:26:13.853
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214.17a6d93e8017025c], Reason = [Created], Message = [Created container filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214] 01/03/24 13:26:13.853
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214.17a6d93e842e80c2], Reason = [Started], Message = [Started container filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214] 01/03/24 13:26:13.854
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2.17a6d93e184d903c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1325/filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2 to jb-1-26-np-adtwo5cmi2] 01/03/24 13:26:13.854
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2.17a6d93e580cceab], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/03/24 13:26:13.854
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2.17a6d93e58dac934], Reason = [Created], Message = [Created container filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2] 01/03/24 13:26:13.854
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2.17a6d93e5c5ecc95], Reason = [Started], Message = [Started container filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2] 01/03/24 13:26:13.854
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e.17a6d93e1a4cf31f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1325/filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e to jb-1-26-np-nqeu5xtrab] 01/03/24 13:26:13.855
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e.17a6d93e58e7f2ad], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/03/24 13:26:13.855
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e.17a6d93e59e85bcd], Reason = [Created], Message = [Created container filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e] 01/03/24 13:26:13.855
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e.17a6d93e5defc120], Reason = [Started], Message = [Started container filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e] 01/03/24 13:26:13.855
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17a6d93f0fc4eeff], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 01/03/24 13:26:13.901
STEP: removing the label node off the node jb-1-26-np-64kerjapxk 01/03/24 13:26:14.919
STEP: verifying the node doesn't have the label node 01/03/24 13:26:14.981
STEP: removing the label node off the node jb-1-26-np-adtwo5cmi2 01/03/24 13:26:14.999
STEP: verifying the node doesn't have the label node 01/03/24 13:26:15.048
STEP: removing the label node off the node jb-1-26-np-nqeu5xtrab 01/03/24 13:26:15.067
STEP: verifying the node doesn't have the label node 01/03/24 13:26:15.111
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:26:15.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1325" for this suite. 01/03/24 13:26:15.165
------------------------------
• [SLOW TEST] [6.013 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:26:09.182
    Jan  3 13:26:09.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename sched-pred 01/03/24 13:26:09.185
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:26:09.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:26:09.27
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan  3 13:26:09.297: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  3 13:26:09.337: INFO: Waiting for terminating namespaces to be deleted...
    Jan  3 13:26:09.357: INFO: 
    Logging pods the apiserver thinks is on node jb-1-26-np-64kerjapxk before test
    Jan  3 13:26:09.388: INFO: calico-node-r98wj from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.388: INFO: 	Container calico-node ready: true, restart count 2
    Jan  3 13:26:09.388: INFO: csi-ionoscloud-t8qkm from kube-system started at 2024-01-03 11:27:28 +0000 UTC (2 container statuses recorded)
    Jan  3 13:26:09.388: INFO: 	Container csi-ionoscloud ready: true, restart count 0
    Jan  3 13:26:09.388: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  3 13:26:09.388: INFO: konnectivity-agent-gnnsp from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.388: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  3 13:26:09.388: INFO: kube-proxy-z7q4m from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.388: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 13:26:09.388: INFO: nginx-proxy-jb-1-26-np-64kerjapxk from kube-system started at 2024-01-03 11:27:28 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.388: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan  3 13:26:09.388: INFO: sonobuoy from sonobuoy started at 2024-01-03 11:39:58 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.388: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  3 13:26:09.388: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-4hltf from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 13:26:09.388: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 13:26:09.388: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 13:26:09.388: INFO: 
    Logging pods the apiserver thinks is on node jb-1-26-np-adtwo5cmi2 before test
    Jan  3 13:26:09.416: INFO: calico-kube-controllers-54564bcfb4-kjb7m from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.416: INFO: 	Container calico-kube-controllers ready: true, restart count 2
    Jan  3 13:26:09.416: INFO: calico-node-j4nfd from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.416: INFO: 	Container calico-node ready: true, restart count 1
    Jan  3 13:26:09.416: INFO: calico-typha-8555b568f6-pqf6q from kube-system started at 2024-01-03 12:14:40 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.416: INFO: 	Container calico-typha ready: true, restart count 0
    Jan  3 13:26:09.416: INFO: csi-ionoscloud-4z7q8 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (2 container statuses recorded)
    Jan  3 13:26:09.416: INFO: 	Container csi-ionoscloud ready: true, restart count 0
    Jan  3 13:26:09.416: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  3 13:26:09.416: INFO: konnectivity-agent-srxbt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.416: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  3 13:26:09.416: INFO: kube-proxy-ml4kt from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.416: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 13:26:09.416: INFO: nginx-proxy-jb-1-26-np-adtwo5cmi2 from kube-system started at 2024-01-03 11:27:37 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.416: INFO: 	Container nginx-proxy ready: true, restart count 0
    Jan  3 13:26:09.416: INFO: snapshot-validation-webhook-684799cdd5-x6hm4 from kube-system started at 2024-01-03 11:29:38 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.416: INFO: 	Container snapshot-validation ready: true, restart count 0
    Jan  3 13:26:09.416: INFO: sonobuoy-e2e-job-61ed32d11fea4649 from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 13:26:09.416: INFO: 	Container e2e ready: true, restart count 0
    Jan  3 13:26:09.416: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 13:26:09.416: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-cb8zx from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 13:26:09.416: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 13:26:09.416: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 13:26:09.416: INFO: 
    Logging pods the apiserver thinks is on node jb-1-26-np-nqeu5xtrab before test
    Jan  3 13:26:09.443: INFO: calico-node-t5pwv from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.443: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 13:26:09.443: INFO: calico-typha-8555b568f6-bdlz4 from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.443: INFO: 	Container calico-typha ready: true, restart count 0
    Jan  3 13:26:09.443: INFO: coredns-89967fcdc-9jbpx from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.443: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 13:26:09.443: INFO: coredns-89967fcdc-zjgbq from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.443: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 13:26:09.443: INFO: csi-ionoscloud-kv8wz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (2 container statuses recorded)
    Jan  3 13:26:09.443: INFO: 	Container csi-ionoscloud ready: true, restart count 0
    Jan  3 13:26:09.443: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan  3 13:26:09.443: INFO: ionos-policy-validator-64d9954f65-dpd5v from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.443: INFO: 	Container ionos-policy-validator ready: true, restart count 0
    Jan  3 13:26:09.443: INFO: konnectivity-agent-v6hrz from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.443: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  3 13:26:09.443: INFO: kube-proxy-hqxkb from kube-system started at 2024-01-03 11:27:51 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.443: INFO: 	Container kube-proxy ready: true, restart count 1
    Jan  3 13:26:09.443: INFO: nginx-proxy-jb-1-26-np-nqeu5xtrab from kube-system started at 2024-01-03 11:30:18 +0000 UTC (1 container statuses recorded)
    Jan  3 13:26:09.443: INFO: 	Container nginx-proxy ready: true, restart count 1
    Jan  3 13:26:09.443: INFO: sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-j7ffw from sonobuoy started at 2024-01-03 11:40:05 +0000 UTC (2 container statuses recorded)
    Jan  3 13:26:09.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 13:26:09.443: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node jb-1-26-np-64kerjapxk 01/03/24 13:26:09.539
    STEP: verifying the node has the label node jb-1-26-np-adtwo5cmi2 01/03/24 13:26:09.58
    STEP: verifying the node has the label node jb-1-26-np-nqeu5xtrab 01/03/24 13:26:09.621
    Jan  3 13:26:09.667: INFO: Pod calico-kube-controllers-54564bcfb4-kjb7m requesting resource cpu=0m on Node jb-1-26-np-adtwo5cmi2
    Jan  3 13:26:09.667: INFO: Pod calico-node-j4nfd requesting resource cpu=250m on Node jb-1-26-np-adtwo5cmi2
    Jan  3 13:26:09.667: INFO: Pod calico-node-r98wj requesting resource cpu=250m on Node jb-1-26-np-64kerjapxk
    Jan  3 13:26:09.667: INFO: Pod calico-node-t5pwv requesting resource cpu=250m on Node jb-1-26-np-nqeu5xtrab
    Jan  3 13:26:09.667: INFO: Pod calico-typha-8555b568f6-bdlz4 requesting resource cpu=0m on Node jb-1-26-np-nqeu5xtrab
    Jan  3 13:26:09.668: INFO: Pod calico-typha-8555b568f6-pqf6q requesting resource cpu=0m on Node jb-1-26-np-adtwo5cmi2
    Jan  3 13:26:09.668: INFO: Pod coredns-89967fcdc-9jbpx requesting resource cpu=100m on Node jb-1-26-np-nqeu5xtrab
    Jan  3 13:26:09.668: INFO: Pod coredns-89967fcdc-zjgbq requesting resource cpu=100m on Node jb-1-26-np-nqeu5xtrab
    Jan  3 13:26:09.668: INFO: Pod csi-ionoscloud-4z7q8 requesting resource cpu=0m on Node jb-1-26-np-adtwo5cmi2
    Jan  3 13:26:09.668: INFO: Pod csi-ionoscloud-kv8wz requesting resource cpu=0m on Node jb-1-26-np-nqeu5xtrab
    Jan  3 13:26:09.668: INFO: Pod csi-ionoscloud-t8qkm requesting resource cpu=0m on Node jb-1-26-np-64kerjapxk
    Jan  3 13:26:09.668: INFO: Pod ionos-policy-validator-64d9954f65-dpd5v requesting resource cpu=5m on Node jb-1-26-np-nqeu5xtrab
    Jan  3 13:26:09.668: INFO: Pod konnectivity-agent-gnnsp requesting resource cpu=16m on Node jb-1-26-np-64kerjapxk
    Jan  3 13:26:09.668: INFO: Pod konnectivity-agent-srxbt requesting resource cpu=16m on Node jb-1-26-np-adtwo5cmi2
    Jan  3 13:26:09.668: INFO: Pod konnectivity-agent-v6hrz requesting resource cpu=16m on Node jb-1-26-np-nqeu5xtrab
    Jan  3 13:26:09.668: INFO: Pod kube-proxy-hqxkb requesting resource cpu=0m on Node jb-1-26-np-nqeu5xtrab
    Jan  3 13:26:09.668: INFO: Pod kube-proxy-ml4kt requesting resource cpu=0m on Node jb-1-26-np-adtwo5cmi2
    Jan  3 13:26:09.668: INFO: Pod kube-proxy-z7q4m requesting resource cpu=0m on Node jb-1-26-np-64kerjapxk
    Jan  3 13:26:09.668: INFO: Pod nginx-proxy-jb-1-26-np-64kerjapxk requesting resource cpu=25m on Node jb-1-26-np-64kerjapxk
    Jan  3 13:26:09.668: INFO: Pod nginx-proxy-jb-1-26-np-adtwo5cmi2 requesting resource cpu=25m on Node jb-1-26-np-adtwo5cmi2
    Jan  3 13:26:09.668: INFO: Pod nginx-proxy-jb-1-26-np-nqeu5xtrab requesting resource cpu=25m on Node jb-1-26-np-nqeu5xtrab
    Jan  3 13:26:09.669: INFO: Pod snapshot-validation-webhook-684799cdd5-x6hm4 requesting resource cpu=0m on Node jb-1-26-np-adtwo5cmi2
    Jan  3 13:26:09.669: INFO: Pod sonobuoy requesting resource cpu=0m on Node jb-1-26-np-64kerjapxk
    Jan  3 13:26:09.669: INFO: Pod sonobuoy-e2e-job-61ed32d11fea4649 requesting resource cpu=0m on Node jb-1-26-np-adtwo5cmi2
    Jan  3 13:26:09.669: INFO: Pod sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-4hltf requesting resource cpu=0m on Node jb-1-26-np-64kerjapxk
    Jan  3 13:26:09.669: INFO: Pod sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-cb8zx requesting resource cpu=0m on Node jb-1-26-np-adtwo5cmi2
    Jan  3 13:26:09.669: INFO: Pod sonobuoy-systemd-logs-daemon-set-f1bdbb872e7f4b25-j7ffw requesting resource cpu=0m on Node jb-1-26-np-nqeu5xtrab
    STEP: Starting Pods to consume most of the cluster CPU. 01/03/24 13:26:09.669
    Jan  3 13:26:09.669: INFO: Creating a pod which consumes cpu=2596m on Node jb-1-26-np-64kerjapxk
    Jan  3 13:26:09.698: INFO: Creating a pod which consumes cpu=2596m on Node jb-1-26-np-adtwo5cmi2
    Jan  3 13:26:09.721: INFO: Creating a pod which consumes cpu=2452m on Node jb-1-26-np-nqeu5xtrab
    Jan  3 13:26:09.749: INFO: Waiting up to 5m0s for pod "filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214" in namespace "sched-pred-1325" to be "running"
    Jan  3 13:26:09.772: INFO: Pod "filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214": Phase="Pending", Reason="", readiness=false. Elapsed: 22.653974ms
    Jan  3 13:26:11.792: INFO: Pod "filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042846186s
    Jan  3 13:26:13.793: INFO: Pod "filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214": Phase="Running", Reason="", readiness=true. Elapsed: 4.04395182s
    Jan  3 13:26:13.793: INFO: Pod "filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214" satisfied condition "running"
    Jan  3 13:26:13.793: INFO: Waiting up to 5m0s for pod "filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2" in namespace "sched-pred-1325" to be "running"
    Jan  3 13:26:13.812: INFO: Pod "filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2": Phase="Running", Reason="", readiness=true. Elapsed: 18.491011ms
    Jan  3 13:26:13.812: INFO: Pod "filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2" satisfied condition "running"
    Jan  3 13:26:13.812: INFO: Waiting up to 5m0s for pod "filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e" in namespace "sched-pred-1325" to be "running"
    Jan  3 13:26:13.832: INFO: Pod "filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e": Phase="Running", Reason="", readiness=true. Elapsed: 20.036223ms
    Jan  3 13:26:13.832: INFO: Pod "filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/03/24 13:26:13.832
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214.17a6d93e16d02f6b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1325/filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214 to jb-1-26-np-64kerjapxk] 01/03/24 13:26:13.853
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214.17a6d93e7f5b90a4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/03/24 13:26:13.853
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214.17a6d93e8017025c], Reason = [Created], Message = [Created container filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214] 01/03/24 13:26:13.853
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214.17a6d93e842e80c2], Reason = [Started], Message = [Started container filler-pod-0bbc6e3c-4075-428f-9c09-9f3694e41214] 01/03/24 13:26:13.854
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2.17a6d93e184d903c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1325/filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2 to jb-1-26-np-adtwo5cmi2] 01/03/24 13:26:13.854
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2.17a6d93e580cceab], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/03/24 13:26:13.854
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2.17a6d93e58dac934], Reason = [Created], Message = [Created container filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2] 01/03/24 13:26:13.854
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2.17a6d93e5c5ecc95], Reason = [Started], Message = [Started container filler-pod-29a19b3f-6b0d-4ee1-a1df-a2842e393ca2] 01/03/24 13:26:13.854
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e.17a6d93e1a4cf31f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1325/filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e to jb-1-26-np-nqeu5xtrab] 01/03/24 13:26:13.855
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e.17a6d93e58e7f2ad], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/03/24 13:26:13.855
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e.17a6d93e59e85bcd], Reason = [Created], Message = [Created container filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e] 01/03/24 13:26:13.855
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e.17a6d93e5defc120], Reason = [Started], Message = [Started container filler-pod-c1ac3aa5-5efc-4430-9345-66609582ba5e] 01/03/24 13:26:13.855
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17a6d93f0fc4eeff], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 01/03/24 13:26:13.901
    STEP: removing the label node off the node jb-1-26-np-64kerjapxk 01/03/24 13:26:14.919
    STEP: verifying the node doesn't have the label node 01/03/24 13:26:14.981
    STEP: removing the label node off the node jb-1-26-np-adtwo5cmi2 01/03/24 13:26:14.999
    STEP: verifying the node doesn't have the label node 01/03/24 13:26:15.048
    STEP: removing the label node off the node jb-1-26-np-nqeu5xtrab 01/03/24 13:26:15.067
    STEP: verifying the node doesn't have the label node 01/03/24 13:26:15.111
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:26:15.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1325" for this suite. 01/03/24 13:26:15.165
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:26:15.198
Jan  3 13:26:15.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 13:26:15.2
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:26:15.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:26:15.277
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-d0d71ae9-027e-4656-afb0-b2804844c488 01/03/24 13:26:15.326
STEP: Creating configMap with name cm-test-opt-upd-5b55aaf5-a301-45dd-a685-1fc4ee2a7792 01/03/24 13:26:15.345
STEP: Creating the pod 01/03/24 13:26:15.362
Jan  3 13:26:15.393: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ad8532ed-7db6-4ed1-8fc3-73cdbcdc7260" in namespace "projected-9174" to be "running and ready"
Jan  3 13:26:15.411: INFO: Pod "pod-projected-configmaps-ad8532ed-7db6-4ed1-8fc3-73cdbcdc7260": Phase="Pending", Reason="", readiness=false. Elapsed: 18.479607ms
Jan  3 13:26:15.411: INFO: The phase of Pod pod-projected-configmaps-ad8532ed-7db6-4ed1-8fc3-73cdbcdc7260 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:26:17.432: INFO: Pod "pod-projected-configmaps-ad8532ed-7db6-4ed1-8fc3-73cdbcdc7260": Phase="Running", Reason="", readiness=true. Elapsed: 2.039221909s
Jan  3 13:26:17.432: INFO: The phase of Pod pod-projected-configmaps-ad8532ed-7db6-4ed1-8fc3-73cdbcdc7260 is Running (Ready = true)
Jan  3 13:26:17.432: INFO: Pod "pod-projected-configmaps-ad8532ed-7db6-4ed1-8fc3-73cdbcdc7260" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-d0d71ae9-027e-4656-afb0-b2804844c488 01/03/24 13:26:17.718
STEP: Updating configmap cm-test-opt-upd-5b55aaf5-a301-45dd-a685-1fc4ee2a7792 01/03/24 13:26:17.738
STEP: Creating configMap with name cm-test-opt-create-69da6ef3-9aef-4a7c-9d8e-f32398d1f058 01/03/24 13:26:17.758
STEP: waiting to observe update in volume 01/03/24 13:26:17.782
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  3 13:26:19.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9174" for this suite. 01/03/24 13:26:19.997
------------------------------
• [4.850 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:26:15.198
    Jan  3 13:26:15.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 13:26:15.2
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:26:15.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:26:15.277
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-d0d71ae9-027e-4656-afb0-b2804844c488 01/03/24 13:26:15.326
    STEP: Creating configMap with name cm-test-opt-upd-5b55aaf5-a301-45dd-a685-1fc4ee2a7792 01/03/24 13:26:15.345
    STEP: Creating the pod 01/03/24 13:26:15.362
    Jan  3 13:26:15.393: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ad8532ed-7db6-4ed1-8fc3-73cdbcdc7260" in namespace "projected-9174" to be "running and ready"
    Jan  3 13:26:15.411: INFO: Pod "pod-projected-configmaps-ad8532ed-7db6-4ed1-8fc3-73cdbcdc7260": Phase="Pending", Reason="", readiness=false. Elapsed: 18.479607ms
    Jan  3 13:26:15.411: INFO: The phase of Pod pod-projected-configmaps-ad8532ed-7db6-4ed1-8fc3-73cdbcdc7260 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:26:17.432: INFO: Pod "pod-projected-configmaps-ad8532ed-7db6-4ed1-8fc3-73cdbcdc7260": Phase="Running", Reason="", readiness=true. Elapsed: 2.039221909s
    Jan  3 13:26:17.432: INFO: The phase of Pod pod-projected-configmaps-ad8532ed-7db6-4ed1-8fc3-73cdbcdc7260 is Running (Ready = true)
    Jan  3 13:26:17.432: INFO: Pod "pod-projected-configmaps-ad8532ed-7db6-4ed1-8fc3-73cdbcdc7260" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-d0d71ae9-027e-4656-afb0-b2804844c488 01/03/24 13:26:17.718
    STEP: Updating configmap cm-test-opt-upd-5b55aaf5-a301-45dd-a685-1fc4ee2a7792 01/03/24 13:26:17.738
    STEP: Creating configMap with name cm-test-opt-create-69da6ef3-9aef-4a7c-9d8e-f32398d1f058 01/03/24 13:26:17.758
    STEP: waiting to observe update in volume 01/03/24 13:26:17.782
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:26:19.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9174" for this suite. 01/03/24 13:26:19.997
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:26:20.048
Jan  3 13:26:20.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename replication-controller 01/03/24 13:26:20.051
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:26:20.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:26:20.154
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 01/03/24 13:26:20.205
STEP: waiting for RC to be added 01/03/24 13:26:20.226
STEP: waiting for available Replicas 01/03/24 13:26:20.227
STEP: patching ReplicationController 01/03/24 13:26:22.389
STEP: waiting for RC to be modified 01/03/24 13:26:22.414
STEP: patching ReplicationController status 01/03/24 13:26:22.415
STEP: waiting for RC to be modified 01/03/24 13:26:22.435
STEP: waiting for available Replicas 01/03/24 13:26:22.436
STEP: fetching ReplicationController status 01/03/24 13:26:22.445
STEP: patching ReplicationController scale 01/03/24 13:26:22.463
STEP: waiting for RC to be modified 01/03/24 13:26:22.485
STEP: waiting for ReplicationController's scale to be the max amount 01/03/24 13:26:22.486
STEP: fetching ReplicationController; ensuring that it's patched 01/03/24 13:26:23.878
STEP: updating ReplicationController status 01/03/24 13:26:23.898
STEP: waiting for RC to be modified 01/03/24 13:26:23.919
STEP: listing all ReplicationControllers 01/03/24 13:26:23.92
STEP: checking that ReplicationController has expected values 01/03/24 13:26:23.937
STEP: deleting ReplicationControllers by collection 01/03/24 13:26:23.938
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/03/24 13:26:23.967
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  3 13:26:24.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7017" for this suite. 01/03/24 13:26:24.099
------------------------------
• [4.076 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:26:20.048
    Jan  3 13:26:20.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename replication-controller 01/03/24 13:26:20.051
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:26:20.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:26:20.154
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 01/03/24 13:26:20.205
    STEP: waiting for RC to be added 01/03/24 13:26:20.226
    STEP: waiting for available Replicas 01/03/24 13:26:20.227
    STEP: patching ReplicationController 01/03/24 13:26:22.389
    STEP: waiting for RC to be modified 01/03/24 13:26:22.414
    STEP: patching ReplicationController status 01/03/24 13:26:22.415
    STEP: waiting for RC to be modified 01/03/24 13:26:22.435
    STEP: waiting for available Replicas 01/03/24 13:26:22.436
    STEP: fetching ReplicationController status 01/03/24 13:26:22.445
    STEP: patching ReplicationController scale 01/03/24 13:26:22.463
    STEP: waiting for RC to be modified 01/03/24 13:26:22.485
    STEP: waiting for ReplicationController's scale to be the max amount 01/03/24 13:26:22.486
    STEP: fetching ReplicationController; ensuring that it's patched 01/03/24 13:26:23.878
    STEP: updating ReplicationController status 01/03/24 13:26:23.898
    STEP: waiting for RC to be modified 01/03/24 13:26:23.919
    STEP: listing all ReplicationControllers 01/03/24 13:26:23.92
    STEP: checking that ReplicationController has expected values 01/03/24 13:26:23.937
    STEP: deleting ReplicationControllers by collection 01/03/24 13:26:23.938
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/03/24 13:26:23.967
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:26:24.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7017" for this suite. 01/03/24 13:26:24.099
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:26:24.125
Jan  3 13:26:24.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename emptydir 01/03/24 13:26:24.128
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:26:24.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:26:24.207
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 01/03/24 13:26:24.234
Jan  3 13:26:24.261: INFO: Waiting up to 5m0s for pod "pod-c01280d7-b80d-4807-8cf9-960d18bcb56d" in namespace "emptydir-8889" to be "Succeeded or Failed"
Jan  3 13:26:24.279: INFO: Pod "pod-c01280d7-b80d-4807-8cf9-960d18bcb56d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.561112ms
Jan  3 13:26:26.299: INFO: Pod "pod-c01280d7-b80d-4807-8cf9-960d18bcb56d": Phase="Running", Reason="", readiness=true. Elapsed: 2.038001483s
Jan  3 13:26:28.299: INFO: Pod "pod-c01280d7-b80d-4807-8cf9-960d18bcb56d": Phase="Running", Reason="", readiness=false. Elapsed: 4.03717707s
Jan  3 13:26:30.299: INFO: Pod "pod-c01280d7-b80d-4807-8cf9-960d18bcb56d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038123474s
STEP: Saw pod success 01/03/24 13:26:30.3
Jan  3 13:26:30.300: INFO: Pod "pod-c01280d7-b80d-4807-8cf9-960d18bcb56d" satisfied condition "Succeeded or Failed"
Jan  3 13:26:30.318: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-c01280d7-b80d-4807-8cf9-960d18bcb56d container test-container: <nil>
STEP: delete the pod 01/03/24 13:26:30.357
Jan  3 13:26:30.391: INFO: Waiting for pod pod-c01280d7-b80d-4807-8cf9-960d18bcb56d to disappear
Jan  3 13:26:30.409: INFO: Pod pod-c01280d7-b80d-4807-8cf9-960d18bcb56d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  3 13:26:30.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8889" for this suite. 01/03/24 13:26:30.44
------------------------------
• [SLOW TEST] [6.342 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:26:24.125
    Jan  3 13:26:24.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename emptydir 01/03/24 13:26:24.128
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:26:24.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:26:24.207
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/03/24 13:26:24.234
    Jan  3 13:26:24.261: INFO: Waiting up to 5m0s for pod "pod-c01280d7-b80d-4807-8cf9-960d18bcb56d" in namespace "emptydir-8889" to be "Succeeded or Failed"
    Jan  3 13:26:24.279: INFO: Pod "pod-c01280d7-b80d-4807-8cf9-960d18bcb56d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.561112ms
    Jan  3 13:26:26.299: INFO: Pod "pod-c01280d7-b80d-4807-8cf9-960d18bcb56d": Phase="Running", Reason="", readiness=true. Elapsed: 2.038001483s
    Jan  3 13:26:28.299: INFO: Pod "pod-c01280d7-b80d-4807-8cf9-960d18bcb56d": Phase="Running", Reason="", readiness=false. Elapsed: 4.03717707s
    Jan  3 13:26:30.299: INFO: Pod "pod-c01280d7-b80d-4807-8cf9-960d18bcb56d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038123474s
    STEP: Saw pod success 01/03/24 13:26:30.3
    Jan  3 13:26:30.300: INFO: Pod "pod-c01280d7-b80d-4807-8cf9-960d18bcb56d" satisfied condition "Succeeded or Failed"
    Jan  3 13:26:30.318: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-c01280d7-b80d-4807-8cf9-960d18bcb56d container test-container: <nil>
    STEP: delete the pod 01/03/24 13:26:30.357
    Jan  3 13:26:30.391: INFO: Waiting for pod pod-c01280d7-b80d-4807-8cf9-960d18bcb56d to disappear
    Jan  3 13:26:30.409: INFO: Pod pod-c01280d7-b80d-4807-8cf9-960d18bcb56d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:26:30.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8889" for this suite. 01/03/24 13:26:30.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:26:30.471
Jan  3 13:26:30.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename container-probe 01/03/24 13:26:30.473
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:26:30.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:26:30.551
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 in namespace container-probe-6508 01/03/24 13:26:30.577
Jan  3 13:26:30.608: INFO: Waiting up to 5m0s for pod "liveness-20d17ff1-9bf4-4413-a4d7-865855147f07" in namespace "container-probe-6508" to be "not pending"
Jan  3 13:26:30.627: INFO: Pod "liveness-20d17ff1-9bf4-4413-a4d7-865855147f07": Phase="Pending", Reason="", readiness=false. Elapsed: 19.398577ms
Jan  3 13:26:32.651: INFO: Pod "liveness-20d17ff1-9bf4-4413-a4d7-865855147f07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042825056s
Jan  3 13:26:34.646: INFO: Pod "liveness-20d17ff1-9bf4-4413-a4d7-865855147f07": Phase="Running", Reason="", readiness=true. Elapsed: 4.03797553s
Jan  3 13:26:34.646: INFO: Pod "liveness-20d17ff1-9bf4-4413-a4d7-865855147f07" satisfied condition "not pending"
Jan  3 13:26:34.646: INFO: Started pod liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 in namespace container-probe-6508
STEP: checking the pod's current state and verifying that restartCount is present 01/03/24 13:26:34.646
Jan  3 13:26:34.664: INFO: Initial restart count of pod liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 is 0
Jan  3 13:26:52.882: INFO: Restart count of pod container-probe-6508/liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 is now 1 (18.218762074s elapsed)
Jan  3 13:27:13.093: INFO: Restart count of pod container-probe-6508/liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 is now 2 (38.42948945s elapsed)
Jan  3 13:27:33.299: INFO: Restart count of pod container-probe-6508/liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 is now 3 (58.635027199s elapsed)
Jan  3 13:27:53.507: INFO: Restart count of pod container-probe-6508/liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 is now 4 (1m18.843761594s elapsed)
Jan  3 13:28:54.238: INFO: Restart count of pod container-probe-6508/liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 is now 5 (2m19.574300674s elapsed)
STEP: deleting the pod 01/03/24 13:28:54.238
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  3 13:28:54.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6508" for this suite. 01/03/24 13:28:54.305
------------------------------
• [SLOW TEST] [143.875 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:26:30.471
    Jan  3 13:26:30.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename container-probe 01/03/24 13:26:30.473
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:26:30.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:26:30.551
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 in namespace container-probe-6508 01/03/24 13:26:30.577
    Jan  3 13:26:30.608: INFO: Waiting up to 5m0s for pod "liveness-20d17ff1-9bf4-4413-a4d7-865855147f07" in namespace "container-probe-6508" to be "not pending"
    Jan  3 13:26:30.627: INFO: Pod "liveness-20d17ff1-9bf4-4413-a4d7-865855147f07": Phase="Pending", Reason="", readiness=false. Elapsed: 19.398577ms
    Jan  3 13:26:32.651: INFO: Pod "liveness-20d17ff1-9bf4-4413-a4d7-865855147f07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042825056s
    Jan  3 13:26:34.646: INFO: Pod "liveness-20d17ff1-9bf4-4413-a4d7-865855147f07": Phase="Running", Reason="", readiness=true. Elapsed: 4.03797553s
    Jan  3 13:26:34.646: INFO: Pod "liveness-20d17ff1-9bf4-4413-a4d7-865855147f07" satisfied condition "not pending"
    Jan  3 13:26:34.646: INFO: Started pod liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 in namespace container-probe-6508
    STEP: checking the pod's current state and verifying that restartCount is present 01/03/24 13:26:34.646
    Jan  3 13:26:34.664: INFO: Initial restart count of pod liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 is 0
    Jan  3 13:26:52.882: INFO: Restart count of pod container-probe-6508/liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 is now 1 (18.218762074s elapsed)
    Jan  3 13:27:13.093: INFO: Restart count of pod container-probe-6508/liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 is now 2 (38.42948945s elapsed)
    Jan  3 13:27:33.299: INFO: Restart count of pod container-probe-6508/liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 is now 3 (58.635027199s elapsed)
    Jan  3 13:27:53.507: INFO: Restart count of pod container-probe-6508/liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 is now 4 (1m18.843761594s elapsed)
    Jan  3 13:28:54.238: INFO: Restart count of pod container-probe-6508/liveness-20d17ff1-9bf4-4413-a4d7-865855147f07 is now 5 (2m19.574300674s elapsed)
    STEP: deleting the pod 01/03/24 13:28:54.238
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:28:54.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6508" for this suite. 01/03/24 13:28:54.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:28:54.348
Jan  3 13:28:54.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename init-container 01/03/24 13:28:54.351
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:28:54.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:28:54.43
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 01/03/24 13:28:54.458
Jan  3 13:28:54.458: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:29:01.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1244" for this suite. 01/03/24 13:29:01.335
------------------------------
• [SLOW TEST] [7.012 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:28:54.348
    Jan  3 13:28:54.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename init-container 01/03/24 13:28:54.351
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:28:54.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:28:54.43
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 01/03/24 13:28:54.458
    Jan  3 13:28:54.458: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:29:01.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1244" for this suite. 01/03/24 13:29:01.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:29:01.378
Jan  3 13:29:01.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename replicaset 01/03/24 13:29:01.379
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:29:01.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:29:01.458
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan  3 13:29:01.486: INFO: Creating ReplicaSet my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e
Jan  3 13:29:01.524: INFO: Pod name my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e: Found 1 pods out of 1
Jan  3 13:29:01.524: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e" is running
Jan  3 13:29:01.524: INFO: Waiting up to 5m0s for pod "my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e-zftfb" in namespace "replicaset-4706" to be "running"
Jan  3 13:29:01.544: INFO: Pod "my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e-zftfb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.36839ms
Jan  3 13:29:03.562: INFO: Pod "my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e-zftfb": Phase="Running", Reason="", readiness=true. Elapsed: 2.038184036s
Jan  3 13:29:03.562: INFO: Pod "my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e-zftfb" satisfied condition "running"
Jan  3 13:29:03.562: INFO: Pod "my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e-zftfb" is running (conditions: [])
Jan  3 13:29:03.563: INFO: Trying to dial the pod
Jan  3 13:29:08.686: INFO: Controller my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e: Got expected result from replica 1 [my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e-zftfb]: "my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e-zftfb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  3 13:29:08.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4706" for this suite. 01/03/24 13:29:08.719
------------------------------
• [SLOW TEST] [7.363 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:29:01.378
    Jan  3 13:29:01.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename replicaset 01/03/24 13:29:01.379
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:29:01.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:29:01.458
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan  3 13:29:01.486: INFO: Creating ReplicaSet my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e
    Jan  3 13:29:01.524: INFO: Pod name my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e: Found 1 pods out of 1
    Jan  3 13:29:01.524: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e" is running
    Jan  3 13:29:01.524: INFO: Waiting up to 5m0s for pod "my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e-zftfb" in namespace "replicaset-4706" to be "running"
    Jan  3 13:29:01.544: INFO: Pod "my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e-zftfb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.36839ms
    Jan  3 13:29:03.562: INFO: Pod "my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e-zftfb": Phase="Running", Reason="", readiness=true. Elapsed: 2.038184036s
    Jan  3 13:29:03.562: INFO: Pod "my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e-zftfb" satisfied condition "running"
    Jan  3 13:29:03.562: INFO: Pod "my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e-zftfb" is running (conditions: [])
    Jan  3 13:29:03.563: INFO: Trying to dial the pod
    Jan  3 13:29:08.686: INFO: Controller my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e: Got expected result from replica 1 [my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e-zftfb]: "my-hostname-basic-6ebd3ee0-a8af-4999-9848-bd3ce421781e-zftfb", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:29:08.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4706" for this suite. 01/03/24 13:29:08.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:29:08.743
Jan  3 13:29:08.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename security-context 01/03/24 13:29:08.745
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:29:08.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:29:08.832
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/03/24 13:29:08.86
Jan  3 13:29:08.885: INFO: Waiting up to 5m0s for pod "security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916" in namespace "security-context-8228" to be "Succeeded or Failed"
Jan  3 13:29:08.904: INFO: Pod "security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916": Phase="Pending", Reason="", readiness=false. Elapsed: 19.803125ms
Jan  3 13:29:10.924: INFO: Pod "security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916": Phase="Running", Reason="", readiness=true. Elapsed: 2.03968913s
Jan  3 13:29:12.926: INFO: Pod "security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916": Phase="Running", Reason="", readiness=false. Elapsed: 4.041529704s
Jan  3 13:29:14.926: INFO: Pod "security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040948272s
STEP: Saw pod success 01/03/24 13:29:14.926
Jan  3 13:29:14.926: INFO: Pod "security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916" satisfied condition "Succeeded or Failed"
Jan  3 13:29:14.947: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916 container test-container: <nil>
STEP: delete the pod 01/03/24 13:29:15.104
Jan  3 13:29:15.144: INFO: Waiting for pod security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916 to disappear
Jan  3 13:29:15.163: INFO: Pod security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  3 13:29:15.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-8228" for this suite. 01/03/24 13:29:15.194
------------------------------
• [SLOW TEST] [6.477 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:29:08.743
    Jan  3 13:29:08.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename security-context 01/03/24 13:29:08.745
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:29:08.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:29:08.832
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/03/24 13:29:08.86
    Jan  3 13:29:08.885: INFO: Waiting up to 5m0s for pod "security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916" in namespace "security-context-8228" to be "Succeeded or Failed"
    Jan  3 13:29:08.904: INFO: Pod "security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916": Phase="Pending", Reason="", readiness=false. Elapsed: 19.803125ms
    Jan  3 13:29:10.924: INFO: Pod "security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916": Phase="Running", Reason="", readiness=true. Elapsed: 2.03968913s
    Jan  3 13:29:12.926: INFO: Pod "security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916": Phase="Running", Reason="", readiness=false. Elapsed: 4.041529704s
    Jan  3 13:29:14.926: INFO: Pod "security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040948272s
    STEP: Saw pod success 01/03/24 13:29:14.926
    Jan  3 13:29:14.926: INFO: Pod "security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916" satisfied condition "Succeeded or Failed"
    Jan  3 13:29:14.947: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916 container test-container: <nil>
    STEP: delete the pod 01/03/24 13:29:15.104
    Jan  3 13:29:15.144: INFO: Waiting for pod security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916 to disappear
    Jan  3 13:29:15.163: INFO: Pod security-context-e63e02c2-4e9c-4fe5-acfe-8433bc8a9916 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:29:15.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-8228" for this suite. 01/03/24 13:29:15.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:29:15.223
Jan  3 13:29:15.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 13:29:15.225
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:29:15.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:29:15.309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 01/03/24 13:29:15.336
Jan  3 13:29:15.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: mark a version not serverd 01/03/24 13:29:22.949
STEP: check the unserved version gets removed 01/03/24 13:29:23.067
STEP: check the other version is not changed 01/03/24 13:29:26.055
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  3 13:29:31.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2586" for this suite. 01/03/24 13:29:31.375
------------------------------
• [SLOW TEST] [16.186 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:29:15.223
    Jan  3 13:29:15.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/24 13:29:15.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:29:15.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:29:15.309
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 01/03/24 13:29:15.336
    Jan  3 13:29:15.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: mark a version not serverd 01/03/24 13:29:22.949
    STEP: check the unserved version gets removed 01/03/24 13:29:23.067
    STEP: check the other version is not changed 01/03/24 13:29:26.055
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:29:31.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2586" for this suite. 01/03/24 13:29:31.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:29:31.41
Jan  3 13:29:31.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename pod-network-test 01/03/24 13:29:31.413
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:29:31.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:29:31.495
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-2249 01/03/24 13:29:31.523
STEP: creating a selector 01/03/24 13:29:31.524
STEP: Creating the service pods in kubernetes 01/03/24 13:29:31.524
Jan  3 13:29:31.524: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  3 13:29:31.646: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2249" to be "running and ready"
Jan  3 13:29:31.670: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.994671ms
Jan  3 13:29:31.670: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 13:29:33.691: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.044881142s
Jan  3 13:29:33.691: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 13:29:35.689: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.043183935s
Jan  3 13:29:35.689: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 13:29:37.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.047239553s
Jan  3 13:29:37.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 13:29:39.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.043877304s
Jan  3 13:29:39.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 13:29:41.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.044654283s
Jan  3 13:29:41.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 13:29:43.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.046280571s
Jan  3 13:29:43.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 13:29:45.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.043866711s
Jan  3 13:29:45.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 13:29:47.691: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.044980758s
Jan  3 13:29:47.691: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 13:29:49.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.044702599s
Jan  3 13:29:49.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 13:29:51.694: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.048114776s
Jan  3 13:29:51.694: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 13:29:53.691: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.045297314s
Jan  3 13:29:53.691: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  3 13:29:53.691: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  3 13:29:53.711: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2249" to be "running and ready"
Jan  3 13:29:53.729: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 18.408ms
Jan  3 13:29:53.729: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  3 13:29:53.729: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  3 13:29:53.748: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2249" to be "running and ready"
Jan  3 13:29:53.770: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 21.715082ms
Jan  3 13:29:53.770: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  3 13:29:53.770: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/03/24 13:29:53.79
Jan  3 13:29:53.834: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2249" to be "running"
Jan  3 13:29:53.853: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.359794ms
Jan  3 13:29:55.877: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.041841959s
Jan  3 13:29:55.877: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  3 13:29:55.894: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2249" to be "running"
Jan  3 13:29:55.918: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 24.054892ms
Jan  3 13:29:55.919: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan  3 13:29:55.939: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan  3 13:29:55.939: INFO: Going to poll 10.221.146.126 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan  3 13:29:55.958: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.221.146.126 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2249 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 13:29:55.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 13:29:55.960: INFO: ExecWithOptions: Clientset creation
Jan  3 13:29:55.960: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2249/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.221.146.126+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  3 13:29:57.291: INFO: Found all 1 expected endpoints: [netserver-0]
Jan  3 13:29:57.291: INFO: Going to poll 10.222.238.204 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan  3 13:29:57.312: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.222.238.204 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2249 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 13:29:57.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 13:29:57.314: INFO: ExecWithOptions: Clientset creation
Jan  3 13:29:57.314: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2249/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.222.238.204+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  3 13:29:58.620: INFO: Found all 1 expected endpoints: [netserver-1]
Jan  3 13:29:58.620: INFO: Going to poll 10.221.146.153 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan  3 13:29:58.638: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.221.146.153 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2249 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 13:29:58.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
Jan  3 13:29:58.640: INFO: ExecWithOptions: Clientset creation
Jan  3 13:29:58.640: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2249/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.221.146.153+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  3 13:29:59.939: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan  3 13:29:59.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2249" for this suite. 01/03/24 13:29:59.971
------------------------------
• [SLOW TEST] [28.590 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:29:31.41
    Jan  3 13:29:31.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename pod-network-test 01/03/24 13:29:31.413
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:29:31.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:29:31.495
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-2249 01/03/24 13:29:31.523
    STEP: creating a selector 01/03/24 13:29:31.524
    STEP: Creating the service pods in kubernetes 01/03/24 13:29:31.524
    Jan  3 13:29:31.524: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  3 13:29:31.646: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2249" to be "running and ready"
    Jan  3 13:29:31.670: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.994671ms
    Jan  3 13:29:31.670: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 13:29:33.691: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.044881142s
    Jan  3 13:29:33.691: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 13:29:35.689: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.043183935s
    Jan  3 13:29:35.689: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 13:29:37.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.047239553s
    Jan  3 13:29:37.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 13:29:39.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.043877304s
    Jan  3 13:29:39.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 13:29:41.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.044654283s
    Jan  3 13:29:41.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 13:29:43.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.046280571s
    Jan  3 13:29:43.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 13:29:45.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.043866711s
    Jan  3 13:29:45.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 13:29:47.691: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.044980758s
    Jan  3 13:29:47.691: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 13:29:49.690: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.044702599s
    Jan  3 13:29:49.690: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 13:29:51.694: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.048114776s
    Jan  3 13:29:51.694: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 13:29:53.691: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.045297314s
    Jan  3 13:29:53.691: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  3 13:29:53.691: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  3 13:29:53.711: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2249" to be "running and ready"
    Jan  3 13:29:53.729: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 18.408ms
    Jan  3 13:29:53.729: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  3 13:29:53.729: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  3 13:29:53.748: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2249" to be "running and ready"
    Jan  3 13:29:53.770: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 21.715082ms
    Jan  3 13:29:53.770: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  3 13:29:53.770: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/03/24 13:29:53.79
    Jan  3 13:29:53.834: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2249" to be "running"
    Jan  3 13:29:53.853: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.359794ms
    Jan  3 13:29:55.877: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.041841959s
    Jan  3 13:29:55.877: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  3 13:29:55.894: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2249" to be "running"
    Jan  3 13:29:55.918: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 24.054892ms
    Jan  3 13:29:55.919: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan  3 13:29:55.939: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan  3 13:29:55.939: INFO: Going to poll 10.221.146.126 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan  3 13:29:55.958: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.221.146.126 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2249 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 13:29:55.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 13:29:55.960: INFO: ExecWithOptions: Clientset creation
    Jan  3 13:29:55.960: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2249/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.221.146.126+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  3 13:29:57.291: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan  3 13:29:57.291: INFO: Going to poll 10.222.238.204 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan  3 13:29:57.312: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.222.238.204 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2249 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 13:29:57.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 13:29:57.314: INFO: ExecWithOptions: Clientset creation
    Jan  3 13:29:57.314: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2249/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.222.238.204+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  3 13:29:58.620: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan  3 13:29:58.620: INFO: Going to poll 10.221.146.153 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan  3 13:29:58.638: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.221.146.153 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2249 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 13:29:58.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    Jan  3 13:29:58.640: INFO: ExecWithOptions: Clientset creation
    Jan  3 13:29:58.640: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2249/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.221.146.153+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  3 13:29:59.939: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:29:59.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2249" for this suite. 01/03/24 13:29:59.971
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:30:00.001
Jan  3 13:30:00.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename events 01/03/24 13:30:00.003
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:30:00.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:30:00.148
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/03/24 13:30:00.175
STEP: get a list of Events with a label in the current namespace 01/03/24 13:30:00.242
STEP: delete a list of events 01/03/24 13:30:00.271
Jan  3 13:30:00.271: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/03/24 13:30:00.365
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan  3 13:30:00.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-696" for this suite. 01/03/24 13:30:00.401
------------------------------
• [0.428 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:30:00.001
    Jan  3 13:30:00.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename events 01/03/24 13:30:00.003
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:30:00.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:30:00.148
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/03/24 13:30:00.175
    STEP: get a list of Events with a label in the current namespace 01/03/24 13:30:00.242
    STEP: delete a list of events 01/03/24 13:30:00.271
    Jan  3 13:30:00.271: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/03/24 13:30:00.365
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:30:00.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-696" for this suite. 01/03/24 13:30:00.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/03/24 13:30:00.434
Jan  3 13:30:00.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
STEP: Building a namespace api object, basename projected 01/03/24 13:30:00.436
STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:30:00.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:30:00.515
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-229fdb94-1ff3-42e2-a5b6-9190fc2be81c 01/03/24 13:30:00.544
STEP: Creating a pod to test consume secrets 01/03/24 13:30:00.564
Jan  3 13:30:00.593: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce" in namespace "projected-5181" to be "Succeeded or Failed"
Jan  3 13:30:00.613: INFO: Pod "pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce": Phase="Pending", Reason="", readiness=false. Elapsed: 18.831353ms
Jan  3 13:30:02.636: INFO: Pod "pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041889588s
Jan  3 13:30:04.632: INFO: Pod "pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038365361s
STEP: Saw pod success 01/03/24 13:30:04.632
Jan  3 13:30:04.633: INFO: Pod "pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce" satisfied condition "Succeeded or Failed"
Jan  3 13:30:04.654: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce container projected-secret-volume-test: <nil>
STEP: delete the pod 01/03/24 13:30:04.819
Jan  3 13:30:04.857: INFO: Waiting for pod pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce to disappear
Jan  3 13:30:04.878: INFO: Pod pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  3 13:30:04.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5181" for this suite. 01/03/24 13:30:04.91
------------------------------
• [4.501 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/03/24 13:30:00.434
    Jan  3 13:30:00.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3764843863
    STEP: Building a namespace api object, basename projected 01/03/24 13:30:00.436
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/24 13:30:00.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/24 13:30:00.515
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-229fdb94-1ff3-42e2-a5b6-9190fc2be81c 01/03/24 13:30:00.544
    STEP: Creating a pod to test consume secrets 01/03/24 13:30:00.564
    Jan  3 13:30:00.593: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce" in namespace "projected-5181" to be "Succeeded or Failed"
    Jan  3 13:30:00.613: INFO: Pod "pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce": Phase="Pending", Reason="", readiness=false. Elapsed: 18.831353ms
    Jan  3 13:30:02.636: INFO: Pod "pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041889588s
    Jan  3 13:30:04.632: INFO: Pod "pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038365361s
    STEP: Saw pod success 01/03/24 13:30:04.632
    Jan  3 13:30:04.633: INFO: Pod "pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce" satisfied condition "Succeeded or Failed"
    Jan  3 13:30:04.654: INFO: Trying to get logs from node jb-1-26-np-64kerjapxk pod pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/03/24 13:30:04.819
    Jan  3 13:30:04.857: INFO: Waiting for pod pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce to disappear
    Jan  3 13:30:04.878: INFO: Pod pod-projected-secrets-e4c8c6bc-dad1-4e1a-b1cc-cc55b68139ce no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  3 13:30:04.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5181" for this suite. 01/03/24 13:30:04.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jan  3 13:30:04.944: INFO: Running AfterSuite actions on node 1
Jan  3 13:30:04.944: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.001 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jan  3 13:30:04.944: INFO: Running AfterSuite actions on node 1
    Jan  3 13:30:04.944: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.084 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6511.547 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h48m32.146702159s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

